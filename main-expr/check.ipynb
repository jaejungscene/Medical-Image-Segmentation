{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params: 123,479,719\n",
      "torch.Size([1, 4, 224, 224])\n",
      "torch.Size([1, 4, 224, 224])\n",
      "torch.Size([1, 4, 224, 224])\n",
      "torch.Size([1, 4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "## TransCASCADE\n",
    "import torch\n",
    "from lib.networks import TransCASCADE\n",
    "from lib.cnn_vit_backbone import CONFIGS as CONFIGS_ViT_seg\n",
    "\n",
    "class args:\n",
    "    vit_name = \"R50-ViT-B_16\"\n",
    "    num_classes = 4\n",
    "    n_skip = 3 \n",
    "    img_size = 224\n",
    "    vit_patches_size = 16\n",
    "\n",
    "config_vit = CONFIGS_ViT_seg[args.vit_name]\n",
    "config_vit.n_classes = args.num_classes\n",
    "config_vit.n_skip = args.n_skip\n",
    "if args.vit_name.find('R50') != -1:\n",
    "    config_vit.patches.grid = (int(args.img_size / args.vit_patches_size), int(args.img_size / args.vit_patches_size))\n",
    "net = TransCASCADE(\n",
    "    config_vit, \n",
    "    img_size=args.img_size, \n",
    "    num_classes=config_vit.n_classes\n",
    ").cuda() # model initialization\n",
    "print('#params: {:,}'.format(sum([p.data.nelement() for p in net.parameters()])))\n",
    "\n",
    "inputs = torch.randn((1,1,224,224)).cuda()\n",
    "outputs = net(inputs)\n",
    "\n",
    "for out in outputs:\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed1.proj.weight\n",
      "patch_embed1.proj.bias\n",
      "patch_embed1.norm.weight\n",
      "patch_embed1.norm.bias\n",
      "patch_embed2.proj.weight\n",
      "patch_embed2.proj.bias\n",
      "patch_embed2.norm.weight\n",
      "patch_embed2.norm.bias\n",
      "patch_embed3.proj.weight\n",
      "patch_embed3.proj.bias\n",
      "patch_embed3.norm.weight\n",
      "patch_embed3.norm.bias\n",
      "patch_embed4.proj.weight\n",
      "patch_embed4.proj.bias\n",
      "patch_embed4.norm.weight\n",
      "patch_embed4.norm.bias\n",
      "block1.0.norm1.weight\n",
      "block1.0.norm1.bias\n",
      "block1.0.attn.q.weight\n",
      "block1.0.attn.q.bias\n",
      "block1.0.attn.kv.weight\n",
      "block1.0.attn.kv.bias\n",
      "block1.0.attn.proj.weight\n",
      "block1.0.attn.proj.bias\n",
      "block1.0.attn.sr.weight\n",
      "block1.0.attn.sr.bias\n",
      "block1.0.attn.norm.weight\n",
      "block1.0.attn.norm.bias\n",
      "block1.0.norm2.weight\n",
      "block1.0.norm2.bias\n",
      "block1.0.mlp.fc1.weight\n",
      "block1.0.mlp.fc1.bias\n",
      "block1.0.mlp.dwconv.dwconv.weight\n",
      "block1.0.mlp.dwconv.dwconv.bias\n",
      "block1.0.mlp.fc2.weight\n",
      "block1.0.mlp.fc2.bias\n",
      "block1.1.norm1.weight\n",
      "block1.1.norm1.bias\n",
      "block1.1.attn.q.weight\n",
      "block1.1.attn.q.bias\n",
      "block1.1.attn.kv.weight\n",
      "block1.1.attn.kv.bias\n",
      "block1.1.attn.proj.weight\n",
      "block1.1.attn.proj.bias\n",
      "block1.1.attn.sr.weight\n",
      "block1.1.attn.sr.bias\n",
      "block1.1.attn.norm.weight\n",
      "block1.1.attn.norm.bias\n",
      "block1.1.norm2.weight\n",
      "block1.1.norm2.bias\n",
      "block1.1.mlp.fc1.weight\n",
      "block1.1.mlp.fc1.bias\n",
      "block1.1.mlp.dwconv.dwconv.weight\n",
      "block1.1.mlp.dwconv.dwconv.bias\n",
      "block1.1.mlp.fc2.weight\n",
      "block1.1.mlp.fc2.bias\n",
      "block1.2.norm1.weight\n",
      "block1.2.norm1.bias\n",
      "block1.2.attn.q.weight\n",
      "block1.2.attn.q.bias\n",
      "block1.2.attn.kv.weight\n",
      "block1.2.attn.kv.bias\n",
      "block1.2.attn.proj.weight\n",
      "block1.2.attn.proj.bias\n",
      "block1.2.attn.sr.weight\n",
      "block1.2.attn.sr.bias\n",
      "block1.2.attn.norm.weight\n",
      "block1.2.attn.norm.bias\n",
      "block1.2.norm2.weight\n",
      "block1.2.norm2.bias\n",
      "block1.2.mlp.fc1.weight\n",
      "block1.2.mlp.fc1.bias\n",
      "block1.2.mlp.dwconv.dwconv.weight\n",
      "block1.2.mlp.dwconv.dwconv.bias\n",
      "block1.2.mlp.fc2.weight\n",
      "block1.2.mlp.fc2.bias\n",
      "norm1.weight\n",
      "norm1.bias\n",
      "block2.0.norm1.weight\n",
      "block2.0.norm1.bias\n",
      "block2.0.attn.q.weight\n",
      "block2.0.attn.q.bias\n",
      "block2.0.attn.kv.weight\n",
      "block2.0.attn.kv.bias\n",
      "block2.0.attn.proj.weight\n",
      "block2.0.attn.proj.bias\n",
      "block2.0.attn.sr.weight\n",
      "block2.0.attn.sr.bias\n",
      "block2.0.attn.norm.weight\n",
      "block2.0.attn.norm.bias\n",
      "block2.0.norm2.weight\n",
      "block2.0.norm2.bias\n",
      "block2.0.mlp.fc1.weight\n",
      "block2.0.mlp.fc1.bias\n",
      "block2.0.mlp.dwconv.dwconv.weight\n",
      "block2.0.mlp.dwconv.dwconv.bias\n",
      "block2.0.mlp.fc2.weight\n",
      "block2.0.mlp.fc2.bias\n",
      "block2.1.norm1.weight\n",
      "block2.1.norm1.bias\n",
      "block2.1.attn.q.weight\n",
      "block2.1.attn.q.bias\n",
      "block2.1.attn.kv.weight\n",
      "block2.1.attn.kv.bias\n",
      "block2.1.attn.proj.weight\n",
      "block2.1.attn.proj.bias\n",
      "block2.1.attn.sr.weight\n",
      "block2.1.attn.sr.bias\n",
      "block2.1.attn.norm.weight\n",
      "block2.1.attn.norm.bias\n",
      "block2.1.norm2.weight\n",
      "block2.1.norm2.bias\n",
      "block2.1.mlp.fc1.weight\n",
      "block2.1.mlp.fc1.bias\n",
      "block2.1.mlp.dwconv.dwconv.weight\n",
      "block2.1.mlp.dwconv.dwconv.bias\n",
      "block2.1.mlp.fc2.weight\n",
      "block2.1.mlp.fc2.bias\n",
      "block2.2.norm1.weight\n",
      "block2.2.norm1.bias\n",
      "block2.2.attn.q.weight\n",
      "block2.2.attn.q.bias\n",
      "block2.2.attn.kv.weight\n",
      "block2.2.attn.kv.bias\n",
      "block2.2.attn.proj.weight\n",
      "block2.2.attn.proj.bias\n",
      "block2.2.attn.sr.weight\n",
      "block2.2.attn.sr.bias\n",
      "block2.2.attn.norm.weight\n",
      "block2.2.attn.norm.bias\n",
      "block2.2.norm2.weight\n",
      "block2.2.norm2.bias\n",
      "block2.2.mlp.fc1.weight\n",
      "block2.2.mlp.fc1.bias\n",
      "block2.2.mlp.dwconv.dwconv.weight\n",
      "block2.2.mlp.dwconv.dwconv.bias\n",
      "block2.2.mlp.fc2.weight\n",
      "block2.2.mlp.fc2.bias\n",
      "block2.3.norm1.weight\n",
      "block2.3.norm1.bias\n",
      "block2.3.attn.q.weight\n",
      "block2.3.attn.q.bias\n",
      "block2.3.attn.kv.weight\n",
      "block2.3.attn.kv.bias\n",
      "block2.3.attn.proj.weight\n",
      "block2.3.attn.proj.bias\n",
      "block2.3.attn.sr.weight\n",
      "block2.3.attn.sr.bias\n",
      "block2.3.attn.norm.weight\n",
      "block2.3.attn.norm.bias\n",
      "block2.3.norm2.weight\n",
      "block2.3.norm2.bias\n",
      "block2.3.mlp.fc1.weight\n",
      "block2.3.mlp.fc1.bias\n",
      "block2.3.mlp.dwconv.dwconv.weight\n",
      "block2.3.mlp.dwconv.dwconv.bias\n",
      "block2.3.mlp.fc2.weight\n",
      "block2.3.mlp.fc2.bias\n",
      "norm2.weight\n",
      "norm2.bias\n",
      "block3.0.norm1.weight\n",
      "block3.0.norm1.bias\n",
      "block3.0.attn.q.weight\n",
      "block3.0.attn.q.bias\n",
      "block3.0.attn.kv.weight\n",
      "block3.0.attn.kv.bias\n",
      "block3.0.attn.proj.weight\n",
      "block3.0.attn.proj.bias\n",
      "block3.0.attn.sr.weight\n",
      "block3.0.attn.sr.bias\n",
      "block3.0.attn.norm.weight\n",
      "block3.0.attn.norm.bias\n",
      "block3.0.norm2.weight\n",
      "block3.0.norm2.bias\n",
      "block3.0.mlp.fc1.weight\n",
      "block3.0.mlp.fc1.bias\n",
      "block3.0.mlp.dwconv.dwconv.weight\n",
      "block3.0.mlp.dwconv.dwconv.bias\n",
      "block3.0.mlp.fc2.weight\n",
      "block3.0.mlp.fc2.bias\n",
      "block3.1.norm1.weight\n",
      "block3.1.norm1.bias\n",
      "block3.1.attn.q.weight\n",
      "block3.1.attn.q.bias\n",
      "block3.1.attn.kv.weight\n",
      "block3.1.attn.kv.bias\n",
      "block3.1.attn.proj.weight\n",
      "block3.1.attn.proj.bias\n",
      "block3.1.attn.sr.weight\n",
      "block3.1.attn.sr.bias\n",
      "block3.1.attn.norm.weight\n",
      "block3.1.attn.norm.bias\n",
      "block3.1.norm2.weight\n",
      "block3.1.norm2.bias\n",
      "block3.1.mlp.fc1.weight\n",
      "block3.1.mlp.fc1.bias\n",
      "block3.1.mlp.dwconv.dwconv.weight\n",
      "block3.1.mlp.dwconv.dwconv.bias\n",
      "block3.1.mlp.fc2.weight\n",
      "block3.1.mlp.fc2.bias\n",
      "block3.2.norm1.weight\n",
      "block3.2.norm1.bias\n",
      "block3.2.attn.q.weight\n",
      "block3.2.attn.q.bias\n",
      "block3.2.attn.kv.weight\n",
      "block3.2.attn.kv.bias\n",
      "block3.2.attn.proj.weight\n",
      "block3.2.attn.proj.bias\n",
      "block3.2.attn.sr.weight\n",
      "block3.2.attn.sr.bias\n",
      "block3.2.attn.norm.weight\n",
      "block3.2.attn.norm.bias\n",
      "block3.2.norm2.weight\n",
      "block3.2.norm2.bias\n",
      "block3.2.mlp.fc1.weight\n",
      "block3.2.mlp.fc1.bias\n",
      "block3.2.mlp.dwconv.dwconv.weight\n",
      "block3.2.mlp.dwconv.dwconv.bias\n",
      "block3.2.mlp.fc2.weight\n",
      "block3.2.mlp.fc2.bias\n",
      "block3.3.norm1.weight\n",
      "block3.3.norm1.bias\n",
      "block3.3.attn.q.weight\n",
      "block3.3.attn.q.bias\n",
      "block3.3.attn.kv.weight\n",
      "block3.3.attn.kv.bias\n",
      "block3.3.attn.proj.weight\n",
      "block3.3.attn.proj.bias\n",
      "block3.3.attn.sr.weight\n",
      "block3.3.attn.sr.bias\n",
      "block3.3.attn.norm.weight\n",
      "block3.3.attn.norm.bias\n",
      "block3.3.norm2.weight\n",
      "block3.3.norm2.bias\n",
      "block3.3.mlp.fc1.weight\n",
      "block3.3.mlp.fc1.bias\n",
      "block3.3.mlp.dwconv.dwconv.weight\n",
      "block3.3.mlp.dwconv.dwconv.bias\n",
      "block3.3.mlp.fc2.weight\n",
      "block3.3.mlp.fc2.bias\n",
      "block3.4.norm1.weight\n",
      "block3.4.norm1.bias\n",
      "block3.4.attn.q.weight\n",
      "block3.4.attn.q.bias\n",
      "block3.4.attn.kv.weight\n",
      "block3.4.attn.kv.bias\n",
      "block3.4.attn.proj.weight\n",
      "block3.4.attn.proj.bias\n",
      "block3.4.attn.sr.weight\n",
      "block3.4.attn.sr.bias\n",
      "block3.4.attn.norm.weight\n",
      "block3.4.attn.norm.bias\n",
      "block3.4.norm2.weight\n",
      "block3.4.norm2.bias\n",
      "block3.4.mlp.fc1.weight\n",
      "block3.4.mlp.fc1.bias\n",
      "block3.4.mlp.dwconv.dwconv.weight\n",
      "block3.4.mlp.dwconv.dwconv.bias\n",
      "block3.4.mlp.fc2.weight\n",
      "block3.4.mlp.fc2.bias\n",
      "block3.5.norm1.weight\n",
      "block3.5.norm1.bias\n",
      "block3.5.attn.q.weight\n",
      "block3.5.attn.q.bias\n",
      "block3.5.attn.kv.weight\n",
      "block3.5.attn.kv.bias\n",
      "block3.5.attn.proj.weight\n",
      "block3.5.attn.proj.bias\n",
      "block3.5.attn.sr.weight\n",
      "block3.5.attn.sr.bias\n",
      "block3.5.attn.norm.weight\n",
      "block3.5.attn.norm.bias\n",
      "block3.5.norm2.weight\n",
      "block3.5.norm2.bias\n",
      "block3.5.mlp.fc1.weight\n",
      "block3.5.mlp.fc1.bias\n",
      "block3.5.mlp.dwconv.dwconv.weight\n",
      "block3.5.mlp.dwconv.dwconv.bias\n",
      "block3.5.mlp.fc2.weight\n",
      "block3.5.mlp.fc2.bias\n",
      "norm3.weight\n",
      "norm3.bias\n",
      "block4.0.norm1.weight\n",
      "block4.0.norm1.bias\n",
      "block4.0.attn.q.weight\n",
      "block4.0.attn.q.bias\n",
      "block4.0.attn.kv.weight\n",
      "block4.0.attn.kv.bias\n",
      "block4.0.attn.proj.weight\n",
      "block4.0.attn.proj.bias\n",
      "block4.0.norm2.weight\n",
      "block4.0.norm2.bias\n",
      "block4.0.mlp.fc1.weight\n",
      "block4.0.mlp.fc1.bias\n",
      "block4.0.mlp.dwconv.dwconv.weight\n",
      "block4.0.mlp.dwconv.dwconv.bias\n",
      "block4.0.mlp.fc2.weight\n",
      "block4.0.mlp.fc2.bias\n",
      "block4.1.norm1.weight\n",
      "block4.1.norm1.bias\n",
      "block4.1.attn.q.weight\n",
      "block4.1.attn.q.bias\n",
      "block4.1.attn.kv.weight\n",
      "block4.1.attn.kv.bias\n",
      "block4.1.attn.proj.weight\n",
      "block4.1.attn.proj.bias\n",
      "block4.1.norm2.weight\n",
      "block4.1.norm2.bias\n",
      "block4.1.mlp.fc1.weight\n",
      "block4.1.mlp.fc1.bias\n",
      "block4.1.mlp.dwconv.dwconv.weight\n",
      "block4.1.mlp.dwconv.dwconv.bias\n",
      "block4.1.mlp.fc2.weight\n",
      "block4.1.mlp.fc2.bias\n",
      "block4.2.norm1.weight\n",
      "block4.2.norm1.bias\n",
      "block4.2.attn.q.weight\n",
      "block4.2.attn.q.bias\n",
      "block4.2.attn.kv.weight\n",
      "block4.2.attn.kv.bias\n",
      "block4.2.attn.proj.weight\n",
      "block4.2.attn.proj.bias\n",
      "block4.2.norm2.weight\n",
      "block4.2.norm2.bias\n",
      "block4.2.mlp.fc1.weight\n",
      "block4.2.mlp.fc1.bias\n",
      "block4.2.mlp.dwconv.dwconv.weight\n",
      "block4.2.mlp.dwconv.dwconv.bias\n",
      "block4.2.mlp.fc2.weight\n",
      "block4.2.mlp.fc2.bias\n",
      "norm4.weight\n",
      "norm4.bias\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "for key in timm.create_model(\"pvt_v2_b2\").state_dict():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params: 35,273,823\n",
      "torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 2, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "## PVT_CASCADE\n",
    "import torch\n",
    "from lib.networks import PVT_CASCADE\n",
    "from lib.cnn_vit_backbone import CONFIGS as CONFIGS_ViT_seg\n",
    "\n",
    "class args:\n",
    "    vit_name = \"R50-ViT-B_16\"\n",
    "    num_classes = 2\n",
    "    n_skip = 3 \n",
    "    img_size = 224\n",
    "    vit_patches_size = 16\n",
    "\n",
    "config_vit = CONFIGS_ViT_seg[args.vit_name]\n",
    "config_vit.n_classes = args.num_classes\n",
    "config_vit.n_skip = args.n_skip\n",
    "if args.vit_name.find('R50') != -1:\n",
    "    config_vit.patches.grid = (int(args.img_size / args.vit_patches_size), int(args.img_size / args.vit_patches_size))\n",
    "net = PVT_CASCADE(n_class=config_vit.n_classes).cuda() # model initialization\n",
    "weight = torch.load(\"./pretrained_pth/pvt/pvt_v2_b2.pth\")\n",
    "net.load_pvt_weights_from(weight)\n",
    "print('#params: {:,}'.format(sum([p.data.nelement() for p in net.parameters()])))\n",
    "\n",
    "inputs = torch.randn((1,1,224,224)).cuda()\n",
    "outputs = net(inputs)\n",
    "\n",
    "for out in outputs:\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed1.proj.weight\n",
      "patch_embed1.proj.bias\n",
      "patch_embed1.norm.weight\n",
      "patch_embed1.norm.bias\n",
      "patch_embed2.proj.weight\n",
      "patch_embed2.proj.bias\n",
      "patch_embed2.norm.weight\n",
      "patch_embed2.norm.bias\n",
      "patch_embed3.proj.weight\n",
      "patch_embed3.proj.bias\n",
      "patch_embed3.norm.weight\n",
      "patch_embed3.norm.bias\n",
      "patch_embed4.proj.weight\n",
      "patch_embed4.proj.bias\n",
      "patch_embed4.norm.weight\n",
      "patch_embed4.norm.bias\n",
      "block1.0.norm1.weight\n",
      "block1.0.norm1.bias\n",
      "block1.0.attn.q.weight\n",
      "block1.0.attn.q.bias\n",
      "block1.0.attn.kv.weight\n",
      "block1.0.attn.kv.bias\n",
      "block1.0.attn.proj.weight\n",
      "block1.0.attn.proj.bias\n",
      "block1.0.attn.sr.weight\n",
      "block1.0.attn.sr.bias\n",
      "block1.0.attn.norm.weight\n",
      "block1.0.attn.norm.bias\n",
      "block1.0.norm2.weight\n",
      "block1.0.norm2.bias\n",
      "block1.0.mlp.fc1.weight\n",
      "block1.0.mlp.fc1.bias\n",
      "block1.0.mlp.dwconv.dwconv.weight\n",
      "block1.0.mlp.dwconv.dwconv.bias\n",
      "block1.0.mlp.fc2.weight\n",
      "block1.0.mlp.fc2.bias\n",
      "block1.1.norm1.weight\n",
      "block1.1.norm1.bias\n",
      "block1.1.attn.q.weight\n",
      "block1.1.attn.q.bias\n",
      "block1.1.attn.kv.weight\n",
      "block1.1.attn.kv.bias\n",
      "block1.1.attn.proj.weight\n",
      "block1.1.attn.proj.bias\n",
      "block1.1.attn.sr.weight\n",
      "block1.1.attn.sr.bias\n",
      "block1.1.attn.norm.weight\n",
      "block1.1.attn.norm.bias\n",
      "block1.1.norm2.weight\n",
      "block1.1.norm2.bias\n",
      "block1.1.mlp.fc1.weight\n",
      "block1.1.mlp.fc1.bias\n",
      "block1.1.mlp.dwconv.dwconv.weight\n",
      "block1.1.mlp.dwconv.dwconv.bias\n",
      "block1.1.mlp.fc2.weight\n",
      "block1.1.mlp.fc2.bias\n",
      "block1.2.norm1.weight\n",
      "block1.2.norm1.bias\n",
      "block1.2.attn.q.weight\n",
      "block1.2.attn.q.bias\n",
      "block1.2.attn.kv.weight\n",
      "block1.2.attn.kv.bias\n",
      "block1.2.attn.proj.weight\n",
      "block1.2.attn.proj.bias\n",
      "block1.2.attn.sr.weight\n",
      "block1.2.attn.sr.bias\n",
      "block1.2.attn.norm.weight\n",
      "block1.2.attn.norm.bias\n",
      "block1.2.norm2.weight\n",
      "block1.2.norm2.bias\n",
      "block1.2.mlp.fc1.weight\n",
      "block1.2.mlp.fc1.bias\n",
      "block1.2.mlp.dwconv.dwconv.weight\n",
      "block1.2.mlp.dwconv.dwconv.bias\n",
      "block1.2.mlp.fc2.weight\n",
      "block1.2.mlp.fc2.bias\n",
      "norm1.weight\n",
      "norm1.bias\n",
      "block2.0.norm1.weight\n",
      "block2.0.norm1.bias\n",
      "block2.0.attn.q.weight\n",
      "block2.0.attn.q.bias\n",
      "block2.0.attn.kv.weight\n",
      "block2.0.attn.kv.bias\n",
      "block2.0.attn.proj.weight\n",
      "block2.0.attn.proj.bias\n",
      "block2.0.attn.sr.weight\n",
      "block2.0.attn.sr.bias\n",
      "block2.0.attn.norm.weight\n",
      "block2.0.attn.norm.bias\n",
      "block2.0.norm2.weight\n",
      "block2.0.norm2.bias\n",
      "block2.0.mlp.fc1.weight\n",
      "block2.0.mlp.fc1.bias\n",
      "block2.0.mlp.dwconv.dwconv.weight\n",
      "block2.0.mlp.dwconv.dwconv.bias\n",
      "block2.0.mlp.fc2.weight\n",
      "block2.0.mlp.fc2.bias\n",
      "block2.1.norm1.weight\n",
      "block2.1.norm1.bias\n",
      "block2.1.attn.q.weight\n",
      "block2.1.attn.q.bias\n",
      "block2.1.attn.kv.weight\n",
      "block2.1.attn.kv.bias\n",
      "block2.1.attn.proj.weight\n",
      "block2.1.attn.proj.bias\n",
      "block2.1.attn.sr.weight\n",
      "block2.1.attn.sr.bias\n",
      "block2.1.attn.norm.weight\n",
      "block2.1.attn.norm.bias\n",
      "block2.1.norm2.weight\n",
      "block2.1.norm2.bias\n",
      "block2.1.mlp.fc1.weight\n",
      "block2.1.mlp.fc1.bias\n",
      "block2.1.mlp.dwconv.dwconv.weight\n",
      "block2.1.mlp.dwconv.dwconv.bias\n",
      "block2.1.mlp.fc2.weight\n",
      "block2.1.mlp.fc2.bias\n",
      "block2.2.norm1.weight\n",
      "block2.2.norm1.bias\n",
      "block2.2.attn.q.weight\n",
      "block2.2.attn.q.bias\n",
      "block2.2.attn.kv.weight\n",
      "block2.2.attn.kv.bias\n",
      "block2.2.attn.proj.weight\n",
      "block2.2.attn.proj.bias\n",
      "block2.2.attn.sr.weight\n",
      "block2.2.attn.sr.bias\n",
      "block2.2.attn.norm.weight\n",
      "block2.2.attn.norm.bias\n",
      "block2.2.norm2.weight\n",
      "block2.2.norm2.bias\n",
      "block2.2.mlp.fc1.weight\n",
      "block2.2.mlp.fc1.bias\n",
      "block2.2.mlp.dwconv.dwconv.weight\n",
      "block2.2.mlp.dwconv.dwconv.bias\n",
      "block2.2.mlp.fc2.weight\n",
      "block2.2.mlp.fc2.bias\n",
      "block2.3.norm1.weight\n",
      "block2.3.norm1.bias\n",
      "block2.3.attn.q.weight\n",
      "block2.3.attn.q.bias\n",
      "block2.3.attn.kv.weight\n",
      "block2.3.attn.kv.bias\n",
      "block2.3.attn.proj.weight\n",
      "block2.3.attn.proj.bias\n",
      "block2.3.attn.sr.weight\n",
      "block2.3.attn.sr.bias\n",
      "block2.3.attn.norm.weight\n",
      "block2.3.attn.norm.bias\n",
      "block2.3.norm2.weight\n",
      "block2.3.norm2.bias\n",
      "block2.3.mlp.fc1.weight\n",
      "block2.3.mlp.fc1.bias\n",
      "block2.3.mlp.dwconv.dwconv.weight\n",
      "block2.3.mlp.dwconv.dwconv.bias\n",
      "block2.3.mlp.fc2.weight\n",
      "block2.3.mlp.fc2.bias\n",
      "norm2.weight\n",
      "norm2.bias\n",
      "block3.0.norm1.weight\n",
      "block3.0.norm1.bias\n",
      "block3.0.attn.q.weight\n",
      "block3.0.attn.q.bias\n",
      "block3.0.attn.kv.weight\n",
      "block3.0.attn.kv.bias\n",
      "block3.0.attn.proj.weight\n",
      "block3.0.attn.proj.bias\n",
      "block3.0.attn.sr.weight\n",
      "block3.0.attn.sr.bias\n",
      "block3.0.attn.norm.weight\n",
      "block3.0.attn.norm.bias\n",
      "block3.0.norm2.weight\n",
      "block3.0.norm2.bias\n",
      "block3.0.mlp.fc1.weight\n",
      "block3.0.mlp.fc1.bias\n",
      "block3.0.mlp.dwconv.dwconv.weight\n",
      "block3.0.mlp.dwconv.dwconv.bias\n",
      "block3.0.mlp.fc2.weight\n",
      "block3.0.mlp.fc2.bias\n",
      "block3.1.norm1.weight\n",
      "block3.1.norm1.bias\n",
      "block3.1.attn.q.weight\n",
      "block3.1.attn.q.bias\n",
      "block3.1.attn.kv.weight\n",
      "block3.1.attn.kv.bias\n",
      "block3.1.attn.proj.weight\n",
      "block3.1.attn.proj.bias\n",
      "block3.1.attn.sr.weight\n",
      "block3.1.attn.sr.bias\n",
      "block3.1.attn.norm.weight\n",
      "block3.1.attn.norm.bias\n",
      "block3.1.norm2.weight\n",
      "block3.1.norm2.bias\n",
      "block3.1.mlp.fc1.weight\n",
      "block3.1.mlp.fc1.bias\n",
      "block3.1.mlp.dwconv.dwconv.weight\n",
      "block3.1.mlp.dwconv.dwconv.bias\n",
      "block3.1.mlp.fc2.weight\n",
      "block3.1.mlp.fc2.bias\n",
      "block3.2.norm1.weight\n",
      "block3.2.norm1.bias\n",
      "block3.2.attn.q.weight\n",
      "block3.2.attn.q.bias\n",
      "block3.2.attn.kv.weight\n",
      "block3.2.attn.kv.bias\n",
      "block3.2.attn.proj.weight\n",
      "block3.2.attn.proj.bias\n",
      "block3.2.attn.sr.weight\n",
      "block3.2.attn.sr.bias\n",
      "block3.2.attn.norm.weight\n",
      "block3.2.attn.norm.bias\n",
      "block3.2.norm2.weight\n",
      "block3.2.norm2.bias\n",
      "block3.2.mlp.fc1.weight\n",
      "block3.2.mlp.fc1.bias\n",
      "block3.2.mlp.dwconv.dwconv.weight\n",
      "block3.2.mlp.dwconv.dwconv.bias\n",
      "block3.2.mlp.fc2.weight\n",
      "block3.2.mlp.fc2.bias\n",
      "block3.3.norm1.weight\n",
      "block3.3.norm1.bias\n",
      "block3.3.attn.q.weight\n",
      "block3.3.attn.q.bias\n",
      "block3.3.attn.kv.weight\n",
      "block3.3.attn.kv.bias\n",
      "block3.3.attn.proj.weight\n",
      "block3.3.attn.proj.bias\n",
      "block3.3.attn.sr.weight\n",
      "block3.3.attn.sr.bias\n",
      "block3.3.attn.norm.weight\n",
      "block3.3.attn.norm.bias\n",
      "block3.3.norm2.weight\n",
      "block3.3.norm2.bias\n",
      "block3.3.mlp.fc1.weight\n",
      "block3.3.mlp.fc1.bias\n",
      "block3.3.mlp.dwconv.dwconv.weight\n",
      "block3.3.mlp.dwconv.dwconv.bias\n",
      "block3.3.mlp.fc2.weight\n",
      "block3.3.mlp.fc2.bias\n",
      "block3.4.norm1.weight\n",
      "block3.4.norm1.bias\n",
      "block3.4.attn.q.weight\n",
      "block3.4.attn.q.bias\n",
      "block3.4.attn.kv.weight\n",
      "block3.4.attn.kv.bias\n",
      "block3.4.attn.proj.weight\n",
      "block3.4.attn.proj.bias\n",
      "block3.4.attn.sr.weight\n",
      "block3.4.attn.sr.bias\n",
      "block3.4.attn.norm.weight\n",
      "block3.4.attn.norm.bias\n",
      "block3.4.norm2.weight\n",
      "block3.4.norm2.bias\n",
      "block3.4.mlp.fc1.weight\n",
      "block3.4.mlp.fc1.bias\n",
      "block3.4.mlp.dwconv.dwconv.weight\n",
      "block3.4.mlp.dwconv.dwconv.bias\n",
      "block3.4.mlp.fc2.weight\n",
      "block3.4.mlp.fc2.bias\n",
      "block3.5.norm1.weight\n",
      "block3.5.norm1.bias\n",
      "block3.5.attn.q.weight\n",
      "block3.5.attn.q.bias\n",
      "block3.5.attn.kv.weight\n",
      "block3.5.attn.kv.bias\n",
      "block3.5.attn.proj.weight\n",
      "block3.5.attn.proj.bias\n",
      "block3.5.attn.sr.weight\n",
      "block3.5.attn.sr.bias\n",
      "block3.5.attn.norm.weight\n",
      "block3.5.attn.norm.bias\n",
      "block3.5.norm2.weight\n",
      "block3.5.norm2.bias\n",
      "block3.5.mlp.fc1.weight\n",
      "block3.5.mlp.fc1.bias\n",
      "block3.5.mlp.dwconv.dwconv.weight\n",
      "block3.5.mlp.dwconv.dwconv.bias\n",
      "block3.5.mlp.fc2.weight\n",
      "block3.5.mlp.fc2.bias\n",
      "norm3.weight\n",
      "norm3.bias\n",
      "block4.0.norm1.weight\n",
      "block4.0.norm1.bias\n",
      "block4.0.attn.q.weight\n",
      "block4.0.attn.q.bias\n",
      "block4.0.attn.kv.weight\n",
      "block4.0.attn.kv.bias\n",
      "block4.0.attn.proj.weight\n",
      "block4.0.attn.proj.bias\n",
      "block4.0.norm2.weight\n",
      "block4.0.norm2.bias\n",
      "block4.0.mlp.fc1.weight\n",
      "block4.0.mlp.fc1.bias\n",
      "block4.0.mlp.dwconv.dwconv.weight\n",
      "block4.0.mlp.dwconv.dwconv.bias\n",
      "block4.0.mlp.fc2.weight\n",
      "block4.0.mlp.fc2.bias\n",
      "block4.1.norm1.weight\n",
      "block4.1.norm1.bias\n",
      "block4.1.attn.q.weight\n",
      "block4.1.attn.q.bias\n",
      "block4.1.attn.kv.weight\n",
      "block4.1.attn.kv.bias\n",
      "block4.1.attn.proj.weight\n",
      "block4.1.attn.proj.bias\n",
      "block4.1.norm2.weight\n",
      "block4.1.norm2.bias\n",
      "block4.1.mlp.fc1.weight\n",
      "block4.1.mlp.fc1.bias\n",
      "block4.1.mlp.dwconv.dwconv.weight\n",
      "block4.1.mlp.dwconv.dwconv.bias\n",
      "block4.1.mlp.fc2.weight\n",
      "block4.1.mlp.fc2.bias\n",
      "block4.2.norm1.weight\n",
      "block4.2.norm1.bias\n",
      "block4.2.attn.q.weight\n",
      "block4.2.attn.q.bias\n",
      "block4.2.attn.kv.weight\n",
      "block4.2.attn.kv.bias\n",
      "block4.2.attn.proj.weight\n",
      "block4.2.attn.proj.bias\n",
      "block4.2.norm2.weight\n",
      "block4.2.norm2.bias\n",
      "block4.2.mlp.fc1.weight\n",
      "block4.2.mlp.fc1.bias\n",
      "block4.2.mlp.dwconv.dwconv.weight\n",
      "block4.2.mlp.dwconv.dwconv.bias\n",
      "block4.2.mlp.fc2.weight\n",
      "block4.2.mlp.fc2.bias\n",
      "norm4.weight\n",
      "norm4.bias\n",
      "head.weight\n",
      "head.bias\n"
     ]
    }
   ],
   "source": [
    "for key in weight:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.0.weight\n",
      "conv.0.bias\n",
      "conv.1.weight\n",
      "conv.1.bias\n",
      "conv.1.running_mean\n",
      "conv.1.running_var\n",
      "conv.1.num_batches_tracked\n",
      "backbone.patch_embed1.proj.weight\n",
      "backbone.patch_embed1.proj.bias\n",
      "backbone.patch_embed1.norm.weight\n",
      "backbone.patch_embed1.norm.bias\n",
      "backbone.patch_embed2.proj.weight\n",
      "backbone.patch_embed2.proj.bias\n",
      "backbone.patch_embed2.norm.weight\n",
      "backbone.patch_embed2.norm.bias\n",
      "backbone.patch_embed3.proj.weight\n",
      "backbone.patch_embed3.proj.bias\n",
      "backbone.patch_embed3.norm.weight\n",
      "backbone.patch_embed3.norm.bias\n",
      "backbone.patch_embed4.proj.weight\n",
      "backbone.patch_embed4.proj.bias\n",
      "backbone.patch_embed4.norm.weight\n",
      "backbone.patch_embed4.norm.bias\n",
      "backbone.block1.0.norm1.weight\n",
      "backbone.block1.0.norm1.bias\n",
      "backbone.block1.0.attn.q.weight\n",
      "backbone.block1.0.attn.q.bias\n",
      "backbone.block1.0.attn.kv.weight\n",
      "backbone.block1.0.attn.kv.bias\n",
      "backbone.block1.0.attn.proj.weight\n",
      "backbone.block1.0.attn.proj.bias\n",
      "backbone.block1.0.attn.sr.weight\n",
      "backbone.block1.0.attn.sr.bias\n",
      "backbone.block1.0.attn.norm.weight\n",
      "backbone.block1.0.attn.norm.bias\n",
      "backbone.block1.0.norm2.weight\n",
      "backbone.block1.0.norm2.bias\n",
      "backbone.block1.0.mlp.fc1.weight\n",
      "backbone.block1.0.mlp.fc1.bias\n",
      "backbone.block1.0.mlp.dwconv.dwconv.weight\n",
      "backbone.block1.0.mlp.dwconv.dwconv.bias\n",
      "backbone.block1.0.mlp.fc2.weight\n",
      "backbone.block1.0.mlp.fc2.bias\n",
      "backbone.block1.1.norm1.weight\n",
      "backbone.block1.1.norm1.bias\n",
      "backbone.block1.1.attn.q.weight\n",
      "backbone.block1.1.attn.q.bias\n",
      "backbone.block1.1.attn.kv.weight\n",
      "backbone.block1.1.attn.kv.bias\n",
      "backbone.block1.1.attn.proj.weight\n",
      "backbone.block1.1.attn.proj.bias\n",
      "backbone.block1.1.attn.sr.weight\n",
      "backbone.block1.1.attn.sr.bias\n",
      "backbone.block1.1.attn.norm.weight\n",
      "backbone.block1.1.attn.norm.bias\n",
      "backbone.block1.1.norm2.weight\n",
      "backbone.block1.1.norm2.bias\n",
      "backbone.block1.1.mlp.fc1.weight\n",
      "backbone.block1.1.mlp.fc1.bias\n",
      "backbone.block1.1.mlp.dwconv.dwconv.weight\n",
      "backbone.block1.1.mlp.dwconv.dwconv.bias\n",
      "backbone.block1.1.mlp.fc2.weight\n",
      "backbone.block1.1.mlp.fc2.bias\n",
      "backbone.block1.2.norm1.weight\n",
      "backbone.block1.2.norm1.bias\n",
      "backbone.block1.2.attn.q.weight\n",
      "backbone.block1.2.attn.q.bias\n",
      "backbone.block1.2.attn.kv.weight\n",
      "backbone.block1.2.attn.kv.bias\n",
      "backbone.block1.2.attn.proj.weight\n",
      "backbone.block1.2.attn.proj.bias\n",
      "backbone.block1.2.attn.sr.weight\n",
      "backbone.block1.2.attn.sr.bias\n",
      "backbone.block1.2.attn.norm.weight\n",
      "backbone.block1.2.attn.norm.bias\n",
      "backbone.block1.2.norm2.weight\n",
      "backbone.block1.2.norm2.bias\n",
      "backbone.block1.2.mlp.fc1.weight\n",
      "backbone.block1.2.mlp.fc1.bias\n",
      "backbone.block1.2.mlp.dwconv.dwconv.weight\n",
      "backbone.block1.2.mlp.dwconv.dwconv.bias\n",
      "backbone.block1.2.mlp.fc2.weight\n",
      "backbone.block1.2.mlp.fc2.bias\n",
      "backbone.norm1.weight\n",
      "backbone.norm1.bias\n",
      "backbone.block2.0.norm1.weight\n",
      "backbone.block2.0.norm1.bias\n",
      "backbone.block2.0.attn.q.weight\n",
      "backbone.block2.0.attn.q.bias\n",
      "backbone.block2.0.attn.kv.weight\n",
      "backbone.block2.0.attn.kv.bias\n",
      "backbone.block2.0.attn.proj.weight\n",
      "backbone.block2.0.attn.proj.bias\n",
      "backbone.block2.0.attn.sr.weight\n",
      "backbone.block2.0.attn.sr.bias\n",
      "backbone.block2.0.attn.norm.weight\n",
      "backbone.block2.0.attn.norm.bias\n",
      "backbone.block2.0.norm2.weight\n",
      "backbone.block2.0.norm2.bias\n",
      "backbone.block2.0.mlp.fc1.weight\n",
      "backbone.block2.0.mlp.fc1.bias\n",
      "backbone.block2.0.mlp.dwconv.dwconv.weight\n",
      "backbone.block2.0.mlp.dwconv.dwconv.bias\n",
      "backbone.block2.0.mlp.fc2.weight\n",
      "backbone.block2.0.mlp.fc2.bias\n",
      "backbone.block2.1.norm1.weight\n",
      "backbone.block2.1.norm1.bias\n",
      "backbone.block2.1.attn.q.weight\n",
      "backbone.block2.1.attn.q.bias\n",
      "backbone.block2.1.attn.kv.weight\n",
      "backbone.block2.1.attn.kv.bias\n",
      "backbone.block2.1.attn.proj.weight\n",
      "backbone.block2.1.attn.proj.bias\n",
      "backbone.block2.1.attn.sr.weight\n",
      "backbone.block2.1.attn.sr.bias\n",
      "backbone.block2.1.attn.norm.weight\n",
      "backbone.block2.1.attn.norm.bias\n",
      "backbone.block2.1.norm2.weight\n",
      "backbone.block2.1.norm2.bias\n",
      "backbone.block2.1.mlp.fc1.weight\n",
      "backbone.block2.1.mlp.fc1.bias\n",
      "backbone.block2.1.mlp.dwconv.dwconv.weight\n",
      "backbone.block2.1.mlp.dwconv.dwconv.bias\n",
      "backbone.block2.1.mlp.fc2.weight\n",
      "backbone.block2.1.mlp.fc2.bias\n",
      "backbone.block2.2.norm1.weight\n",
      "backbone.block2.2.norm1.bias\n",
      "backbone.block2.2.attn.q.weight\n",
      "backbone.block2.2.attn.q.bias\n",
      "backbone.block2.2.attn.kv.weight\n",
      "backbone.block2.2.attn.kv.bias\n",
      "backbone.block2.2.attn.proj.weight\n",
      "backbone.block2.2.attn.proj.bias\n",
      "backbone.block2.2.attn.sr.weight\n",
      "backbone.block2.2.attn.sr.bias\n",
      "backbone.block2.2.attn.norm.weight\n",
      "backbone.block2.2.attn.norm.bias\n",
      "backbone.block2.2.norm2.weight\n",
      "backbone.block2.2.norm2.bias\n",
      "backbone.block2.2.mlp.fc1.weight\n",
      "backbone.block2.2.mlp.fc1.bias\n",
      "backbone.block2.2.mlp.dwconv.dwconv.weight\n",
      "backbone.block2.2.mlp.dwconv.dwconv.bias\n",
      "backbone.block2.2.mlp.fc2.weight\n",
      "backbone.block2.2.mlp.fc2.bias\n",
      "backbone.block2.3.norm1.weight\n",
      "backbone.block2.3.norm1.bias\n",
      "backbone.block2.3.attn.q.weight\n",
      "backbone.block2.3.attn.q.bias\n",
      "backbone.block2.3.attn.kv.weight\n",
      "backbone.block2.3.attn.kv.bias\n",
      "backbone.block2.3.attn.proj.weight\n",
      "backbone.block2.3.attn.proj.bias\n",
      "backbone.block2.3.attn.sr.weight\n",
      "backbone.block2.3.attn.sr.bias\n",
      "backbone.block2.3.attn.norm.weight\n",
      "backbone.block2.3.attn.norm.bias\n",
      "backbone.block2.3.norm2.weight\n",
      "backbone.block2.3.norm2.bias\n",
      "backbone.block2.3.mlp.fc1.weight\n",
      "backbone.block2.3.mlp.fc1.bias\n",
      "backbone.block2.3.mlp.dwconv.dwconv.weight\n",
      "backbone.block2.3.mlp.dwconv.dwconv.bias\n",
      "backbone.block2.3.mlp.fc2.weight\n",
      "backbone.block2.3.mlp.fc2.bias\n",
      "backbone.norm2.weight\n",
      "backbone.norm2.bias\n",
      "backbone.block3.0.norm1.weight\n",
      "backbone.block3.0.norm1.bias\n",
      "backbone.block3.0.attn.q.weight\n",
      "backbone.block3.0.attn.q.bias\n",
      "backbone.block3.0.attn.kv.weight\n",
      "backbone.block3.0.attn.kv.bias\n",
      "backbone.block3.0.attn.proj.weight\n",
      "backbone.block3.0.attn.proj.bias\n",
      "backbone.block3.0.attn.sr.weight\n",
      "backbone.block3.0.attn.sr.bias\n",
      "backbone.block3.0.attn.norm.weight\n",
      "backbone.block3.0.attn.norm.bias\n",
      "backbone.block3.0.norm2.weight\n",
      "backbone.block3.0.norm2.bias\n",
      "backbone.block3.0.mlp.fc1.weight\n",
      "backbone.block3.0.mlp.fc1.bias\n",
      "backbone.block3.0.mlp.dwconv.dwconv.weight\n",
      "backbone.block3.0.mlp.dwconv.dwconv.bias\n",
      "backbone.block3.0.mlp.fc2.weight\n",
      "backbone.block3.0.mlp.fc2.bias\n",
      "backbone.block3.1.norm1.weight\n",
      "backbone.block3.1.norm1.bias\n",
      "backbone.block3.1.attn.q.weight\n",
      "backbone.block3.1.attn.q.bias\n",
      "backbone.block3.1.attn.kv.weight\n",
      "backbone.block3.1.attn.kv.bias\n",
      "backbone.block3.1.attn.proj.weight\n",
      "backbone.block3.1.attn.proj.bias\n",
      "backbone.block3.1.attn.sr.weight\n",
      "backbone.block3.1.attn.sr.bias\n",
      "backbone.block3.1.attn.norm.weight\n",
      "backbone.block3.1.attn.norm.bias\n",
      "backbone.block3.1.norm2.weight\n",
      "backbone.block3.1.norm2.bias\n",
      "backbone.block3.1.mlp.fc1.weight\n",
      "backbone.block3.1.mlp.fc1.bias\n",
      "backbone.block3.1.mlp.dwconv.dwconv.weight\n",
      "backbone.block3.1.mlp.dwconv.dwconv.bias\n",
      "backbone.block3.1.mlp.fc2.weight\n",
      "backbone.block3.1.mlp.fc2.bias\n",
      "backbone.block3.2.norm1.weight\n",
      "backbone.block3.2.norm1.bias\n",
      "backbone.block3.2.attn.q.weight\n",
      "backbone.block3.2.attn.q.bias\n",
      "backbone.block3.2.attn.kv.weight\n",
      "backbone.block3.2.attn.kv.bias\n",
      "backbone.block3.2.attn.proj.weight\n",
      "backbone.block3.2.attn.proj.bias\n",
      "backbone.block3.2.attn.sr.weight\n",
      "backbone.block3.2.attn.sr.bias\n",
      "backbone.block3.2.attn.norm.weight\n",
      "backbone.block3.2.attn.norm.bias\n",
      "backbone.block3.2.norm2.weight\n",
      "backbone.block3.2.norm2.bias\n",
      "backbone.block3.2.mlp.fc1.weight\n",
      "backbone.block3.2.mlp.fc1.bias\n",
      "backbone.block3.2.mlp.dwconv.dwconv.weight\n",
      "backbone.block3.2.mlp.dwconv.dwconv.bias\n",
      "backbone.block3.2.mlp.fc2.weight\n",
      "backbone.block3.2.mlp.fc2.bias\n",
      "backbone.block3.3.norm1.weight\n",
      "backbone.block3.3.norm1.bias\n",
      "backbone.block3.3.attn.q.weight\n",
      "backbone.block3.3.attn.q.bias\n",
      "backbone.block3.3.attn.kv.weight\n",
      "backbone.block3.3.attn.kv.bias\n",
      "backbone.block3.3.attn.proj.weight\n",
      "backbone.block3.3.attn.proj.bias\n",
      "backbone.block3.3.attn.sr.weight\n",
      "backbone.block3.3.attn.sr.bias\n",
      "backbone.block3.3.attn.norm.weight\n",
      "backbone.block3.3.attn.norm.bias\n",
      "backbone.block3.3.norm2.weight\n",
      "backbone.block3.3.norm2.bias\n",
      "backbone.block3.3.mlp.fc1.weight\n",
      "backbone.block3.3.mlp.fc1.bias\n",
      "backbone.block3.3.mlp.dwconv.dwconv.weight\n",
      "backbone.block3.3.mlp.dwconv.dwconv.bias\n",
      "backbone.block3.3.mlp.fc2.weight\n",
      "backbone.block3.3.mlp.fc2.bias\n",
      "backbone.block3.4.norm1.weight\n",
      "backbone.block3.4.norm1.bias\n",
      "backbone.block3.4.attn.q.weight\n",
      "backbone.block3.4.attn.q.bias\n",
      "backbone.block3.4.attn.kv.weight\n",
      "backbone.block3.4.attn.kv.bias\n",
      "backbone.block3.4.attn.proj.weight\n",
      "backbone.block3.4.attn.proj.bias\n",
      "backbone.block3.4.attn.sr.weight\n",
      "backbone.block3.4.attn.sr.bias\n",
      "backbone.block3.4.attn.norm.weight\n",
      "backbone.block3.4.attn.norm.bias\n",
      "backbone.block3.4.norm2.weight\n",
      "backbone.block3.4.norm2.bias\n",
      "backbone.block3.4.mlp.fc1.weight\n",
      "backbone.block3.4.mlp.fc1.bias\n",
      "backbone.block3.4.mlp.dwconv.dwconv.weight\n",
      "backbone.block3.4.mlp.dwconv.dwconv.bias\n",
      "backbone.block3.4.mlp.fc2.weight\n",
      "backbone.block3.4.mlp.fc2.bias\n",
      "backbone.block3.5.norm1.weight\n",
      "backbone.block3.5.norm1.bias\n",
      "backbone.block3.5.attn.q.weight\n",
      "backbone.block3.5.attn.q.bias\n",
      "backbone.block3.5.attn.kv.weight\n",
      "backbone.block3.5.attn.kv.bias\n",
      "backbone.block3.5.attn.proj.weight\n",
      "backbone.block3.5.attn.proj.bias\n",
      "backbone.block3.5.attn.sr.weight\n",
      "backbone.block3.5.attn.sr.bias\n",
      "backbone.block3.5.attn.norm.weight\n",
      "backbone.block3.5.attn.norm.bias\n",
      "backbone.block3.5.norm2.weight\n",
      "backbone.block3.5.norm2.bias\n",
      "backbone.block3.5.mlp.fc1.weight\n",
      "backbone.block3.5.mlp.fc1.bias\n",
      "backbone.block3.5.mlp.dwconv.dwconv.weight\n",
      "backbone.block3.5.mlp.dwconv.dwconv.bias\n",
      "backbone.block3.5.mlp.fc2.weight\n",
      "backbone.block3.5.mlp.fc2.bias\n",
      "backbone.norm3.weight\n",
      "backbone.norm3.bias\n",
      "backbone.block4.0.norm1.weight\n",
      "backbone.block4.0.norm1.bias\n",
      "backbone.block4.0.attn.q.weight\n",
      "backbone.block4.0.attn.q.bias\n",
      "backbone.block4.0.attn.kv.weight\n",
      "backbone.block4.0.attn.kv.bias\n",
      "backbone.block4.0.attn.proj.weight\n",
      "backbone.block4.0.attn.proj.bias\n",
      "backbone.block4.0.norm2.weight\n",
      "backbone.block4.0.norm2.bias\n",
      "backbone.block4.0.mlp.fc1.weight\n",
      "backbone.block4.0.mlp.fc1.bias\n",
      "backbone.block4.0.mlp.dwconv.dwconv.weight\n",
      "backbone.block4.0.mlp.dwconv.dwconv.bias\n",
      "backbone.block4.0.mlp.fc2.weight\n",
      "backbone.block4.0.mlp.fc2.bias\n",
      "backbone.block4.1.norm1.weight\n",
      "backbone.block4.1.norm1.bias\n",
      "backbone.block4.1.attn.q.weight\n",
      "backbone.block4.1.attn.q.bias\n",
      "backbone.block4.1.attn.kv.weight\n",
      "backbone.block4.1.attn.kv.bias\n",
      "backbone.block4.1.attn.proj.weight\n",
      "backbone.block4.1.attn.proj.bias\n",
      "backbone.block4.1.norm2.weight\n",
      "backbone.block4.1.norm2.bias\n",
      "backbone.block4.1.mlp.fc1.weight\n",
      "backbone.block4.1.mlp.fc1.bias\n",
      "backbone.block4.1.mlp.dwconv.dwconv.weight\n",
      "backbone.block4.1.mlp.dwconv.dwconv.bias\n",
      "backbone.block4.1.mlp.fc2.weight\n",
      "backbone.block4.1.mlp.fc2.bias\n",
      "backbone.block4.2.norm1.weight\n",
      "backbone.block4.2.norm1.bias\n",
      "backbone.block4.2.attn.q.weight\n",
      "backbone.block4.2.attn.q.bias\n",
      "backbone.block4.2.attn.kv.weight\n",
      "backbone.block4.2.attn.kv.bias\n",
      "backbone.block4.2.attn.proj.weight\n",
      "backbone.block4.2.attn.proj.bias\n",
      "backbone.block4.2.norm2.weight\n",
      "backbone.block4.2.norm2.bias\n",
      "backbone.block4.2.mlp.fc1.weight\n",
      "backbone.block4.2.mlp.fc1.bias\n",
      "backbone.block4.2.mlp.dwconv.dwconv.weight\n",
      "backbone.block4.2.mlp.dwconv.dwconv.bias\n",
      "backbone.block4.2.mlp.fc2.weight\n",
      "backbone.block4.2.mlp.fc2.bias\n",
      "backbone.norm4.weight\n",
      "backbone.norm4.bias\n",
      "decoder.Conv_1x1.weight\n",
      "decoder.Conv_1x1.bias\n",
      "decoder.ConvBlock4.conv.0.weight\n",
      "decoder.ConvBlock4.conv.0.bias\n",
      "decoder.ConvBlock4.conv.1.weight\n",
      "decoder.ConvBlock4.conv.1.bias\n",
      "decoder.ConvBlock4.conv.1.running_mean\n",
      "decoder.ConvBlock4.conv.1.running_var\n",
      "decoder.ConvBlock4.conv.1.num_batches_tracked\n",
      "decoder.ConvBlock4.conv.3.weight\n",
      "decoder.ConvBlock4.conv.3.bias\n",
      "decoder.ConvBlock4.conv.4.weight\n",
      "decoder.ConvBlock4.conv.4.bias\n",
      "decoder.ConvBlock4.conv.4.running_mean\n",
      "decoder.ConvBlock4.conv.4.running_var\n",
      "decoder.ConvBlock4.conv.4.num_batches_tracked\n",
      "decoder.Up3.up.1.weight\n",
      "decoder.Up3.up.1.bias\n",
      "decoder.Up3.up.2.weight\n",
      "decoder.Up3.up.2.bias\n",
      "decoder.Up3.up.2.running_mean\n",
      "decoder.Up3.up.2.running_var\n",
      "decoder.Up3.up.2.num_batches_tracked\n",
      "decoder.AG3.W_g.0.weight\n",
      "decoder.AG3.W_g.0.bias\n",
      "decoder.AG3.W_g.1.weight\n",
      "decoder.AG3.W_g.1.bias\n",
      "decoder.AG3.W_g.1.running_mean\n",
      "decoder.AG3.W_g.1.running_var\n",
      "decoder.AG3.W_g.1.num_batches_tracked\n",
      "decoder.AG3.W_x.0.weight\n",
      "decoder.AG3.W_x.0.bias\n",
      "decoder.AG3.W_x.1.weight\n",
      "decoder.AG3.W_x.1.bias\n",
      "decoder.AG3.W_x.1.running_mean\n",
      "decoder.AG3.W_x.1.running_var\n",
      "decoder.AG3.W_x.1.num_batches_tracked\n",
      "decoder.AG3.psi.0.weight\n",
      "decoder.AG3.psi.0.bias\n",
      "decoder.AG3.psi.1.weight\n",
      "decoder.AG3.psi.1.bias\n",
      "decoder.AG3.psi.1.running_mean\n",
      "decoder.AG3.psi.1.running_var\n",
      "decoder.AG3.psi.1.num_batches_tracked\n",
      "decoder.ConvBlock3.conv.0.weight\n",
      "decoder.ConvBlock3.conv.0.bias\n",
      "decoder.ConvBlock3.conv.1.weight\n",
      "decoder.ConvBlock3.conv.1.bias\n",
      "decoder.ConvBlock3.conv.1.running_mean\n",
      "decoder.ConvBlock3.conv.1.running_var\n",
      "decoder.ConvBlock3.conv.1.num_batches_tracked\n",
      "decoder.ConvBlock3.conv.3.weight\n",
      "decoder.ConvBlock3.conv.3.bias\n",
      "decoder.ConvBlock3.conv.4.weight\n",
      "decoder.ConvBlock3.conv.4.bias\n",
      "decoder.ConvBlock3.conv.4.running_mean\n",
      "decoder.ConvBlock3.conv.4.running_var\n",
      "decoder.ConvBlock3.conv.4.num_batches_tracked\n",
      "decoder.Up2.up.1.weight\n",
      "decoder.Up2.up.1.bias\n",
      "decoder.Up2.up.2.weight\n",
      "decoder.Up2.up.2.bias\n",
      "decoder.Up2.up.2.running_mean\n",
      "decoder.Up2.up.2.running_var\n",
      "decoder.Up2.up.2.num_batches_tracked\n",
      "decoder.AG2.W_g.0.weight\n",
      "decoder.AG2.W_g.0.bias\n",
      "decoder.AG2.W_g.1.weight\n",
      "decoder.AG2.W_g.1.bias\n",
      "decoder.AG2.W_g.1.running_mean\n",
      "decoder.AG2.W_g.1.running_var\n",
      "decoder.AG2.W_g.1.num_batches_tracked\n",
      "decoder.AG2.W_x.0.weight\n",
      "decoder.AG2.W_x.0.bias\n",
      "decoder.AG2.W_x.1.weight\n",
      "decoder.AG2.W_x.1.bias\n",
      "decoder.AG2.W_x.1.running_mean\n",
      "decoder.AG2.W_x.1.running_var\n",
      "decoder.AG2.W_x.1.num_batches_tracked\n",
      "decoder.AG2.psi.0.weight\n",
      "decoder.AG2.psi.0.bias\n",
      "decoder.AG2.psi.1.weight\n",
      "decoder.AG2.psi.1.bias\n",
      "decoder.AG2.psi.1.running_mean\n",
      "decoder.AG2.psi.1.running_var\n",
      "decoder.AG2.psi.1.num_batches_tracked\n",
      "decoder.ConvBlock2.conv.0.weight\n",
      "decoder.ConvBlock2.conv.0.bias\n",
      "decoder.ConvBlock2.conv.1.weight\n",
      "decoder.ConvBlock2.conv.1.bias\n",
      "decoder.ConvBlock2.conv.1.running_mean\n",
      "decoder.ConvBlock2.conv.1.running_var\n",
      "decoder.ConvBlock2.conv.1.num_batches_tracked\n",
      "decoder.ConvBlock2.conv.3.weight\n",
      "decoder.ConvBlock2.conv.3.bias\n",
      "decoder.ConvBlock2.conv.4.weight\n",
      "decoder.ConvBlock2.conv.4.bias\n",
      "decoder.ConvBlock2.conv.4.running_mean\n",
      "decoder.ConvBlock2.conv.4.running_var\n",
      "decoder.ConvBlock2.conv.4.num_batches_tracked\n",
      "decoder.Up1.up.1.weight\n",
      "decoder.Up1.up.1.bias\n",
      "decoder.Up1.up.2.weight\n",
      "decoder.Up1.up.2.bias\n",
      "decoder.Up1.up.2.running_mean\n",
      "decoder.Up1.up.2.running_var\n",
      "decoder.Up1.up.2.num_batches_tracked\n",
      "decoder.AG1.W_g.0.weight\n",
      "decoder.AG1.W_g.0.bias\n",
      "decoder.AG1.W_g.1.weight\n",
      "decoder.AG1.W_g.1.bias\n",
      "decoder.AG1.W_g.1.running_mean\n",
      "decoder.AG1.W_g.1.running_var\n",
      "decoder.AG1.W_g.1.num_batches_tracked\n",
      "decoder.AG1.W_x.0.weight\n",
      "decoder.AG1.W_x.0.bias\n",
      "decoder.AG1.W_x.1.weight\n",
      "decoder.AG1.W_x.1.bias\n",
      "decoder.AG1.W_x.1.running_mean\n",
      "decoder.AG1.W_x.1.running_var\n",
      "decoder.AG1.W_x.1.num_batches_tracked\n",
      "decoder.AG1.psi.0.weight\n",
      "decoder.AG1.psi.0.bias\n",
      "decoder.AG1.psi.1.weight\n",
      "decoder.AG1.psi.1.bias\n",
      "decoder.AG1.psi.1.running_mean\n",
      "decoder.AG1.psi.1.running_var\n",
      "decoder.AG1.psi.1.num_batches_tracked\n",
      "decoder.ConvBlock1.conv.0.weight\n",
      "decoder.ConvBlock1.conv.0.bias\n",
      "decoder.ConvBlock1.conv.1.weight\n",
      "decoder.ConvBlock1.conv.1.bias\n",
      "decoder.ConvBlock1.conv.1.running_mean\n",
      "decoder.ConvBlock1.conv.1.running_var\n",
      "decoder.ConvBlock1.conv.1.num_batches_tracked\n",
      "decoder.ConvBlock1.conv.3.weight\n",
      "decoder.ConvBlock1.conv.3.bias\n",
      "decoder.ConvBlock1.conv.4.weight\n",
      "decoder.ConvBlock1.conv.4.bias\n",
      "decoder.ConvBlock1.conv.4.running_mean\n",
      "decoder.ConvBlock1.conv.4.running_var\n",
      "decoder.ConvBlock1.conv.4.num_batches_tracked\n",
      "decoder.CA4.fc1.weight\n",
      "decoder.CA4.fc2.weight\n",
      "decoder.CA3.fc1.weight\n",
      "decoder.CA3.fc2.weight\n",
      "decoder.CA2.fc1.weight\n",
      "decoder.CA2.fc2.weight\n",
      "decoder.CA1.fc1.weight\n",
      "decoder.CA1.fc2.weight\n",
      "decoder.SA.conv1.weight\n",
      "out_head1.weight\n",
      "out_head1.bias\n",
      "out_head2.weight\n",
      "out_head2.bias\n",
      "out_head3.weight\n",
      "out_head3.bias\n",
      "out_head4.weight\n",
      "out_head4.bias\n"
     ]
    }
   ],
   "source": [
    "for key in net.state_dict():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PVT_CASCADE(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (backbone): pvt_v2_b2(\n",
      "    (patch_embed1): OverlapPatchEmbed(\n",
      "      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed2): OverlapPatchEmbed(\n",
      "      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed3): OverlapPatchEmbed(\n",
      "      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed4): OverlapPatchEmbed(\n",
      "      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (block1): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "    (block2): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "    (block3): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "    (block4): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): CASCADE(\n",
      "    (Conv_1x1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (ConvBlock4): conv_block(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Up3): up_conv(\n",
      "      (up): Sequential(\n",
      "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "        (1): Conv2d(512, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (AG3): Attention_block(\n",
      "      (W_g): Sequential(\n",
      "        (0): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (W_x): Sequential(\n",
      "        (0): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (psi): Sequential(\n",
      "        (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Sigmoid()\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (ConvBlock3): conv_block(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Up2): up_conv(\n",
      "      (up): Sequential(\n",
      "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "        (1): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (AG2): Attention_block(\n",
      "      (W_g): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (W_x): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (psi): Sequential(\n",
      "        (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Sigmoid()\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (ConvBlock2): conv_block(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Up1): up_conv(\n",
      "      (up): Sequential(\n",
      "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "        (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (AG1): Attention_block(\n",
      "      (W_g): Sequential(\n",
      "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (W_x): Sequential(\n",
      "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (psi): Sequential(\n",
      "        (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Sigmoid()\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (ConvBlock1): conv_block(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (CA4): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (relu1): ReLU()\n",
      "      (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (CA3): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (relu1): ReLU()\n",
      "      (fc2): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (CA2): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (relu1): ReLU()\n",
      "      (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (CA1): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (relu1): ReLU()\n",
      "      (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (SA): SpatialAttention(\n",
      "      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (out_head1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (out_head2): Conv2d(320, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (out_head3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (out_head4): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
