[2022/12/28 23:11] | Loading pretrained weights from url (https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth)
[2022/12/28 23:11] | load model weight from data/pretrained/swin_tiny_patch4_window7_224.pth
[2022/12/28 23:11] | popping out head
[2022/12/28 23:11] | ---------------------------------------------------------------------------------
[2022/12/28 23:11] |                                    INFORMATION
[2022/12/28 23:11] | ---------------------------------------------------------------------------------
[2022/12/28 23:11] | Project Name              | MECLA
[2022/12/28 23:11] | Project Administrator     | jaejung
[2022/12/28 23:11] | Experiment Name           | eyepacs_v1_swin_tiny_patch4_window7_224_v0
[2022/12/28 23:11] | Experiment Start Time     | 2022-12-28 23:10:54
[2022/12/28 23:11] | Experiment Model Name     | swin_tiny_patch4_window7_224
[2022/12/28 23:11] | Experiment Log Directory  | log/eyepacs_v1_swin_tiny_patch4_window7_224_v0
[2022/12/28 23:11] | ---------------------------------------------------------------------------------
[2022/12/28 23:11] |                                 EXPERIMENT SETUP
[2022/12/28 23:11] | ---------------------------------------------------------------------------------
[2022/12/28 23:11] | train_size                | (224, 224)
[2022/12/28 23:11] | test_size                 | (224, 224)
[2022/12/28 23:11] | center_crop_ptr           | 0.875
[2022/12/28 23:11] | interpolation             | bicubic
[2022/12/28 23:11] | mean                      | (0.485, 0.456, 0.406)
[2022/12/28 23:11] | std                       | (0.229, 0.224, 0.225)
[2022/12/28 23:11] | hflip                     | 0.5
[2022/12/28 23:11] | auto_aug                  | False
[2022/12/28 23:11] | cutmix                    | None
[2022/12/28 23:11] | mixup                     | None
[2022/12/28 23:11] | remode                    | 0.2
[2022/12/28 23:11] | model_name                | swin_tiny_patch4_window7_224
[2022/12/28 23:11] | lr                        | 0.001
[2022/12/28 23:11] | epoch                     | 100
[2022/12/28 23:11] | criterion                 | ce
[2022/12/28 23:11] | optimizer                 | adamw
[2022/12/28 23:11] | weight_decay              | 0.0001
[2022/12/28 23:11] | scheduler                 | cosine
[2022/12/28 23:11] | warmup_epoch              | 1
[2022/12/28 23:11] | batch_size                | 32
[2022/12/28 23:11] | ---------------------------------------------------------------------------------
[2022/12/28 23:11] |                                   DATA & MODEL
[2022/12/28 23:11] | ---------------------------------------------------------------------------------
[2022/12/28 23:11] | Model Parameters(M)       | 27523199
[2022/12/28 23:11] | Number of Train Examples  | 28100
[2022/12/28 23:11] | Number of Valid Examples  | 7026
[2022/12/28 23:11] | Number of Class           | 5
[2022/12/28 23:11] | Task                      | multiclass
[2022/12/28 23:11] | ---------------------------------------------------------------------------------
[2022/12/28 23:11] | TRAIN(000): [ 50/879] Batch: 0.1054 (0.1527) Data: 0.0096 (0.0492) Loss: 0.6757 (0.9943)
[2022/12/28 23:11] | TRAIN(000): [100/879] Batch: 0.1101 (0.1326) Data: 0.0099 (0.0302) Loss: 0.8717 (0.9332)
[2022/12/28 23:11] | TRAIN(000): [150/879] Batch: 0.1260 (0.1280) Data: 0.0121 (0.0240) Loss: 0.7427 (0.8970)
[2022/12/28 23:11] | TRAIN(000): [200/879] Batch: 0.1119 (0.1250) Data: 0.0098 (0.0207) Loss: 0.7346 (0.8708)
[2022/12/28 23:11] | TRAIN(000): [250/879] Batch: 0.1007 (0.1225) Data: 0.0097 (0.0186) Loss: 0.5141 (0.8608)
[2022/12/28 23:11] | TRAIN(000): [300/879] Batch: 0.1161 (0.1204) Data: 0.0104 (0.0171) Loss: 0.9388 (0.8641)
[2022/12/28 23:11] | TRAIN(000): [350/879] Batch: 0.1129 (0.1191) Data: 0.0080 (0.0161) Loss: 0.9121 (0.8583)
[2022/12/28 23:11] | TRAIN(000): [400/879] Batch: 0.1109 (0.1181) Data: 0.0102 (0.0153) Loss: 1.0492 (0.8573)
[2022/12/28 23:11] | TRAIN(000): [450/879] Batch: 0.1085 (0.1178) Data: 0.0091 (0.0148) Loss: 0.8874 (0.8559)
[2022/12/28 23:12] | TRAIN(000): [500/879] Batch: 0.1208 (0.1174) Data: 0.0121 (0.0144) Loss: 1.0555 (0.8579)
[2022/12/28 23:12] | TRAIN(000): [550/879] Batch: 0.1319 (0.1174) Data: 0.0106 (0.0141) Loss: 0.6528 (0.8596)
[2022/12/28 23:12] | TRAIN(000): [600/879] Batch: 0.1179 (0.1173) Data: 0.0119 (0.0139) Loss: 0.8234 (0.8600)
[2022/12/28 23:12] | TRAIN(000): [650/879] Batch: 0.1169 (0.1173) Data: 0.0080 (0.0137) Loss: 0.6857 (0.8616)
[2022/12/28 23:12] | TRAIN(000): [700/879] Batch: 0.1195 (0.1172) Data: 0.0167 (0.0135) Loss: 0.8111 (0.8647)
[2022/12/28 23:12] | TRAIN(000): [750/879] Batch: 0.1191 (0.1173) Data: 0.0165 (0.0134) Loss: 1.0927 (0.8646)
[2022/12/28 23:12] | TRAIN(000): [800/879] Batch: 0.1129 (0.1175) Data: 0.0111 (0.0132) Loss: 0.8663 (0.8642)
[2022/12/28 23:12] | TRAIN(000): [850/879] Batch: 0.1243 (0.1174) Data: 0.0111 (0.0131) Loss: 0.7474 (0.8669)
[2022/12/28 23:12] | ------------------------------------------------------------
[2022/12/28 23:12] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:12] | ------------------------------------------------------------
[2022/12/28 23:12] |     TRAIN(0)     0:01:43     0:00:11     0:01:31      0.8654
[2022/12/28 23:12] | ------------------------------------------------------------
[2022/12/28 23:12] | VALID(000): [ 50/220] Batch: 0.0397 (0.0660) Data: 0.0234 (0.0494) Loss: 0.7708 (0.8477)
[2022/12/28 23:12] | VALID(000): [100/220] Batch: 0.0386 (0.0522) Data: 0.0262 (0.0369) Loss: 1.1245 (0.8720)
[2022/12/28 23:12] | VALID(000): [150/220] Batch: 0.0381 (0.0475) Data: 0.0211 (0.0332) Loss: 0.7640 (0.8634)
[2022/12/28 23:12] | VALID(000): [200/220] Batch: 0.0355 (0.0452) Data: 0.0274 (0.0316) Loss: 0.5297 (0.8707)
[2022/12/28 23:12] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:12] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:12] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:12] |     VALID(0)      0.8706      0.7347      0.5131      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:12] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:12] | ####################################################################################################
[2022/12/28 23:13] | TRAIN(001): [ 50/879] Batch: 0.1021 (0.1473) Data: 0.0093 (0.0440) Loss: 0.7155 (0.8817)
[2022/12/28 23:13] | TRAIN(001): [100/879] Batch: 0.1276 (0.1330) Data: 0.0112 (0.0278) Loss: 1.1030 (0.8891)
[2022/12/28 23:13] | TRAIN(001): [150/879] Batch: 0.1063 (0.1254) Data: 0.0092 (0.0218) Loss: 1.1625 (0.8762)
[2022/12/28 23:13] | TRAIN(001): [200/879] Batch: 0.1124 (0.1219) Data: 0.0094 (0.0189) Loss: 0.8774 (0.8834)
[2022/12/28 23:13] | TRAIN(001): [250/879] Batch: 0.1032 (0.1206) Data: 0.0100 (0.0172) Loss: 0.7674 (0.8779)
[2022/12/28 23:13] | TRAIN(001): [300/879] Batch: 0.1155 (0.1190) Data: 0.0107 (0.0160) Loss: 1.0228 (0.8781)
[2022/12/28 23:13] | TRAIN(001): [350/879] Batch: 0.1108 (0.1184) Data: 0.0102 (0.0152) Loss: 0.8379 (0.8771)
[2022/12/28 23:13] | TRAIN(001): [400/879] Batch: 0.1057 (0.1177) Data: 0.0105 (0.0146) Loss: 0.5638 (0.8710)
[2022/12/28 23:13] | TRAIN(001): [450/879] Batch: 0.1112 (0.1173) Data: 0.0090 (0.0141) Loss: 0.8186 (0.8755)
[2022/12/28 23:13] | TRAIN(001): [500/879] Batch: 0.1252 (0.1171) Data: 0.0115 (0.0138) Loss: 1.0388 (0.8790)
[2022/12/28 23:14] | TRAIN(001): [550/879] Batch: 0.1063 (0.1166) Data: 0.0089 (0.0134) Loss: 0.9484 (0.8767)
[2022/12/28 23:14] | TRAIN(001): [600/879] Batch: 0.1136 (0.1164) Data: 0.0113 (0.0132) Loss: 0.7224 (0.8745)
[2022/12/28 23:14] | TRAIN(001): [650/879] Batch: 0.1277 (0.1163) Data: 0.0112 (0.0130) Loss: 0.8530 (0.8736)
[2022/12/28 23:14] | TRAIN(001): [700/879] Batch: 0.1138 (0.1162) Data: 0.0078 (0.0128) Loss: 0.9023 (0.8751)
[2022/12/28 23:14] | TRAIN(001): [750/879] Batch: 0.1036 (0.1158) Data: 0.0090 (0.0126) Loss: 0.9992 (0.8749)
[2022/12/28 23:14] | TRAIN(001): [800/879] Batch: 0.1134 (0.1157) Data: 0.0099 (0.0125) Loss: 1.5551 (0.8749)
[2022/12/28 23:14] | TRAIN(001): [850/879] Batch: 0.1120 (0.1155) Data: 0.0094 (0.0123) Loss: 0.8377 (0.8758)
[2022/12/28 23:14] | ------------------------------------------------------------
[2022/12/28 23:14] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:14] | ------------------------------------------------------------
[2022/12/28 23:14] |     TRAIN(1)     0:01:41     0:00:10     0:01:30      0.8763
[2022/12/28 23:14] | ------------------------------------------------------------
[2022/12/28 23:14] | VALID(001): [ 50/220] Batch: 0.0413 (0.0659) Data: 0.0288 (0.0532) Loss: 0.7871 (0.8523)
[2022/12/28 23:14] | VALID(001): [100/220] Batch: 0.0366 (0.0521) Data: 0.0271 (0.0400) Loss: 1.1279 (0.8774)
[2022/12/28 23:14] | VALID(001): [150/220] Batch: 0.0373 (0.0476) Data: 0.0263 (0.0356) Loss: 0.7590 (0.8685)
[2022/12/28 23:14] | VALID(001): [200/220] Batch: 0.0371 (0.0453) Data: 0.0282 (0.0334) Loss: 0.4717 (0.8769)
[2022/12/28 23:14] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:14] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:14] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:14] |     VALID(1)      0.8764      0.7347      0.5154      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:14] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:14] | ####################################################################################################
[2022/12/28 23:14] | TRAIN(002): [ 50/879] Batch: 0.1108 (0.1502) Data: 0.0106 (0.0472) Loss: 0.9654 (0.8705)
[2022/12/28 23:15] | TRAIN(002): [100/879] Batch: 0.1146 (0.1318) Data: 0.0115 (0.0290) Loss: 1.0655 (0.8810)
[2022/12/28 23:15] | TRAIN(002): [150/879] Batch: 0.1102 (0.1276) Data: 0.0111 (0.0232) Loss: 1.0047 (0.8897)
[2022/12/28 23:15] | TRAIN(002): [200/879] Batch: 0.1158 (0.1243) Data: 0.0118 (0.0201) Loss: 0.7420 (0.8864)
[2022/12/28 23:15] | TRAIN(002): [250/879] Batch: 0.1085 (0.1216) Data: 0.0092 (0.0180) Loss: 1.0504 (0.8871)
[2022/12/28 23:15] | TRAIN(002): [300/879] Batch: 0.1161 (0.1196) Data: 0.0091 (0.0166) Loss: 1.1867 (0.8786)
[2022/12/28 23:15] | TRAIN(002): [350/879] Batch: 0.1256 (0.1188) Data: 0.0104 (0.0157) Loss: 1.0297 (0.8817)
[2022/12/28 23:15] | TRAIN(002): [400/879] Batch: 0.1142 (0.1179) Data: 0.0114 (0.0150) Loss: 0.7573 (0.8759)
[2022/12/28 23:15] | TRAIN(002): [450/879] Batch: 0.1093 (0.1172) Data: 0.0091 (0.0145) Loss: 0.8248 (0.8749)
[2022/12/28 23:15] | TRAIN(002): [500/879] Batch: 0.1149 (0.1172) Data: 0.0111 (0.0141) Loss: 0.8580 (0.8758)
[2022/12/28 23:15] | TRAIN(002): [550/879] Batch: 0.1066 (0.1168) Data: 0.0093 (0.0138) Loss: 1.2204 (0.8763)
[2022/12/28 23:15] | TRAIN(002): [600/879] Batch: 0.1195 (0.1166) Data: 0.0106 (0.0135) Loss: 0.6335 (0.8791)
[2022/12/28 23:16] | TRAIN(002): [650/879] Batch: 0.1142 (0.1168) Data: 0.0110 (0.0133) Loss: 1.0497 (0.8749)
[2022/12/28 23:16] | TRAIN(002): [700/879] Batch: 0.1268 (0.1168) Data: 0.0111 (0.0131) Loss: 0.9483 (0.8756)
[2022/12/28 23:16] | TRAIN(002): [750/879] Batch: 0.1172 (0.1167) Data: 0.0097 (0.0129) Loss: 1.0132 (0.8760)
[2022/12/28 23:16] | TRAIN(002): [800/879] Batch: 0.1139 (0.1166) Data: 0.0097 (0.0128) Loss: 0.8713 (0.8758)
[2022/12/28 23:16] | TRAIN(002): [850/879] Batch: 0.1111 (0.1166) Data: 0.0085 (0.0126) Loss: 0.9437 (0.8753)
[2022/12/28 23:16] | ------------------------------------------------------------
[2022/12/28 23:16] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:16] | ------------------------------------------------------------
[2022/12/28 23:16] |     TRAIN(2)     0:01:42     0:00:11     0:01:31      0.8732
[2022/12/28 23:16] | ------------------------------------------------------------
[2022/12/28 23:16] | VALID(002): [ 50/220] Batch: 0.0391 (0.0655) Data: 0.0281 (0.0510) Loss: 0.7954 (0.8581)
[2022/12/28 23:16] | VALID(002): [100/220] Batch: 0.0347 (0.0520) Data: 0.0255 (0.0384) Loss: 1.1631 (0.8832)
[2022/12/28 23:16] | VALID(002): [150/220] Batch: 0.0384 (0.0476) Data: 0.0267 (0.0341) Loss: 0.7595 (0.8742)
[2022/12/28 23:16] | VALID(002): [200/220] Batch: 0.0310 (0.0454) Data: 0.0282 (0.0316) Loss: 0.4718 (0.8827)
[2022/12/28 23:16] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:16] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:16] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:16] |     VALID(2)      0.8828      0.7347      0.5177      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:16] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:16] | ####################################################################################################
[2022/12/28 23:16] | TRAIN(003): [ 50/879] Batch: 0.1109 (0.1500) Data: 0.0096 (0.0448) Loss: 0.9758 (0.8897)
[2022/12/28 23:16] | TRAIN(003): [100/879] Batch: 0.1249 (0.1331) Data: 0.0117 (0.0280) Loss: 0.7799 (0.8946)
[2022/12/28 23:16] | TRAIN(003): [150/879] Batch: 0.1098 (0.1262) Data: 0.0100 (0.0222) Loss: 0.7753 (0.8785)
[2022/12/28 23:17] | TRAIN(003): [200/879] Batch: 0.1123 (0.1227) Data: 0.0092 (0.0192) Loss: 0.7552 (0.8713)
[2022/12/28 23:17] | TRAIN(003): [250/879] Batch: 0.1186 (0.1209) Data: 0.0097 (0.0174) Loss: 0.8823 (0.8729)
[2022/12/28 23:17] | TRAIN(003): [300/879] Batch: 0.1083 (0.1192) Data: 0.0099 (0.0162) Loss: 0.7005 (0.8721)
[2022/12/28 23:17] | TRAIN(003): [350/879] Batch: 0.1223 (0.1179) Data: 0.0115 (0.0152) Loss: 0.7608 (0.8803)
[2022/12/28 23:17] | TRAIN(003): [400/879] Batch: 0.1124 (0.1169) Data: 0.0093 (0.0145) Loss: 1.0678 (0.8809)
[2022/12/28 23:17] | TRAIN(003): [450/879] Batch: 0.1107 (0.1160) Data: 0.0087 (0.0139) Loss: 0.7445 (0.8842)
[2022/12/28 23:17] | TRAIN(003): [500/879] Batch: 0.1092 (0.1152) Data: 0.0088 (0.0134) Loss: 0.8447 (0.8792)
[2022/12/28 23:17] | TRAIN(003): [550/879] Batch: 0.1271 (0.1152) Data: 0.0114 (0.0131) Loss: 0.6324 (0.8788)
[2022/12/28 23:17] | TRAIN(003): [600/879] Batch: 0.1192 (0.1155) Data: 0.0112 (0.0130) Loss: 1.0499 (0.8777)
[2022/12/28 23:17] | TRAIN(003): [650/879] Batch: 0.1275 (0.1156) Data: 0.0114 (0.0128) Loss: 0.5839 (0.8789)
[2022/12/28 23:18] | TRAIN(003): [700/879] Batch: 0.1154 (0.1159) Data: 0.0115 (0.0127) Loss: 0.8347 (0.8770)
[2022/12/28 23:18] | TRAIN(003): [750/879] Batch: 0.1220 (0.1161) Data: 0.0115 (0.0126) Loss: 0.6134 (0.8751)
[2022/12/28 23:18] | TRAIN(003): [800/879] Batch: 0.1197 (0.1162) Data: 0.0116 (0.0125) Loss: 1.1073 (0.8757)
[2022/12/28 23:18] | TRAIN(003): [850/879] Batch: 0.1132 (0.1164) Data: 0.0115 (0.0124) Loss: 0.6678 (0.8741)
[2022/12/28 23:18] | ------------------------------------------------------------
[2022/12/28 23:18] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:18] | ------------------------------------------------------------
[2022/12/28 23:18] |     TRAIN(3)     0:01:42     0:00:10     0:01:31      0.8719
[2022/12/28 23:18] | ------------------------------------------------------------
[2022/12/28 23:18] | VALID(003): [ 50/220] Batch: 0.0384 (0.0657) Data: 0.0253 (0.0528) Loss: 0.7762 (0.8579)
[2022/12/28 23:18] | VALID(003): [100/220] Batch: 0.0381 (0.0523) Data: 0.0239 (0.0383) Loss: 1.1427 (0.8869)
[2022/12/28 23:18] | VALID(003): [150/220] Batch: 0.0378 (0.0478) Data: 0.0268 (0.0335) Loss: 0.7563 (0.8763)
[2022/12/28 23:18] | VALID(003): [200/220] Batch: 0.0411 (0.0456) Data: 0.0235 (0.0315) Loss: 0.4729 (0.8848)
[2022/12/28 23:18] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:18] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:18] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:18] |     VALID(3)      0.8839      0.7347      0.5223      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:18] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:18] | ####################################################################################################
[2022/12/28 23:18] | TRAIN(004): [ 50/879] Batch: 0.1047 (0.1491) Data: 0.0166 (0.0470) Loss: 0.7945 (0.8672)
[2022/12/28 23:18] | TRAIN(004): [100/879] Batch: 0.1125 (0.1307) Data: 0.0098 (0.0287) Loss: 0.8218 (0.8855)
[2022/12/28 23:18] | TRAIN(004): [150/879] Batch: 0.1129 (0.1252) Data: 0.0098 (0.0228) Loss: 0.8443 (0.8615)
[2022/12/28 23:18] | TRAIN(004): [200/879] Batch: 0.1104 (0.1225) Data: 0.0094 (0.0198) Loss: 0.8909 (0.8611)
[2022/12/28 23:19] | TRAIN(004): [250/879] Batch: 0.1029 (0.1205) Data: 0.0121 (0.0180) Loss: 0.8737 (0.8650)
[2022/12/28 23:19] | TRAIN(004): [300/879] Batch: 0.1166 (0.1187) Data: 0.0094 (0.0166) Loss: 0.8002 (0.8675)
[2022/12/28 23:19] | TRAIN(004): [350/879] Batch: 0.1206 (0.1176) Data: 0.0113 (0.0157) Loss: 0.7580 (0.8700)
[2022/12/28 23:19] | TRAIN(004): [400/879] Batch: 0.1142 (0.1171) Data: 0.0092 (0.0151) Loss: 0.5029 (0.8748)
[2022/12/28 23:19] | TRAIN(004): [450/879] Batch: 0.1056 (0.1162) Data: 0.0095 (0.0145) Loss: 0.3951 (0.8759)
[2022/12/28 23:19] | TRAIN(004): [500/879] Batch: 0.1381 (0.1158) Data: 0.0135 (0.0141) Loss: 0.8586 (0.8730)
[2022/12/28 23:19] | TRAIN(004): [550/879] Batch: 0.1051 (0.1160) Data: 0.0086 (0.0139) Loss: 1.0617 (0.8709)
[2022/12/28 23:19] | TRAIN(004): [600/879] Batch: 0.1043 (0.1154) Data: 0.0090 (0.0135) Loss: 1.0271 (0.8755)
[2022/12/28 23:19] | TRAIN(004): [650/879] Batch: 0.1052 (0.1151) Data: 0.0095 (0.0132) Loss: 1.0121 (0.8738)
[2022/12/28 23:19] | TRAIN(004): [700/879] Batch: 0.1114 (0.1147) Data: 0.0113 (0.0130) Loss: 1.0267 (0.8736)
[2022/12/28 23:19] | TRAIN(004): [750/879] Batch: 0.1081 (0.1143) Data: 0.0089 (0.0127) Loss: 0.6710 (0.8741)
[2022/12/28 23:20] | TRAIN(004): [800/879] Batch: 0.1045 (0.1140) Data: 0.0095 (0.0125) Loss: 1.0375 (0.8764)
[2022/12/28 23:20] | TRAIN(004): [850/879] Batch: 0.1121 (0.1138) Data: 0.0087 (0.0124) Loss: 0.8155 (0.8761)
[2022/12/28 23:20] | ------------------------------------------------------------
[2022/12/28 23:20] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:20] | ------------------------------------------------------------
[2022/12/28 23:20] |     TRAIN(4)     0:01:39     0:00:10     0:01:29      0.8743
[2022/12/28 23:20] | ------------------------------------------------------------
[2022/12/28 23:20] | VALID(004): [ 50/220] Batch: 0.0391 (0.0653) Data: 0.0263 (0.0528) Loss: 0.7762 (0.8519)
[2022/12/28 23:20] | VALID(004): [100/220] Batch: 0.0361 (0.0518) Data: 0.0227 (0.0398) Loss: 1.0952 (0.8752)
[2022/12/28 23:20] | VALID(004): [150/220] Batch: 0.0392 (0.0474) Data: 0.0237 (0.0343) Loss: 0.7632 (0.8667)
[2022/12/28 23:20] | VALID(004): [200/220] Batch: 0.0352 (0.0452) Data: 0.0250 (0.0316) Loss: 0.5422 (0.8741)
[2022/12/28 23:20] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:20] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:20] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:20] |     VALID(4)      0.8731      0.7347      0.5161      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:20] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:20] | ####################################################################################################
[2022/12/28 23:20] | TRAIN(005): [ 50/879] Batch: 0.1030 (0.1435) Data: 0.0097 (0.0427) Loss: 0.9000 (0.9135)
[2022/12/28 23:20] | TRAIN(005): [100/879] Batch: 0.1168 (0.1264) Data: 0.0095 (0.0263) Loss: 0.6426 (0.8912)
[2022/12/28 23:20] | TRAIN(005): [150/879] Batch: 0.1158 (0.1232) Data: 0.0089 (0.0213) Loss: 0.9276 (0.8878)
[2022/12/28 23:20] | TRAIN(005): [200/879] Batch: 0.1227 (0.1210) Data: 0.0113 (0.0186) Loss: 0.6872 (0.8741)
[2022/12/28 23:20] | TRAIN(005): [250/879] Batch: 0.1122 (0.1190) Data: 0.0114 (0.0169) Loss: 0.9502 (0.8746)
[2022/12/28 23:20] | TRAIN(005): [300/879] Batch: 0.1078 (0.1179) Data: 0.0096 (0.0157) Loss: 0.9437 (0.8711)
[2022/12/28 23:21] | TRAIN(005): [350/879] Batch: 0.1143 (0.1175) Data: 0.0093 (0.0150) Loss: 1.0227 (0.8653)
[2022/12/28 23:21] | TRAIN(005): [400/879] Batch: 0.1175 (0.1164) Data: 0.0095 (0.0143) Loss: 0.7584 (0.8676)
[2022/12/28 23:21] | TRAIN(005): [450/879] Batch: 0.1068 (0.1156) Data: 0.0092 (0.0138) Loss: 0.9021 (0.8643)
[2022/12/28 23:21] | TRAIN(005): [500/879] Batch: 0.1098 (0.1149) Data: 0.0095 (0.0133) Loss: 1.0617 (0.8674)
[2022/12/28 23:21] | TRAIN(005): [550/879] Batch: 0.1117 (0.1144) Data: 0.0094 (0.0130) Loss: 1.1155 (0.8665)
[2022/12/28 23:21] | TRAIN(005): [600/879] Batch: 0.1171 (0.1139) Data: 0.0090 (0.0127) Loss: 0.9273 (0.8696)
[2022/12/28 23:21] | TRAIN(005): [650/879] Batch: 0.1090 (0.1135) Data: 0.0089 (0.0124) Loss: 0.7662 (0.8661)
[2022/12/28 23:21] | TRAIN(005): [700/879] Batch: 0.1047 (0.1133) Data: 0.0091 (0.0122) Loss: 0.9520 (0.8689)
[2022/12/28 23:21] | TRAIN(005): [750/879] Batch: 0.1192 (0.1131) Data: 0.0114 (0.0121) Loss: 0.6450 (0.8695)
[2022/12/28 23:21] | TRAIN(005): [800/879] Batch: 0.1064 (0.1134) Data: 0.0106 (0.0120) Loss: 0.9120 (0.8701)
[2022/12/28 23:21] | TRAIN(005): [850/879] Batch: 0.1130 (0.1134) Data: 0.0113 (0.0119) Loss: 0.6728 (0.8733)
[2022/12/28 23:22] | ------------------------------------------------------------
[2022/12/28 23:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:22] | ------------------------------------------------------------
[2022/12/28 23:22] |     TRAIN(5)     0:01:39     0:00:10     0:01:29      0.8737
[2022/12/28 23:22] | ------------------------------------------------------------
[2022/12/28 23:22] | VALID(005): [ 50/220] Batch: 0.0370 (0.0666) Data: 0.0247 (0.0494) Loss: 0.7771 (0.8492)
[2022/12/28 23:22] | VALID(005): [100/220] Batch: 0.0392 (0.0524) Data: 0.0256 (0.0375) Loss: 1.1020 (0.8713)
[2022/12/28 23:22] | VALID(005): [150/220] Batch: 0.0368 (0.0476) Data: 0.0249 (0.0334) Loss: 0.7643 (0.8637)
[2022/12/28 23:22] | VALID(005): [200/220] Batch: 0.0398 (0.0453) Data: 0.0230 (0.0308) Loss: 0.5369 (0.8711)
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] |     VALID(5)      0.8707      0.7347      0.4954      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] | ####################################################################################################
[2022/12/28 23:22] | TRAIN(006): [ 50/879] Batch: 0.1225 (0.1526) Data: 0.0115 (0.0465) Loss: 1.3170 (0.8895)
[2022/12/28 23:22] | TRAIN(006): [100/879] Batch: 0.1053 (0.1322) Data: 0.0093 (0.0285) Loss: 0.7720 (0.8579)
[2022/12/28 23:22] | TRAIN(006): [150/879] Batch: 0.1102 (0.1242) Data: 0.0096 (0.0222) Loss: 0.8774 (0.8730)
[2022/12/28 23:22] | TRAIN(006): [200/879] Batch: 0.1161 (0.1202) Data: 0.0077 (0.0191) Loss: 0.8370 (0.8802)
[2022/12/28 23:22] | TRAIN(006): [250/879] Batch: 0.1130 (0.1185) Data: 0.0079 (0.0173) Loss: 0.7796 (0.8811)
[2022/12/28 23:22] | TRAIN(006): [300/879] Batch: 0.1031 (0.1171) Data: 0.0089 (0.0160) Loss: 1.1114 (0.8830)
[2022/12/28 23:22] | TRAIN(006): [350/879] Batch: 0.1129 (0.1158) Data: 0.0091 (0.0151) Loss: 0.4983 (0.8732)
[2022/12/28 23:22] | TRAIN(006): [400/879] Batch: 0.1084 (0.1152) Data: 0.0163 (0.0145) Loss: 0.7350 (0.8738)
[2022/12/28 23:23] | TRAIN(006): [450/879] Batch: 0.1189 (0.1158) Data: 0.0114 (0.0142) Loss: 0.9736 (0.8786)
[2022/12/28 23:23] | TRAIN(006): [500/879] Batch: 0.1017 (0.1155) Data: 0.0095 (0.0138) Loss: 0.4897 (0.8753)
[2022/12/28 23:23] | TRAIN(006): [550/879] Batch: 0.1247 (0.1151) Data: 0.0112 (0.0134) Loss: 0.9244 (0.8728)
[2022/12/28 23:23] | TRAIN(006): [600/879] Batch: 0.1260 (0.1151) Data: 0.0113 (0.0132) Loss: 0.6977 (0.8743)
[2022/12/28 23:23] | TRAIN(006): [650/879] Batch: 0.1205 (0.1148) Data: 0.0112 (0.0130) Loss: 0.9527 (0.8756)
[2022/12/28 23:23] | TRAIN(006): [700/879] Batch: 0.1066 (0.1145) Data: 0.0099 (0.0127) Loss: 0.8978 (0.8717)
[2022/12/28 23:23] | TRAIN(006): [750/879] Batch: 0.1258 (0.1142) Data: 0.0137 (0.0125) Loss: 0.8817 (0.8715)
[2022/12/28 23:23] | TRAIN(006): [800/879] Batch: 0.1121 (0.1146) Data: 0.0122 (0.0125) Loss: 1.0734 (0.8712)
[2022/12/28 23:23] | TRAIN(006): [850/879] Batch: 0.1140 (0.1145) Data: 0.0092 (0.0124) Loss: 0.8147 (0.8698)
[2022/12/28 23:23] | ------------------------------------------------------------
[2022/12/28 23:23] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:23] | ------------------------------------------------------------
[2022/12/28 23:23] |     TRAIN(6)     0:01:40     0:00:10     0:01:29      0.8714
[2022/12/28 23:23] | ------------------------------------------------------------
[2022/12/28 23:23] | VALID(006): [ 50/220] Batch: 0.0382 (0.0651) Data: 0.0273 (0.0527) Loss: 0.7954 (0.8497)
[2022/12/28 23:23] | VALID(006): [100/220] Batch: 0.0411 (0.0517) Data: 0.0230 (0.0378) Loss: 1.0893 (0.8701)
[2022/12/28 23:24] | VALID(006): [150/220] Batch: 0.0395 (0.0472) Data: 0.0233 (0.0329) Loss: 0.7725 (0.8632)
[2022/12/28 23:24] | VALID(006): [200/220] Batch: 0.0347 (0.0449) Data: 0.0279 (0.0306) Loss: 0.5303 (0.8703)
[2022/12/28 23:24] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:24] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:24] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:24] |     VALID(6)      0.8701      0.7347      0.5149      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:24] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:24] | ####################################################################################################
[2022/12/28 23:24] | TRAIN(007): [ 50/879] Batch: 0.1167 (0.1471) Data: 0.0091 (0.0455) Loss: 0.9080 (0.8622)
[2022/12/28 23:24] | TRAIN(007): [100/879] Batch: 0.1158 (0.1294) Data: 0.0095 (0.0280) Loss: 1.1784 (0.8815)
[2022/12/28 23:24] | TRAIN(007): [150/879] Batch: 0.1050 (0.1220) Data: 0.0088 (0.0219) Loss: 0.9493 (0.8723)
[2022/12/28 23:24] | TRAIN(007): [200/879] Batch: 0.1141 (0.1194) Data: 0.0111 (0.0190) Loss: 0.9391 (0.8829)
[2022/12/28 23:24] | TRAIN(007): [250/879] Batch: 0.1038 (0.1173) Data: 0.0091 (0.0171) Loss: 0.8263 (0.8807)
[2022/12/28 23:24] | TRAIN(007): [300/879] Batch: 0.1070 (0.1159) Data: 0.0095 (0.0159) Loss: 0.6710 (0.8841)
[2022/12/28 23:24] | TRAIN(007): [350/879] Batch: 0.1060 (0.1159) Data: 0.0095 (0.0151) Loss: 0.9104 (0.8752)
[2022/12/28 23:24] | TRAIN(007): [400/879] Batch: 0.1152 (0.1160) Data: 0.0123 (0.0146) Loss: 0.9381 (0.8757)
[2022/12/28 23:24] | TRAIN(007): [450/879] Batch: 0.1198 (0.1159) Data: 0.0140 (0.0142) Loss: 0.7623 (0.8726)
[2022/12/28 23:25] | TRAIN(007): [500/879] Batch: 0.1044 (0.1157) Data: 0.0097 (0.0138) Loss: 0.8170 (0.8748)
[2022/12/28 23:25] | TRAIN(007): [550/879] Batch: 0.1027 (0.1154) Data: 0.0104 (0.0135) Loss: 0.8443 (0.8773)
[2022/12/28 23:25] | TRAIN(007): [600/879] Batch: 0.1101 (0.1150) Data: 0.0114 (0.0132) Loss: 0.7170 (0.8757)
[2022/12/28 23:25] | TRAIN(007): [650/879] Batch: 0.1062 (0.1145) Data: 0.0092 (0.0129) Loss: 0.7119 (0.8757)
[2022/12/28 23:25] | TRAIN(007): [700/879] Batch: 0.1066 (0.1141) Data: 0.0095 (0.0126) Loss: 0.7879 (0.8762)
[2022/12/28 23:25] | TRAIN(007): [750/879] Batch: 0.1033 (0.1137) Data: 0.0094 (0.0124) Loss: 0.5455 (0.8735)
[2022/12/28 23:25] | TRAIN(007): [800/879] Batch: 0.1164 (0.1134) Data: 0.0098 (0.0122) Loss: 0.7366 (0.8739)
[2022/12/28 23:25] | TRAIN(007): [850/879] Batch: 0.1141 (0.1134) Data: 0.0112 (0.0121) Loss: 0.8687 (0.8728)
[2022/12/28 23:25] | ------------------------------------------------------------
[2022/12/28 23:25] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:25] | ------------------------------------------------------------
[2022/12/28 23:25] |     TRAIN(7)     0:01:39     0:00:10     0:01:28      0.8717
[2022/12/28 23:25] | ------------------------------------------------------------
[2022/12/28 23:25] | VALID(007): [ 50/220] Batch: 0.0405 (0.0643) Data: 0.0230 (0.0486) Loss: 0.7745 (0.8564)
[2022/12/28 23:25] | VALID(007): [100/220] Batch: 0.0358 (0.0514) Data: 0.0250 (0.0368) Loss: 1.1634 (0.8836)
[2022/12/28 23:25] | VALID(007): [150/220] Batch: 0.0392 (0.0472) Data: 0.0250 (0.0326) Loss: 0.7482 (0.8735)
[2022/12/28 23:25] | VALID(007): [200/220] Batch: 0.0399 (0.0451) Data: 0.0233 (0.0303) Loss: 0.4784 (0.8823)
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] |     VALID(7)      0.8818      0.7347      0.5174      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] | ####################################################################################################
[2022/12/28 23:26] | TRAIN(008): [ 50/879] Batch: 0.1264 (0.1539) Data: 0.0118 (0.0465) Loss: 0.8598 (0.8298)
[2022/12/28 23:26] | TRAIN(008): [100/879] Batch: 0.1116 (0.1369) Data: 0.0113 (0.0291) Loss: 0.8431 (0.8793)
[2022/12/28 23:26] | TRAIN(008): [150/879] Batch: 0.1262 (0.1313) Data: 0.0115 (0.0233) Loss: 0.8090 (0.8727)
[2022/12/28 23:26] | TRAIN(008): [200/879] Batch: 0.1235 (0.1284) Data: 0.0115 (0.0203) Loss: 0.6699 (0.8667)
[2022/12/28 23:26] | TRAIN(008): [250/879] Batch: 0.1092 (0.1256) Data: 0.0093 (0.0183) Loss: 1.0588 (0.8656)
[2022/12/28 23:26] | TRAIN(008): [300/879] Batch: 0.1046 (0.1228) Data: 0.0102 (0.0168) Loss: 0.9112 (0.8749)
[2022/12/28 23:26] | TRAIN(008): [350/879] Batch: 0.1139 (0.1209) Data: 0.0090 (0.0158) Loss: 0.7669 (0.8696)
[2022/12/28 23:26] | TRAIN(008): [400/879] Batch: 0.1137 (0.1195) Data: 0.0091 (0.0150) Loss: 0.9108 (0.8674)
[2022/12/28 23:26] | TRAIN(008): [450/879] Batch: 0.1072 (0.1183) Data: 0.0093 (0.0144) Loss: 0.7000 (0.8687)
[2022/12/28 23:26] | TRAIN(008): [500/879] Batch: 0.1125 (0.1181) Data: 0.0100 (0.0140) Loss: 0.8160 (0.8665)
[2022/12/28 23:26] | TRAIN(008): [550/879] Batch: 0.1050 (0.1173) Data: 0.0096 (0.0137) Loss: 0.9730 (0.8659)
[2022/12/28 23:27] | TRAIN(008): [600/879] Batch: 0.1135 (0.1168) Data: 0.0088 (0.0133) Loss: 0.9549 (0.8653)
[2022/12/28 23:27] | TRAIN(008): [650/879] Batch: 0.1116 (0.1162) Data: 0.0093 (0.0130) Loss: 0.6843 (0.8669)
[2022/12/28 23:27] | TRAIN(008): [700/879] Batch: 0.1137 (0.1159) Data: 0.0089 (0.0128) Loss: 1.0012 (0.8674)
[2022/12/28 23:27] | TRAIN(008): [750/879] Batch: 0.1043 (0.1154) Data: 0.0094 (0.0126) Loss: 0.9467 (0.8692)
[2022/12/28 23:27] | TRAIN(008): [800/879] Batch: 0.1147 (0.1150) Data: 0.0096 (0.0124) Loss: 0.7750 (0.8714)
[2022/12/28 23:27] | TRAIN(008): [850/879] Batch: 0.1151 (0.1150) Data: 0.0095 (0.0123) Loss: 0.9157 (0.8703)
[2022/12/28 23:27] | ------------------------------------------------------------
[2022/12/28 23:27] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:27] | ------------------------------------------------------------
[2022/12/28 23:27] |     TRAIN(8)     0:01:40     0:00:10     0:01:30      0.8718
[2022/12/28 23:27] | ------------------------------------------------------------
[2022/12/28 23:27] | VALID(008): [ 50/220] Batch: 0.0430 (0.0660) Data: 0.0246 (0.0522) Loss: 0.8065 (0.8542)
[2022/12/28 23:27] | VALID(008): [100/220] Batch: 0.0382 (0.0522) Data: 0.0237 (0.0386) Loss: 1.0863 (0.8734)
[2022/12/28 23:27] | VALID(008): [150/220] Batch: 0.0383 (0.0476) Data: 0.0266 (0.0345) Loss: 0.7733 (0.8667)
[2022/12/28 23:27] | VALID(008): [200/220] Batch: 0.0377 (0.0454) Data: 0.0209 (0.0321) Loss: 0.5575 (0.8729)
[2022/12/28 23:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:27] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:27] |     VALID(8)      0.8731      0.7347      0.5139      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:27] | ####################################################################################################
[2022/12/28 23:27] | TRAIN(009): [ 50/879] Batch: 0.1023 (0.1468) Data: 0.0103 (0.0442) Loss: 0.9259 (0.8904)
[2022/12/28 23:27] | TRAIN(009): [100/879] Batch: 0.1121 (0.1286) Data: 0.0149 (0.0274) Loss: 0.8616 (0.8426)
[2022/12/28 23:28] | TRAIN(009): [150/879] Batch: 0.1111 (0.1249) Data: 0.0089 (0.0220) Loss: 1.2793 (0.8595)
[2022/12/28 23:28] | TRAIN(009): [200/879] Batch: 0.1082 (0.1209) Data: 0.0113 (0.0190) Loss: 0.9102 (0.8664)
[2022/12/28 23:28] | TRAIN(009): [250/879] Batch: 0.1154 (0.1189) Data: 0.0095 (0.0172) Loss: 0.9232 (0.8593)
[2022/12/28 23:28] | TRAIN(009): [300/879] Batch: 0.1060 (0.1176) Data: 0.0088 (0.0160) Loss: 0.8617 (0.8648)
[2022/12/28 23:28] | TRAIN(009): [350/879] Batch: 0.1016 (0.1167) Data: 0.0092 (0.0151) Loss: 1.0131 (0.8715)
[2022/12/28 23:28] | TRAIN(009): [400/879] Batch: 0.1186 (0.1160) Data: 0.0113 (0.0145) Loss: 0.7165 (0.8719)
[2022/12/28 23:28] | TRAIN(009): [450/879] Batch: 0.1019 (0.1161) Data: 0.0089 (0.0141) Loss: 0.7144 (0.8680)
[2022/12/28 23:28] | TRAIN(009): [500/879] Batch: 0.1038 (0.1153) Data: 0.0093 (0.0136) Loss: 0.9474 (0.8702)
[2022/12/28 23:28] | TRAIN(009): [550/879] Batch: 0.1067 (0.1147) Data: 0.0090 (0.0132) Loss: 0.6731 (0.8663)
[2022/12/28 23:28] | TRAIN(009): [600/879] Batch: 0.1099 (0.1143) Data: 0.0094 (0.0129) Loss: 0.8071 (0.8677)
[2022/12/28 23:28] | TRAIN(009): [650/879] Batch: 0.1116 (0.1142) Data: 0.0102 (0.0127) Loss: 0.7023 (0.8680)
[2022/12/28 23:29] | TRAIN(009): [700/879] Batch: 0.1134 (0.1138) Data: 0.0103 (0.0124) Loss: 0.7100 (0.8665)
[2022/12/28 23:29] | TRAIN(009): [750/879] Batch: 0.1137 (0.1141) Data: 0.0117 (0.0123) Loss: 0.9209 (0.8676)
[2022/12/28 23:29] | TRAIN(009): [800/879] Batch: 0.1138 (0.1138) Data: 0.0119 (0.0122) Loss: 0.9354 (0.8680)
[2022/12/28 23:29] | TRAIN(009): [850/879] Batch: 0.1041 (0.1137) Data: 0.0093 (0.0120) Loss: 0.7565 (0.8705)
[2022/12/28 23:29] | ------------------------------------------------------------
[2022/12/28 23:29] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:29] | ------------------------------------------------------------
[2022/12/28 23:29] |     TRAIN(9)     0:01:39     0:00:10     0:01:29      0.8705
[2022/12/28 23:29] | ------------------------------------------------------------
[2022/12/28 23:29] | VALID(009): [ 50/220] Batch: 0.0395 (0.0645) Data: 0.0284 (0.0518) Loss: 0.7898 (0.8474)
[2022/12/28 23:29] | VALID(009): [100/220] Batch: 0.0404 (0.0515) Data: 0.0277 (0.0393) Loss: 1.1002 (0.8689)
[2022/12/28 23:29] | VALID(009): [150/220] Batch: 0.0368 (0.0472) Data: 0.0246 (0.0350) Loss: 0.7545 (0.8612)
[2022/12/28 23:29] | VALID(009): [200/220] Batch: 0.0375 (0.0450) Data: 0.0277 (0.0330) Loss: 0.5247 (0.8683)
[2022/12/28 23:29] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:29] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:29] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:29] |     VALID(9)      0.8682      0.7347      0.5185      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:29] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:29] | ####################################################################################################
[2022/12/28 23:29] | TRAIN(010): [ 50/879] Batch: 0.1119 (0.1444) Data: 0.0091 (0.0449) Loss: 0.9378 (0.8515)
[2022/12/28 23:29] | TRAIN(010): [100/879] Batch: 0.1321 (0.1278) Data: 0.0159 (0.0282) Loss: 0.8177 (0.8663)
[2022/12/28 23:29] | TRAIN(010): [150/879] Batch: 0.1029 (0.1223) Data: 0.0093 (0.0223) Loss: 0.7723 (0.8531)
[2022/12/28 23:29] | TRAIN(010): [200/879] Batch: 0.1037 (0.1195) Data: 0.0088 (0.0194) Loss: 0.8985 (0.8620)
[2022/12/28 23:30] | TRAIN(010): [250/879] Batch: 0.1154 (0.1174) Data: 0.0103 (0.0175) Loss: 0.6625 (0.8580)
[2022/12/28 23:30] | TRAIN(010): [300/879] Batch: 0.1052 (0.1165) Data: 0.0093 (0.0163) Loss: 0.5350 (0.8588)
[2022/12/28 23:30] | TRAIN(010): [350/879] Batch: 0.1079 (0.1155) Data: 0.0090 (0.0153) Loss: 0.7237 (0.8555)
[2022/12/28 23:30] | TRAIN(010): [400/879] Batch: 0.1212 (0.1155) Data: 0.0095 (0.0148) Loss: 1.1051 (0.8584)
[2022/12/28 23:30] | TRAIN(010): [450/879] Batch: 0.1101 (0.1153) Data: 0.0101 (0.0143) Loss: 0.7903 (0.8625)
[2022/12/28 23:30] | TRAIN(010): [500/879] Batch: 0.1237 (0.1148) Data: 0.0114 (0.0138) Loss: 0.9451 (0.8645)
[2022/12/28 23:30] | TRAIN(010): [550/879] Batch: 0.1105 (0.1145) Data: 0.0091 (0.0135) Loss: 0.8772 (0.8668)
[2022/12/28 23:30] | TRAIN(010): [600/879] Batch: 0.1041 (0.1143) Data: 0.0097 (0.0132) Loss: 0.8983 (0.8672)
[2022/12/28 23:30] | TRAIN(010): [650/879] Batch: 0.1075 (0.1139) Data: 0.0092 (0.0129) Loss: 0.9427 (0.8675)
[2022/12/28 23:30] | TRAIN(010): [700/879] Batch: 0.1171 (0.1135) Data: 0.0091 (0.0127) Loss: 0.5796 (0.8691)
[2022/12/28 23:30] | TRAIN(010): [750/879] Batch: 0.1141 (0.1132) Data: 0.0104 (0.0124) Loss: 0.8164 (0.8672)
[2022/12/28 23:31] | TRAIN(010): [800/879] Batch: 0.1242 (0.1129) Data: 0.0116 (0.0123) Loss: 0.6746 (0.8679)
[2022/12/28 23:31] | TRAIN(010): [850/879] Batch: 0.1059 (0.1130) Data: 0.0096 (0.0122) Loss: 0.7138 (0.8694)
[2022/12/28 23:31] | ------------------------------------------------------------
[2022/12/28 23:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:31] | ------------------------------------------------------------
[2022/12/28 23:31] |    TRAIN(10)     0:01:39     0:00:10     0:01:28      0.8701
[2022/12/28 23:31] | ------------------------------------------------------------
[2022/12/28 23:31] | VALID(010): [ 50/220] Batch: 0.0375 (0.0659) Data: 0.0193 (0.0501) Loss: 0.7713 (0.8493)
[2022/12/28 23:31] | VALID(010): [100/220] Batch: 0.0377 (0.0520) Data: 0.0242 (0.0366) Loss: 1.1132 (0.8726)
[2022/12/28 23:31] | VALID(010): [150/220] Batch: 0.0382 (0.0474) Data: 0.0228 (0.0321) Loss: 0.7576 (0.8642)
[2022/12/28 23:31] | VALID(010): [200/220] Batch: 0.0395 (0.0451) Data: 0.0234 (0.0297) Loss: 0.5378 (0.8719)
[2022/12/28 23:31] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:31] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:31] |    VALID(10)      0.8713      0.7347      0.4951      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:31] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:31] | ####################################################################################################
[2022/12/28 23:31] | TRAIN(011): [ 50/879] Batch: 0.1021 (0.1426) Data: 0.0097 (0.0436) Loss: 0.8962 (0.8591)
[2022/12/28 23:31] | TRAIN(011): [100/879] Batch: 0.1055 (0.1263) Data: 0.0089 (0.0270) Loss: 0.7768 (0.8631)
[2022/12/28 23:31] | TRAIN(011): [150/879] Batch: 0.1231 (0.1235) Data: 0.0115 (0.0217) Loss: 1.2033 (0.8643)
[2022/12/28 23:31] | TRAIN(011): [200/879] Batch: 0.1133 (0.1202) Data: 0.0096 (0.0188) Loss: 0.8760 (0.8678)
[2022/12/28 23:31] | TRAIN(011): [250/879] Batch: 0.1109 (0.1179) Data: 0.0092 (0.0169) Loss: 0.7918 (0.8692)
[2022/12/28 23:31] | TRAIN(011): [300/879] Batch: 0.1035 (0.1163) Data: 0.0095 (0.0157) Loss: 0.9138 (0.8668)
[2022/12/28 23:32] | TRAIN(011): [350/879] Batch: 0.1132 (0.1150) Data: 0.0094 (0.0148) Loss: 0.8674 (0.8718)
[2022/12/28 23:32] | TRAIN(011): [400/879] Batch: 0.1055 (0.1140) Data: 0.0096 (0.0141) Loss: 1.0008 (0.8721)
[2022/12/28 23:32] | TRAIN(011): [450/879] Batch: 0.1058 (0.1133) Data: 0.0099 (0.0136) Loss: 0.8044 (0.8728)
[2022/12/28 23:32] | TRAIN(011): [500/879] Batch: 0.1099 (0.1127) Data: 0.0093 (0.0132) Loss: 0.8343 (0.8770)
[2022/12/28 23:32] | TRAIN(011): [550/879] Batch: 0.1026 (0.1122) Data: 0.0092 (0.0129) Loss: 0.8360 (0.8752)
[2022/12/28 23:32] | TRAIN(011): [600/879] Batch: 0.1151 (0.1118) Data: 0.0093 (0.0126) Loss: 0.9906 (0.8791)
[2022/12/28 23:32] | TRAIN(011): [650/879] Batch: 0.1273 (0.1115) Data: 0.0112 (0.0124) Loss: 0.7229 (0.8777)
[2022/12/28 23:32] | TRAIN(011): [700/879] Batch: 0.1246 (0.1118) Data: 0.0114 (0.0122) Loss: 0.5566 (0.8783)
[2022/12/28 23:32] | TRAIN(011): [750/879] Batch: 0.1009 (0.1115) Data: 0.0094 (0.0121) Loss: 1.1370 (0.8766)
[2022/12/28 23:32] | TRAIN(011): [800/879] Batch: 0.1223 (0.1114) Data: 0.0114 (0.0119) Loss: 0.8548 (0.8741)
[2022/12/28 23:32] | TRAIN(011): [850/879] Batch: 0.1207 (0.1119) Data: 0.0119 (0.0119) Loss: 0.8568 (0.8719)
[2022/12/28 23:33] | ------------------------------------------------------------
[2022/12/28 23:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:33] | ------------------------------------------------------------
[2022/12/28 23:33] |    TRAIN(11)     0:01:38     0:00:10     0:01:28      0.8715
[2022/12/28 23:33] | ------------------------------------------------------------
[2022/12/28 23:33] | VALID(011): [ 50/220] Batch: 0.0386 (0.0649) Data: 0.0240 (0.0497) Loss: 0.7771 (0.8488)
[2022/12/28 23:33] | VALID(011): [100/220] Batch: 0.0338 (0.0515) Data: 0.0281 (0.0362) Loss: 1.1050 (0.8711)
[2022/12/28 23:33] | VALID(011): [150/220] Batch: 0.0388 (0.0471) Data: 0.0235 (0.0318) Loss: 0.7622 (0.8633)
[2022/12/28 23:33] | VALID(011): [200/220] Batch: 0.0426 (0.0448) Data: 0.0217 (0.0295) Loss: 0.5359 (0.8708)
[2022/12/28 23:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:33] |    VALID(11)      0.8704      0.7347      0.4936      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:33] | ####################################################################################################
[2022/12/28 23:33] | TRAIN(012): [ 50/879] Batch: 0.1064 (0.1495) Data: 0.0099 (0.0476) Loss: 1.1697 (0.8974)
[2022/12/28 23:33] | TRAIN(012): [100/879] Batch: 0.1056 (0.1311) Data: 0.0104 (0.0295) Loss: 0.5540 (0.8724)
[2022/12/28 23:33] | TRAIN(012): [150/879] Batch: 0.1040 (0.1243) Data: 0.0096 (0.0233) Loss: 0.6345 (0.8833)
[2022/12/28 23:33] | TRAIN(012): [200/879] Batch: 0.1017 (0.1205) Data: 0.0090 (0.0200) Loss: 0.8496 (0.8719)
[2022/12/28 23:33] | TRAIN(012): [250/879] Batch: 0.1015 (0.1183) Data: 0.0099 (0.0182) Loss: 0.7080 (0.8647)
[2022/12/28 23:33] | TRAIN(012): [300/879] Batch: 0.1120 (0.1173) Data: 0.0095 (0.0169) Loss: 0.8997 (0.8693)
[2022/12/28 23:33] | TRAIN(012): [350/879] Batch: 0.1105 (0.1158) Data: 0.0095 (0.0159) Loss: 0.9554 (0.8706)
[2022/12/28 23:33] | TRAIN(012): [400/879] Batch: 0.1007 (0.1147) Data: 0.0091 (0.0151) Loss: 0.8574 (0.8662)
[2022/12/28 23:34] | TRAIN(012): [450/879] Batch: 0.1233 (0.1143) Data: 0.0117 (0.0146) Loss: 0.8445 (0.8682)
[2022/12/28 23:34] | TRAIN(012): [500/879] Batch: 0.1194 (0.1148) Data: 0.0113 (0.0142) Loss: 0.7873 (0.8694)
[2022/12/28 23:34] | TRAIN(012): [550/879] Batch: 0.1155 (0.1144) Data: 0.0089 (0.0138) Loss: 1.0610 (0.8703)
[2022/12/28 23:34] | TRAIN(012): [600/879] Batch: 0.1060 (0.1138) Data: 0.0101 (0.0135) Loss: 1.0736 (0.8722)
[2022/12/28 23:34] | TRAIN(012): [650/879] Batch: 0.1021 (0.1138) Data: 0.0092 (0.0132) Loss: 0.9696 (0.8735)
[2022/12/28 23:34] | TRAIN(012): [700/879] Batch: 0.1039 (0.1139) Data: 0.0091 (0.0130) Loss: 0.9107 (0.8728)
[2022/12/28 23:34] | TRAIN(012): [750/879] Batch: 0.1228 (0.1137) Data: 0.0093 (0.0128) Loss: 1.3058 (0.8722)
[2022/12/28 23:34] | TRAIN(012): [800/879] Batch: 0.1059 (0.1133) Data: 0.0100 (0.0126) Loss: 0.8700 (0.8696)
[2022/12/28 23:34] | TRAIN(012): [850/879] Batch: 0.1158 (0.1132) Data: 0.0111 (0.0124) Loss: 1.0122 (0.8705)
[2022/12/28 23:34] | ------------------------------------------------------------
[2022/12/28 23:34] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:34] | ------------------------------------------------------------
[2022/12/28 23:34] |    TRAIN(12)     0:01:39     0:00:10     0:01:28      0.8708
[2022/12/28 23:34] | ------------------------------------------------------------
[2022/12/28 23:34] | VALID(012): [ 50/220] Batch: 0.0383 (0.0654) Data: 0.0280 (0.0527) Loss: 0.7786 (0.8463)
[2022/12/28 23:34] | VALID(012): [100/220] Batch: 0.0354 (0.0516) Data: 0.0219 (0.0394) Loss: 1.1138 (0.8692)
[2022/12/28 23:34] | VALID(012): [150/220] Batch: 0.0357 (0.0470) Data: 0.0284 (0.0350) Loss: 0.7552 (0.8609)
[2022/12/28 23:35] | VALID(012): [200/220] Batch: 0.0374 (0.0447) Data: 0.0276 (0.0328) Loss: 0.5164 (0.8686)
[2022/12/28 23:35] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:35] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:35] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:35] |    VALID(12)      0.8683      0.7347      0.4959      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:35] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:35] | ####################################################################################################
[2022/12/28 23:35] | TRAIN(013): [ 50/879] Batch: 0.1128 (0.1426) Data: 0.0103 (0.0446) Loss: 0.6803 (0.8879)
[2022/12/28 23:35] | TRAIN(013): [100/879] Batch: 0.1198 (0.1282) Data: 0.0112 (0.0278) Loss: 0.8288 (0.8849)
[2022/12/28 23:35] | TRAIN(013): [150/879] Batch: 0.1003 (0.1211) Data: 0.0093 (0.0218) Loss: 1.1435 (0.8801)
[2022/12/28 23:35] | TRAIN(013): [200/879] Batch: 0.1105 (0.1188) Data: 0.0079 (0.0190) Loss: 0.7292 (0.8719)
[2022/12/28 23:35] | TRAIN(013): [250/879] Batch: 0.1251 (0.1168) Data: 0.0109 (0.0171) Loss: 0.6625 (0.8683)
[2022/12/28 23:35] | TRAIN(013): [300/879] Batch: 0.1075 (0.1159) Data: 0.0092 (0.0159) Loss: 0.7787 (0.8685)
[2022/12/28 23:35] | TRAIN(013): [350/879] Batch: 0.1004 (0.1154) Data: 0.0091 (0.0151) Loss: 1.0723 (0.8724)
[2022/12/28 23:35] | TRAIN(013): [400/879] Batch: 0.1031 (0.1146) Data: 0.0099 (0.0144) Loss: 0.7939 (0.8718)
[2022/12/28 23:35] | TRAIN(013): [450/879] Batch: 0.1148 (0.1139) Data: 0.0093 (0.0139) Loss: 0.7818 (0.8730)
[2022/12/28 23:35] | TRAIN(013): [500/879] Batch: 0.1007 (0.1131) Data: 0.0092 (0.0135) Loss: 1.0621 (0.8735)
[2022/12/28 23:36] | TRAIN(013): [550/879] Batch: 0.1118 (0.1128) Data: 0.0092 (0.0131) Loss: 0.8123 (0.8711)
[2022/12/28 23:36] | TRAIN(013): [600/879] Batch: 0.1258 (0.1131) Data: 0.0115 (0.0129) Loss: 1.0383 (0.8740)
[2022/12/28 23:36] | TRAIN(013): [650/879] Batch: 0.1026 (0.1128) Data: 0.0092 (0.0127) Loss: 0.8472 (0.8739)
[2022/12/28 23:36] | TRAIN(013): [700/879] Batch: 0.1146 (0.1124) Data: 0.0091 (0.0124) Loss: 0.7878 (0.8740)
[2022/12/28 23:36] | TRAIN(013): [750/879] Batch: 0.1139 (0.1121) Data: 0.0092 (0.0122) Loss: 0.7126 (0.8724)
[2022/12/28 23:36] | TRAIN(013): [800/879] Batch: 0.1182 (0.1124) Data: 0.0113 (0.0121) Loss: 1.0433 (0.8732)
[2022/12/28 23:36] | TRAIN(013): [850/879] Batch: 0.1267 (0.1128) Data: 0.0112 (0.0121) Loss: 0.9523 (0.8711)
[2022/12/28 23:36] | ------------------------------------------------------------
[2022/12/28 23:36] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:36] | ------------------------------------------------------------
[2022/12/28 23:36] |    TRAIN(13)     0:01:39     0:00:10     0:01:28      0.8699
[2022/12/28 23:36] | ------------------------------------------------------------
[2022/12/28 23:36] | VALID(013): [ 50/220] Batch: 0.0405 (0.0656) Data: 0.0281 (0.0523) Loss: 0.7751 (0.8491)
[2022/12/28 23:36] | VALID(013): [100/220] Batch: 0.0394 (0.0519) Data: 0.0264 (0.0394) Loss: 1.1309 (0.8740)
[2022/12/28 23:36] | VALID(013): [150/220] Batch: 0.0381 (0.0473) Data: 0.0280 (0.0351) Loss: 0.7542 (0.8652)
[2022/12/28 23:36] | VALID(013): [200/220] Batch: 0.0374 (0.0450) Data: 0.0264 (0.0329) Loss: 0.4882 (0.8737)
[2022/12/28 23:36] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:36] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:36] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:36] |    VALID(13)      0.8732      0.7347      0.5054      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:36] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:36] | ####################################################################################################
[2022/12/28 23:36] | TRAIN(014): [ 50/879] Batch: 0.1047 (0.1456) Data: 0.0101 (0.0457) Loss: 0.6112 (0.8580)
[2022/12/28 23:37] | TRAIN(014): [100/879] Batch: 0.1028 (0.1262) Data: 0.0096 (0.0279) Loss: 1.0178 (0.8562)
[2022/12/28 23:37] | TRAIN(014): [150/879] Batch: 0.1144 (0.1197) Data: 0.0095 (0.0218) Loss: 1.1756 (0.8719)
[2022/12/28 23:37] | TRAIN(014): [200/879] Batch: 0.1092 (0.1167) Data: 0.0086 (0.0187) Loss: 0.8909 (0.8704)
[2022/12/28 23:37] | TRAIN(014): [250/879] Batch: 0.1024 (0.1154) Data: 0.0090 (0.0170) Loss: 0.5671 (0.8748)
[2022/12/28 23:37] | TRAIN(014): [300/879] Batch: 0.1061 (0.1141) Data: 0.0095 (0.0158) Loss: 1.4325 (0.8694)
[2022/12/28 23:37] | TRAIN(014): [350/879] Batch: 0.1201 (0.1138) Data: 0.0113 (0.0150) Loss: 1.0816 (0.8718)
[2022/12/28 23:37] | TRAIN(014): [400/879] Batch: 0.1053 (0.1135) Data: 0.0095 (0.0144) Loss: 0.7493 (0.8695)
[2022/12/28 23:37] | TRAIN(014): [450/879] Batch: 0.1028 (0.1132) Data: 0.0088 (0.0139) Loss: 0.8890 (0.8681)
[2022/12/28 23:37] | TRAIN(014): [500/879] Batch: 0.1034 (0.1127) Data: 0.0116 (0.0134) Loss: 0.6367 (0.8707)
[2022/12/28 23:37] | TRAIN(014): [550/879] Batch: 0.1025 (0.1123) Data: 0.0087 (0.0131) Loss: 0.8718 (0.8687)
[2022/12/28 23:37] | TRAIN(014): [600/879] Batch: 0.1141 (0.1120) Data: 0.0113 (0.0128) Loss: 0.8652 (0.8677)
[2022/12/28 23:38] | TRAIN(014): [650/879] Batch: 0.1045 (0.1117) Data: 0.0093 (0.0125) Loss: 1.3284 (0.8675)
[2022/12/28 23:38] | TRAIN(014): [700/879] Batch: 0.1098 (0.1115) Data: 0.0101 (0.0123) Loss: 0.7624 (0.8707)
[2022/12/28 23:38] | TRAIN(014): [750/879] Batch: 0.1145 (0.1112) Data: 0.0092 (0.0121) Loss: 0.8545 (0.8731)
[2022/12/28 23:38] | TRAIN(014): [800/879] Batch: 0.1083 (0.1110) Data: 0.0094 (0.0120) Loss: 0.9288 (0.8730)
[2022/12/28 23:38] | TRAIN(014): [850/879] Batch: 0.1193 (0.1114) Data: 0.0113 (0.0119) Loss: 0.6448 (0.8720)
[2022/12/28 23:38] | ------------------------------------------------------------
[2022/12/28 23:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:38] | ------------------------------------------------------------
[2022/12/28 23:38] |    TRAIN(14)     0:01:37     0:00:10     0:01:27      0.8701
[2022/12/28 23:38] | ------------------------------------------------------------
[2022/12/28 23:38] | VALID(014): [ 50/220] Batch: 0.0382 (0.0630) Data: 0.0260 (0.0496) Loss: 0.7704 (0.8517)
[2022/12/28 23:38] | VALID(014): [100/220] Batch: 0.0382 (0.0507) Data: 0.0216 (0.0367) Loss: 1.1415 (0.8780)
[2022/12/28 23:38] | VALID(014): [150/220] Batch: 0.0387 (0.0466) Data: 0.0255 (0.0330) Loss: 0.7503 (0.8686)
[2022/12/28 23:38] | VALID(014): [200/220] Batch: 0.0361 (0.0445) Data: 0.0224 (0.0310) Loss: 0.4852 (0.8772)
[2022/12/28 23:38] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:38] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:38] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:38] |    VALID(14)      0.8765      0.7347      0.5162      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:38] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:38] | ####################################################################################################
[2022/12/28 23:38] | TRAIN(015): [ 50/879] Batch: 0.1183 (0.1506) Data: 0.0099 (0.0446) Loss: 0.7641 (0.8496)
[2022/12/28 23:38] | TRAIN(015): [100/879] Batch: 0.1122 (0.1319) Data: 0.0114 (0.0281) Loss: 0.8669 (0.8779)
[2022/12/28 23:38] | TRAIN(015): [150/879] Batch: 0.1099 (0.1257) Data: 0.0090 (0.0225) Loss: 0.9530 (0.8852)
[2022/12/28 23:39] | TRAIN(015): [200/879] Batch: 0.1139 (0.1221) Data: 0.0169 (0.0197) Loss: 0.7092 (0.8819)
[2022/12/28 23:39] | TRAIN(015): [250/879] Batch: 0.1056 (0.1198) Data: 0.0101 (0.0178) Loss: 0.9142 (0.8769)
[2022/12/28 23:39] | TRAIN(015): [300/879] Batch: 0.1162 (0.1183) Data: 0.0105 (0.0167) Loss: 0.9339 (0.8751)
[2022/12/28 23:39] | TRAIN(015): [350/879] Batch: 0.1024 (0.1171) Data: 0.0091 (0.0158) Loss: 0.8679 (0.8743)
[2022/12/28 23:39] | TRAIN(015): [400/879] Batch: 0.1091 (0.1166) Data: 0.0097 (0.0152) Loss: 0.6291 (0.8740)
[2022/12/28 23:39] | TRAIN(015): [450/879] Batch: 0.1129 (0.1159) Data: 0.0149 (0.0148) Loss: 1.2046 (0.8732)
[2022/12/28 23:39] | TRAIN(015): [500/879] Batch: 0.1035 (0.1153) Data: 0.0106 (0.0143) Loss: 1.0074 (0.8722)
[2022/12/28 23:39] | TRAIN(015): [550/879] Batch: 0.1159 (0.1151) Data: 0.0095 (0.0140) Loss: 0.8344 (0.8711)
[2022/12/28 23:39] | TRAIN(015): [600/879] Batch: 0.1123 (0.1146) Data: 0.0130 (0.0137) Loss: 1.1943 (0.8723)
[2022/12/28 23:39] | TRAIN(015): [650/879] Batch: 0.1047 (0.1146) Data: 0.0162 (0.0136) Loss: 0.7095 (0.8699)
[2022/12/28 23:39] | TRAIN(015): [700/879] Batch: 0.1061 (0.1144) Data: 0.0091 (0.0133) Loss: 0.5725 (0.8692)
[2022/12/28 23:40] | TRAIN(015): [750/879] Batch: 0.1146 (0.1140) Data: 0.0097 (0.0131) Loss: 0.9196 (0.8681)
[2022/12/28 23:40] | TRAIN(015): [800/879] Batch: 0.1053 (0.1139) Data: 0.0092 (0.0130) Loss: 0.8440 (0.8682)
[2022/12/28 23:40] | TRAIN(015): [850/879] Batch: 0.1194 (0.1138) Data: 0.0114 (0.0129) Loss: 0.7215 (0.8710)
[2022/12/28 23:40] | ------------------------------------------------------------
[2022/12/28 23:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:40] | ------------------------------------------------------------
[2022/12/28 23:40] |    TRAIN(15)     0:01:39     0:00:11     0:01:28      0.8699
[2022/12/28 23:40] | ------------------------------------------------------------
[2022/12/28 23:40] | VALID(015): [ 50/220] Batch: 0.0391 (0.0652) Data: 0.0258 (0.0520) Loss: 0.7814 (0.8466)
[2022/12/28 23:40] | VALID(015): [100/220] Batch: 0.0393 (0.0519) Data: 0.0236 (0.0378) Loss: 1.1157 (0.8690)
[2022/12/28 23:40] | VALID(015): [150/220] Batch: 0.0387 (0.0474) Data: 0.0240 (0.0329) Loss: 0.7542 (0.8610)
[2022/12/28 23:40] | VALID(015): [200/220] Batch: 0.0318 (0.0452) Data: 0.0287 (0.0305) Loss: 0.5173 (0.8686)
[2022/12/28 23:40] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:40] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:40] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:40] |    VALID(15)      0.8685      0.7347      0.5132      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:40] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:40] | ####################################################################################################
[2022/12/28 23:40] | TRAIN(016): [ 50/879] Batch: 0.1153 (0.1483) Data: 0.0099 (0.0461) Loss: 0.9908 (0.8562)
[2022/12/28 23:40] | TRAIN(016): [100/879] Batch: 0.1262 (0.1308) Data: 0.0118 (0.0285) Loss: 0.7303 (0.8956)
[2022/12/28 23:40] | TRAIN(016): [150/879] Batch: 0.1239 (0.1265) Data: 0.0119 (0.0227) Loss: 0.7848 (0.8806)
[2022/12/28 23:40] | TRAIN(016): [200/879] Batch: 0.1147 (0.1249) Data: 0.0161 (0.0200) Loss: 0.9047 (0.8844)
[2022/12/28 23:40] | TRAIN(016): [250/879] Batch: 0.1228 (0.1221) Data: 0.0097 (0.0180) Loss: 0.6517 (0.8770)
[2022/12/28 23:41] | TRAIN(016): [300/879] Batch: 0.1077 (0.1198) Data: 0.0088 (0.0166) Loss: 0.8387 (0.8725)
[2022/12/28 23:41] | TRAIN(016): [350/879] Batch: 0.1039 (0.1197) Data: 0.0094 (0.0158) Loss: 0.7226 (0.8650)
[2022/12/28 23:41] | TRAIN(016): [400/879] Batch: 0.1229 (0.1185) Data: 0.0109 (0.0151) Loss: 0.9547 (0.8713)
[2022/12/28 23:41] | TRAIN(016): [450/879] Batch: 0.1195 (0.1186) Data: 0.0115 (0.0147) Loss: 0.5868 (0.8636)
[2022/12/28 23:41] | TRAIN(016): [500/879] Batch: 0.1159 (0.1186) Data: 0.0093 (0.0143) Loss: 1.1323 (0.8691)
[2022/12/28 23:41] | TRAIN(016): [550/879] Batch: 0.1068 (0.1179) Data: 0.0093 (0.0139) Loss: 0.6436 (0.8688)
[2022/12/28 23:41] | TRAIN(016): [600/879] Batch: 0.1010 (0.1172) Data: 0.0087 (0.0135) Loss: 1.1396 (0.8703)
[2022/12/28 23:41] | TRAIN(016): [650/879] Batch: 0.1013 (0.1164) Data: 0.0093 (0.0132) Loss: 0.8509 (0.8688)
[2022/12/28 23:41] | TRAIN(016): [700/879] Batch: 0.1018 (0.1160) Data: 0.0088 (0.0130) Loss: 0.8716 (0.8688)
[2022/12/28 23:41] | TRAIN(016): [750/879] Batch: 0.1080 (0.1155) Data: 0.0112 (0.0128) Loss: 1.0080 (0.8681)
[2022/12/28 23:42] | TRAIN(016): [800/879] Batch: 0.1033 (0.1152) Data: 0.0105 (0.0126) Loss: 0.7959 (0.8699)
[2022/12/28 23:42] | TRAIN(016): [850/879] Batch: 0.1058 (0.1148) Data: 0.0092 (0.0124) Loss: 0.7323 (0.8699)
[2022/12/28 23:42] | ------------------------------------------------------------
[2022/12/28 23:42] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:42] | ------------------------------------------------------------
[2022/12/28 23:42] |    TRAIN(16)     0:01:40     0:00:10     0:01:29      0.8696
[2022/12/28 23:42] | ------------------------------------------------------------
[2022/12/28 23:42] | VALID(016): [ 50/220] Batch: 0.0381 (0.0650) Data: 0.0206 (0.0489) Loss: 0.7756 (0.8500)
[2022/12/28 23:42] | VALID(016): [100/220] Batch: 0.0384 (0.0518) Data: 0.0240 (0.0361) Loss: 1.1060 (0.8747)
[2022/12/28 23:42] | VALID(016): [150/220] Batch: 0.0350 (0.0474) Data: 0.0218 (0.0317) Loss: 0.7592 (0.8663)
[2022/12/28 23:42] | VALID(016): [200/220] Batch: 0.0392 (0.0451) Data: 0.0266 (0.0301) Loss: 0.4951 (0.8745)
[2022/12/28 23:42] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:42] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:42] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:42] |    VALID(16)      0.8736      0.7347      0.5138      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:42] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:42] | ####################################################################################################
[2022/12/28 23:42] | TRAIN(017): [ 50/879] Batch: 0.1091 (0.1457) Data: 0.0097 (0.0455) Loss: 0.8474 (0.8731)
[2022/12/28 23:42] | TRAIN(017): [100/879] Batch: 0.1138 (0.1291) Data: 0.0092 (0.0288) Loss: 0.6929 (0.8742)
[2022/12/28 23:42] | TRAIN(017): [150/879] Batch: 0.1070 (0.1231) Data: 0.0090 (0.0228) Loss: 0.7174 (0.8675)
[2022/12/28 23:42] | TRAIN(017): [200/879] Batch: 0.1097 (0.1201) Data: 0.0102 (0.0197) Loss: 0.9844 (0.8631)
[2022/12/28 23:42] | TRAIN(017): [250/879] Batch: 0.1227 (0.1189) Data: 0.0115 (0.0180) Loss: 0.9325 (0.8660)
[2022/12/28 23:42] | TRAIN(017): [300/879] Batch: 0.1061 (0.1179) Data: 0.0099 (0.0168) Loss: 1.1166 (0.8698)
[2022/12/28 23:42] | TRAIN(017): [350/879] Batch: 0.1114 (0.1170) Data: 0.0099 (0.0159) Loss: 0.8116 (0.8721)
[2022/12/28 23:43] | TRAIN(017): [400/879] Batch: 0.1023 (0.1160) Data: 0.0128 (0.0152) Loss: 1.0186 (0.8710)
[2022/12/28 23:43] | TRAIN(017): [450/879] Batch: 0.1235 (0.1153) Data: 0.0120 (0.0146) Loss: 1.0832 (0.8730)
[2022/12/28 23:43] | TRAIN(017): [500/879] Batch: 0.1126 (0.1152) Data: 0.0095 (0.0142) Loss: 0.7465 (0.8680)
[2022/12/28 23:43] | TRAIN(017): [550/879] Batch: 0.1175 (0.1147) Data: 0.0099 (0.0139) Loss: 0.6681 (0.8684)
[2022/12/28 23:43] | TRAIN(017): [600/879] Batch: 0.1252 (0.1143) Data: 0.0160 (0.0136) Loss: 0.8012 (0.8696)
[2022/12/28 23:43] | TRAIN(017): [650/879] Batch: 0.1248 (0.1146) Data: 0.0111 (0.0134) Loss: 0.6188 (0.8700)
[2022/12/28 23:43] | TRAIN(017): [700/879] Batch: 0.1033 (0.1145) Data: 0.0091 (0.0132) Loss: 0.8811 (0.8712)
[2022/12/28 23:43] | TRAIN(017): [750/879] Batch: 0.1075 (0.1142) Data: 0.0099 (0.0130) Loss: 0.8596 (0.8727)
[2022/12/28 23:43] | TRAIN(017): [800/879] Batch: 0.1306 (0.1140) Data: 0.0115 (0.0129) Loss: 1.1327 (0.8716)
[2022/12/28 23:43] | TRAIN(017): [850/879] Batch: 0.1007 (0.1142) Data: 0.0079 (0.0128) Loss: 1.0678 (0.8709)
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] |    TRAIN(17)     0:01:40     0:00:11     0:01:29      0.8699
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:44] | VALID(017): [ 50/220] Batch: 0.0402 (0.0670) Data: 0.0278 (0.0538) Loss: 0.7864 (0.8469)
[2022/12/28 23:44] | VALID(017): [100/220] Batch: 0.0377 (0.0526) Data: 0.0286 (0.0404) Loss: 1.1077 (0.8686)
[2022/12/28 23:44] | VALID(017): [150/220] Batch: 0.0400 (0.0479) Data: 0.0274 (0.0356) Loss: 0.7601 (0.8608)
[2022/12/28 23:44] | VALID(017): [200/220] Batch: 0.0354 (0.0456) Data: 0.0244 (0.0332) Loss: 0.5238 (0.8682)
[2022/12/28 23:44] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:44] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:44] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:44] |    VALID(17)      0.8681      0.7347      0.5066      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:44] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:44] | ####################################################################################################
[2022/12/28 23:44] | TRAIN(018): [ 50/879] Batch: 0.1014 (0.1488) Data: 0.0091 (0.0462) Loss: 0.5104 (0.8555)
[2022/12/28 23:44] | TRAIN(018): [100/879] Batch: 0.1051 (0.1307) Data: 0.0094 (0.0290) Loss: 1.0219 (0.8741)
[2022/12/28 23:44] | TRAIN(018): [150/879] Batch: 0.1135 (0.1246) Data: 0.0165 (0.0230) Loss: 0.9936 (0.8617)
[2022/12/28 23:44] | TRAIN(018): [200/879] Batch: 0.1069 (0.1211) Data: 0.0111 (0.0200) Loss: 0.8523 (0.8716)
[2022/12/28 23:44] | TRAIN(018): [250/879] Batch: 0.1155 (0.1190) Data: 0.0092 (0.0181) Loss: 0.9327 (0.8639)
[2022/12/28 23:44] | TRAIN(018): [300/879] Batch: 0.1104 (0.1175) Data: 0.0096 (0.0168) Loss: 0.8888 (0.8637)
[2022/12/28 23:44] | TRAIN(018): [350/879] Batch: 0.1030 (0.1164) Data: 0.0095 (0.0158) Loss: 0.5980 (0.8717)
[2022/12/28 23:44] | TRAIN(018): [400/879] Batch: 0.1088 (0.1155) Data: 0.0118 (0.0151) Loss: 0.8188 (0.8744)
[2022/12/28 23:45] | TRAIN(018): [450/879] Batch: 0.1247 (0.1149) Data: 0.0080 (0.0146) Loss: 0.8560 (0.8731)
[2022/12/28 23:45] | TRAIN(018): [500/879] Batch: 0.1114 (0.1142) Data: 0.0094 (0.0141) Loss: 0.9347 (0.8722)
[2022/12/28 23:45] | TRAIN(018): [550/879] Batch: 0.1078 (0.1138) Data: 0.0089 (0.0137) Loss: 0.7227 (0.8709)
[2022/12/28 23:45] | TRAIN(018): [600/879] Batch: 0.1093 (0.1135) Data: 0.0073 (0.0133) Loss: 0.9744 (0.8707)
[2022/12/28 23:45] | TRAIN(018): [650/879] Batch: 0.1201 (0.1131) Data: 0.0101 (0.0131) Loss: 0.9653 (0.8697)
[2022/12/28 23:45] | TRAIN(018): [700/879] Batch: 0.1042 (0.1129) Data: 0.0094 (0.0129) Loss: 1.1176 (0.8713)
[2022/12/28 23:45] | TRAIN(018): [750/879] Batch: 0.1134 (0.1128) Data: 0.0096 (0.0127) Loss: 0.8641 (0.8709)
[2022/12/28 23:45] | TRAIN(018): [800/879] Batch: 0.1066 (0.1129) Data: 0.0079 (0.0125) Loss: 0.8493 (0.8702)
[2022/12/28 23:45] | TRAIN(018): [850/879] Batch: 0.1085 (0.1126) Data: 0.0124 (0.0124) Loss: 0.7538 (0.8694)
[2022/12/28 23:45] | ------------------------------------------------------------
[2022/12/28 23:45] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:45] | ------------------------------------------------------------
[2022/12/28 23:45] |    TRAIN(18)     0:01:38     0:00:10     0:01:28      0.8697
[2022/12/28 23:45] | ------------------------------------------------------------
[2022/12/28 23:45] | VALID(018): [ 50/220] Batch: 0.0396 (0.0650) Data: 0.0285 (0.0523) Loss: 0.7804 (0.8470)
[2022/12/28 23:45] | VALID(018): [100/220] Batch: 0.0390 (0.0519) Data: 0.0233 (0.0380) Loss: 1.1072 (0.8690)
[2022/12/28 23:45] | VALID(018): [150/220] Batch: 0.0428 (0.0474) Data: 0.0231 (0.0331) Loss: 0.7610 (0.8614)
[2022/12/28 23:45] | VALID(018): [200/220] Batch: 0.0386 (0.0452) Data: 0.0266 (0.0313) Loss: 0.5256 (0.8689)
[2022/12/28 23:45] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:45] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:45] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:45] |    VALID(18)      0.8687      0.7347      0.5051      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:45] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:45] | ####################################################################################################
[2022/12/28 23:46] | TRAIN(019): [ 50/879] Batch: 0.1050 (0.1485) Data: 0.0158 (0.0446) Loss: 0.7495 (0.8927)
[2022/12/28 23:46] | TRAIN(019): [100/879] Batch: 0.1163 (0.1310) Data: 0.0103 (0.0283) Loss: 0.7848 (0.8807)
[2022/12/28 23:46] | TRAIN(019): [150/879] Batch: 0.1043 (0.1255) Data: 0.0095 (0.0227) Loss: 1.1229 (0.8815)
[2022/12/28 23:46] | TRAIN(019): [200/879] Batch: 0.1020 (0.1220) Data: 0.0096 (0.0196) Loss: 0.9878 (0.8804)
[2022/12/28 23:46] | TRAIN(019): [250/879] Batch: 0.1130 (0.1194) Data: 0.0118 (0.0177) Loss: 0.8491 (0.8820)
[2022/12/28 23:46] | TRAIN(019): [300/879] Batch: 0.1090 (0.1183) Data: 0.0102 (0.0165) Loss: 0.7425 (0.8761)
[2022/12/28 23:46] | TRAIN(019): [350/879] Batch: 0.1227 (0.1176) Data: 0.0114 (0.0157) Loss: 0.9261 (0.8754)
[2022/12/28 23:46] | TRAIN(019): [400/879] Batch: 0.1060 (0.1170) Data: 0.0091 (0.0150) Loss: 0.7620 (0.8760)
[2022/12/28 23:46] | TRAIN(019): [450/879] Batch: 0.1234 (0.1173) Data: 0.0112 (0.0146) Loss: 0.9429 (0.8741)
[2022/12/28 23:46] | TRAIN(019): [500/879] Batch: 0.1166 (0.1173) Data: 0.0097 (0.0142) Loss: 0.8490 (0.8739)
[2022/12/28 23:47] | TRAIN(019): [550/879] Batch: 0.1037 (0.1167) Data: 0.0089 (0.0138) Loss: 0.9081 (0.8716)
[2022/12/28 23:47] | TRAIN(019): [600/879] Batch: 0.1024 (0.1162) Data: 0.0090 (0.0135) Loss: 1.0515 (0.8708)
[2022/12/28 23:47] | TRAIN(019): [650/879] Batch: 0.1021 (0.1156) Data: 0.0096 (0.0132) Loss: 0.9190 (0.8716)
[2022/12/28 23:47] | TRAIN(019): [700/879] Batch: 0.1140 (0.1151) Data: 0.0097 (0.0129) Loss: 1.0692 (0.8700)
[2022/12/28 23:47] | TRAIN(019): [750/879] Batch: 0.1110 (0.1149) Data: 0.0113 (0.0127) Loss: 0.8895 (0.8682)
[2022/12/28 23:47] | TRAIN(019): [800/879] Batch: 0.1111 (0.1146) Data: 0.0088 (0.0125) Loss: 0.8229 (0.8701)
[2022/12/28 23:47] | TRAIN(019): [850/879] Batch: 0.1217 (0.1143) Data: 0.0116 (0.0123) Loss: 1.0037 (0.8688)
[2022/12/28 23:47] | ------------------------------------------------------------
[2022/12/28 23:47] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:47] | ------------------------------------------------------------
[2022/12/28 23:47] |    TRAIN(19)     0:01:40     0:00:10     0:01:29      0.8697
[2022/12/28 23:47] | ------------------------------------------------------------
[2022/12/28 23:47] | VALID(019): [ 50/220] Batch: 0.0394 (0.0649) Data: 0.0213 (0.0519) Loss: 0.7919 (0.8521)
[2022/12/28 23:47] | VALID(019): [100/220] Batch: 0.0414 (0.0518) Data: 0.0238 (0.0378) Loss: 1.0869 (0.8733)
[2022/12/28 23:47] | VALID(019): [150/220] Batch: 0.0414 (0.0474) Data: 0.0285 (0.0341) Loss: 0.7702 (0.8657)
[2022/12/28 23:47] | VALID(019): [200/220] Batch: 0.0381 (0.0453) Data: 0.0221 (0.0317) Loss: 0.5420 (0.8728)
[2022/12/28 23:47] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:47] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:47] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:47] |    VALID(19)      0.8723      0.7347      0.5133      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:47] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:47] | ####################################################################################################
[2022/12/28 23:47] | TRAIN(020): [ 50/879] Batch: 0.1030 (0.1448) Data: 0.0100 (0.0449) Loss: 0.6140 (0.8864)
[2022/12/28 23:48] | TRAIN(020): [100/879] Batch: 0.1039 (0.1258) Data: 0.0091 (0.0274) Loss: 0.8353 (0.8829)
[2022/12/28 23:48] | TRAIN(020): [150/879] Batch: 0.1264 (0.1210) Data: 0.0114 (0.0215) Loss: 1.2070 (0.8833)
[2022/12/28 23:48] | TRAIN(020): [200/879] Batch: 0.1109 (0.1190) Data: 0.0091 (0.0187) Loss: 0.8334 (0.8794)
[2022/12/28 23:48] | TRAIN(020): [250/879] Batch: 0.1152 (0.1174) Data: 0.0092 (0.0170) Loss: 0.8186 (0.8770)
[2022/12/28 23:48] | TRAIN(020): [300/879] Batch: 0.1090 (0.1159) Data: 0.0098 (0.0158) Loss: 0.8739 (0.8757)
[2022/12/28 23:48] | TRAIN(020): [350/879] Batch: 0.1142 (0.1159) Data: 0.0115 (0.0151) Loss: 0.9185 (0.8738)
[2022/12/28 23:48] | TRAIN(020): [400/879] Batch: 0.1131 (0.1163) Data: 0.0093 (0.0146) Loss: 0.6623 (0.8762)
[2022/12/28 23:48] | TRAIN(020): [450/879] Batch: 0.1012 (0.1154) Data: 0.0090 (0.0140) Loss: 1.0062 (0.8756)
[2022/12/28 23:48] | TRAIN(020): [500/879] Batch: 0.1210 (0.1150) Data: 0.0114 (0.0136) Loss: 0.9822 (0.8787)
[2022/12/28 23:48] | TRAIN(020): [550/879] Batch: 0.1128 (0.1154) Data: 0.0115 (0.0135) Loss: 1.3305 (0.8813)
[2022/12/28 23:48] | TRAIN(020): [600/879] Batch: 0.1143 (0.1158) Data: 0.0110 (0.0133) Loss: 1.0119 (0.8786)
[2022/12/28 23:49] | TRAIN(020): [650/879] Batch: 0.1044 (0.1158) Data: 0.0094 (0.0131) Loss: 1.0339 (0.8743)
[2022/12/28 23:49] | TRAIN(020): [700/879] Batch: 0.1044 (0.1153) Data: 0.0091 (0.0128) Loss: 0.8167 (0.8709)
[2022/12/28 23:49] | TRAIN(020): [750/879] Batch: 0.1150 (0.1149) Data: 0.0101 (0.0126) Loss: 1.1266 (0.8702)
[2022/12/28 23:49] | TRAIN(020): [800/879] Batch: 0.1110 (0.1147) Data: 0.0094 (0.0124) Loss: 1.0064 (0.8691)
[2022/12/28 23:49] | TRAIN(020): [850/879] Batch: 0.1054 (0.1143) Data: 0.0089 (0.0123) Loss: 0.8437 (0.8695)
[2022/12/28 23:49] | ------------------------------------------------------------
[2022/12/28 23:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:49] | ------------------------------------------------------------
[2022/12/28 23:49] |    TRAIN(20)     0:01:40     0:00:10     0:01:29      0.8694
[2022/12/28 23:49] | ------------------------------------------------------------
[2022/12/28 23:49] | VALID(020): [ 50/220] Batch: 0.0410 (0.0666) Data: 0.0239 (0.0509) Loss: 0.7905 (0.8480)
[2022/12/28 23:49] | VALID(020): [100/220] Batch: 0.0393 (0.0529) Data: 0.0245 (0.0375) Loss: 1.1024 (0.8688)
[2022/12/28 23:49] | VALID(020): [150/220] Batch: 0.0396 (0.0483) Data: 0.0244 (0.0330) Loss: 0.7624 (0.8616)
[2022/12/28 23:49] | VALID(020): [200/220] Batch: 0.0415 (0.0459) Data: 0.0249 (0.0306) Loss: 0.5295 (0.8686)
[2022/12/28 23:49] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:49] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:49] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:49] |    VALID(20)      0.8686      0.7347      0.5082      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:49] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:49] | ####################################################################################################
[2022/12/28 23:49] | TRAIN(021): [ 50/879] Batch: 0.1138 (0.1490) Data: 0.0105 (0.0471) Loss: 1.1180 (0.9003)
[2022/12/28 23:49] | TRAIN(021): [100/879] Batch: 0.1122 (0.1288) Data: 0.0097 (0.0286) Loss: 0.7437 (0.8713)
[2022/12/28 23:49] | TRAIN(021): [150/879] Batch: 0.1012 (0.1217) Data: 0.0087 (0.0222) Loss: 0.7584 (0.8599)
[2022/12/28 23:50] | TRAIN(021): [200/879] Batch: 0.1151 (0.1183) Data: 0.0102 (0.0190) Loss: 0.9340 (0.8748)
[2022/12/28 23:50] | TRAIN(021): [250/879] Batch: 0.1155 (0.1165) Data: 0.0099 (0.0171) Loss: 1.0041 (0.8743)
[2022/12/28 23:50] | TRAIN(021): [300/879] Batch: 0.1241 (0.1164) Data: 0.0116 (0.0161) Loss: 0.8790 (0.8704)
[2022/12/28 23:50] | TRAIN(021): [350/879] Batch: 0.1146 (0.1154) Data: 0.0096 (0.0152) Loss: 1.0481 (0.8702)
[2022/12/28 23:50] | TRAIN(021): [400/879] Batch: 0.1151 (0.1146) Data: 0.0100 (0.0145) Loss: 1.0969 (0.8677)
[2022/12/28 23:50] | TRAIN(021): [450/879] Batch: 0.1057 (0.1138) Data: 0.0102 (0.0140) Loss: 0.7923 (0.8673)
[2022/12/28 23:50] | TRAIN(021): [500/879] Batch: 0.1091 (0.1135) Data: 0.0116 (0.0136) Loss: 0.8409 (0.8659)
[2022/12/28 23:50] | TRAIN(021): [550/879] Batch: 0.1161 (0.1131) Data: 0.0097 (0.0132) Loss: 0.6402 (0.8661)
[2022/12/28 23:50] | TRAIN(021): [600/879] Batch: 0.1014 (0.1128) Data: 0.0093 (0.0129) Loss: 0.6615 (0.8665)
[2022/12/28 23:50] | TRAIN(021): [650/879] Batch: 0.1092 (0.1124) Data: 0.0114 (0.0127) Loss: 0.8031 (0.8679)
[2022/12/28 23:50] | TRAIN(021): [700/879] Batch: 0.1212 (0.1124) Data: 0.0114 (0.0125) Loss: 0.7926 (0.8676)
[2022/12/28 23:51] | TRAIN(021): [750/879] Batch: 0.1225 (0.1128) Data: 0.0115 (0.0124) Loss: 0.8908 (0.8681)
[2022/12/28 23:51] | TRAIN(021): [800/879] Batch: 0.1131 (0.1130) Data: 0.0095 (0.0123) Loss: 0.7166 (0.8676)
[2022/12/28 23:51] | TRAIN(021): [850/879] Batch: 0.1026 (0.1127) Data: 0.0096 (0.0122) Loss: 1.1925 (0.8692)
[2022/12/28 23:51] | ------------------------------------------------------------
[2022/12/28 23:51] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:51] | ------------------------------------------------------------
[2022/12/28 23:51] |    TRAIN(21)     0:01:38     0:00:10     0:01:28      0.8696
[2022/12/28 23:51] | ------------------------------------------------------------
[2022/12/28 23:51] | VALID(021): [ 50/220] Batch: 0.0386 (0.0655) Data: 0.0271 (0.0520) Loss: 0.7758 (0.8473)
[2022/12/28 23:51] | VALID(021): [100/220] Batch: 0.0377 (0.0518) Data: 0.0271 (0.0391) Loss: 1.1040 (0.8704)
[2022/12/28 23:51] | VALID(021): [150/220] Batch: 0.0378 (0.0472) Data: 0.0274 (0.0348) Loss: 0.7563 (0.8621)
[2022/12/28 23:51] | VALID(021): [200/220] Batch: 0.0418 (0.0449) Data: 0.0237 (0.0326) Loss: 0.5248 (0.8696)
[2022/12/28 23:51] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:51] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:51] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:51] |    VALID(21)      0.8691      0.7347      0.5071      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:51] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:51] | ####################################################################################################
[2022/12/28 23:51] | TRAIN(022): [ 50/879] Batch: 0.1034 (0.1445) Data: 0.0165 (0.0447) Loss: 0.8587 (0.8460)
[2022/12/28 23:51] | TRAIN(022): [100/879] Batch: 0.1071 (0.1271) Data: 0.0092 (0.0278) Loss: 0.9109 (0.8607)
[2022/12/28 23:51] | TRAIN(022): [150/879] Batch: 0.1117 (0.1216) Data: 0.0156 (0.0222) Loss: 0.6384 (0.8645)
[2022/12/28 23:51] | TRAIN(022): [200/879] Batch: 0.1097 (0.1184) Data: 0.0092 (0.0193) Loss: 0.7488 (0.8617)
[2022/12/28 23:51] | TRAIN(022): [250/879] Batch: 0.1151 (0.1170) Data: 0.0112 (0.0176) Loss: 0.8668 (0.8703)
[2022/12/28 23:52] | TRAIN(022): [300/879] Batch: 0.1227 (0.1163) Data: 0.0091 (0.0166) Loss: 0.5608 (0.8754)
[2022/12/28 23:52] | TRAIN(022): [350/879] Batch: 0.1041 (0.1152) Data: 0.0090 (0.0157) Loss: 1.1160 (0.8752)
[2022/12/28 23:52] | TRAIN(022): [400/879] Batch: 0.1078 (0.1146) Data: 0.0121 (0.0151) Loss: 1.1365 (0.8741)
[2022/12/28 23:52] | TRAIN(022): [450/879] Batch: 0.1092 (0.1147) Data: 0.0114 (0.0148) Loss: 0.5703 (0.8683)
[2022/12/28 23:52] | TRAIN(022): [500/879] Batch: 0.1388 (0.1147) Data: 0.0153 (0.0145) Loss: 0.8642 (0.8645)
[2022/12/28 23:52] | TRAIN(022): [550/879] Batch: 0.1260 (0.1147) Data: 0.0115 (0.0142) Loss: 0.8820 (0.8664)
[2022/12/28 23:52] | TRAIN(022): [600/879] Batch: 0.1285 (0.1146) Data: 0.0126 (0.0141) Loss: 0.8903 (0.8681)
[2022/12/28 23:52] | TRAIN(022): [650/879] Batch: 0.1089 (0.1144) Data: 0.0093 (0.0139) Loss: 0.8491 (0.8683)
[2022/12/28 23:52] | TRAIN(022): [700/879] Batch: 0.1097 (0.1143) Data: 0.0166 (0.0137) Loss: 1.1834 (0.8695)
[2022/12/28 23:52] | TRAIN(022): [750/879] Batch: 0.1227 (0.1140) Data: 0.0114 (0.0135) Loss: 1.0236 (0.8702)
[2022/12/28 23:52] | TRAIN(022): [800/879] Batch: 0.1105 (0.1139) Data: 0.0158 (0.0134) Loss: 0.8202 (0.8698)
[2022/12/28 23:53] | TRAIN(022): [850/879] Batch: 0.1232 (0.1138) Data: 0.0111 (0.0133) Loss: 0.7759 (0.8693)
[2022/12/28 23:53] | ------------------------------------------------------------
[2022/12/28 23:53] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:53] | ------------------------------------------------------------
[2022/12/28 23:53] |    TRAIN(22)     0:01:39     0:00:11     0:01:28      0.8697
[2022/12/28 23:53] | ------------------------------------------------------------
[2022/12/28 23:53] | VALID(022): [ 50/220] Batch: 0.0378 (0.0643) Data: 0.0275 (0.0504) Loss: 0.7894 (0.8474)
[2022/12/28 23:53] | VALID(022): [100/220] Batch: 0.0389 (0.0512) Data: 0.0266 (0.0384) Loss: 1.1082 (0.8698)
[2022/12/28 23:53] | VALID(022): [150/220] Batch: 0.0380 (0.0468) Data: 0.0259 (0.0343) Loss: 0.7612 (0.8619)
[2022/12/28 23:53] | VALID(022): [200/220] Batch: 0.0366 (0.0446) Data: 0.0290 (0.0322) Loss: 0.5000 (0.8696)
[2022/12/28 23:53] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:53] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:53] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:53] |    VALID(22)      0.8694      0.7347      0.4984      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:53] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:53] | ####################################################################################################
[2022/12/28 23:53] | TRAIN(023): [ 50/879] Batch: 0.1086 (0.1461) Data: 0.0118 (0.0458) Loss: 0.9205 (0.8714)
[2022/12/28 23:53] | TRAIN(023): [100/879] Batch: 0.1217 (0.1295) Data: 0.0166 (0.0288) Loss: 0.8317 (0.8913)
[2022/12/28 23:53] | TRAIN(023): [150/879] Batch: 0.1023 (0.1238) Data: 0.0094 (0.0229) Loss: 0.8495 (0.8759)
[2022/12/28 23:53] | TRAIN(023): [200/879] Batch: 0.1258 (0.1212) Data: 0.0101 (0.0200) Loss: 1.1023 (0.8747)
[2022/12/28 23:53] | TRAIN(023): [250/879] Batch: 0.1091 (0.1190) Data: 0.0110 (0.0183) Loss: 0.9663 (0.8756)
[2022/12/28 23:53] | TRAIN(023): [300/879] Batch: 0.1091 (0.1178) Data: 0.0178 (0.0170) Loss: 1.1594 (0.8733)
[2022/12/28 23:53] | TRAIN(023): [350/879] Batch: 0.1100 (0.1169) Data: 0.0166 (0.0162) Loss: 0.7819 (0.8757)
[2022/12/28 23:54] | TRAIN(023): [400/879] Batch: 0.1087 (0.1167) Data: 0.0089 (0.0157) Loss: 1.0934 (0.8715)
[2022/12/28 23:54] | TRAIN(023): [450/879] Batch: 0.1126 (0.1159) Data: 0.0098 (0.0151) Loss: 1.1410 (0.8716)
[2022/12/28 23:54] | TRAIN(023): [500/879] Batch: 0.1144 (0.1154) Data: 0.0093 (0.0146) Loss: 0.7684 (0.8680)
[2022/12/28 23:54] | TRAIN(023): [550/879] Batch: 0.1193 (0.1150) Data: 0.0118 (0.0142) Loss: 0.6345 (0.8634)
[2022/12/28 23:54] | TRAIN(023): [600/879] Batch: 0.1022 (0.1147) Data: 0.0097 (0.0139) Loss: 1.2060 (0.8626)
[2022/12/28 23:54] | TRAIN(023): [650/879] Batch: 0.1052 (0.1143) Data: 0.0095 (0.0136) Loss: 0.7974 (0.8627)
[2022/12/28 23:54] | TRAIN(023): [700/879] Batch: 0.1086 (0.1140) Data: 0.0091 (0.0134) Loss: 1.2345 (0.8648)
[2022/12/28 23:54] | TRAIN(023): [750/879] Batch: 0.1111 (0.1139) Data: 0.0126 (0.0132) Loss: 0.7890 (0.8669)
[2022/12/28 23:54] | TRAIN(023): [800/879] Batch: 0.1027 (0.1139) Data: 0.0099 (0.0131) Loss: 0.6792 (0.8665)
[2022/12/28 23:54] | TRAIN(023): [850/879] Batch: 0.1157 (0.1139) Data: 0.0113 (0.0130) Loss: 1.0129 (0.8687)
[2022/12/28 23:54] | ------------------------------------------------------------
[2022/12/28 23:54] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:54] | ------------------------------------------------------------
[2022/12/28 23:54] |    TRAIN(23)     0:01:40     0:00:11     0:01:28      0.8692
[2022/12/28 23:54] | ------------------------------------------------------------
[2022/12/28 23:55] | VALID(023): [ 50/220] Batch: 0.0419 (0.0653) Data: 0.0247 (0.0506) Loss: 0.7911 (0.8479)
[2022/12/28 23:55] | VALID(023): [100/220] Batch: 0.0381 (0.0517) Data: 0.0244 (0.0379) Loss: 1.0970 (0.8689)
[2022/12/28 23:55] | VALID(023): [150/220] Batch: 0.0351 (0.0472) Data: 0.0281 (0.0334) Loss: 0.7663 (0.8616)
[2022/12/28 23:55] | VALID(023): [200/220] Batch: 0.0408 (0.0450) Data: 0.0249 (0.0314) Loss: 0.5271 (0.8688)
[2022/12/28 23:55] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:55] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:55] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:55] |    VALID(23)      0.8686      0.7347      0.4926      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:55] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:55] | ####################################################################################################
[2022/12/28 23:55] | TRAIN(024): [ 50/879] Batch: 0.1120 (0.1492) Data: 0.0093 (0.0471) Loss: 0.8734 (0.8935)
[2022/12/28 23:55] | TRAIN(024): [100/879] Batch: 0.1103 (0.1315) Data: 0.0092 (0.0293) Loss: 1.0552 (0.8826)
[2022/12/28 23:55] | TRAIN(024): [150/879] Batch: 0.1139 (0.1262) Data: 0.0180 (0.0237) Loss: 0.8717 (0.8732)
[2022/12/28 23:55] | TRAIN(024): [200/879] Batch: 0.1086 (0.1233) Data: 0.0153 (0.0207) Loss: 0.6058 (0.8688)
[2022/12/28 23:55] | TRAIN(024): [250/879] Batch: 0.1109 (0.1211) Data: 0.0088 (0.0187) Loss: 0.9197 (0.8651)
[2022/12/28 23:55] | TRAIN(024): [300/879] Batch: 0.1089 (0.1199) Data: 0.0094 (0.0175) Loss: 0.6487 (0.8595)
[2022/12/28 23:55] | TRAIN(024): [350/879] Batch: 0.1182 (0.1190) Data: 0.0079 (0.0166) Loss: 0.6610 (0.8574)
[2022/12/28 23:55] | TRAIN(024): [400/879] Batch: 0.1146 (0.1182) Data: 0.0098 (0.0158) Loss: 0.7602 (0.8608)
[2022/12/28 23:56] | TRAIN(024): [450/879] Batch: 0.1143 (0.1173) Data: 0.0106 (0.0153) Loss: 0.6682 (0.8632)
[2022/12/28 23:56] | TRAIN(024): [500/879] Batch: 0.1059 (0.1165) Data: 0.0095 (0.0147) Loss: 0.9697 (0.8678)
[2022/12/28 23:56] | TRAIN(024): [550/879] Batch: 0.1226 (0.1159) Data: 0.0091 (0.0143) Loss: 0.6927 (0.8690)
[2022/12/28 23:56] | TRAIN(024): [600/879] Batch: 0.1037 (0.1155) Data: 0.0094 (0.0140) Loss: 0.5593 (0.8675)
[2022/12/28 23:56] | TRAIN(024): [650/879] Batch: 0.1136 (0.1151) Data: 0.0093 (0.0138) Loss: 0.9056 (0.8695)
[2022/12/28 23:56] | TRAIN(024): [700/879] Batch: 0.1024 (0.1148) Data: 0.0091 (0.0135) Loss: 1.0309 (0.8678)
[2022/12/28 23:56] | TRAIN(024): [750/879] Batch: 0.1141 (0.1145) Data: 0.0090 (0.0133) Loss: 0.8033 (0.8672)
[2022/12/28 23:56] | TRAIN(024): [800/879] Batch: 0.1024 (0.1143) Data: 0.0095 (0.0132) Loss: 0.7214 (0.8702)
[2022/12/28 23:56] | TRAIN(024): [850/879] Batch: 0.1052 (0.1142) Data: 0.0153 (0.0131) Loss: 0.9587 (0.8699)
[2022/12/28 23:56] | ------------------------------------------------------------
[2022/12/28 23:56] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:56] | ------------------------------------------------------------
[2022/12/28 23:56] |    TRAIN(24)     0:01:40     0:00:11     0:01:28      0.8692
[2022/12/28 23:56] | ------------------------------------------------------------
[2022/12/28 23:56] | VALID(024): [ 50/220] Batch: 0.0394 (0.0653) Data: 0.0262 (0.0506) Loss: 0.7770 (0.8477)
[2022/12/28 23:56] | VALID(024): [100/220] Batch: 0.0379 (0.0519) Data: 0.0258 (0.0377) Loss: 1.1176 (0.8719)
[2022/12/28 23:56] | VALID(024): [150/220] Batch: 0.0394 (0.0474) Data: 0.0267 (0.0340) Loss: 0.7543 (0.8633)
[2022/12/28 23:56] | VALID(024): [200/220] Batch: 0.0346 (0.0452) Data: 0.0246 (0.0321) Loss: 0.4924 (0.8716)
[2022/12/28 23:56] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:56] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:56] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:56] |    VALID(24)      0.8710      0.7347      0.5177      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:56] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:56] | ####################################################################################################
[2022/12/28 23:57] | TRAIN(025): [ 50/879] Batch: 0.1169 (0.1521) Data: 0.0172 (0.0500) Loss: 0.5723 (0.8508)
[2022/12/28 23:57] | TRAIN(025): [100/879] Batch: 0.1176 (0.1324) Data: 0.0079 (0.0307) Loss: 0.7750 (0.8609)
[2022/12/28 23:57] | TRAIN(025): [150/879] Batch: 0.1095 (0.1251) Data: 0.0162 (0.0241) Loss: 0.6611 (0.8709)
[2022/12/28 23:57] | TRAIN(025): [200/879] Batch: 0.1063 (0.1220) Data: 0.0106 (0.0209) Loss: 1.1677 (0.8695)
[2022/12/28 23:57] | TRAIN(025): [250/879] Batch: 0.1129 (0.1205) Data: 0.0092 (0.0189) Loss: 0.9101 (0.8681)
[2022/12/28 23:57] | TRAIN(025): [300/879] Batch: 0.1079 (0.1192) Data: 0.0093 (0.0176) Loss: 0.7522 (0.8680)
[2022/12/28 23:57] | TRAIN(025): [350/879] Batch: 0.1200 (0.1182) Data: 0.0095 (0.0167) Loss: 0.9842 (0.8718)
[2022/12/28 23:57] | TRAIN(025): [400/879] Batch: 0.1055 (0.1173) Data: 0.0093 (0.0160) Loss: 1.1600 (0.8718)
[2022/12/28 23:57] | TRAIN(025): [450/879] Batch: 0.1127 (0.1168) Data: 0.0156 (0.0155) Loss: 0.5591 (0.8665)
[2022/12/28 23:57] | TRAIN(025): [500/879] Batch: 0.1110 (0.1165) Data: 0.0092 (0.0151) Loss: 1.0583 (0.8734)
[2022/12/28 23:58] | TRAIN(025): [550/879] Batch: 0.1076 (0.1159) Data: 0.0105 (0.0147) Loss: 0.6750 (0.8734)
[2022/12/28 23:58] | TRAIN(025): [600/879] Batch: 0.1148 (0.1156) Data: 0.0156 (0.0144) Loss: 0.8630 (0.8751)
[2022/12/28 23:58] | TRAIN(025): [650/879] Batch: 0.1041 (0.1153) Data: 0.0093 (0.0141) Loss: 0.6690 (0.8759)
[2022/12/28 23:58] | TRAIN(025): [700/879] Batch: 0.1101 (0.1152) Data: 0.0099 (0.0139) Loss: 0.9890 (0.8761)
[2022/12/28 23:58] | TRAIN(025): [750/879] Batch: 0.1094 (0.1150) Data: 0.0092 (0.0137) Loss: 0.8534 (0.8724)
[2022/12/28 23:58] | TRAIN(025): [800/879] Batch: 0.1264 (0.1151) Data: 0.0113 (0.0136) Loss: 0.8565 (0.8692)
[2022/12/28 23:58] | TRAIN(025): [850/879] Batch: 0.1325 (0.1151) Data: 0.0160 (0.0135) Loss: 1.4442 (0.8682)
[2022/12/28 23:58] | ------------------------------------------------------------
[2022/12/28 23:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:58] | ------------------------------------------------------------
[2022/12/28 23:58] |    TRAIN(25)     0:01:40     0:00:11     0:01:29      0.8693
[2022/12/28 23:58] | ------------------------------------------------------------
[2022/12/28 23:58] | VALID(025): [ 50/220] Batch: 0.0347 (0.0647) Data: 0.0284 (0.0510) Loss: 0.7981 (0.8547)
[2022/12/28 23:58] | VALID(025): [100/220] Batch: 0.0405 (0.0515) Data: 0.0269 (0.0383) Loss: 1.1009 (0.8739)
[2022/12/28 23:58] | VALID(025): [150/220] Batch: 0.0379 (0.0471) Data: 0.0269 (0.0342) Loss: 0.7746 (0.8671)
[2022/12/28 23:58] | VALID(025): [200/220] Batch: 0.0469 (0.0450) Data: 0.0188 (0.0317) Loss: 0.5695 (0.8737)
[2022/12/28 23:58] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:58] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:58] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:58] |    VALID(25)      0.8739      0.7347      0.5022      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:58] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:58] | ####################################################################################################
[2022/12/28 23:58] | TRAIN(026): [ 50/879] Batch: 0.1039 (0.1459) Data: 0.0088 (0.0455) Loss: 0.8710 (0.8661)
[2022/12/28 23:59] | TRAIN(026): [100/879] Batch: 0.1051 (0.1291) Data: 0.0097 (0.0282) Loss: 0.9161 (0.8691)
[2022/12/28 23:59] | TRAIN(026): [150/879] Batch: 0.1097 (0.1233) Data: 0.0107 (0.0226) Loss: 0.8249 (0.8845)
[2022/12/28 23:59] | TRAIN(026): [200/879] Batch: 0.1179 (0.1210) Data: 0.0113 (0.0197) Loss: 0.7673 (0.8709)
[2022/12/28 23:59] | TRAIN(026): [250/879] Batch: 0.1146 (0.1194) Data: 0.0095 (0.0180) Loss: 0.8854 (0.8765)
[2022/12/28 23:59] | TRAIN(026): [300/879] Batch: 0.1240 (0.1185) Data: 0.0109 (0.0170) Loss: 0.8264 (0.8731)
[2022/12/28 23:59] | TRAIN(026): [350/879] Batch: 0.1026 (0.1173) Data: 0.0101 (0.0161) Loss: 0.6559 (0.8732)
[2022/12/28 23:59] | TRAIN(026): [400/879] Batch: 0.1043 (0.1164) Data: 0.0093 (0.0154) Loss: 0.7435 (0.8697)
[2022/12/28 23:59] | TRAIN(026): [450/879] Batch: 0.1152 (0.1164) Data: 0.0093 (0.0150) Loss: 0.7319 (0.8701)
[2022/12/28 23:59] | TRAIN(026): [500/879] Batch: 0.1106 (0.1160) Data: 0.0091 (0.0145) Loss: 0.9490 (0.8703)
[2022/12/28 23:59] | TRAIN(026): [550/879] Batch: 0.1092 (0.1155) Data: 0.0103 (0.0142) Loss: 1.1226 (0.8685)
[2022/12/28 23:59] | TRAIN(026): [600/879] Batch: 0.1286 (0.1152) Data: 0.0164 (0.0139) Loss: 1.0235 (0.8668)
[2022/12/29 00:00] | TRAIN(026): [650/879] Batch: 0.1149 (0.1149) Data: 0.0098 (0.0137) Loss: 0.8830 (0.8689)
[2022/12/29 00:00] | TRAIN(026): [700/879] Batch: 0.1031 (0.1148) Data: 0.0089 (0.0135) Loss: 1.0298 (0.8715)
[2022/12/29 00:00] | TRAIN(026): [750/879] Batch: 0.1065 (0.1145) Data: 0.0132 (0.0133) Loss: 0.7868 (0.8728)
[2022/12/29 00:00] | TRAIN(026): [800/879] Batch: 0.1072 (0.1143) Data: 0.0095 (0.0131) Loss: 1.1408 (0.8710)
[2022/12/29 00:00] | TRAIN(026): [850/879] Batch: 0.1076 (0.1143) Data: 0.0099 (0.0130) Loss: 0.9314 (0.8698)
[2022/12/29 00:00] | ------------------------------------------------------------
[2022/12/29 00:00] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:00] | ------------------------------------------------------------
[2022/12/29 00:00] |    TRAIN(26)     0:01:40     0:00:11     0:01:28      0.8693
[2022/12/29 00:00] | ------------------------------------------------------------
[2022/12/29 00:00] | VALID(026): [ 50/220] Batch: 0.0412 (0.0651) Data: 0.0276 (0.0514) Loss: 0.7825 (0.8466)
[2022/12/29 00:00] | VALID(026): [100/220] Batch: 0.0386 (0.0519) Data: 0.0263 (0.0378) Loss: 1.1109 (0.8693)
[2022/12/29 00:00] | VALID(026): [150/220] Batch: 0.0392 (0.0475) Data: 0.0265 (0.0339) Loss: 0.7545 (0.8610)
[2022/12/29 00:00] | VALID(026): [200/220] Batch: 0.0382 (0.0453) Data: 0.0275 (0.0320) Loss: 0.5086 (0.8687)
[2022/12/29 00:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:00] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:00] |    VALID(26)      0.8683      0.7347      0.5041      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:00] | ####################################################################################################
[2022/12/29 00:00] | TRAIN(027): [ 50/879] Batch: 0.1012 (0.1478) Data: 0.0104 (0.0449) Loss: 0.8195 (0.9109)
[2022/12/29 00:00] | TRAIN(027): [100/879] Batch: 0.1126 (0.1316) Data: 0.0121 (0.0284) Loss: 0.8956 (0.8970)
[2022/12/29 00:00] | TRAIN(027): [150/879] Batch: 0.1045 (0.1253) Data: 0.0092 (0.0225) Loss: 0.8182 (0.8867)
[2022/12/29 00:01] | TRAIN(027): [200/879] Batch: 0.1281 (0.1219) Data: 0.0114 (0.0197) Loss: 0.9492 (0.8812)
[2022/12/29 00:01] | TRAIN(027): [250/879] Batch: 0.1213 (0.1207) Data: 0.0113 (0.0179) Loss: 0.8176 (0.8841)
[2022/12/29 00:01] | TRAIN(027): [300/879] Batch: 0.1158 (0.1197) Data: 0.0109 (0.0168) Loss: 0.9428 (0.8736)
[2022/12/29 00:01] | TRAIN(027): [350/879] Batch: 0.1231 (0.1184) Data: 0.0117 (0.0158) Loss: 0.6923 (0.8731)
[2022/12/29 00:01] | TRAIN(027): [400/879] Batch: 0.1259 (0.1180) Data: 0.0110 (0.0152) Loss: 0.8443 (0.8676)
[2022/12/29 00:01] | TRAIN(027): [450/879] Batch: 0.1154 (0.1174) Data: 0.0097 (0.0147) Loss: 0.4232 (0.8645)
[2022/12/29 00:01] | TRAIN(027): [500/879] Batch: 0.1276 (0.1168) Data: 0.0104 (0.0143) Loss: 0.8783 (0.8648)
[2022/12/29 00:01] | TRAIN(027): [550/879] Batch: 0.1118 (0.1162) Data: 0.0104 (0.0140) Loss: 0.7019 (0.8644)
[2022/12/29 00:01] | TRAIN(027): [600/879] Batch: 0.1152 (0.1157) Data: 0.0094 (0.0137) Loss: 1.3727 (0.8683)
[2022/12/29 00:01] | TRAIN(027): [650/879] Batch: 0.1086 (0.1154) Data: 0.0160 (0.0135) Loss: 1.1064 (0.8650)
[2022/12/29 00:02] | TRAIN(027): [700/879] Batch: 0.1089 (0.1151) Data: 0.0098 (0.0133) Loss: 0.6871 (0.8658)
[2022/12/29 00:02] | TRAIN(027): [750/879] Batch: 0.1072 (0.1149) Data: 0.0090 (0.0131) Loss: 0.6027 (0.8678)
[2022/12/29 00:02] | TRAIN(027): [800/879] Batch: 0.1018 (0.1146) Data: 0.0095 (0.0129) Loss: 0.8785 (0.8692)
[2022/12/29 00:02] | TRAIN(027): [850/879] Batch: 0.1076 (0.1143) Data: 0.0092 (0.0128) Loss: 0.9472 (0.8691)
[2022/12/29 00:02] | ------------------------------------------------------------
[2022/12/29 00:02] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:02] | ------------------------------------------------------------
[2022/12/29 00:02] |    TRAIN(27)     0:01:40     0:00:11     0:01:29      0.8693
[2022/12/29 00:02] | ------------------------------------------------------------
[2022/12/29 00:02] | VALID(027): [ 50/220] Batch: 0.0397 (0.0637) Data: 0.0260 (0.0493) Loss: 0.7766 (0.8464)
[2022/12/29 00:02] | VALID(027): [100/220] Batch: 0.0334 (0.0511) Data: 0.0274 (0.0372) Loss: 1.1096 (0.8698)
[2022/12/29 00:02] | VALID(027): [150/220] Batch: 0.0376 (0.0470) Data: 0.0270 (0.0337) Loss: 0.7505 (0.8613)
[2022/12/29 00:02] | VALID(027): [200/220] Batch: 0.0376 (0.0449) Data: 0.0274 (0.0320) Loss: 0.5164 (0.8690)
[2022/12/29 00:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:02] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:02] |    VALID(27)      0.8685      0.7347      0.5082      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:02] | ####################################################################################################
[2022/12/29 00:02] | TRAIN(028): [ 50/879] Batch: 0.1232 (0.1460) Data: 0.0171 (0.0459) Loss: 1.1523 (0.8802)
[2022/12/29 00:02] | TRAIN(028): [100/879] Batch: 0.1073 (0.1299) Data: 0.0096 (0.0286) Loss: 0.8485 (0.8662)
[2022/12/29 00:02] | TRAIN(028): [150/879] Batch: 0.1036 (0.1252) Data: 0.0092 (0.0229) Loss: 1.1123 (0.8661)
[2022/12/29 00:02] | TRAIN(028): [200/879] Batch: 0.1153 (0.1224) Data: 0.0091 (0.0199) Loss: 0.6893 (0.8728)
[2022/12/29 00:03] | TRAIN(028): [250/879] Batch: 0.1055 (0.1201) Data: 0.0092 (0.0179) Loss: 0.7041 (0.8739)
[2022/12/29 00:03] | TRAIN(028): [300/879] Batch: 0.1117 (0.1193) Data: 0.0108 (0.0168) Loss: 0.8062 (0.8783)
[2022/12/29 00:03] | TRAIN(028): [350/879] Batch: 0.1206 (0.1183) Data: 0.0114 (0.0160) Loss: 0.9881 (0.8740)
[2022/12/29 00:03] | TRAIN(028): [400/879] Batch: 0.1244 (0.1177) Data: 0.0103 (0.0153) Loss: 1.1440 (0.8705)
[2022/12/29 00:03] | TRAIN(028): [450/879] Batch: 0.1014 (0.1178) Data: 0.0098 (0.0149) Loss: 1.2494 (0.8735)
[2022/12/29 00:03] | TRAIN(028): [500/879] Batch: 0.1083 (0.1170) Data: 0.0114 (0.0144) Loss: 1.0030 (0.8736)
[2022/12/29 00:03] | TRAIN(028): [550/879] Batch: 0.1029 (0.1164) Data: 0.0090 (0.0140) Loss: 0.7473 (0.8726)
[2022/12/29 00:03] | TRAIN(028): [600/879] Batch: 0.1079 (0.1160) Data: 0.0098 (0.0137) Loss: 0.8215 (0.8705)
[2022/12/29 00:03] | TRAIN(028): [650/879] Batch: 0.1107 (0.1159) Data: 0.0110 (0.0135) Loss: 1.0075 (0.8695)
[2022/12/29 00:03] | TRAIN(028): [700/879] Batch: 0.1139 (0.1154) Data: 0.0156 (0.0132) Loss: 0.5572 (0.8689)
[2022/12/29 00:03] | TRAIN(028): [750/879] Batch: 0.1095 (0.1150) Data: 0.0093 (0.0130) Loss: 0.6238 (0.8723)
[2022/12/29 00:04] | TRAIN(028): [800/879] Batch: 0.1137 (0.1149) Data: 0.0093 (0.0128) Loss: 0.5964 (0.8710)
[2022/12/29 00:04] | TRAIN(028): [850/879] Batch: 0.1250 (0.1146) Data: 0.0114 (0.0127) Loss: 0.8050 (0.8699)
[2022/12/29 00:04] | ------------------------------------------------------------
[2022/12/29 00:04] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:04] | ------------------------------------------------------------
[2022/12/29 00:04] |    TRAIN(28)     0:01:40     0:00:11     0:01:29      0.8685
[2022/12/29 00:04] | ------------------------------------------------------------
[2022/12/29 00:04] | VALID(028): [ 50/220] Batch: 0.0360 (0.0665) Data: 0.0275 (0.0522) Loss: 0.7896 (0.8482)
[2022/12/29 00:04] | VALID(028): [100/220] Batch: 0.0402 (0.0525) Data: 0.0228 (0.0389) Loss: 1.1170 (0.8703)
[2022/12/29 00:04] | VALID(028): [150/220] Batch: 0.0454 (0.0479) Data: 0.0212 (0.0342) Loss: 0.7516 (0.8624)
[2022/12/29 00:04] | VALID(028): [200/220] Batch: 0.0444 (0.0455) Data: 0.0235 (0.0322) Loss: 0.5111 (0.8697)
[2022/12/29 00:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:04] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:04] |    VALID(28)      0.8696      0.7347      0.5109      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:04] | ####################################################################################################
[2022/12/29 00:04] | TRAIN(029): [ 50/879] Batch: 0.1162 (0.1454) Data: 0.0095 (0.0448) Loss: 0.5038 (0.8594)
[2022/12/29 00:04] | TRAIN(029): [100/879] Batch: 0.1063 (0.1269) Data: 0.0103 (0.0274) Loss: 0.7593 (0.8488)
[2022/12/29 00:04] | TRAIN(029): [150/879] Batch: 0.1253 (0.1238) Data: 0.0113 (0.0221) Loss: 0.8481 (0.8446)
[2022/12/29 00:04] | TRAIN(029): [200/879] Batch: 0.1035 (0.1205) Data: 0.0092 (0.0191) Loss: 0.7962 (0.8564)
[2022/12/29 00:04] | TRAIN(029): [250/879] Batch: 0.1154 (0.1183) Data: 0.0094 (0.0173) Loss: 0.8361 (0.8646)
[2022/12/29 00:04] | TRAIN(029): [300/879] Batch: 0.1146 (0.1175) Data: 0.0091 (0.0161) Loss: 0.9950 (0.8656)
[2022/12/29 00:05] | TRAIN(029): [350/879] Batch: 0.1185 (0.1164) Data: 0.0097 (0.0152) Loss: 0.9169 (0.8630)
[2022/12/29 00:05] | TRAIN(029): [400/879] Batch: 0.1198 (0.1155) Data: 0.0091 (0.0146) Loss: 0.9916 (0.8615)
[2022/12/29 00:05] | TRAIN(029): [450/879] Batch: 0.1063 (0.1150) Data: 0.0095 (0.0141) Loss: 0.9682 (0.8673)
[2022/12/29 00:05] | TRAIN(029): [500/879] Batch: 0.1066 (0.1144) Data: 0.0093 (0.0137) Loss: 0.8419 (0.8674)
[2022/12/29 00:05] | TRAIN(029): [550/879] Batch: 0.1245 (0.1145) Data: 0.0113 (0.0134) Loss: 0.9892 (0.8668)
[2022/12/29 00:05] | TRAIN(029): [600/879] Batch: 0.1045 (0.1143) Data: 0.0090 (0.0131) Loss: 0.9246 (0.8682)
[2022/12/29 00:05] | TRAIN(029): [650/879] Batch: 0.1002 (0.1141) Data: 0.0089 (0.0129) Loss: 0.7369 (0.8660)
[2022/12/29 00:05] | TRAIN(029): [700/879] Batch: 0.1178 (0.1144) Data: 0.0078 (0.0128) Loss: 0.7226 (0.8671)
[2022/12/29 00:05] | TRAIN(029): [750/879] Batch: 0.1233 (0.1141) Data: 0.0116 (0.0126) Loss: 1.1594 (0.8679)
[2022/12/29 00:05] | TRAIN(029): [800/879] Batch: 0.1176 (0.1145) Data: 0.0103 (0.0125) Loss: 0.8141 (0.8693)
[2022/12/29 00:05] | TRAIN(029): [850/879] Batch: 0.1191 (0.1142) Data: 0.0111 (0.0123) Loss: 0.8727 (0.8691)
[2022/12/29 00:06] | ------------------------------------------------------------
[2022/12/29 00:06] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:06] | ------------------------------------------------------------
[2022/12/29 00:06] |    TRAIN(29)     0:01:40     0:00:10     0:01:29      0.8689
[2022/12/29 00:06] | ------------------------------------------------------------
[2022/12/29 00:06] | VALID(029): [ 50/220] Batch: 0.0389 (0.0639) Data: 0.0239 (0.0481) Loss: 0.7855 (0.8466)
[2022/12/29 00:06] | VALID(029): [100/220] Batch: 0.0382 (0.0510) Data: 0.0237 (0.0355) Loss: 1.1085 (0.8684)
[2022/12/29 00:06] | VALID(029): [150/220] Batch: 0.0361 (0.0467) Data: 0.0261 (0.0312) Loss: 0.7595 (0.8606)
[2022/12/29 00:06] | VALID(029): [200/220] Batch: 0.0377 (0.0445) Data: 0.0242 (0.0297) Loss: 0.5188 (0.8681)
[2022/12/29 00:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:06] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:06] |    VALID(29)      0.8679      0.7347      0.4965      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:06] | ####################################################################################################
[2022/12/29 00:06] | TRAIN(030): [ 50/879] Batch: 0.1001 (0.1425) Data: 0.0098 (0.0436) Loss: 0.9483 (0.9027)
[2022/12/29 00:06] | TRAIN(030): [100/879] Batch: 0.1250 (0.1304) Data: 0.0113 (0.0277) Loss: 0.9024 (0.8973)
[2022/12/29 00:06] | TRAIN(030): [150/879] Batch: 0.1111 (0.1268) Data: 0.0090 (0.0223) Loss: 1.1379 (0.9006)
[2022/12/29 00:06] | TRAIN(030): [200/879] Batch: 0.1076 (0.1222) Data: 0.0101 (0.0192) Loss: 1.0102 (0.8874)
[2022/12/29 00:06] | TRAIN(030): [250/879] Batch: 0.1066 (0.1189) Data: 0.0100 (0.0173) Loss: 0.9497 (0.8860)
[2022/12/29 00:06] | TRAIN(030): [300/879] Batch: 0.1244 (0.1170) Data: 0.0116 (0.0160) Loss: 0.5303 (0.8788)
[2022/12/29 00:06] | TRAIN(030): [350/879] Batch: 0.1196 (0.1173) Data: 0.0113 (0.0154) Loss: 0.6025 (0.8750)
[2022/12/29 00:06] | TRAIN(030): [400/879] Batch: 0.1148 (0.1177) Data: 0.0117 (0.0149) Loss: 0.5463 (0.8716)
[2022/12/29 00:07] | TRAIN(030): [450/879] Batch: 0.1151 (0.1179) Data: 0.0109 (0.0145) Loss: 1.1496 (0.8740)
[2022/12/29 00:07] | TRAIN(030): [500/879] Batch: 0.1155 (0.1171) Data: 0.0087 (0.0140) Loss: 0.6817 (0.8727)
[2022/12/29 00:07] | TRAIN(030): [550/879] Batch: 0.1017 (0.1165) Data: 0.0091 (0.0136) Loss: 0.8763 (0.8724)
[2022/12/29 00:07] | TRAIN(030): [600/879] Batch: 0.1066 (0.1157) Data: 0.0088 (0.0133) Loss: 0.9216 (0.8707)
[2022/12/29 00:07] | TRAIN(030): [650/879] Batch: 0.1124 (0.1151) Data: 0.0103 (0.0130) Loss: 1.2056 (0.8695)
[2022/12/29 00:07] | TRAIN(030): [700/879] Batch: 0.1048 (0.1146) Data: 0.0087 (0.0128) Loss: 0.7262 (0.8674)
[2022/12/29 00:07] | TRAIN(030): [750/879] Batch: 0.1121 (0.1141) Data: 0.0098 (0.0126) Loss: 0.9634 (0.8694)
[2022/12/29 00:07] | TRAIN(030): [800/879] Batch: 0.1088 (0.1137) Data: 0.0115 (0.0124) Loss: 0.8906 (0.8698)
[2022/12/29 00:07] | TRAIN(030): [850/879] Batch: 0.1124 (0.1138) Data: 0.0099 (0.0123) Loss: 0.9598 (0.8683)
[2022/12/29 00:07] | ------------------------------------------------------------
[2022/12/29 00:07] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:07] | ------------------------------------------------------------
[2022/12/29 00:07] |    TRAIN(30)     0:01:39     0:00:10     0:01:29      0.8693
[2022/12/29 00:07] | ------------------------------------------------------------
[2022/12/29 00:07] | VALID(030): [ 50/220] Batch: 0.0399 (0.0650) Data: 0.0277 (0.0512) Loss: 0.7784 (0.8469)
[2022/12/29 00:07] | VALID(030): [100/220] Batch: 0.0387 (0.0516) Data: 0.0220 (0.0388) Loss: 1.1033 (0.8697)
[2022/12/29 00:07] | VALID(030): [150/220] Batch: 0.0388 (0.0471) Data: 0.0274 (0.0346) Loss: 0.7596 (0.8616)
[2022/12/29 00:08] | VALID(030): [200/220] Batch: 0.0373 (0.0449) Data: 0.0230 (0.0325) Loss: 0.5207 (0.8692)
[2022/12/29 00:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:08] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:08] |    VALID(30)      0.8688      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:08] | ####################################################################################################
[2022/12/29 00:08] | TRAIN(031): [ 50/879] Batch: 0.1019 (0.1461) Data: 0.0164 (0.0457) Loss: 0.7612 (0.8949)
[2022/12/29 00:08] | TRAIN(031): [100/879] Batch: 0.1042 (0.1285) Data: 0.0092 (0.0285) Loss: 1.0083 (0.8638)
[2022/12/29 00:08] | TRAIN(031): [150/879] Batch: 0.1101 (0.1232) Data: 0.0164 (0.0229) Loss: 1.1799 (0.8687)
[2022/12/29 00:08] | TRAIN(031): [200/879] Batch: 0.1106 (0.1206) Data: 0.0112 (0.0200) Loss: 0.7179 (0.8665)
[2022/12/29 00:08] | TRAIN(031): [250/879] Batch: 0.1069 (0.1192) Data: 0.0163 (0.0182) Loss: 0.9366 (0.8634)
[2022/12/29 00:08] | TRAIN(031): [300/879] Batch: 0.1258 (0.1186) Data: 0.0124 (0.0171) Loss: 0.9313 (0.8679)
[2022/12/29 00:08] | TRAIN(031): [350/879] Batch: 0.1158 (0.1180) Data: 0.0178 (0.0163) Loss: 0.8345 (0.8754)
[2022/12/29 00:08] | TRAIN(031): [400/879] Batch: 0.1196 (0.1175) Data: 0.0164 (0.0158) Loss: 0.7204 (0.8721)
[2022/12/29 00:08] | TRAIN(031): [450/879] Batch: 0.1068 (0.1168) Data: 0.0160 (0.0153) Loss: 0.7505 (0.8695)
[2022/12/29 00:08] | TRAIN(031): [500/879] Batch: 0.1026 (0.1163) Data: 0.0162 (0.0149) Loss: 1.0208 (0.8722)
[2022/12/29 00:09] | TRAIN(031): [550/879] Batch: 0.1072 (0.1160) Data: 0.0098 (0.0146) Loss: 1.2535 (0.8716)
[2022/12/29 00:09] | TRAIN(031): [600/879] Batch: 0.1213 (0.1158) Data: 0.0164 (0.0143) Loss: 0.8349 (0.8745)
[2022/12/29 00:09] | TRAIN(031): [650/879] Batch: 0.1030 (0.1157) Data: 0.0160 (0.0141) Loss: 0.7048 (0.8713)
[2022/12/29 00:09] | TRAIN(031): [700/879] Batch: 0.1125 (0.1158) Data: 0.0161 (0.0140) Loss: 0.9383 (0.8679)
[2022/12/29 00:09] | TRAIN(031): [750/879] Batch: 0.1405 (0.1158) Data: 0.0117 (0.0139) Loss: 0.9168 (0.8664)
[2022/12/29 00:09] | TRAIN(031): [800/879] Batch: 0.1211 (0.1158) Data: 0.0112 (0.0137) Loss: 1.0633 (0.8658)
[2022/12/29 00:09] | TRAIN(031): [850/879] Batch: 0.1272 (0.1157) Data: 0.0111 (0.0136) Loss: 0.8852 (0.8679)
[2022/12/29 00:09] | ------------------------------------------------------------
[2022/12/29 00:09] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:09] | ------------------------------------------------------------
[2022/12/29 00:09] |    TRAIN(31)     0:01:41     0:00:11     0:01:29      0.8688
[2022/12/29 00:09] | ------------------------------------------------------------
[2022/12/29 00:09] | VALID(031): [ 50/220] Batch: 0.0370 (0.0630) Data: 0.0232 (0.0491) Loss: 0.7878 (0.8480)
[2022/12/29 00:09] | VALID(031): [100/220] Batch: 0.0385 (0.0507) Data: 0.0274 (0.0367) Loss: 1.1047 (0.8691)
[2022/12/29 00:09] | VALID(031): [150/220] Batch: 0.0370 (0.0466) Data: 0.0274 (0.0332) Loss: 0.7625 (0.8615)
[2022/12/29 00:09] | VALID(031): [200/220] Batch: 0.0399 (0.0446) Data: 0.0258 (0.0312) Loss: 0.5342 (0.8687)
[2022/12/29 00:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:09] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:09] |    VALID(31)      0.8686      0.7347      0.4962      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:09] | ####################################################################################################
[2022/12/29 00:09] | TRAIN(032): [ 50/879] Batch: 0.1018 (0.1471) Data: 0.0121 (0.0442) Loss: 0.9721 (0.9024)
[2022/12/29 00:10] | TRAIN(032): [100/879] Batch: 0.1036 (0.1296) Data: 0.0098 (0.0279) Loss: 0.7538 (0.8686)
[2022/12/29 00:10] | TRAIN(032): [150/879] Batch: 0.1252 (0.1225) Data: 0.0176 (0.0221) Loss: 1.1019 (0.8768)
[2022/12/29 00:10] | TRAIN(032): [200/879] Batch: 0.1009 (0.1207) Data: 0.0096 (0.0195) Loss: 0.9682 (0.8701)
[2022/12/29 00:10] | TRAIN(032): [250/879] Batch: 0.1249 (0.1187) Data: 0.0095 (0.0179) Loss: 0.8497 (0.8711)
[2022/12/29 00:10] | TRAIN(032): [300/879] Batch: 0.1168 (0.1178) Data: 0.0108 (0.0168) Loss: 0.9181 (0.8677)
[2022/12/29 00:10] | TRAIN(032): [350/879] Batch: 0.1261 (0.1166) Data: 0.0093 (0.0159) Loss: 1.0837 (0.8715)
[2022/12/29 00:10] | TRAIN(032): [400/879] Batch: 0.1082 (0.1162) Data: 0.0106 (0.0155) Loss: 0.8931 (0.8761)
[2022/12/29 00:10] | TRAIN(032): [450/879] Batch: 0.1088 (0.1156) Data: 0.0092 (0.0150) Loss: 0.7930 (0.8759)
[2022/12/29 00:10] | TRAIN(032): [500/879] Batch: 0.1170 (0.1154) Data: 0.0155 (0.0147) Loss: 0.9540 (0.8737)
[2022/12/29 00:10] | TRAIN(032): [550/879] Batch: 0.1062 (0.1152) Data: 0.0092 (0.0144) Loss: 0.8506 (0.8741)
[2022/12/29 00:11] | TRAIN(032): [600/879] Batch: 0.1034 (0.1148) Data: 0.0101 (0.0142) Loss: 1.0954 (0.8741)
[2022/12/29 00:11] | TRAIN(032): [650/879] Batch: 0.1177 (0.1145) Data: 0.0093 (0.0139) Loss: 0.7783 (0.8755)
[2022/12/29 00:11] | TRAIN(032): [700/879] Batch: 0.1095 (0.1143) Data: 0.0101 (0.0137) Loss: 0.6768 (0.8748)
[2022/12/29 00:11] | TRAIN(032): [750/879] Batch: 0.1075 (0.1145) Data: 0.0096 (0.0136) Loss: 0.5940 (0.8710)
[2022/12/29 00:11] | TRAIN(032): [800/879] Batch: 0.1133 (0.1142) Data: 0.0111 (0.0134) Loss: 1.0960 (0.8701)
[2022/12/29 00:11] | TRAIN(032): [850/879] Batch: 0.1118 (0.1141) Data: 0.0097 (0.0133) Loss: 0.8677 (0.8684)
[2022/12/29 00:11] | ------------------------------------------------------------
[2022/12/29 00:11] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:11] | ------------------------------------------------------------
[2022/12/29 00:11] |    TRAIN(32)     0:01:40     0:00:11     0:01:28      0.8687
[2022/12/29 00:11] | ------------------------------------------------------------
[2022/12/29 00:11] | VALID(032): [ 50/220] Batch: 0.0391 (0.0645) Data: 0.0267 (0.0511) Loss: 0.7847 (0.8468)
[2022/12/29 00:11] | VALID(032): [100/220] Batch: 0.0371 (0.0513) Data: 0.0292 (0.0383) Loss: 1.1114 (0.8689)
[2022/12/29 00:11] | VALID(032): [150/220] Batch: 0.0388 (0.0470) Data: 0.0289 (0.0343) Loss: 0.7579 (0.8609)
[2022/12/29 00:11] | VALID(032): [200/220] Batch: 0.0423 (0.0449) Data: 0.0269 (0.0323) Loss: 0.5193 (0.8684)
[2022/12/29 00:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:11] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:11] |    VALID(32)      0.8682      0.7347      0.5039      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:11] | ####################################################################################################
[2022/12/29 00:11] | TRAIN(033): [ 50/879] Batch: 0.1241 (0.1446) Data: 0.0078 (0.0436) Loss: 0.8852 (0.8862)
[2022/12/29 00:11] | TRAIN(033): [100/879] Batch: 0.1150 (0.1303) Data: 0.0096 (0.0277) Loss: 0.8440 (0.8719)
[2022/12/29 00:12] | TRAIN(033): [150/879] Batch: 0.1221 (0.1244) Data: 0.0089 (0.0220) Loss: 0.5282 (0.8581)
[2022/12/29 00:12] | TRAIN(033): [200/879] Batch: 0.1025 (0.1208) Data: 0.0095 (0.0192) Loss: 0.7599 (0.8631)
[2022/12/29 00:12] | TRAIN(033): [250/879] Batch: 0.1190 (0.1186) Data: 0.0118 (0.0176) Loss: 0.7357 (0.8526)
[2022/12/29 00:12] | TRAIN(033): [300/879] Batch: 0.1007 (0.1169) Data: 0.0102 (0.0164) Loss: 1.0036 (0.8560)
[2022/12/29 00:12] | TRAIN(033): [350/879] Batch: 0.1300 (0.1158) Data: 0.0090 (0.0156) Loss: 0.9426 (0.8601)
[2022/12/29 00:12] | TRAIN(033): [400/879] Batch: 0.1201 (0.1154) Data: 0.0162 (0.0151) Loss: 0.6567 (0.8631)
[2022/12/29 00:12] | TRAIN(033): [450/879] Batch: 0.1167 (0.1151) Data: 0.0093 (0.0147) Loss: 0.6441 (0.8633)
[2022/12/29 00:12] | TRAIN(033): [500/879] Batch: 0.1061 (0.1146) Data: 0.0092 (0.0143) Loss: 0.8882 (0.8681)
[2022/12/29 00:12] | TRAIN(033): [550/879] Batch: 0.1204 (0.1144) Data: 0.0121 (0.0141) Loss: 0.8294 (0.8667)
[2022/12/29 00:12] | TRAIN(033): [600/879] Batch: 0.1153 (0.1141) Data: 0.0108 (0.0138) Loss: 0.9601 (0.8653)
[2022/12/29 00:12] | TRAIN(033): [650/879] Batch: 0.1151 (0.1138) Data: 0.0095 (0.0135) Loss: 0.6452 (0.8680)
[2022/12/29 00:13] | TRAIN(033): [700/879] Batch: 0.1136 (0.1136) Data: 0.0101 (0.0133) Loss: 1.0965 (0.8696)
[2022/12/29 00:13] | TRAIN(033): [750/879] Batch: 0.1203 (0.1133) Data: 0.0098 (0.0132) Loss: 0.9676 (0.8726)
[2022/12/29 00:13] | TRAIN(033): [800/879] Batch: 0.1120 (0.1133) Data: 0.0158 (0.0131) Loss: 0.8806 (0.8717)
[2022/12/29 00:13] | TRAIN(033): [850/879] Batch: 0.1082 (0.1132) Data: 0.0155 (0.0130) Loss: 0.8579 (0.8697)
[2022/12/29 00:13] | ------------------------------------------------------------
[2022/12/29 00:13] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:13] | ------------------------------------------------------------
[2022/12/29 00:13] |    TRAIN(33)     0:01:39     0:00:11     0:01:27      0.8695
[2022/12/29 00:13] | ------------------------------------------------------------
[2022/12/29 00:13] | VALID(033): [ 50/220] Batch: 0.0341 (0.0708) Data: 0.0263 (0.0549) Loss: 0.7794 (0.8464)
[2022/12/29 00:13] | VALID(033): [100/220] Batch: 0.0394 (0.0548) Data: 0.0238 (0.0390) Loss: 1.1172 (0.8698)
[2022/12/29 00:13] | VALID(033): [150/220] Batch: 0.0395 (0.0493) Data: 0.0237 (0.0337) Loss: 0.7546 (0.8614)
[2022/12/29 00:13] | VALID(033): [200/220] Batch: 0.0384 (0.0467) Data: 0.0243 (0.0312) Loss: 0.5047 (0.8692)
[2022/12/29 00:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:13] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:13] |    VALID(33)      0.8689      0.7347      0.4967      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:13] | ####################################################################################################
[2022/12/29 00:13] | TRAIN(034): [ 50/879] Batch: 0.1247 (0.1548) Data: 0.0118 (0.0485) Loss: 0.8550 (0.8623)
[2022/12/29 00:13] | TRAIN(034): [100/879] Batch: 0.1261 (0.1374) Data: 0.0165 (0.0303) Loss: 0.9079 (0.8617)
[2022/12/29 00:13] | TRAIN(034): [150/879] Batch: 0.1083 (0.1281) Data: 0.0091 (0.0238) Loss: 0.9725 (0.8499)
[2022/12/29 00:13] | TRAIN(034): [200/879] Batch: 0.1043 (0.1229) Data: 0.0097 (0.0203) Loss: 0.9695 (0.8495)
[2022/12/29 00:14] | TRAIN(034): [250/879] Batch: 0.1040 (0.1201) Data: 0.0160 (0.0184) Loss: 0.6462 (0.8531)
[2022/12/29 00:14] | TRAIN(034): [300/879] Batch: 0.1081 (0.1187) Data: 0.0086 (0.0171) Loss: 0.6583 (0.8588)
[2022/12/29 00:14] | TRAIN(034): [350/879] Batch: 0.1224 (0.1171) Data: 0.0114 (0.0160) Loss: 0.8346 (0.8631)
[2022/12/29 00:14] | TRAIN(034): [400/879] Batch: 0.1091 (0.1168) Data: 0.0102 (0.0154) Loss: 0.6947 (0.8637)
[2022/12/29 00:14] | TRAIN(034): [450/879] Batch: 0.1136 (0.1159) Data: 0.0104 (0.0148) Loss: 0.5967 (0.8617)
[2022/12/29 00:14] | TRAIN(034): [500/879] Batch: 0.1031 (0.1155) Data: 0.0114 (0.0144) Loss: 0.8835 (0.8632)
[2022/12/29 00:14] | TRAIN(034): [550/879] Batch: 0.1053 (0.1151) Data: 0.0089 (0.0141) Loss: 0.9110 (0.8664)
[2022/12/29 00:14] | TRAIN(034): [600/879] Batch: 0.1103 (0.1146) Data: 0.0101 (0.0137) Loss: 0.6091 (0.8662)
[2022/12/29 00:14] | TRAIN(034): [650/879] Batch: 0.1213 (0.1141) Data: 0.0114 (0.0135) Loss: 1.0769 (0.8663)
[2022/12/29 00:14] | TRAIN(034): [700/879] Batch: 0.1175 (0.1143) Data: 0.0122 (0.0133) Loss: 0.8051 (0.8661)
[2022/12/29 00:14] | TRAIN(034): [750/879] Batch: 0.1063 (0.1141) Data: 0.0099 (0.0131) Loss: 0.9711 (0.8695)
[2022/12/29 00:15] | TRAIN(034): [800/879] Batch: 0.1227 (0.1140) Data: 0.0118 (0.0130) Loss: 0.8198 (0.8690)
[2022/12/29 00:15] | TRAIN(034): [850/879] Batch: 0.1133 (0.1137) Data: 0.0097 (0.0128) Loss: 0.6741 (0.8688)
[2022/12/29 00:15] | ------------------------------------------------------------
[2022/12/29 00:15] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:15] | ------------------------------------------------------------
[2022/12/29 00:15] |    TRAIN(34)     0:01:39     0:00:11     0:01:28      0.8687
[2022/12/29 00:15] | ------------------------------------------------------------
[2022/12/29 00:15] | VALID(034): [ 50/220] Batch: 0.0398 (0.0703) Data: 0.0268 (0.0579) Loss: 0.7893 (0.8470)
[2022/12/29 00:15] | VALID(034): [100/220] Batch: 0.0391 (0.0544) Data: 0.0278 (0.0423) Loss: 1.1025 (0.8685)
[2022/12/29 00:15] | VALID(034): [150/220] Batch: 0.0398 (0.0491) Data: 0.0286 (0.0365) Loss: 0.7637 (0.8611)
[2022/12/29 00:15] | VALID(034): [200/220] Batch: 0.0377 (0.0464) Data: 0.0295 (0.0339) Loss: 0.5154 (0.8685)
[2022/12/29 00:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:15] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:15] |    VALID(34)      0.8683      0.7347      0.5001      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:15] | ####################################################################################################
[2022/12/29 00:15] | TRAIN(035): [ 50/879] Batch: 0.1214 (0.1567) Data: 0.0132 (0.0519) Loss: 0.6417 (0.8874)
[2022/12/29 00:15] | TRAIN(035): [100/879] Batch: 0.1060 (0.1377) Data: 0.0104 (0.0320) Loss: 0.9652 (0.8976)
[2022/12/29 00:15] | TRAIN(035): [150/879] Batch: 0.1270 (0.1309) Data: 0.0114 (0.0252) Loss: 0.6303 (0.8817)
[2022/12/29 00:15] | TRAIN(035): [200/879] Batch: 0.1167 (0.1271) Data: 0.0124 (0.0218) Loss: 0.8567 (0.8764)
[2022/12/29 00:15] | TRAIN(035): [250/879] Batch: 0.1021 (0.1255) Data: 0.0093 (0.0198) Loss: 0.7402 (0.8746)
[2022/12/29 00:15] | TRAIN(035): [300/879] Batch: 0.1159 (0.1226) Data: 0.0114 (0.0182) Loss: 0.6565 (0.8763)
[2022/12/29 00:16] | TRAIN(035): [350/879] Batch: 0.1136 (0.1205) Data: 0.0107 (0.0171) Loss: 0.8658 (0.8782)
[2022/12/29 00:16] | TRAIN(035): [400/879] Batch: 0.1034 (0.1189) Data: 0.0104 (0.0162) Loss: 1.0887 (0.8759)
[2022/12/29 00:16] | TRAIN(035): [450/879] Batch: 0.1122 (0.1184) Data: 0.0108 (0.0157) Loss: 0.8307 (0.8804)
[2022/12/29 00:16] | TRAIN(035): [500/879] Batch: 0.1157 (0.1175) Data: 0.0122 (0.0152) Loss: 0.9772 (0.8783)
[2022/12/29 00:16] | TRAIN(035): [550/879] Batch: 0.1159 (0.1171) Data: 0.0108 (0.0148) Loss: 1.1825 (0.8759)
[2022/12/29 00:16] | TRAIN(035): [600/879] Batch: 0.1084 (0.1169) Data: 0.0106 (0.0145) Loss: 1.1358 (0.8727)
[2022/12/29 00:16] | TRAIN(035): [650/879] Batch: 0.1112 (0.1162) Data: 0.0104 (0.0142) Loss: 0.8525 (0.8716)
[2022/12/29 00:16] | TRAIN(035): [700/879] Batch: 0.1247 (0.1164) Data: 0.0127 (0.0140) Loss: 0.5854 (0.8713)
[2022/12/29 00:16] | TRAIN(035): [750/879] Batch: 0.1138 (0.1160) Data: 0.0095 (0.0138) Loss: 0.6477 (0.8679)
[2022/12/29 00:16] | TRAIN(035): [800/879] Batch: 0.1066 (0.1159) Data: 0.0096 (0.0136) Loss: 0.7075 (0.8670)
[2022/12/29 00:17] | TRAIN(035): [850/879] Batch: 0.1100 (0.1154) Data: 0.0097 (0.0134) Loss: 0.7283 (0.8675)
[2022/12/29 00:17] | ------------------------------------------------------------
[2022/12/29 00:17] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:17] | ------------------------------------------------------------
[2022/12/29 00:17] |    TRAIN(35)     0:01:41     0:00:11     0:01:29      0.8689
[2022/12/29 00:17] | ------------------------------------------------------------
[2022/12/29 00:17] | VALID(035): [ 50/220] Batch: 0.0391 (0.0654) Data: 0.0305 (0.0524) Loss: 0.7870 (0.8481)
[2022/12/29 00:17] | VALID(035): [100/220] Batch: 0.0389 (0.0520) Data: 0.0275 (0.0396) Loss: 1.1020 (0.8694)
[2022/12/29 00:17] | VALID(035): [150/220] Batch: 0.0370 (0.0476) Data: 0.0242 (0.0345) Loss: 0.7628 (0.8617)
[2022/12/29 00:17] | VALID(035): [200/220] Batch: 0.0377 (0.0453) Data: 0.0281 (0.0323) Loss: 0.5350 (0.8689)
[2022/12/29 00:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:17] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:17] |    VALID(35)      0.8688      0.7347      0.4999      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:17] | ####################################################################################################
[2022/12/29 00:17] | TRAIN(036): [ 50/879] Batch: 0.1271 (0.1507) Data: 0.0178 (0.0518) Loss: 1.4528 (0.8640)
[2022/12/29 00:17] | TRAIN(036): [100/879] Batch: 0.1112 (0.1298) Data: 0.0100 (0.0313) Loss: 0.8356 (0.8658)
[2022/12/29 00:17] | TRAIN(036): [150/879] Batch: 0.1246 (0.1246) Data: 0.0116 (0.0246) Loss: 0.8913 (0.8739)
[2022/12/29 00:17] | TRAIN(036): [200/879] Batch: 0.1189 (0.1234) Data: 0.0121 (0.0214) Loss: 0.8580 (0.8725)
[2022/12/29 00:17] | TRAIN(036): [250/879] Batch: 0.1226 (0.1215) Data: 0.0118 (0.0194) Loss: 0.8253 (0.8745)
[2022/12/29 00:17] | TRAIN(036): [300/879] Batch: 0.1065 (0.1195) Data: 0.0108 (0.0179) Loss: 0.7931 (0.8738)
[2022/12/29 00:17] | TRAIN(036): [350/879] Batch: 0.1094 (0.1179) Data: 0.0101 (0.0168) Loss: 0.8108 (0.8644)
[2022/12/29 00:18] | TRAIN(036): [400/879] Batch: 0.1121 (0.1166) Data: 0.0094 (0.0160) Loss: 0.9438 (0.8707)
[2022/12/29 00:18] | TRAIN(036): [450/879] Batch: 0.1037 (0.1161) Data: 0.0102 (0.0154) Loss: 0.7547 (0.8669)
[2022/12/29 00:18] | TRAIN(036): [500/879] Batch: 0.1085 (0.1153) Data: 0.0097 (0.0148) Loss: 0.8198 (0.8667)
[2022/12/29 00:18] | TRAIN(036): [550/879] Batch: 0.1160 (0.1155) Data: 0.0120 (0.0145) Loss: 1.0024 (0.8646)
[2022/12/29 00:18] | TRAIN(036): [600/879] Batch: 0.1116 (0.1157) Data: 0.0101 (0.0143) Loss: 0.8690 (0.8681)
[2022/12/29 00:18] | TRAIN(036): [650/879] Batch: 0.1094 (0.1152) Data: 0.0094 (0.0140) Loss: 0.8861 (0.8670)
[2022/12/29 00:18] | TRAIN(036): [700/879] Batch: 0.1259 (0.1150) Data: 0.0124 (0.0138) Loss: 1.1345 (0.8687)
[2022/12/29 00:18] | TRAIN(036): [750/879] Batch: 0.1209 (0.1150) Data: 0.0113 (0.0136) Loss: 0.9193 (0.8683)
[2022/12/29 00:18] | TRAIN(036): [800/879] Batch: 0.1186 (0.1153) Data: 0.0115 (0.0135) Loss: 0.8419 (0.8687)
[2022/12/29 00:18] | TRAIN(036): [850/879] Batch: 0.1061 (0.1149) Data: 0.0106 (0.0133) Loss: 0.8928 (0.8678)
[2022/12/29 00:18] | ------------------------------------------------------------
[2022/12/29 00:18] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:18] | ------------------------------------------------------------
[2022/12/29 00:18] |    TRAIN(36)     0:01:40     0:00:11     0:01:29      0.8691
[2022/12/29 00:18] | ------------------------------------------------------------
[2022/12/29 00:18] | VALID(036): [ 50/220] Batch: 0.0422 (0.0662) Data: 0.0287 (0.0525) Loss: 0.7846 (0.8467)
[2022/12/29 00:18] | VALID(036): [100/220] Batch: 0.0364 (0.0522) Data: 0.0252 (0.0388) Loss: 1.1081 (0.8686)
[2022/12/29 00:19] | VALID(036): [150/220] Batch: 0.0366 (0.0475) Data: 0.0246 (0.0341) Loss: 0.7593 (0.8607)
[2022/12/29 00:19] | VALID(036): [200/220] Batch: 0.0394 (0.0452) Data: 0.0263 (0.0317) Loss: 0.5221 (0.8681)
[2022/12/29 00:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:19] |    VALID(36)      0.8680      0.7347      0.5072      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:19] | ####################################################################################################
[2022/12/29 00:19] | TRAIN(037): [ 50/879] Batch: 0.1135 (0.1504) Data: 0.0071 (0.0508) Loss: 1.1464 (0.8608)
[2022/12/29 00:19] | TRAIN(037): [100/879] Batch: 0.1138 (0.1327) Data: 0.0114 (0.0315) Loss: 0.7622 (0.8588)
[2022/12/29 00:19] | TRAIN(037): [150/879] Batch: 0.1141 (0.1256) Data: 0.0084 (0.0250) Loss: 0.8931 (0.8504)
[2022/12/29 00:19] | TRAIN(037): [200/879] Batch: 0.1157 (0.1220) Data: 0.0119 (0.0215) Loss: 1.0328 (0.8635)
[2022/12/29 00:19] | TRAIN(037): [250/879] Batch: 0.1223 (0.1200) Data: 0.0136 (0.0194) Loss: 1.0683 (0.8617)
[2022/12/29 00:19] | TRAIN(037): [300/879] Batch: 0.1200 (0.1185) Data: 0.0104 (0.0180) Loss: 1.0960 (0.8674)
[2022/12/29 00:19] | TRAIN(037): [350/879] Batch: 0.1067 (0.1173) Data: 0.0089 (0.0169) Loss: 0.9592 (0.8667)
[2022/12/29 00:19] | TRAIN(037): [400/879] Batch: 0.1238 (0.1162) Data: 0.0117 (0.0161) Loss: 0.8798 (0.8687)
[2022/12/29 00:19] | TRAIN(037): [450/879] Batch: 0.1167 (0.1153) Data: 0.0089 (0.0156) Loss: 0.8570 (0.8714)
[2022/12/29 00:20] | TRAIN(037): [500/879] Batch: 0.1001 (0.1147) Data: 0.0174 (0.0150) Loss: 0.7023 (0.8707)
[2022/12/29 00:20] | TRAIN(037): [550/879] Batch: 0.1036 (0.1142) Data: 0.0110 (0.0146) Loss: 0.7599 (0.8687)
[2022/12/29 00:20] | TRAIN(037): [600/879] Batch: 0.1094 (0.1140) Data: 0.0136 (0.0143) Loss: 0.8835 (0.8682)
[2022/12/29 00:20] | TRAIN(037): [650/879] Batch: 0.1206 (0.1135) Data: 0.0144 (0.0140) Loss: 1.1832 (0.8674)
[2022/12/29 00:20] | TRAIN(037): [700/879] Batch: 0.1067 (0.1136) Data: 0.0107 (0.0139) Loss: 1.2250 (0.8668)
[2022/12/29 00:20] | TRAIN(037): [750/879] Batch: 0.1181 (0.1132) Data: 0.0122 (0.0136) Loss: 0.9319 (0.8687)
[2022/12/29 00:20] | TRAIN(037): [800/879] Batch: 0.1131 (0.1129) Data: 0.0097 (0.0134) Loss: 0.7831 (0.8700)
[2022/12/29 00:20] | TRAIN(037): [850/879] Batch: 0.1169 (0.1129) Data: 0.0114 (0.0133) Loss: 0.9873 (0.8697)
[2022/12/29 00:20] | ------------------------------------------------------------
[2022/12/29 00:20] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:20] | ------------------------------------------------------------
[2022/12/29 00:20] |    TRAIN(37)     0:01:39     0:00:11     0:01:27      0.8687
[2022/12/29 00:20] | ------------------------------------------------------------
[2022/12/29 00:20] | VALID(037): [ 50/220] Batch: 0.0352 (0.0664) Data: 0.0250 (0.0524) Loss: 0.7873 (0.8469)
[2022/12/29 00:20] | VALID(037): [100/220] Batch: 0.0388 (0.0523) Data: 0.0270 (0.0395) Loss: 1.1132 (0.8691)
[2022/12/29 00:20] | VALID(037): [150/220] Batch: 0.0384 (0.0476) Data: 0.0278 (0.0351) Loss: 0.7586 (0.8612)
[2022/12/29 00:20] | VALID(037): [200/220] Batch: 0.0385 (0.0453) Data: 0.0210 (0.0327) Loss: 0.5081 (0.8687)
[2022/12/29 00:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:20] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:20] |    VALID(37)      0.8686      0.7347      0.5063      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:20] | ####################################################################################################
[2022/12/29 00:21] | TRAIN(038): [ 50/879] Batch: 0.1080 (0.1394) Data: 0.0143 (0.0429) Loss: 1.1598 (0.8535)
[2022/12/29 00:21] | TRAIN(038): [100/879] Batch: 0.1079 (0.1237) Data: 0.0097 (0.0266) Loss: 1.1676 (0.8721)
[2022/12/29 00:21] | TRAIN(038): [150/879] Batch: 0.1006 (0.1185) Data: 0.0101 (0.0212) Loss: 1.0152 (0.8892)
[2022/12/29 00:21] | TRAIN(038): [200/879] Batch: 0.1188 (0.1181) Data: 0.0118 (0.0188) Loss: 0.8024 (0.8769)
[2022/12/29 00:21] | TRAIN(038): [250/879] Batch: 0.1214 (0.1185) Data: 0.0111 (0.0174) Loss: 1.1030 (0.8720)
[2022/12/29 00:21] | TRAIN(038): [300/879] Batch: 0.1077 (0.1169) Data: 0.0104 (0.0162) Loss: 0.7861 (0.8760)
[2022/12/29 00:21] | TRAIN(038): [350/879] Batch: 0.1052 (0.1155) Data: 0.0081 (0.0154) Loss: 1.1095 (0.8764)
[2022/12/29 00:21] | TRAIN(038): [400/879] Batch: 0.1145 (0.1156) Data: 0.0118 (0.0149) Loss: 1.0097 (0.8749)
[2022/12/29 00:21] | TRAIN(038): [450/879] Batch: 0.1024 (0.1148) Data: 0.0094 (0.0143) Loss: 0.7691 (0.8769)
[2022/12/29 00:21] | TRAIN(038): [500/879] Batch: 0.1125 (0.1140) Data: 0.0099 (0.0139) Loss: 0.8025 (0.8750)
[2022/12/29 00:21] | TRAIN(038): [550/879] Batch: 0.1026 (0.1133) Data: 0.0091 (0.0135) Loss: 0.7485 (0.8745)
[2022/12/29 00:22] | TRAIN(038): [600/879] Batch: 0.1054 (0.1130) Data: 0.0098 (0.0133) Loss: 0.8011 (0.8735)
[2022/12/29 00:22] | TRAIN(038): [650/879] Batch: 0.1089 (0.1124) Data: 0.0101 (0.0130) Loss: 0.9112 (0.8713)
[2022/12/29 00:22] | TRAIN(038): [700/879] Batch: 0.1062 (0.1121) Data: 0.0101 (0.0128) Loss: 1.3610 (0.8694)
[2022/12/29 00:22] | TRAIN(038): [750/879] Batch: 0.1032 (0.1119) Data: 0.0097 (0.0127) Loss: 1.1391 (0.8700)
[2022/12/29 00:22] | TRAIN(038): [800/879] Batch: 0.1065 (0.1117) Data: 0.0109 (0.0125) Loss: 1.0924 (0.8693)
[2022/12/29 00:22] | TRAIN(038): [850/879] Batch: 0.1264 (0.1117) Data: 0.0127 (0.0124) Loss: 1.1806 (0.8695)
[2022/12/29 00:22] | ------------------------------------------------------------
[2022/12/29 00:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:22] | ------------------------------------------------------------
[2022/12/29 00:22] |    TRAIN(38)     0:01:38     0:00:10     0:01:27      0.8687
[2022/12/29 00:22] | ------------------------------------------------------------
[2022/12/29 00:22] | VALID(038): [ 50/220] Batch: 0.0402 (0.0649) Data: 0.0270 (0.0521) Loss: 0.7821 (0.8465)
[2022/12/29 00:22] | VALID(038): [100/220] Batch: 0.0387 (0.0517) Data: 0.0233 (0.0384) Loss: 1.1156 (0.8693)
[2022/12/29 00:22] | VALID(038): [150/220] Batch: 0.0389 (0.0472) Data: 0.0268 (0.0342) Loss: 0.7568 (0.8612)
[2022/12/29 00:22] | VALID(038): [200/220] Batch: 0.0382 (0.0450) Data: 0.0246 (0.0317) Loss: 0.5065 (0.8690)
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] |    VALID(38)      0.8688      0.7347      0.5004      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] | ####################################################################################################
[2022/12/29 00:22] | TRAIN(039): [ 50/879] Batch: 0.1306 (0.1483) Data: 0.0116 (0.0485) Loss: 1.0934 (0.8441)
[2022/12/29 00:22] | TRAIN(039): [100/879] Batch: 0.1026 (0.1290) Data: 0.0097 (0.0297) Loss: 0.6750 (0.8401)
[2022/12/29 00:23] | TRAIN(039): [150/879] Batch: 0.1251 (0.1238) Data: 0.0114 (0.0235) Loss: 0.6819 (0.8568)
[2022/12/29 00:23] | TRAIN(039): [200/879] Batch: 0.1216 (0.1227) Data: 0.0123 (0.0206) Loss: 0.7440 (0.8584)
[2022/12/29 00:23] | TRAIN(039): [250/879] Batch: 0.1127 (0.1207) Data: 0.0096 (0.0186) Loss: 0.8595 (0.8617)
[2022/12/29 00:23] | TRAIN(039): [300/879] Batch: 0.0993 (0.1185) Data: 0.0097 (0.0171) Loss: 1.0718 (0.8637)
[2022/12/29 00:23] | TRAIN(039): [350/879] Batch: 0.1098 (0.1168) Data: 0.0116 (0.0162) Loss: 0.7244 (0.8673)
[2022/12/29 00:23] | TRAIN(039): [400/879] Batch: 0.1213 (0.1168) Data: 0.0121 (0.0156) Loss: 0.6194 (0.8699)
[2022/12/29 00:23] | TRAIN(039): [450/879] Batch: 0.1130 (0.1172) Data: 0.0120 (0.0152) Loss: 0.8156 (0.8718)
[2022/12/29 00:23] | TRAIN(039): [500/879] Batch: 0.1197 (0.1168) Data: 0.0115 (0.0148) Loss: 0.7770 (0.8720)
[2022/12/29 00:23] | TRAIN(039): [550/879] Batch: 0.1052 (0.1164) Data: 0.0102 (0.0144) Loss: 1.0476 (0.8682)
[2022/12/29 00:23] | TRAIN(039): [600/879] Batch: 0.1135 (0.1158) Data: 0.0101 (0.0141) Loss: 1.2427 (0.8689)
[2022/12/29 00:23] | TRAIN(039): [650/879] Batch: 0.1122 (0.1153) Data: 0.0121 (0.0138) Loss: 0.6283 (0.8685)
[2022/12/29 00:24] | TRAIN(039): [700/879] Batch: 0.1063 (0.1153) Data: 0.0117 (0.0136) Loss: 0.6026 (0.8649)
[2022/12/29 00:24] | TRAIN(039): [750/879] Batch: 0.1035 (0.1148) Data: 0.0105 (0.0134) Loss: 0.7584 (0.8660)
[2022/12/29 00:24] | TRAIN(039): [800/879] Batch: 0.1087 (0.1144) Data: 0.0102 (0.0132) Loss: 0.7222 (0.8681)
[2022/12/29 00:24] | TRAIN(039): [850/879] Batch: 0.1043 (0.1140) Data: 0.0103 (0.0130) Loss: 0.7015 (0.8702)
[2022/12/29 00:24] | ------------------------------------------------------------
[2022/12/29 00:24] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:24] | ------------------------------------------------------------
[2022/12/29 00:24] |    TRAIN(39)     0:01:40     0:00:11     0:01:28      0.8687
[2022/12/29 00:24] | ------------------------------------------------------------
[2022/12/29 00:24] | VALID(039): [ 50/220] Batch: 0.0398 (0.0660) Data: 0.0235 (0.0504) Loss: 0.7831 (0.8462)
[2022/12/29 00:24] | VALID(039): [100/220] Batch: 0.0394 (0.0522) Data: 0.0242 (0.0369) Loss: 1.1108 (0.8688)
[2022/12/29 00:24] | VALID(039): [150/220] Batch: 0.0405 (0.0477) Data: 0.0231 (0.0325) Loss: 0.7579 (0.8608)
[2022/12/29 00:24] | VALID(039): [200/220] Batch: 0.0384 (0.0455) Data: 0.0270 (0.0310) Loss: 0.5092 (0.8684)
[2022/12/29 00:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:24] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:24] |    VALID(39)      0.8682      0.7347      0.4911      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:24] | ####################################################################################################
[2022/12/29 00:24] | TRAIN(040): [ 50/879] Batch: 0.1041 (0.1430) Data: 0.0083 (0.0442) Loss: 0.8730 (0.8635)
[2022/12/29 00:24] | TRAIN(040): [100/879] Batch: 0.1150 (0.1275) Data: 0.0099 (0.0277) Loss: 0.6610 (0.8560)
[2022/12/29 00:24] | TRAIN(040): [150/879] Batch: 0.1244 (0.1230) Data: 0.0114 (0.0222) Loss: 0.7949 (0.8453)
[2022/12/29 00:24] | TRAIN(040): [200/879] Batch: 0.1233 (0.1205) Data: 0.0129 (0.0194) Loss: 0.8560 (0.8537)
[2022/12/29 00:25] | TRAIN(040): [250/879] Batch: 0.1161 (0.1195) Data: 0.0125 (0.0178) Loss: 0.9442 (0.8536)
[2022/12/29 00:25] | TRAIN(040): [300/879] Batch: 0.1030 (0.1178) Data: 0.0099 (0.0166) Loss: 0.9114 (0.8554)
[2022/12/29 00:25] | TRAIN(040): [350/879] Batch: 0.1094 (0.1166) Data: 0.0101 (0.0156) Loss: 0.7845 (0.8578)
[2022/12/29 00:25] | TRAIN(040): [400/879] Batch: 0.1142 (0.1158) Data: 0.0103 (0.0150) Loss: 0.7870 (0.8570)
[2022/12/29 00:25] | TRAIN(040): [450/879] Batch: 0.1055 (0.1153) Data: 0.0102 (0.0145) Loss: 0.7912 (0.8563)
[2022/12/29 00:25] | TRAIN(040): [500/879] Batch: 0.1030 (0.1145) Data: 0.0115 (0.0141) Loss: 0.6838 (0.8587)
[2022/12/29 00:25] | TRAIN(040): [550/879] Batch: 0.1020 (0.1142) Data: 0.0094 (0.0138) Loss: 0.9761 (0.8558)
[2022/12/29 00:25] | TRAIN(040): [600/879] Batch: 0.1046 (0.1137) Data: 0.0096 (0.0135) Loss: 0.9298 (0.8577)
[2022/12/29 00:25] | TRAIN(040): [650/879] Batch: 0.1154 (0.1132) Data: 0.0120 (0.0132) Loss: 0.9794 (0.8608)
[2022/12/29 00:25] | TRAIN(040): [700/879] Batch: 0.1102 (0.1128) Data: 0.0100 (0.0130) Loss: 0.7787 (0.8610)
[2022/12/29 00:25] | TRAIN(040): [750/879] Batch: 0.1035 (0.1130) Data: 0.0116 (0.0129) Loss: 0.9095 (0.8620)
[2022/12/29 00:26] | TRAIN(040): [800/879] Batch: 0.1027 (0.1126) Data: 0.0088 (0.0127) Loss: 0.8594 (0.8653)
[2022/12/29 00:26] | TRAIN(040): [850/879] Batch: 0.1053 (0.1124) Data: 0.0104 (0.0126) Loss: 0.8981 (0.8669)
[2022/12/29 00:26] | ------------------------------------------------------------
[2022/12/29 00:26] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:26] | ------------------------------------------------------------
[2022/12/29 00:26] |    TRAIN(40)     0:01:38     0:00:10     0:01:27      0.8684
[2022/12/29 00:26] | ------------------------------------------------------------
[2022/12/29 00:26] | VALID(040): [ 50/220] Batch: 0.0408 (0.0677) Data: 0.0274 (0.0544) Loss: 0.7880 (0.8478)
[2022/12/29 00:26] | VALID(040): [100/220] Batch: 0.0373 (0.0531) Data: 0.0271 (0.0407) Loss: 1.0987 (0.8690)
[2022/12/29 00:26] | VALID(040): [150/220] Batch: 0.0394 (0.0482) Data: 0.0266 (0.0361) Loss: 0.7645 (0.8615)
[2022/12/29 00:26] | VALID(040): [200/220] Batch: 0.0396 (0.0458) Data: 0.0267 (0.0337) Loss: 0.5319 (0.8687)
[2022/12/29 00:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:26] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:26] |    VALID(40)      0.8685      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:26] | ####################################################################################################
[2022/12/29 00:26] | TRAIN(041): [ 50/879] Batch: 0.1244 (0.1545) Data: 0.0122 (0.0505) Loss: 1.0577 (0.8518)
[2022/12/29 00:26] | TRAIN(041): [100/879] Batch: 0.1035 (0.1357) Data: 0.0106 (0.0313) Loss: 0.5490 (0.8644)
[2022/12/29 00:26] | TRAIN(041): [150/879] Batch: 0.1093 (0.1263) Data: 0.0118 (0.0243) Loss: 0.9609 (0.8671)
[2022/12/29 00:26] | TRAIN(041): [200/879] Batch: 0.1029 (0.1224) Data: 0.0098 (0.0209) Loss: 0.7285 (0.8744)
[2022/12/29 00:26] | TRAIN(041): [250/879] Batch: 0.1321 (0.1220) Data: 0.0113 (0.0191) Loss: 0.7500 (0.8674)
[2022/12/29 00:26] | TRAIN(041): [300/879] Batch: 0.1054 (0.1203) Data: 0.0107 (0.0177) Loss: 0.7314 (0.8687)
[2022/12/29 00:27] | TRAIN(041): [350/879] Batch: 0.1119 (0.1186) Data: 0.0114 (0.0167) Loss: 1.0330 (0.8704)
[2022/12/29 00:27] | TRAIN(041): [400/879] Batch: 0.1040 (0.1174) Data: 0.0098 (0.0159) Loss: 0.4886 (0.8715)
[2022/12/29 00:27] | TRAIN(041): [450/879] Batch: 0.1127 (0.1164) Data: 0.0089 (0.0152) Loss: 0.6074 (0.8707)
[2022/12/29 00:27] | TRAIN(041): [500/879] Batch: 0.1038 (0.1154) Data: 0.0093 (0.0147) Loss: 0.7617 (0.8728)
[2022/12/29 00:27] | TRAIN(041): [550/879] Batch: 0.1099 (0.1147) Data: 0.0096 (0.0143) Loss: 1.0490 (0.8712)
[2022/12/29 00:27] | TRAIN(041): [600/879] Batch: 0.1151 (0.1142) Data: 0.0098 (0.0139) Loss: 0.8000 (0.8717)
[2022/12/29 00:27] | TRAIN(041): [650/879] Batch: 0.1118 (0.1138) Data: 0.0083 (0.0136) Loss: 1.0289 (0.8714)
[2022/12/29 00:27] | TRAIN(041): [700/879] Batch: 0.1109 (0.1135) Data: 0.0101 (0.0133) Loss: 0.7494 (0.8738)
[2022/12/29 00:27] | TRAIN(041): [750/879] Batch: 0.1109 (0.1132) Data: 0.0099 (0.0131) Loss: 1.0710 (0.8763)
[2022/12/29 00:27] | TRAIN(041): [800/879] Batch: 0.1029 (0.1132) Data: 0.0097 (0.0130) Loss: 0.7936 (0.8724)
[2022/12/29 00:27] | TRAIN(041): [850/879] Batch: 0.1069 (0.1130) Data: 0.0099 (0.0129) Loss: 0.8432 (0.8703)
[2022/12/29 00:28] | ------------------------------------------------------------
[2022/12/29 00:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:28] | ------------------------------------------------------------
[2022/12/29 00:28] |    TRAIN(41)     0:01:39     0:00:11     0:01:27      0.8685
[2022/12/29 00:28] | ------------------------------------------------------------
[2022/12/29 00:28] | VALID(041): [ 50/220] Batch: 0.0404 (0.0663) Data: 0.0265 (0.0538) Loss: 0.7977 (0.8500)
[2022/12/29 00:28] | VALID(041): [100/220] Batch: 0.0344 (0.0524) Data: 0.0297 (0.0397) Loss: 1.1202 (0.8708)
[2022/12/29 00:28] | VALID(041): [150/220] Batch: 0.0370 (0.0477) Data: 0.0232 (0.0349) Loss: 0.7641 (0.8635)
[2022/12/29 00:28] | VALID(041): [200/220] Batch: 0.0395 (0.0455) Data: 0.0239 (0.0322) Loss: 0.5183 (0.8708)
[2022/12/29 00:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:28] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:28] |    VALID(41)      0.8711      0.7347      0.5059      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:28] | ####################################################################################################
[2022/12/29 00:28] | TRAIN(042): [ 50/879] Batch: 0.1038 (0.1534) Data: 0.0190 (0.0492) Loss: 0.7889 (0.8866)
[2022/12/29 00:28] | TRAIN(042): [100/879] Batch: 0.1141 (0.1347) Data: 0.0100 (0.0305) Loss: 0.8499 (0.8780)
[2022/12/29 00:28] | TRAIN(042): [150/879] Batch: 0.1123 (0.1272) Data: 0.0082 (0.0243) Loss: 0.8102 (0.8688)
[2022/12/29 00:28] | TRAIN(042): [200/879] Batch: 0.1065 (0.1241) Data: 0.0137 (0.0212) Loss: 0.9474 (0.8613)
[2022/12/29 00:28] | TRAIN(042): [250/879] Batch: 0.1206 (0.1219) Data: 0.0112 (0.0193) Loss: 0.7590 (0.8606)
[2022/12/29 00:28] | TRAIN(042): [300/879] Batch: 0.1100 (0.1209) Data: 0.0099 (0.0181) Loss: 1.0226 (0.8618)
[2022/12/29 00:28] | TRAIN(042): [350/879] Batch: 0.1267 (0.1203) Data: 0.0121 (0.0173) Loss: 0.7851 (0.8684)
[2022/12/29 00:28] | TRAIN(042): [400/879] Batch: 0.1138 (0.1193) Data: 0.0097 (0.0165) Loss: 0.7439 (0.8663)
[2022/12/29 00:29] | TRAIN(042): [450/879] Batch: 0.1172 (0.1186) Data: 0.0167 (0.0160) Loss: 0.7643 (0.8685)
[2022/12/29 00:29] | TRAIN(042): [500/879] Batch: 0.0995 (0.1183) Data: 0.0100 (0.0156) Loss: 0.8306 (0.8695)
[2022/12/29 00:29] | TRAIN(042): [550/879] Batch: 0.1137 (0.1176) Data: 0.0086 (0.0152) Loss: 0.6676 (0.8707)
[2022/12/29 00:29] | TRAIN(042): [600/879] Batch: 0.1213 (0.1170) Data: 0.0115 (0.0149) Loss: 0.8343 (0.8699)
[2022/12/29 00:29] | TRAIN(042): [650/879] Batch: 0.1060 (0.1165) Data: 0.0099 (0.0146) Loss: 0.9740 (0.8694)
[2022/12/29 00:29] | TRAIN(042): [700/879] Batch: 0.1140 (0.1162) Data: 0.0082 (0.0144) Loss: 0.7359 (0.8687)
[2022/12/29 00:29] | TRAIN(042): [750/879] Batch: 0.1239 (0.1159) Data: 0.0111 (0.0142) Loss: 0.8897 (0.8679)
[2022/12/29 00:29] | TRAIN(042): [800/879] Batch: 0.1176 (0.1158) Data: 0.0109 (0.0141) Loss: 1.2711 (0.8700)
[2022/12/29 00:29] | TRAIN(042): [850/879] Batch: 0.1293 (0.1158) Data: 0.0133 (0.0140) Loss: 1.1231 (0.8692)
[2022/12/29 00:29] | ------------------------------------------------------------
[2022/12/29 00:29] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:29] | ------------------------------------------------------------
[2022/12/29 00:29] |    TRAIN(42)     0:01:41     0:00:12     0:01:29      0.8686
[2022/12/29 00:29] | ------------------------------------------------------------
[2022/12/29 00:29] | VALID(042): [ 50/220] Batch: 0.0381 (0.0685) Data: 0.0290 (0.0557) Loss: 0.7813 (0.8463)
[2022/12/29 00:29] | VALID(042): [100/220] Batch: 0.0395 (0.0535) Data: 0.0242 (0.0397) Loss: 1.1062 (0.8688)
[2022/12/29 00:29] | VALID(042): [150/220] Batch: 0.0407 (0.0486) Data: 0.0236 (0.0343) Loss: 0.7596 (0.8609)
[2022/12/29 00:30] | VALID(042): [200/220] Batch: 0.0384 (0.0460) Data: 0.0268 (0.0317) Loss: 0.5135 (0.8685)
[2022/12/29 00:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:30] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:30] |    VALID(42)      0.8682      0.7347      0.5201      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:30] | ####################################################################################################
[2022/12/29 00:30] | TRAIN(043): [ 50/879] Batch: 0.1231 (0.1523) Data: 0.0122 (0.0519) Loss: 0.7185 (0.9119)
[2022/12/29 00:30] | TRAIN(043): [100/879] Batch: 0.1070 (0.1319) Data: 0.0103 (0.0320) Loss: 1.0201 (0.8916)
[2022/12/29 00:30] | TRAIN(043): [150/879] Batch: 0.1140 (0.1262) Data: 0.0096 (0.0251) Loss: 1.0762 (0.8834)
[2022/12/29 00:30] | TRAIN(043): [200/879] Batch: 0.1161 (0.1222) Data: 0.0095 (0.0217) Loss: 0.7026 (0.8772)
[2022/12/29 00:30] | TRAIN(043): [250/879] Batch: 0.1230 (0.1196) Data: 0.0114 (0.0195) Loss: 0.7314 (0.8706)
[2022/12/29 00:30] | TRAIN(043): [300/879] Batch: 0.1070 (0.1185) Data: 0.0102 (0.0181) Loss: 0.7574 (0.8668)
[2022/12/29 00:30] | TRAIN(043): [350/879] Batch: 0.1220 (0.1172) Data: 0.0100 (0.0170) Loss: 1.2117 (0.8703)
[2022/12/29 00:30] | TRAIN(043): [400/879] Batch: 0.1193 (0.1161) Data: 0.0119 (0.0162) Loss: 0.8597 (0.8698)
[2022/12/29 00:30] | TRAIN(043): [450/879] Batch: 0.1040 (0.1155) Data: 0.0119 (0.0157) Loss: 0.6274 (0.8690)
[2022/12/29 00:31] | TRAIN(043): [500/879] Batch: 0.1159 (0.1155) Data: 0.0102 (0.0152) Loss: 0.8121 (0.8659)
[2022/12/29 00:31] | TRAIN(043): [550/879] Batch: 0.1263 (0.1150) Data: 0.0188 (0.0148) Loss: 0.4149 (0.8680)
[2022/12/29 00:31] | TRAIN(043): [600/879] Batch: 0.1064 (0.1147) Data: 0.0142 (0.0144) Loss: 0.6349 (0.8667)
[2022/12/29 00:31] | TRAIN(043): [650/879] Batch: 0.1252 (0.1143) Data: 0.0115 (0.0141) Loss: 0.7432 (0.8689)
[2022/12/29 00:31] | TRAIN(043): [700/879] Batch: 0.1016 (0.1144) Data: 0.0100 (0.0139) Loss: 0.9279 (0.8686)
[2022/12/29 00:31] | TRAIN(043): [750/879] Batch: 0.1276 (0.1143) Data: 0.0122 (0.0138) Loss: 0.6653 (0.8676)
[2022/12/29 00:31] | TRAIN(043): [800/879] Batch: 0.1066 (0.1141) Data: 0.0106 (0.0136) Loss: 0.8836 (0.8690)
[2022/12/29 00:31] | TRAIN(043): [850/879] Batch: 0.1009 (0.1138) Data: 0.0095 (0.0134) Loss: 0.9315 (0.8691)
[2022/12/29 00:31] | ------------------------------------------------------------
[2022/12/29 00:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:31] | ------------------------------------------------------------
[2022/12/29 00:31] |    TRAIN(43)     0:01:40     0:00:11     0:01:28      0.8685
[2022/12/29 00:31] | ------------------------------------------------------------
[2022/12/29 00:31] | VALID(043): [ 50/220] Batch: 0.0357 (0.0659) Data: 0.0208 (0.0510) Loss: 0.7822 (0.8464)
[2022/12/29 00:31] | VALID(043): [100/220] Batch: 0.0338 (0.0518) Data: 0.0317 (0.0383) Loss: 1.1126 (0.8685)
[2022/12/29 00:31] | VALID(043): [150/220] Batch: 0.0376 (0.0473) Data: 0.0233 (0.0335) Loss: 0.7583 (0.8607)
[2022/12/29 00:31] | VALID(043): [200/220] Batch: 0.0380 (0.0450) Data: 0.0247 (0.0314) Loss: 0.5197 (0.8682)
[2022/12/29 00:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:31] |    VALID(43)      0.8681      0.7347      0.4999      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:31] | ####################################################################################################
[2022/12/29 00:31] | TRAIN(044): [ 50/879] Batch: 0.1067 (0.1479) Data: 0.0100 (0.0484) Loss: 0.9063 (0.8794)
[2022/12/29 00:32] | TRAIN(044): [100/879] Batch: 0.1140 (0.1271) Data: 0.0109 (0.0293) Loss: 1.0532 (0.8591)
[2022/12/29 00:32] | TRAIN(044): [150/879] Batch: 0.1126 (0.1207) Data: 0.0094 (0.0228) Loss: 0.7297 (0.8683)
[2022/12/29 00:32] | TRAIN(044): [200/879] Batch: 0.1043 (0.1179) Data: 0.0104 (0.0198) Loss: 0.9883 (0.8621)
[2022/12/29 00:32] | TRAIN(044): [250/879] Batch: 0.1076 (0.1156) Data: 0.0104 (0.0178) Loss: 0.9119 (0.8668)
[2022/12/29 00:32] | TRAIN(044): [300/879] Batch: 0.1096 (0.1142) Data: 0.0092 (0.0165) Loss: 1.0831 (0.8696)
[2022/12/29 00:32] | TRAIN(044): [350/879] Batch: 0.1111 (0.1133) Data: 0.0106 (0.0156) Loss: 1.0708 (0.8720)
[2022/12/29 00:32] | TRAIN(044): [400/879] Batch: 0.1034 (0.1128) Data: 0.0121 (0.0149) Loss: 0.8225 (0.8679)
[2022/12/29 00:32] | TRAIN(044): [450/879] Batch: 0.1012 (0.1121) Data: 0.0091 (0.0143) Loss: 0.9662 (0.8743)
[2022/12/29 00:32] | TRAIN(044): [500/879] Batch: 0.1005 (0.1116) Data: 0.0096 (0.0139) Loss: 0.8187 (0.8764)
[2022/12/29 00:32] | TRAIN(044): [550/879] Batch: 0.1007 (0.1111) Data: 0.0090 (0.0135) Loss: 0.6780 (0.8710)
[2022/12/29 00:32] | TRAIN(044): [600/879] Batch: 0.1041 (0.1109) Data: 0.0094 (0.0132) Loss: 0.8451 (0.8695)
[2022/12/29 00:33] | TRAIN(044): [650/879] Batch: 0.1193 (0.1106) Data: 0.0113 (0.0130) Loss: 1.2341 (0.8692)
[2022/12/29 00:33] | TRAIN(044): [700/879] Batch: 0.1034 (0.1106) Data: 0.0103 (0.0128) Loss: 0.7463 (0.8683)
[2022/12/29 00:33] | TRAIN(044): [750/879] Batch: 0.1163 (0.1106) Data: 0.0092 (0.0127) Loss: 0.7628 (0.8700)
[2022/12/29 00:33] | TRAIN(044): [800/879] Batch: 0.1133 (0.1105) Data: 0.0092 (0.0125) Loss: 0.8047 (0.8685)
[2022/12/29 00:33] | TRAIN(044): [850/879] Batch: 0.1055 (0.1105) Data: 0.0096 (0.0124) Loss: 1.1404 (0.8703)
[2022/12/29 00:33] | ------------------------------------------------------------
[2022/12/29 00:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:33] | ------------------------------------------------------------
[2022/12/29 00:33] |    TRAIN(44)     0:01:36     0:00:10     0:01:26      0.8685
[2022/12/29 00:33] | ------------------------------------------------------------
[2022/12/29 00:33] | VALID(044): [ 50/220] Batch: 0.0395 (0.0640) Data: 0.0270 (0.0509) Loss: 0.7794 (0.8472)
[2022/12/29 00:33] | VALID(044): [100/220] Batch: 0.0354 (0.0509) Data: 0.0222 (0.0382) Loss: 1.1239 (0.8708)
[2022/12/29 00:33] | VALID(044): [150/220] Batch: 0.0418 (0.0466) Data: 0.0252 (0.0340) Loss: 0.7526 (0.8621)
[2022/12/29 00:33] | VALID(044): [200/220] Batch: 0.0383 (0.0444) Data: 0.0275 (0.0319) Loss: 0.5055 (0.8700)
[2022/12/29 00:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:33] |    VALID(44)      0.8697      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:33] | ####################################################################################################
[2022/12/29 00:33] | TRAIN(045): [ 50/879] Batch: 0.1195 (0.1466) Data: 0.0099 (0.0450) Loss: 1.0445 (0.8990)
[2022/12/29 00:33] | TRAIN(045): [100/879] Batch: 0.1006 (0.1270) Data: 0.0097 (0.0275) Loss: 0.8161 (0.8980)
[2022/12/29 00:33] | TRAIN(045): [150/879] Batch: 0.1014 (0.1222) Data: 0.0094 (0.0219) Loss: 0.7652 (0.8820)
[2022/12/29 00:34] | TRAIN(045): [200/879] Batch: 0.1129 (0.1188) Data: 0.0101 (0.0189) Loss: 0.4818 (0.8801)
[2022/12/29 00:34] | TRAIN(045): [250/879] Batch: 0.1029 (0.1159) Data: 0.0145 (0.0171) Loss: 0.7673 (0.8759)
[2022/12/29 00:34] | TRAIN(045): [300/879] Batch: 0.1129 (0.1143) Data: 0.0095 (0.0158) Loss: 0.5852 (0.8785)
[2022/12/29 00:34] | TRAIN(045): [350/879] Batch: 0.1058 (0.1133) Data: 0.0105 (0.0150) Loss: 0.6293 (0.8740)
[2022/12/29 00:34] | TRAIN(045): [400/879] Batch: 0.1131 (0.1131) Data: 0.0105 (0.0145) Loss: 1.0768 (0.8732)
[2022/12/29 00:34] | TRAIN(045): [450/879] Batch: 0.1227 (0.1128) Data: 0.0095 (0.0141) Loss: 0.9796 (0.8739)
[2022/12/29 00:34] | TRAIN(045): [500/879] Batch: 0.1191 (0.1129) Data: 0.0124 (0.0139) Loss: 0.8177 (0.8701)
[2022/12/29 00:34] | TRAIN(045): [550/879] Batch: 0.1153 (0.1127) Data: 0.0093 (0.0136) Loss: 1.0080 (0.8702)
[2022/12/29 00:34] | TRAIN(045): [600/879] Batch: 0.1007 (0.1125) Data: 0.0105 (0.0134) Loss: 0.8948 (0.8715)
[2022/12/29 00:34] | TRAIN(045): [650/879] Batch: 0.1079 (0.1124) Data: 0.0119 (0.0132) Loss: 1.0960 (0.8700)
[2022/12/29 00:34] | TRAIN(045): [700/879] Batch: 0.1205 (0.1123) Data: 0.0100 (0.0130) Loss: 0.8593 (0.8705)
[2022/12/29 00:35] | TRAIN(045): [750/879] Batch: 0.1172 (0.1125) Data: 0.0102 (0.0129) Loss: 0.6088 (0.8702)
[2022/12/29 00:35] | TRAIN(045): [800/879] Batch: 0.1260 (0.1122) Data: 0.0112 (0.0127) Loss: 0.8379 (0.8692)
[2022/12/29 00:35] | TRAIN(045): [850/879] Batch: 0.1074 (0.1121) Data: 0.0110 (0.0126) Loss: 0.7922 (0.8692)
[2022/12/29 00:35] | ------------------------------------------------------------
[2022/12/29 00:35] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:35] | ------------------------------------------------------------
[2022/12/29 00:35] |    TRAIN(45)     0:01:38     0:00:11     0:01:27      0.8687
[2022/12/29 00:35] | ------------------------------------------------------------
[2022/12/29 00:35] | VALID(045): [ 50/220] Batch: 0.0418 (0.0642) Data: 0.0249 (0.0496) Loss: 0.7831 (0.8464)
[2022/12/29 00:35] | VALID(045): [100/220] Batch: 0.0411 (0.0511) Data: 0.0243 (0.0374) Loss: 1.1105 (0.8686)
[2022/12/29 00:35] | VALID(045): [150/220] Batch: 0.0386 (0.0468) Data: 0.0232 (0.0330) Loss: 0.7585 (0.8607)
[2022/12/29 00:35] | VALID(045): [200/220] Batch: 0.0387 (0.0447) Data: 0.0207 (0.0304) Loss: 0.5182 (0.8682)
[2022/12/29 00:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:35] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:35] |    VALID(45)      0.8680      0.7347      0.4999      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:35] | ####################################################################################################
[2022/12/29 00:35] | TRAIN(046): [ 50/879] Batch: 0.1174 (0.1523) Data: 0.0104 (0.0522) Loss: 0.9321 (0.8519)
[2022/12/29 00:35] | TRAIN(046): [100/879] Batch: 0.1143 (0.1319) Data: 0.0157 (0.0321) Loss: 1.0033 (0.8651)
[2022/12/29 00:35] | TRAIN(046): [150/879] Batch: 0.1264 (0.1247) Data: 0.0101 (0.0250) Loss: 0.6691 (0.8564)
[2022/12/29 00:35] | TRAIN(046): [200/879] Batch: 0.1078 (0.1220) Data: 0.0072 (0.0215) Loss: 1.0007 (0.8554)
[2022/12/29 00:35] | TRAIN(046): [250/879] Batch: 0.1195 (0.1201) Data: 0.0085 (0.0195) Loss: 0.7805 (0.8616)
[2022/12/29 00:36] | TRAIN(046): [300/879] Batch: 0.1016 (0.1198) Data: 0.0096 (0.0181) Loss: 0.6249 (0.8625)
[2022/12/29 00:36] | TRAIN(046): [350/879] Batch: 0.1081 (0.1185) Data: 0.0095 (0.0171) Loss: 1.0088 (0.8628)
[2022/12/29 00:36] | TRAIN(046): [400/879] Batch: 0.1077 (0.1176) Data: 0.0107 (0.0163) Loss: 0.8494 (0.8680)
[2022/12/29 00:36] | TRAIN(046): [450/879] Batch: 0.1065 (0.1170) Data: 0.0099 (0.0157) Loss: 0.7957 (0.8667)
[2022/12/29 00:36] | TRAIN(046): [500/879] Batch: 0.1032 (0.1165) Data: 0.0116 (0.0152) Loss: 0.7934 (0.8662)
[2022/12/29 00:36] | TRAIN(046): [550/879] Batch: 0.1042 (0.1158) Data: 0.0098 (0.0148) Loss: 0.9348 (0.8686)
[2022/12/29 00:36] | TRAIN(046): [600/879] Batch: 0.1157 (0.1155) Data: 0.0104 (0.0145) Loss: 0.8606 (0.8658)
[2022/12/29 00:36] | TRAIN(046): [650/879] Batch: 0.1079 (0.1150) Data: 0.0096 (0.0142) Loss: 0.6105 (0.8655)
[2022/12/29 00:36] | TRAIN(046): [700/879] Batch: 0.1213 (0.1147) Data: 0.0116 (0.0139) Loss: 0.9205 (0.8653)
[2022/12/29 00:36] | TRAIN(046): [750/879] Batch: 0.1117 (0.1143) Data: 0.0111 (0.0137) Loss: 0.8148 (0.8663)
[2022/12/29 00:36] | TRAIN(046): [800/879] Batch: 0.1180 (0.1142) Data: 0.0097 (0.0135) Loss: 0.9702 (0.8657)
[2022/12/29 00:37] | TRAIN(046): [850/879] Batch: 0.1172 (0.1139) Data: 0.0112 (0.0134) Loss: 0.8544 (0.8677)
[2022/12/29 00:37] | ------------------------------------------------------------
[2022/12/29 00:37] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:37] | ------------------------------------------------------------
[2022/12/29 00:37] |    TRAIN(46)     0:01:39     0:00:11     0:01:28      0.8683
[2022/12/29 00:37] | ------------------------------------------------------------
[2022/12/29 00:37] | VALID(046): [ 50/220] Batch: 0.0414 (0.0658) Data: 0.0256 (0.0503) Loss: 0.7888 (0.8473)
[2022/12/29 00:37] | VALID(046): [100/220] Batch: 0.0381 (0.0521) Data: 0.0220 (0.0378) Loss: 1.1026 (0.8688)
[2022/12/29 00:37] | VALID(046): [150/220] Batch: 0.0387 (0.0475) Data: 0.0255 (0.0337) Loss: 0.7625 (0.8611)
[2022/12/29 00:37] | VALID(046): [200/220] Batch: 0.0389 (0.0452) Data: 0.0232 (0.0314) Loss: 0.5217 (0.8684)
[2022/12/29 00:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:37] |    VALID(46)      0.8683      0.7347      0.5005      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:37] | ####################################################################################################
[2022/12/29 00:37] | TRAIN(047): [ 50/879] Batch: 0.1039 (0.1539) Data: 0.0096 (0.0522) Loss: 0.6743 (0.8623)
[2022/12/29 00:37] | TRAIN(047): [100/879] Batch: 0.1100 (0.1329) Data: 0.0179 (0.0321) Loss: 0.8014 (0.8710)
[2022/12/29 00:37] | TRAIN(047): [150/879] Batch: 0.1134 (0.1266) Data: 0.0111 (0.0252) Loss: 0.9474 (0.8643)
[2022/12/29 00:37] | TRAIN(047): [200/879] Batch: 0.1107 (0.1229) Data: 0.0094 (0.0219) Loss: 1.0615 (0.8553)
[2022/12/29 00:37] | TRAIN(047): [250/879] Batch: 0.1309 (0.1210) Data: 0.0144 (0.0200) Loss: 0.8194 (0.8626)
[2022/12/29 00:37] | TRAIN(047): [300/879] Batch: 0.1147 (0.1198) Data: 0.0091 (0.0187) Loss: 0.8172 (0.8646)
[2022/12/29 00:37] | TRAIN(047): [350/879] Batch: 0.1239 (0.1191) Data: 0.0155 (0.0177) Loss: 0.8778 (0.8649)
[2022/12/29 00:38] | TRAIN(047): [400/879] Batch: 0.1083 (0.1180) Data: 0.0113 (0.0169) Loss: 0.9136 (0.8637)
[2022/12/29 00:38] | TRAIN(047): [450/879] Batch: 0.1218 (0.1174) Data: 0.0091 (0.0163) Loss: 1.0874 (0.8655)
[2022/12/29 00:38] | TRAIN(047): [500/879] Batch: 0.1118 (0.1168) Data: 0.0102 (0.0157) Loss: 0.8043 (0.8673)
[2022/12/29 00:38] | TRAIN(047): [550/879] Batch: 0.1088 (0.1164) Data: 0.0143 (0.0153) Loss: 0.9437 (0.8682)
[2022/12/29 00:38] | TRAIN(047): [600/879] Batch: 0.1274 (0.1162) Data: 0.0090 (0.0151) Loss: 0.6786 (0.8660)
[2022/12/29 00:38] | TRAIN(047): [650/879] Batch: 0.1115 (0.1157) Data: 0.0180 (0.0148) Loss: 0.7550 (0.8665)
[2022/12/29 00:38] | TRAIN(047): [700/879] Batch: 0.1047 (0.1154) Data: 0.0130 (0.0145) Loss: 0.9239 (0.8665)
[2022/12/29 00:38] | TRAIN(047): [750/879] Batch: 0.1009 (0.1152) Data: 0.0091 (0.0143) Loss: 0.9757 (0.8672)
[2022/12/29 00:38] | TRAIN(047): [800/879] Batch: 0.1234 (0.1152) Data: 0.0147 (0.0142) Loss: 0.7105 (0.8678)
[2022/12/29 00:38] | TRAIN(047): [850/879] Batch: 0.1044 (0.1151) Data: 0.0122 (0.0140) Loss: 0.6889 (0.8681)
[2022/12/29 00:38] | ------------------------------------------------------------
[2022/12/29 00:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:38] | ------------------------------------------------------------
[2022/12/29 00:38] |    TRAIN(47)     0:01:41     0:00:12     0:01:28      0.8684
[2022/12/29 00:38] | ------------------------------------------------------------
[2022/12/29 00:39] | VALID(047): [ 50/220] Batch: 0.0371 (0.0696) Data: 0.0299 (0.0546) Loss: 0.7874 (0.8470)
[2022/12/29 00:39] | VALID(047): [100/220] Batch: 0.0401 (0.0540) Data: 0.0265 (0.0399) Loss: 1.1077 (0.8686)
[2022/12/29 00:39] | VALID(047): [150/220] Batch: 0.0384 (0.0488) Data: 0.0257 (0.0350) Loss: 0.7605 (0.8609)
[2022/12/29 00:39] | VALID(047): [200/220] Batch: 0.0371 (0.0461) Data: 0.0271 (0.0326) Loss: 0.5230 (0.8682)
[2022/12/29 00:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:39] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:39] |    VALID(47)      0.8682      0.7347      0.4936      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:39] | ####################################################################################################
[2022/12/29 00:39] | TRAIN(048): [ 50/879] Batch: 0.1064 (0.1469) Data: 0.0079 (0.0460) Loss: 0.7668 (0.8337)
[2022/12/29 00:39] | TRAIN(048): [100/879] Batch: 0.1108 (0.1284) Data: 0.0102 (0.0283) Loss: 0.8086 (0.8398)
[2022/12/29 00:39] | TRAIN(048): [150/879] Batch: 0.1134 (0.1209) Data: 0.0090 (0.0223) Loss: 0.9220 (0.8409)
[2022/12/29 00:39] | TRAIN(048): [200/879] Batch: 0.0999 (0.1183) Data: 0.0156 (0.0194) Loss: 0.7212 (0.8414)
[2022/12/29 00:39] | TRAIN(048): [250/879] Batch: 0.1067 (0.1169) Data: 0.0098 (0.0177) Loss: 0.8824 (0.8472)
[2022/12/29 00:39] | TRAIN(048): [300/879] Batch: 0.1075 (0.1159) Data: 0.0098 (0.0165) Loss: 1.3884 (0.8531)
[2022/12/29 00:39] | TRAIN(048): [350/879] Batch: 0.1341 (0.1152) Data: 0.0118 (0.0157) Loss: 0.9529 (0.8521)
[2022/12/29 00:39] | TRAIN(048): [400/879] Batch: 0.1156 (0.1150) Data: 0.0127 (0.0151) Loss: 0.9999 (0.8579)
[2022/12/29 00:40] | TRAIN(048): [450/879] Batch: 0.1059 (0.1145) Data: 0.0097 (0.0146) Loss: 0.6830 (0.8632)
[2022/12/29 00:40] | TRAIN(048): [500/879] Batch: 0.1048 (0.1147) Data: 0.0095 (0.0143) Loss: 0.9199 (0.8672)
[2022/12/29 00:40] | TRAIN(048): [550/879] Batch: 0.1106 (0.1144) Data: 0.0115 (0.0140) Loss: 0.7411 (0.8676)
[2022/12/29 00:40] | TRAIN(048): [600/879] Batch: 0.1019 (0.1143) Data: 0.0146 (0.0138) Loss: 0.9874 (0.8661)
[2022/12/29 00:40] | TRAIN(048): [650/879] Batch: 0.1032 (0.1141) Data: 0.0094 (0.0135) Loss: 0.9345 (0.8687)
[2022/12/29 00:40] | TRAIN(048): [700/879] Batch: 0.1060 (0.1138) Data: 0.0111 (0.0134) Loss: 1.1360 (0.8686)
[2022/12/29 00:40] | TRAIN(048): [750/879] Batch: 0.1138 (0.1137) Data: 0.0104 (0.0132) Loss: 0.8962 (0.8695)
[2022/12/29 00:40] | TRAIN(048): [800/879] Batch: 0.1090 (0.1136) Data: 0.0124 (0.0131) Loss: 0.7468 (0.8693)
[2022/12/29 00:40] | TRAIN(048): [850/879] Batch: 0.1303 (0.1134) Data: 0.0119 (0.0130) Loss: 1.0382 (0.8695)
[2022/12/29 00:40] | ------------------------------------------------------------
[2022/12/29 00:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:40] | ------------------------------------------------------------
[2022/12/29 00:40] |    TRAIN(48)     0:01:39     0:00:11     0:01:28      0.8683
[2022/12/29 00:40] | ------------------------------------------------------------
[2022/12/29 00:40] | VALID(048): [ 50/220] Batch: 0.0388 (0.0656) Data: 0.0273 (0.0529) Loss: 0.7827 (0.8462)
[2022/12/29 00:40] | VALID(048): [100/220] Batch: 0.0428 (0.0519) Data: 0.0226 (0.0387) Loss: 1.1105 (0.8689)
[2022/12/29 00:40] | VALID(048): [150/220] Batch: 0.0405 (0.0475) Data: 0.0220 (0.0334) Loss: 0.7573 (0.8607)
[2022/12/29 00:40] | VALID(048): [200/220] Batch: 0.0287 (0.0452) Data: 0.0235 (0.0307) Loss: 0.5112 (0.8684)
[2022/12/29 00:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:40] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:40] |    VALID(48)      0.8681      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:40] | ####################################################################################################
[2022/12/29 00:41] | TRAIN(049): [ 50/879] Batch: 0.1052 (0.1435) Data: 0.0137 (0.0437) Loss: 1.0165 (0.8926)
[2022/12/29 00:41] | TRAIN(049): [100/879] Batch: 0.1145 (0.1250) Data: 0.0096 (0.0270) Loss: 0.7461 (0.8727)
[2022/12/29 00:41] | TRAIN(049): [150/879] Batch: 0.1151 (0.1205) Data: 0.0083 (0.0216) Loss: 0.8466 (0.8737)
[2022/12/29 00:41] | TRAIN(049): [200/879] Batch: 0.1199 (0.1179) Data: 0.0115 (0.0188) Loss: 0.7633 (0.8775)
[2022/12/29 00:41] | TRAIN(049): [250/879] Batch: 0.1176 (0.1170) Data: 0.0100 (0.0172) Loss: 0.8616 (0.8733)
[2022/12/29 00:41] | TRAIN(049): [300/879] Batch: 0.1050 (0.1169) Data: 0.0094 (0.0163) Loss: 0.8296 (0.8736)
[2022/12/29 00:41] | TRAIN(049): [350/879] Batch: 0.1221 (0.1162) Data: 0.0115 (0.0155) Loss: 0.6232 (0.8715)
[2022/12/29 00:41] | TRAIN(049): [400/879] Batch: 0.1080 (0.1159) Data: 0.0124 (0.0150) Loss: 0.7511 (0.8694)
[2022/12/29 00:41] | TRAIN(049): [450/879] Batch: 0.1116 (0.1151) Data: 0.0101 (0.0145) Loss: 1.0494 (0.8738)
[2022/12/29 00:41] | TRAIN(049): [500/879] Batch: 0.1090 (0.1148) Data: 0.0103 (0.0142) Loss: 0.4958 (0.8698)
[2022/12/29 00:42] | TRAIN(049): [550/879] Batch: 0.1040 (0.1145) Data: 0.0094 (0.0139) Loss: 0.9544 (0.8665)
[2022/12/29 00:42] | TRAIN(049): [600/879] Batch: 0.1196 (0.1141) Data: 0.0101 (0.0136) Loss: 0.6379 (0.8662)
[2022/12/29 00:42] | TRAIN(049): [650/879] Batch: 0.1079 (0.1139) Data: 0.0099 (0.0134) Loss: 1.0694 (0.8661)
[2022/12/29 00:42] | TRAIN(049): [700/879] Batch: 0.1224 (0.1136) Data: 0.0136 (0.0132) Loss: 0.9689 (0.8678)
[2022/12/29 00:42] | TRAIN(049): [750/879] Batch: 0.1212 (0.1136) Data: 0.0088 (0.0131) Loss: 0.6762 (0.8676)
[2022/12/29 00:42] | TRAIN(049): [800/879] Batch: 0.1128 (0.1136) Data: 0.0148 (0.0130) Loss: 1.0017 (0.8704)
[2022/12/29 00:42] | TRAIN(049): [850/879] Batch: 0.1049 (0.1134) Data: 0.0109 (0.0129) Loss: 0.9935 (0.8697)
[2022/12/29 00:42] | ------------------------------------------------------------
[2022/12/29 00:42] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:42] | ------------------------------------------------------------
[2022/12/29 00:42] |    TRAIN(49)     0:01:39     0:00:11     0:01:28      0.8685
[2022/12/29 00:42] | ------------------------------------------------------------
[2022/12/29 00:42] | VALID(049): [ 50/220] Batch: 0.0433 (0.0697) Data: 0.0249 (0.0554) Loss: 0.7860 (0.8466)
[2022/12/29 00:42] | VALID(049): [100/220] Batch: 0.0383 (0.0541) Data: 0.0265 (0.0401) Loss: 1.1111 (0.8687)
[2022/12/29 00:42] | VALID(049): [150/220] Batch: 0.0397 (0.0489) Data: 0.0251 (0.0352) Loss: 0.7588 (0.8609)
[2022/12/29 00:42] | VALID(049): [200/220] Batch: 0.0406 (0.0464) Data: 0.0260 (0.0326) Loss: 0.5112 (0.8684)
[2022/12/29 00:42] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:42] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:42] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:42] |    VALID(49)      0.8683      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:42] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:42] | ####################################################################################################
[2022/12/29 00:42] | TRAIN(050): [ 50/879] Batch: 0.1066 (0.1487) Data: 0.0101 (0.0474) Loss: 0.8547 (0.8391)
[2022/12/29 00:43] | TRAIN(050): [100/879] Batch: 0.1110 (0.1289) Data: 0.0100 (0.0290) Loss: 1.0239 (0.8402)
[2022/12/29 00:43] | TRAIN(050): [150/879] Batch: 0.1109 (0.1220) Data: 0.0127 (0.0228) Loss: 0.8370 (0.8486)
[2022/12/29 00:43] | TRAIN(050): [200/879] Batch: 0.1259 (0.1188) Data: 0.0112 (0.0197) Loss: 1.0362 (0.8562)
[2022/12/29 00:43] | TRAIN(050): [250/879] Batch: 0.1133 (0.1166) Data: 0.0098 (0.0178) Loss: 0.8387 (0.8594)
[2022/12/29 00:43] | TRAIN(050): [300/879] Batch: 0.1137 (0.1157) Data: 0.0092 (0.0166) Loss: 0.5571 (0.8618)
[2022/12/29 00:43] | TRAIN(050): [350/879] Batch: 0.1133 (0.1153) Data: 0.0092 (0.0157) Loss: 0.9161 (0.8620)
[2022/12/29 00:43] | TRAIN(050): [400/879] Batch: 0.1111 (0.1148) Data: 0.0103 (0.0151) Loss: 0.8090 (0.8668)
[2022/12/29 00:43] | TRAIN(050): [450/879] Batch: 0.1033 (0.1143) Data: 0.0095 (0.0145) Loss: 0.5247 (0.8659)
[2022/12/29 00:43] | TRAIN(050): [500/879] Batch: 0.1070 (0.1138) Data: 0.0106 (0.0141) Loss: 1.0616 (0.8661)
[2022/12/29 00:43] | TRAIN(050): [550/879] Batch: 0.1046 (0.1132) Data: 0.0106 (0.0138) Loss: 0.7849 (0.8685)
[2022/12/29 00:43] | TRAIN(050): [600/879] Batch: 0.1076 (0.1128) Data: 0.0100 (0.0135) Loss: 0.6984 (0.8705)
[2022/12/29 00:44] | TRAIN(050): [650/879] Batch: 0.1160 (0.1128) Data: 0.0099 (0.0133) Loss: 0.6314 (0.8676)
[2022/12/29 00:44] | TRAIN(050): [700/879] Batch: 0.1267 (0.1127) Data: 0.0112 (0.0131) Loss: 1.0626 (0.8664)
[2022/12/29 00:44] | TRAIN(050): [750/879] Batch: 0.1145 (0.1128) Data: 0.0108 (0.0130) Loss: 1.0792 (0.8654)
[2022/12/29 00:44] | TRAIN(050): [800/879] Batch: 0.1271 (0.1129) Data: 0.0123 (0.0129) Loss: 0.9836 (0.8647)
[2022/12/29 00:44] | TRAIN(050): [850/879] Batch: 0.1232 (0.1128) Data: 0.0123 (0.0127) Loss: 0.7248 (0.8649)
[2022/12/29 00:44] | ------------------------------------------------------------
[2022/12/29 00:44] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:44] | ------------------------------------------------------------
[2022/12/29 00:44] |    TRAIN(50)     0:01:38     0:00:11     0:01:27      0.8678
[2022/12/29 00:44] | ------------------------------------------------------------
[2022/12/29 00:44] | VALID(050): [ 50/220] Batch: 0.0392 (0.0651) Data: 0.0266 (0.0523) Loss: 0.7942 (0.8515)
[2022/12/29 00:44] | VALID(050): [100/220] Batch: 0.0386 (0.0517) Data: 0.0259 (0.0389) Loss: 1.0961 (0.8713)
[2022/12/29 00:44] | VALID(050): [150/220] Batch: 0.0386 (0.0472) Data: 0.0232 (0.0346) Loss: 0.7723 (0.8645)
[2022/12/29 00:44] | VALID(050): [200/220] Batch: 0.0377 (0.0450) Data: 0.0226 (0.0321) Loss: 0.5538 (0.8713)
[2022/12/29 00:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:44] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:44] |    VALID(50)      0.8713      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:44] | ####################################################################################################
[2022/12/29 00:44] | TRAIN(051): [ 50/879] Batch: 0.1006 (0.1429) Data: 0.0144 (0.0441) Loss: 0.9245 (0.8831)
[2022/12/29 00:44] | TRAIN(051): [100/879] Batch: 0.1095 (0.1280) Data: 0.0122 (0.0279) Loss: 0.9765 (0.8544)
[2022/12/29 00:44] | TRAIN(051): [150/879] Batch: 0.1167 (0.1223) Data: 0.0125 (0.0222) Loss: 0.8644 (0.8544)
[2022/12/29 00:45] | TRAIN(051): [200/879] Batch: 0.1094 (0.1195) Data: 0.0104 (0.0193) Loss: 0.8154 (0.8609)
[2022/12/29 00:45] | TRAIN(051): [250/879] Batch: 0.1075 (0.1180) Data: 0.0101 (0.0177) Loss: 0.9979 (0.8530)
[2022/12/29 00:45] | TRAIN(051): [300/879] Batch: 0.1152 (0.1169) Data: 0.0094 (0.0166) Loss: 1.0039 (0.8602)
[2022/12/29 00:45] | TRAIN(051): [350/879] Batch: 0.1182 (0.1161) Data: 0.0155 (0.0158) Loss: 0.9463 (0.8632)
[2022/12/29 00:45] | TRAIN(051): [400/879] Batch: 0.1057 (0.1156) Data: 0.0090 (0.0152) Loss: 0.6047 (0.8632)
[2022/12/29 00:45] | TRAIN(051): [450/879] Batch: 0.1073 (0.1151) Data: 0.0101 (0.0147) Loss: 0.9466 (0.8632)
[2022/12/29 00:45] | TRAIN(051): [500/879] Batch: 0.1061 (0.1146) Data: 0.0102 (0.0143) Loss: 0.9651 (0.8639)
[2022/12/29 00:45] | TRAIN(051): [550/879] Batch: 0.1280 (0.1142) Data: 0.0167 (0.0141) Loss: 0.9201 (0.8643)
[2022/12/29 00:45] | TRAIN(051): [600/879] Batch: 0.1190 (0.1141) Data: 0.0093 (0.0138) Loss: 0.9375 (0.8671)
[2022/12/29 00:45] | TRAIN(051): [650/879] Batch: 0.1137 (0.1140) Data: 0.0214 (0.0136) Loss: 0.7676 (0.8661)
[2022/12/29 00:45] | TRAIN(051): [700/879] Batch: 0.1058 (0.1140) Data: 0.0100 (0.0134) Loss: 1.0162 (0.8669)
[2022/12/29 00:46] | TRAIN(051): [750/879] Batch: 0.1144 (0.1138) Data: 0.0114 (0.0133) Loss: 0.7767 (0.8664)
[2022/12/29 00:46] | TRAIN(051): [800/879] Batch: 0.1143 (0.1137) Data: 0.0098 (0.0131) Loss: 0.9538 (0.8659)
[2022/12/29 00:46] | TRAIN(051): [850/879] Batch: 0.1082 (0.1136) Data: 0.0104 (0.0130) Loss: 1.0393 (0.8680)
[2022/12/29 00:46] | ------------------------------------------------------------
[2022/12/29 00:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:46] | ------------------------------------------------------------
[2022/12/29 00:46] |    TRAIN(51)     0:01:39     0:00:11     0:01:28      0.8682
[2022/12/29 00:46] | ------------------------------------------------------------
[2022/12/29 00:46] | VALID(051): [ 50/220] Batch: 0.0286 (0.0705) Data: 0.0275 (0.0556) Loss: 0.7821 (0.8469)
[2022/12/29 00:46] | VALID(051): [100/220] Batch: 0.0328 (0.0542) Data: 0.0207 (0.0405) Loss: 1.1020 (0.8696)
[2022/12/29 00:46] | VALID(051): [150/220] Batch: 0.0383 (0.0487) Data: 0.0243 (0.0354) Loss: 0.7605 (0.8615)
[2022/12/29 00:46] | VALID(051): [200/220] Batch: 0.0260 (0.0460) Data: 0.0269 (0.0328) Loss: 0.5153 (0.8692)
[2022/12/29 00:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:46] |    VALID(51)      0.8687      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:46] | ####################################################################################################
[2022/12/29 00:46] | TRAIN(052): [ 50/879] Batch: 0.1216 (0.1502) Data: 0.0112 (0.0478) Loss: 1.1911 (0.8644)
[2022/12/29 00:46] | TRAIN(052): [100/879] Batch: 0.1156 (0.1312) Data: 0.0191 (0.0303) Loss: 0.7289 (0.8669)
[2022/12/29 00:46] | TRAIN(052): [150/879] Batch: 0.1022 (0.1249) Data: 0.0107 (0.0240) Loss: 0.8902 (0.8611)
[2022/12/29 00:46] | TRAIN(052): [200/879] Batch: 0.1086 (0.1197) Data: 0.0083 (0.0207) Loss: 0.7224 (0.8641)
[2022/12/29 00:46] | TRAIN(052): [250/879] Batch: 0.0989 (0.1176) Data: 0.0091 (0.0187) Loss: 0.9471 (0.8714)
[2022/12/29 00:47] | TRAIN(052): [300/879] Batch: 0.1046 (0.1164) Data: 0.0107 (0.0174) Loss: 0.8652 (0.8742)
[2022/12/29 00:47] | TRAIN(052): [350/879] Batch: 0.1005 (0.1156) Data: 0.0099 (0.0165) Loss: 0.7377 (0.8764)
[2022/12/29 00:47] | TRAIN(052): [400/879] Batch: 0.1137 (0.1152) Data: 0.0094 (0.0158) Loss: 0.9419 (0.8755)
[2022/12/29 00:47] | TRAIN(052): [450/879] Batch: 0.1030 (0.1147) Data: 0.0096 (0.0152) Loss: 1.1428 (0.8731)
[2022/12/29 00:47] | TRAIN(052): [500/879] Batch: 0.1197 (0.1142) Data: 0.0108 (0.0148) Loss: 0.7686 (0.8736)
[2022/12/29 00:47] | TRAIN(052): [550/879] Batch: 0.1075 (0.1138) Data: 0.0105 (0.0144) Loss: 0.4768 (0.8709)
[2022/12/29 00:47] | TRAIN(052): [600/879] Batch: 0.1014 (0.1133) Data: 0.0141 (0.0141) Loss: 0.6341 (0.8712)
[2022/12/29 00:47] | TRAIN(052): [650/879] Batch: 0.1139 (0.1130) Data: 0.0096 (0.0139) Loss: 0.9151 (0.8726)
[2022/12/29 00:47] | TRAIN(052): [700/879] Batch: 0.1108 (0.1128) Data: 0.0113 (0.0136) Loss: 0.8920 (0.8701)
[2022/12/29 00:47] | TRAIN(052): [750/879] Batch: 0.1097 (0.1126) Data: 0.0090 (0.0134) Loss: 0.7046 (0.8697)
[2022/12/29 00:47] | TRAIN(052): [800/879] Batch: 0.1216 (0.1125) Data: 0.0118 (0.0133) Loss: 1.0169 (0.8683)
[2022/12/29 00:48] | TRAIN(052): [850/879] Batch: 0.1037 (0.1121) Data: 0.0105 (0.0131) Loss: 0.7337 (0.8680)
[2022/12/29 00:48] | ------------------------------------------------------------
[2022/12/29 00:48] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:48] | ------------------------------------------------------------
[2022/12/29 00:48] |    TRAIN(52)     0:01:38     0:00:11     0:01:27      0.8685
[2022/12/29 00:48] | ------------------------------------------------------------
[2022/12/29 00:48] | VALID(052): [ 50/220] Batch: 0.0393 (0.0642) Data: 0.0238 (0.0489) Loss: 0.7840 (0.8469)
[2022/12/29 00:48] | VALID(052): [100/220] Batch: 0.0377 (0.0510) Data: 0.0264 (0.0370) Loss: 1.1076 (0.8687)
[2022/12/29 00:48] | VALID(052): [150/220] Batch: 0.0371 (0.0466) Data: 0.0260 (0.0327) Loss: 0.7596 (0.8609)
[2022/12/29 00:48] | VALID(052): [200/220] Batch: 0.0391 (0.0445) Data: 0.0229 (0.0305) Loss: 0.5275 (0.8682)
[2022/12/29 00:48] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:48] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:48] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:48] |    VALID(52)      0.8681      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:48] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:48] | ####################################################################################################
[2022/12/29 00:48] | TRAIN(053): [ 50/879] Batch: 0.0992 (0.1420) Data: 0.0089 (0.0452) Loss: 1.0005 (0.8486)
[2022/12/29 00:48] | TRAIN(053): [100/879] Batch: 0.1053 (0.1250) Data: 0.0100 (0.0281) Loss: 1.1598 (0.8637)
[2022/12/29 00:48] | TRAIN(053): [150/879] Batch: 0.1090 (0.1219) Data: 0.0104 (0.0225) Loss: 1.1794 (0.8737)
[2022/12/29 00:48] | TRAIN(053): [200/879] Batch: 0.1191 (0.1202) Data: 0.0094 (0.0195) Loss: 0.8542 (0.8809)
[2022/12/29 00:48] | TRAIN(053): [250/879] Batch: 0.1032 (0.1184) Data: 0.0119 (0.0176) Loss: 0.9213 (0.8838)
[2022/12/29 00:48] | TRAIN(053): [300/879] Batch: 0.1211 (0.1177) Data: 0.0108 (0.0165) Loss: 0.9496 (0.8789)
[2022/12/29 00:48] | TRAIN(053): [350/879] Batch: 0.1209 (0.1173) Data: 0.0104 (0.0157) Loss: 0.7640 (0.8801)
[2022/12/29 00:49] | TRAIN(053): [400/879] Batch: 0.1008 (0.1166) Data: 0.0100 (0.0150) Loss: 1.0163 (0.8775)
[2022/12/29 00:49] | TRAIN(053): [450/879] Batch: 0.1068 (0.1159) Data: 0.0175 (0.0146) Loss: 1.0124 (0.8779)
[2022/12/29 00:49] | TRAIN(053): [500/879] Batch: 0.1006 (0.1155) Data: 0.0158 (0.0142) Loss: 0.6607 (0.8748)
[2022/12/29 00:49] | TRAIN(053): [550/879] Batch: 0.1118 (0.1151) Data: 0.0098 (0.0139) Loss: 0.7331 (0.8757)
[2022/12/29 00:49] | TRAIN(053): [600/879] Batch: 0.1079 (0.1147) Data: 0.0091 (0.0136) Loss: 0.9041 (0.8765)
[2022/12/29 00:49] | TRAIN(053): [650/879] Batch: 0.1110 (0.1144) Data: 0.0111 (0.0133) Loss: 0.9590 (0.8737)
[2022/12/29 00:49] | TRAIN(053): [700/879] Batch: 0.1012 (0.1142) Data: 0.0074 (0.0132) Loss: 0.5612 (0.8718)
[2022/12/29 00:49] | TRAIN(053): [750/879] Batch: 0.1128 (0.1139) Data: 0.0127 (0.0130) Loss: 1.0594 (0.8713)
[2022/12/29 00:49] | TRAIN(053): [800/879] Batch: 0.1062 (0.1138) Data: 0.0142 (0.0129) Loss: 0.9294 (0.8676)
[2022/12/29 00:49] | TRAIN(053): [850/879] Batch: 0.1318 (0.1137) Data: 0.0144 (0.0127) Loss: 1.0255 (0.8671)
[2022/12/29 00:49] | ------------------------------------------------------------
[2022/12/29 00:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:49] | ------------------------------------------------------------
[2022/12/29 00:49] |    TRAIN(53)     0:01:39     0:00:11     0:01:28      0.8679
[2022/12/29 00:49] | ------------------------------------------------------------
[2022/12/29 00:49] | VALID(053): [ 50/220] Batch: 0.0375 (0.0686) Data: 0.0247 (0.0552) Loss: 0.7846 (0.8469)
[2022/12/29 00:50] | VALID(053): [100/220] Batch: 0.0323 (0.0533) Data: 0.0290 (0.0399) Loss: 1.1064 (0.8686)
[2022/12/29 00:50] | VALID(053): [150/220] Batch: 0.0392 (0.0483) Data: 0.0262 (0.0348) Loss: 0.7602 (0.8609)
[2022/12/29 00:50] | VALID(053): [200/220] Batch: 0.0397 (0.0457) Data: 0.0266 (0.0324) Loss: 0.5266 (0.8682)
[2022/12/29 00:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:50] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:50] |    VALID(53)      0.8681      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:50] | ####################################################################################################
[2022/12/29 00:50] | TRAIN(054): [ 50/879] Batch: 0.1097 (0.1450) Data: 0.0119 (0.0464) Loss: 0.8951 (0.8817)
[2022/12/29 00:50] | TRAIN(054): [100/879] Batch: 0.1087 (0.1271) Data: 0.0094 (0.0286) Loss: 0.7161 (0.8815)
[2022/12/29 00:50] | TRAIN(054): [150/879] Batch: 0.1018 (0.1217) Data: 0.0106 (0.0227) Loss: 0.9404 (0.8677)
[2022/12/29 00:50] | TRAIN(054): [200/879] Batch: 0.1021 (0.1183) Data: 0.0101 (0.0196) Loss: 0.6863 (0.8769)
[2022/12/29 00:50] | TRAIN(054): [250/879] Batch: 0.1041 (0.1159) Data: 0.0102 (0.0177) Loss: 0.5637 (0.8791)
[2022/12/29 00:50] | TRAIN(054): [300/879] Batch: 0.1144 (0.1150) Data: 0.0105 (0.0166) Loss: 0.6364 (0.8786)
[2022/12/29 00:50] | TRAIN(054): [350/879] Batch: 0.1003 (0.1139) Data: 0.0130 (0.0156) Loss: 0.7546 (0.8739)
[2022/12/29 00:50] | TRAIN(054): [400/879] Batch: 0.1113 (0.1129) Data: 0.0094 (0.0149) Loss: 0.8364 (0.8705)
[2022/12/29 00:50] | TRAIN(054): [450/879] Batch: 0.1037 (0.1121) Data: 0.0097 (0.0144) Loss: 0.4929 (0.8716)
[2022/12/29 00:51] | TRAIN(054): [500/879] Batch: 0.1143 (0.1119) Data: 0.0105 (0.0140) Loss: 1.0094 (0.8691)
[2022/12/29 00:51] | TRAIN(054): [550/879] Batch: 0.1033 (0.1116) Data: 0.0100 (0.0137) Loss: 0.6279 (0.8663)
[2022/12/29 00:51] | TRAIN(054): [600/879] Batch: 0.1124 (0.1115) Data: 0.0110 (0.0134) Loss: 0.4785 (0.8671)
[2022/12/29 00:51] | TRAIN(054): [650/879] Batch: 0.1135 (0.1113) Data: 0.0105 (0.0131) Loss: 0.6820 (0.8694)
[2022/12/29 00:51] | TRAIN(054): [700/879] Batch: 0.1006 (0.1110) Data: 0.0099 (0.0129) Loss: 0.9444 (0.8672)
[2022/12/29 00:51] | TRAIN(054): [750/879] Batch: 0.1115 (0.1108) Data: 0.0094 (0.0127) Loss: 1.0527 (0.8691)
[2022/12/29 00:51] | TRAIN(054): [800/879] Batch: 0.1075 (0.1109) Data: 0.0117 (0.0126) Loss: 0.7397 (0.8667)
[2022/12/29 00:51] | TRAIN(054): [850/879] Batch: 0.1101 (0.1109) Data: 0.0108 (0.0124) Loss: 1.1848 (0.8664)
[2022/12/29 00:51] | ------------------------------------------------------------
[2022/12/29 00:51] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:51] | ------------------------------------------------------------
[2022/12/29 00:51] |    TRAIN(54)     0:01:37     0:00:10     0:01:26      0.8680
[2022/12/29 00:51] | ------------------------------------------------------------
[2022/12/29 00:51] | VALID(054): [ 50/220] Batch: 0.0386 (0.0644) Data: 0.0289 (0.0509) Loss: 0.7862 (0.8475)
[2022/12/29 00:51] | VALID(054): [100/220] Batch: 0.0334 (0.0514) Data: 0.0325 (0.0378) Loss: 1.1047 (0.8689)
[2022/12/29 00:51] | VALID(054): [150/220] Batch: 0.0393 (0.0471) Data: 0.0279 (0.0340) Loss: 0.7618 (0.8613)
[2022/12/29 00:51] | VALID(054): [200/220] Batch: 0.0372 (0.0449) Data: 0.0278 (0.0320) Loss: 0.5317 (0.8685)
[2022/12/29 00:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:51] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:51] |    VALID(54)      0.8684      0.7347      0.5013      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:51] | ####################################################################################################
[2022/12/29 00:52] | TRAIN(055): [ 50/879] Batch: 0.1052 (0.1463) Data: 0.0150 (0.0465) Loss: 0.8857 (0.8574)
[2022/12/29 00:52] | TRAIN(055): [100/879] Batch: 0.1112 (0.1261) Data: 0.0130 (0.0284) Loss: 1.0021 (0.8847)
[2022/12/29 00:52] | TRAIN(055): [150/879] Batch: 0.1102 (0.1216) Data: 0.0100 (0.0225) Loss: 0.7343 (0.8689)
[2022/12/29 00:52] | TRAIN(055): [200/879] Batch: 0.1172 (0.1170) Data: 0.0100 (0.0194) Loss: 0.7440 (0.8651)
[2022/12/29 00:52] | TRAIN(055): [250/879] Batch: 0.1148 (0.1162) Data: 0.0087 (0.0177) Loss: 0.8829 (0.8713)
[2022/12/29 00:52] | TRAIN(055): [300/879] Batch: 0.1192 (0.1153) Data: 0.0094 (0.0166) Loss: 0.9079 (0.8654)
[2022/12/29 00:52] | TRAIN(055): [350/879] Batch: 0.0999 (0.1148) Data: 0.0164 (0.0158) Loss: 0.6739 (0.8669)
[2022/12/29 00:52] | TRAIN(055): [400/879] Batch: 0.1129 (0.1143) Data: 0.0096 (0.0152) Loss: 0.9338 (0.8663)
[2022/12/29 00:52] | TRAIN(055): [450/879] Batch: 0.1072 (0.1140) Data: 0.0104 (0.0148) Loss: 1.1938 (0.8671)
[2022/12/29 00:52] | TRAIN(055): [500/879] Batch: 0.1118 (0.1135) Data: 0.0096 (0.0143) Loss: 0.8557 (0.8666)
[2022/12/29 00:52] | TRAIN(055): [550/879] Batch: 0.1119 (0.1132) Data: 0.0133 (0.0141) Loss: 0.8028 (0.8676)
[2022/12/29 00:53] | TRAIN(055): [600/879] Batch: 0.1025 (0.1129) Data: 0.0098 (0.0138) Loss: 0.9583 (0.8718)
[2022/12/29 00:53] | TRAIN(055): [650/879] Batch: 0.1181 (0.1126) Data: 0.0130 (0.0135) Loss: 1.1124 (0.8712)
[2022/12/29 00:53] | TRAIN(055): [700/879] Batch: 0.1073 (0.1125) Data: 0.0105 (0.0133) Loss: 0.8821 (0.8700)
[2022/12/29 00:53] | TRAIN(055): [750/879] Batch: 0.1097 (0.1123) Data: 0.0095 (0.0131) Loss: 0.8046 (0.8694)
[2022/12/29 00:53] | TRAIN(055): [800/879] Batch: 0.1196 (0.1122) Data: 0.0123 (0.0129) Loss: 0.7565 (0.8713)
[2022/12/29 00:53] | TRAIN(055): [850/879] Batch: 0.1171 (0.1122) Data: 0.0096 (0.0128) Loss: 0.7937 (0.8690)
[2022/12/29 00:53] | ------------------------------------------------------------
[2022/12/29 00:53] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:53] | ------------------------------------------------------------
[2022/12/29 00:53] |    TRAIN(55)     0:01:38     0:00:11     0:01:27      0.8682
[2022/12/29 00:53] | ------------------------------------------------------------
[2022/12/29 00:53] | VALID(055): [ 50/220] Batch: 0.0366 (0.0689) Data: 0.0313 (0.0548) Loss: 0.7817 (0.8462)
[2022/12/29 00:53] | VALID(055): [100/220] Batch: 0.0365 (0.0536) Data: 0.0237 (0.0399) Loss: 1.1095 (0.8687)
[2022/12/29 00:53] | VALID(055): [150/220] Batch: 0.0372 (0.0484) Data: 0.0280 (0.0349) Loss: 0.7586 (0.8608)
[2022/12/29 00:53] | VALID(055): [200/220] Batch: 0.0410 (0.0459) Data: 0.0282 (0.0325) Loss: 0.5141 (0.8684)
[2022/12/29 00:53] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:53] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:53] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:53] |    VALID(55)      0.8681      0.7347      0.4974      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:53] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:53] | ####################################################################################################
[2022/12/29 00:53] | TRAIN(056): [ 50/879] Batch: 0.1019 (0.1443) Data: 0.0106 (0.0467) Loss: 0.9033 (0.8749)
[2022/12/29 00:53] | TRAIN(056): [100/879] Batch: 0.1042 (0.1262) Data: 0.0101 (0.0286) Loss: 0.8363 (0.8490)
[2022/12/29 00:54] | TRAIN(056): [150/879] Batch: 0.1193 (0.1222) Data: 0.0098 (0.0228) Loss: 0.6492 (0.8579)
[2022/12/29 00:54] | TRAIN(056): [200/879] Batch: 0.1011 (0.1189) Data: 0.0099 (0.0197) Loss: 0.9065 (0.8624)
[2022/12/29 00:54] | TRAIN(056): [250/879] Batch: 0.1194 (0.1170) Data: 0.0114 (0.0179) Loss: 0.8485 (0.8593)
[2022/12/29 00:54] | TRAIN(056): [300/879] Batch: 0.1008 (0.1160) Data: 0.0101 (0.0166) Loss: 0.7032 (0.8511)
[2022/12/29 00:54] | TRAIN(056): [350/879] Batch: 0.1147 (0.1149) Data: 0.0131 (0.0157) Loss: 1.0242 (0.8563)
[2022/12/29 00:54] | TRAIN(056): [400/879] Batch: 0.1019 (0.1138) Data: 0.0096 (0.0150) Loss: 0.9908 (0.8588)
[2022/12/29 00:54] | TRAIN(056): [450/879] Batch: 0.1120 (0.1135) Data: 0.0115 (0.0145) Loss: 0.8397 (0.8574)
[2022/12/29 00:54] | TRAIN(056): [500/879] Batch: 0.1152 (0.1132) Data: 0.0107 (0.0142) Loss: 0.9594 (0.8635)
[2022/12/29 00:54] | TRAIN(056): [550/879] Batch: 0.1175 (0.1129) Data: 0.0100 (0.0139) Loss: 0.7530 (0.8623)
[2022/12/29 00:54] | TRAIN(056): [600/879] Batch: 0.1225 (0.1124) Data: 0.0113 (0.0136) Loss: 1.1094 (0.8637)
[2022/12/29 00:54] | TRAIN(056): [650/879] Batch: 0.1240 (0.1128) Data: 0.0123 (0.0134) Loss: 0.8670 (0.8675)
[2022/12/29 00:55] | TRAIN(056): [700/879] Batch: 0.1321 (0.1125) Data: 0.0152 (0.0132) Loss: 1.1401 (0.8703)
[2022/12/29 00:55] | TRAIN(056): [750/879] Batch: 0.1036 (0.1124) Data: 0.0102 (0.0130) Loss: 0.9135 (0.8696)
[2022/12/29 00:55] | TRAIN(056): [800/879] Batch: 0.1101 (0.1124) Data: 0.0123 (0.0129) Loss: 0.8952 (0.8690)
[2022/12/29 00:55] | TRAIN(056): [850/879] Batch: 0.1092 (0.1124) Data: 0.0095 (0.0128) Loss: 0.9376 (0.8675)
[2022/12/29 00:55] | ------------------------------------------------------------
[2022/12/29 00:55] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:55] | ------------------------------------------------------------
[2022/12/29 00:55] |    TRAIN(56)     0:01:38     0:00:11     0:01:27      0.8680
[2022/12/29 00:55] | ------------------------------------------------------------
[2022/12/29 00:55] | VALID(056): [ 50/220] Batch: 0.0396 (0.0651) Data: 0.0237 (0.0505) Loss: 0.7832 (0.8464)
[2022/12/29 00:55] | VALID(056): [100/220] Batch: 0.0368 (0.0518) Data: 0.0264 (0.0381) Loss: 1.1075 (0.8685)
[2022/12/29 00:55] | VALID(056): [150/220] Batch: 0.0404 (0.0474) Data: 0.0218 (0.0339) Loss: 0.7589 (0.8606)
[2022/12/29 00:55] | VALID(056): [200/220] Batch: 0.0350 (0.0452) Data: 0.0232 (0.0315) Loss: 0.5194 (0.8681)
[2022/12/29 00:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:55] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:55] |    VALID(56)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:55] | ####################################################################################################
[2022/12/29 00:55] | TRAIN(057): [ 50/879] Batch: 0.1066 (0.1468) Data: 0.0114 (0.0490) Loss: 1.1447 (0.8783)
[2022/12/29 00:55] | TRAIN(057): [100/879] Batch: 0.1034 (0.1275) Data: 0.0112 (0.0300) Loss: 0.7794 (0.8817)
[2022/12/29 00:55] | TRAIN(057): [150/879] Batch: 0.1034 (0.1207) Data: 0.0101 (0.0234) Loss: 1.1196 (0.8665)
[2022/12/29 00:55] | TRAIN(057): [200/879] Batch: 0.1073 (0.1176) Data: 0.0097 (0.0201) Loss: 0.8729 (0.8630)
[2022/12/29 00:56] | TRAIN(057): [250/879] Batch: 0.1032 (0.1152) Data: 0.0097 (0.0181) Loss: 0.9777 (0.8572)
[2022/12/29 00:56] | TRAIN(057): [300/879] Batch: 0.1102 (0.1137) Data: 0.0106 (0.0168) Loss: 0.9605 (0.8606)
[2022/12/29 00:56] | TRAIN(057): [350/879] Batch: 0.1031 (0.1130) Data: 0.0104 (0.0159) Loss: 0.7074 (0.8622)
[2022/12/29 00:56] | TRAIN(057): [400/879] Batch: 0.1017 (0.1123) Data: 0.0096 (0.0151) Loss: 0.6793 (0.8599)
[2022/12/29 00:56] | TRAIN(057): [450/879] Batch: 0.1193 (0.1122) Data: 0.0128 (0.0146) Loss: 1.0394 (0.8590)
[2022/12/29 00:56] | TRAIN(057): [500/879] Batch: 0.1228 (0.1123) Data: 0.0123 (0.0142) Loss: 1.0922 (0.8637)
[2022/12/29 00:56] | TRAIN(057): [550/879] Batch: 0.1217 (0.1120) Data: 0.0118 (0.0139) Loss: 0.9695 (0.8649)
[2022/12/29 00:56] | TRAIN(057): [600/879] Batch: 0.1084 (0.1119) Data: 0.0104 (0.0136) Loss: 1.0648 (0.8600)
[2022/12/29 00:56] | TRAIN(057): [650/879] Batch: 0.1058 (0.1119) Data: 0.0103 (0.0134) Loss: 0.8568 (0.8645)
[2022/12/29 00:56] | TRAIN(057): [700/879] Batch: 0.1211 (0.1120) Data: 0.0093 (0.0132) Loss: 0.7746 (0.8700)
[2022/12/29 00:56] | TRAIN(057): [750/879] Batch: 0.1016 (0.1119) Data: 0.0100 (0.0130) Loss: 0.9638 (0.8697)
[2022/12/29 00:57] | TRAIN(057): [800/879] Batch: 0.1066 (0.1118) Data: 0.0102 (0.0129) Loss: 0.7519 (0.8694)
[2022/12/29 00:57] | TRAIN(057): [850/879] Batch: 0.1083 (0.1117) Data: 0.0107 (0.0128) Loss: 0.7778 (0.8697)
[2022/12/29 00:57] | ------------------------------------------------------------
[2022/12/29 00:57] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:57] | ------------------------------------------------------------
[2022/12/29 00:57] |    TRAIN(57)     0:01:38     0:00:11     0:01:26      0.8679
[2022/12/29 00:57] | ------------------------------------------------------------
[2022/12/29 00:57] | VALID(057): [ 50/220] Batch: 0.0391 (0.0646) Data: 0.0268 (0.0514) Loss: 0.7837 (0.8466)
[2022/12/29 00:57] | VALID(057): [100/220] Batch: 0.0394 (0.0515) Data: 0.0243 (0.0387) Loss: 1.1181 (0.8691)
[2022/12/29 00:57] | VALID(057): [150/220] Batch: 0.0375 (0.0471) Data: 0.0277 (0.0343) Loss: 0.7566 (0.8611)
[2022/12/29 00:57] | VALID(057): [200/220] Batch: 0.0431 (0.0450) Data: 0.0256 (0.0322) Loss: 0.5108 (0.8687)
[2022/12/29 00:57] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:57] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:57] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:57] |    VALID(57)      0.8686      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:57] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:57] | ####################################################################################################
[2022/12/29 00:57] | TRAIN(058): [ 50/879] Batch: 0.1164 (0.1452) Data: 0.0106 (0.0485) Loss: 0.6894 (0.9216)
[2022/12/29 00:57] | TRAIN(058): [100/879] Batch: 0.1089 (0.1275) Data: 0.0099 (0.0294) Loss: 0.8140 (0.9139)
[2022/12/29 00:57] | TRAIN(058): [150/879] Batch: 0.1148 (0.1206) Data: 0.0098 (0.0230) Loss: 0.9627 (0.8940)
[2022/12/29 00:57] | TRAIN(058): [200/879] Batch: 0.1148 (0.1177) Data: 0.0093 (0.0198) Loss: 1.0784 (0.8816)
[2022/12/29 00:57] | TRAIN(058): [250/879] Batch: 0.1056 (0.1168) Data: 0.0097 (0.0181) Loss: 0.7468 (0.8756)
[2022/12/29 00:57] | TRAIN(058): [300/879] Batch: 0.1165 (0.1160) Data: 0.0079 (0.0169) Loss: 1.0566 (0.8738)
[2022/12/29 00:57] | TRAIN(058): [350/879] Batch: 0.1030 (0.1153) Data: 0.0100 (0.0160) Loss: 0.9711 (0.8710)
[2022/12/29 00:58] | TRAIN(058): [400/879] Batch: 0.1018 (0.1148) Data: 0.0094 (0.0153) Loss: 0.8070 (0.8676)
[2022/12/29 00:58] | TRAIN(058): [450/879] Batch: 0.1070 (0.1141) Data: 0.0095 (0.0148) Loss: 1.0429 (0.8697)
[2022/12/29 00:58] | TRAIN(058): [500/879] Batch: 0.1033 (0.1135) Data: 0.0104 (0.0143) Loss: 1.0971 (0.8705)
[2022/12/29 00:58] | TRAIN(058): [550/879] Batch: 0.1016 (0.1131) Data: 0.0095 (0.0140) Loss: 0.9214 (0.8722)
[2022/12/29 00:58] | TRAIN(058): [600/879] Batch: 0.1134 (0.1127) Data: 0.0096 (0.0136) Loss: 0.8426 (0.8710)
[2022/12/29 00:58] | TRAIN(058): [650/879] Batch: 0.1109 (0.1124) Data: 0.0115 (0.0134) Loss: 1.2027 (0.8720)
[2022/12/29 00:58] | TRAIN(058): [700/879] Batch: 0.1149 (0.1122) Data: 0.0133 (0.0132) Loss: 0.9215 (0.8694)
[2022/12/29 00:58] | TRAIN(058): [750/879] Batch: 0.1002 (0.1122) Data: 0.0082 (0.0130) Loss: 0.8529 (0.8685)
[2022/12/29 00:58] | TRAIN(058): [800/879] Batch: 0.1127 (0.1127) Data: 0.0096 (0.0130) Loss: 0.8436 (0.8668)
[2022/12/29 00:58] | TRAIN(058): [850/879] Batch: 0.1100 (0.1125) Data: 0.0092 (0.0128) Loss: 0.9248 (0.8660)
[2022/12/29 00:58] | ------------------------------------------------------------
[2022/12/29 00:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:58] | ------------------------------------------------------------
[2022/12/29 00:58] |    TRAIN(58)     0:01:38     0:00:11     0:01:27      0.8680
[2022/12/29 00:58] | ------------------------------------------------------------
[2022/12/29 00:59] | VALID(058): [ 50/220] Batch: 0.0376 (0.0685) Data: 0.0246 (0.0547) Loss: 0.7834 (0.8472)
[2022/12/29 00:59] | VALID(058): [100/220] Batch: 0.0371 (0.0536) Data: 0.0225 (0.0396) Loss: 1.1044 (0.8690)
[2022/12/29 00:59] | VALID(058): [150/220] Batch: 0.0364 (0.0486) Data: 0.0255 (0.0349) Loss: 0.7609 (0.8612)
[2022/12/29 00:59] | VALID(058): [200/220] Batch: 0.0418 (0.0460) Data: 0.0263 (0.0325) Loss: 0.5304 (0.8685)
[2022/12/29 00:59] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:59] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:59] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:59] |    VALID(58)      0.8683      0.7347      0.4991      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:59] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:59] | ####################################################################################################
[2022/12/29 00:59] | TRAIN(059): [ 50/879] Batch: 0.1010 (0.1469) Data: 0.0099 (0.0483) Loss: 1.1099 (0.8129)
[2022/12/29 00:59] | TRAIN(059): [100/879] Batch: 0.1089 (0.1271) Data: 0.0104 (0.0294) Loss: 0.9079 (0.8393)
[2022/12/29 00:59] | TRAIN(059): [150/879] Batch: 0.1115 (0.1210) Data: 0.0099 (0.0230) Loss: 0.5777 (0.8271)
[2022/12/29 00:59] | TRAIN(059): [200/879] Batch: 0.1029 (0.1177) Data: 0.0100 (0.0198) Loss: 0.6838 (0.8432)
[2022/12/29 00:59] | TRAIN(059): [250/879] Batch: 0.1059 (0.1155) Data: 0.0102 (0.0178) Loss: 0.7677 (0.8572)
[2022/12/29 00:59] | TRAIN(059): [300/879] Batch: 0.1103 (0.1139) Data: 0.0113 (0.0166) Loss: 1.0546 (0.8550)
[2022/12/29 00:59] | TRAIN(059): [350/879] Batch: 0.1053 (0.1132) Data: 0.0090 (0.0156) Loss: 0.8377 (0.8575)
[2022/12/29 00:59] | TRAIN(059): [400/879] Batch: 0.1145 (0.1125) Data: 0.0092 (0.0149) Loss: 0.8709 (0.8571)
[2022/12/29 00:59] | TRAIN(059): [450/879] Batch: 0.1135 (0.1116) Data: 0.0096 (0.0143) Loss: 0.7746 (0.8589)
[2022/12/29 01:00] | TRAIN(059): [500/879] Batch: 0.1118 (0.1112) Data: 0.0096 (0.0139) Loss: 0.9726 (0.8585)
[2022/12/29 01:00] | TRAIN(059): [550/879] Batch: 0.1013 (0.1108) Data: 0.0137 (0.0136) Loss: 0.9354 (0.8600)
[2022/12/29 01:00] | TRAIN(059): [600/879] Batch: 0.1428 (0.1110) Data: 0.0145 (0.0134) Loss: 1.1437 (0.8599)
[2022/12/29 01:00] | TRAIN(059): [650/879] Batch: 0.1018 (0.1110) Data: 0.0098 (0.0133) Loss: 0.7434 (0.8627)
[2022/12/29 01:00] | TRAIN(059): [700/879] Batch: 0.1104 (0.1110) Data: 0.0099 (0.0131) Loss: 0.7429 (0.8630)
[2022/12/29 01:00] | TRAIN(059): [750/879] Batch: 0.1005 (0.1109) Data: 0.0095 (0.0130) Loss: 0.7358 (0.8646)
[2022/12/29 01:00] | TRAIN(059): [800/879] Batch: 0.1110 (0.1108) Data: 0.0104 (0.0128) Loss: 0.8271 (0.8673)
[2022/12/29 01:00] | TRAIN(059): [850/879] Batch: 0.1239 (0.1108) Data: 0.0096 (0.0127) Loss: 0.8308 (0.8676)
[2022/12/29 01:00] | ------------------------------------------------------------
[2022/12/29 01:00] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:00] | ------------------------------------------------------------
[2022/12/29 01:00] |    TRAIN(59)     0:01:37     0:00:11     0:01:26      0.8680
[2022/12/29 01:00] | ------------------------------------------------------------
[2022/12/29 01:00] | VALID(059): [ 50/220] Batch: 0.0379 (0.0672) Data: 0.0262 (0.0516) Loss: 0.7809 (0.8464)
[2022/12/29 01:00] | VALID(059): [100/220] Batch: 0.0397 (0.0526) Data: 0.0241 (0.0375) Loss: 1.1077 (0.8687)
[2022/12/29 01:00] | VALID(059): [150/220] Batch: 0.0358 (0.0478) Data: 0.0245 (0.0330) Loss: 0.7585 (0.8607)
[2022/12/29 01:00] | VALID(059): [200/220] Batch: 0.0375 (0.0454) Data: 0.0231 (0.0308) Loss: 0.5226 (0.8682)
[2022/12/29 01:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:00] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:00] |    VALID(59)      0.8680      0.7347      0.5001      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:00] | ####################################################################################################
[2022/12/29 01:01] | TRAIN(060): [ 50/879] Batch: 0.1053 (0.1525) Data: 0.0145 (0.0489) Loss: 0.6024 (0.8623)
[2022/12/29 01:01] | TRAIN(060): [100/879] Batch: 0.1156 (0.1299) Data: 0.0093 (0.0298) Loss: 0.7083 (0.8665)
[2022/12/29 01:01] | TRAIN(060): [150/879] Batch: 0.1258 (0.1229) Data: 0.0117 (0.0235) Loss: 0.8584 (0.8598)
[2022/12/29 01:01] | TRAIN(060): [200/879] Batch: 0.1061 (0.1202) Data: 0.0088 (0.0203) Loss: 0.6356 (0.8612)
[2022/12/29 01:01] | TRAIN(060): [250/879] Batch: 0.1115 (0.1177) Data: 0.0088 (0.0183) Loss: 0.6133 (0.8637)
[2022/12/29 01:01] | TRAIN(060): [300/879] Batch: 0.1016 (0.1162) Data: 0.0097 (0.0170) Loss: 0.7337 (0.8642)
[2022/12/29 01:01] | TRAIN(060): [350/879] Batch: 0.1145 (0.1155) Data: 0.0101 (0.0161) Loss: 0.7822 (0.8628)
[2022/12/29 01:01] | TRAIN(060): [400/879] Batch: 0.1059 (0.1146) Data: 0.0095 (0.0154) Loss: 1.0424 (0.8677)
[2022/12/29 01:01] | TRAIN(060): [450/879] Batch: 0.1005 (0.1139) Data: 0.0105 (0.0148) Loss: 0.6149 (0.8698)
[2022/12/29 01:01] | TRAIN(060): [500/879] Batch: 0.1006 (0.1135) Data: 0.0104 (0.0144) Loss: 0.5737 (0.8694)
[2022/12/29 01:01] | TRAIN(060): [550/879] Batch: 0.1172 (0.1132) Data: 0.0107 (0.0140) Loss: 0.7384 (0.8690)
[2022/12/29 01:02] | TRAIN(060): [600/879] Batch: 0.1036 (0.1127) Data: 0.0091 (0.0137) Loss: 1.0245 (0.8702)
[2022/12/29 01:02] | TRAIN(060): [650/879] Batch: 0.1040 (0.1124) Data: 0.0097 (0.0134) Loss: 0.8614 (0.8695)
[2022/12/29 01:02] | TRAIN(060): [700/879] Batch: 0.1086 (0.1120) Data: 0.0091 (0.0132) Loss: 0.7291 (0.8697)
[2022/12/29 01:02] | TRAIN(060): [750/879] Batch: 0.1170 (0.1117) Data: 0.0097 (0.0130) Loss: 0.9074 (0.8679)
[2022/12/29 01:02] | TRAIN(060): [800/879] Batch: 0.1038 (0.1114) Data: 0.0093 (0.0128) Loss: 0.5728 (0.8660)
[2022/12/29 01:02] | TRAIN(060): [850/879] Batch: 0.1039 (0.1110) Data: 0.0104 (0.0127) Loss: 1.0987 (0.8670)
[2022/12/29 01:02] | ------------------------------------------------------------
[2022/12/29 01:02] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:02] | ------------------------------------------------------------
[2022/12/29 01:02] |    TRAIN(60)     0:01:37     0:00:11     0:01:26      0.8679
[2022/12/29 01:02] | ------------------------------------------------------------
[2022/12/29 01:02] | VALID(060): [ 50/220] Batch: 0.0388 (0.0642) Data: 0.0262 (0.0514) Loss: 0.7865 (0.8468)
[2022/12/29 01:02] | VALID(060): [100/220] Batch: 0.0414 (0.0510) Data: 0.0256 (0.0385) Loss: 1.1059 (0.8686)
[2022/12/29 01:02] | VALID(060): [150/220] Batch: 0.0387 (0.0466) Data: 0.0261 (0.0342) Loss: 0.7614 (0.8609)
[2022/12/29 01:02] | VALID(060): [200/220] Batch: 0.0391 (0.0445) Data: 0.0232 (0.0315) Loss: 0.5177 (0.8684)
[2022/12/29 01:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:02] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:02] |    VALID(60)      0.8682      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:02] | ####################################################################################################
[2022/12/29 01:02] | TRAIN(061): [ 50/879] Batch: 0.1234 (0.1540) Data: 0.0109 (0.0470) Loss: 1.0351 (0.8562)
[2022/12/29 01:02] | TRAIN(061): [100/879] Batch: 0.1088 (0.1356) Data: 0.0098 (0.0295) Loss: 0.9474 (0.8566)
[2022/12/29 01:03] | TRAIN(061): [150/879] Batch: 0.1246 (0.1268) Data: 0.0119 (0.0232) Loss: 1.1713 (0.8654)
[2022/12/29 01:03] | TRAIN(061): [200/879] Batch: 0.1096 (0.1241) Data: 0.0098 (0.0203) Loss: 1.1898 (0.8607)
[2022/12/29 01:03] | TRAIN(061): [250/879] Batch: 0.1131 (0.1206) Data: 0.0096 (0.0183) Loss: 0.6154 (0.8606)
[2022/12/29 01:03] | TRAIN(061): [300/879] Batch: 0.1148 (0.1184) Data: 0.0116 (0.0169) Loss: 0.8250 (0.8611)
[2022/12/29 01:03] | TRAIN(061): [350/879] Batch: 0.1225 (0.1171) Data: 0.0121 (0.0160) Loss: 0.9002 (0.8613)
[2022/12/29 01:03] | TRAIN(061): [400/879] Batch: 0.1015 (0.1160) Data: 0.0095 (0.0153) Loss: 1.1662 (0.8558)
[2022/12/29 01:03] | TRAIN(061): [450/879] Batch: 0.1058 (0.1154) Data: 0.0104 (0.0148) Loss: 0.8385 (0.8573)
[2022/12/29 01:03] | TRAIN(061): [500/879] Batch: 0.1250 (0.1146) Data: 0.0122 (0.0144) Loss: 0.8908 (0.8592)
[2022/12/29 01:03] | TRAIN(061): [550/879] Batch: 0.1134 (0.1141) Data: 0.0096 (0.0140) Loss: 1.2604 (0.8598)
[2022/12/29 01:03] | TRAIN(061): [600/879] Batch: 0.1128 (0.1136) Data: 0.0099 (0.0137) Loss: 0.9188 (0.8623)
[2022/12/29 01:03] | TRAIN(061): [650/879] Batch: 0.1056 (0.1130) Data: 0.0096 (0.0134) Loss: 0.8474 (0.8635)
[2022/12/29 01:04] | TRAIN(061): [700/879] Batch: 0.1145 (0.1125) Data: 0.0088 (0.0132) Loss: 0.5808 (0.8655)
[2022/12/29 01:04] | TRAIN(061): [750/879] Batch: 0.1014 (0.1123) Data: 0.0164 (0.0130) Loss: 0.7388 (0.8636)
[2022/12/29 01:04] | TRAIN(061): [800/879] Batch: 0.1151 (0.1124) Data: 0.0097 (0.0128) Loss: 0.9793 (0.8657)
[2022/12/29 01:04] | TRAIN(061): [850/879] Batch: 0.1152 (0.1124) Data: 0.0109 (0.0127) Loss: 1.0199 (0.8681)
[2022/12/29 01:04] | ------------------------------------------------------------
[2022/12/29 01:04] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:04] | ------------------------------------------------------------
[2022/12/29 01:04] |    TRAIN(61)     0:01:38     0:00:11     0:01:27      0.8680
[2022/12/29 01:04] | ------------------------------------------------------------
[2022/12/29 01:04] | VALID(061): [ 50/220] Batch: 0.0373 (0.0692) Data: 0.0283 (0.0563) Loss: 0.7817 (0.8466)
[2022/12/29 01:04] | VALID(061): [100/220] Batch: 0.0339 (0.0537) Data: 0.0229 (0.0405) Loss: 1.1087 (0.8687)
[2022/12/29 01:04] | VALID(061): [150/220] Batch: 0.0376 (0.0485) Data: 0.0243 (0.0347) Loss: 0.7590 (0.8608)
[2022/12/29 01:04] | VALID(061): [200/220] Batch: 0.0384 (0.0460) Data: 0.0250 (0.0318) Loss: 0.5253 (0.8682)
[2022/12/29 01:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:04] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:04] |    VALID(61)      0.8681      0.7347      0.4950      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:04] | ####################################################################################################
[2022/12/29 01:04] | TRAIN(062): [ 50/879] Batch: 0.1223 (0.1427) Data: 0.0173 (0.0466) Loss: 1.0687 (0.8861)
[2022/12/29 01:04] | TRAIN(062): [100/879] Batch: 0.1188 (0.1258) Data: 0.0115 (0.0289) Loss: 0.5741 (0.8561)
[2022/12/29 01:04] | TRAIN(062): [150/879] Batch: 0.1012 (0.1217) Data: 0.0098 (0.0230) Loss: 0.7193 (0.8445)
[2022/12/29 01:04] | TRAIN(062): [200/879] Batch: 0.1063 (0.1180) Data: 0.0100 (0.0197) Loss: 0.9572 (0.8634)
[2022/12/29 01:05] | TRAIN(062): [250/879] Batch: 0.1034 (0.1159) Data: 0.0108 (0.0178) Loss: 0.7975 (0.8642)
[2022/12/29 01:05] | TRAIN(062): [300/879] Batch: 0.1097 (0.1157) Data: 0.0107 (0.0166) Loss: 0.9202 (0.8651)
[2022/12/29 01:05] | TRAIN(062): [350/879] Batch: 0.1200 (0.1154) Data: 0.0119 (0.0158) Loss: 0.6805 (0.8666)
[2022/12/29 01:05] | TRAIN(062): [400/879] Batch: 0.1110 (0.1149) Data: 0.0092 (0.0152) Loss: 0.9933 (0.8731)
[2022/12/29 01:05] | TRAIN(062): [450/879] Batch: 0.1093 (0.1141) Data: 0.0099 (0.0146) Loss: 0.7147 (0.8704)
[2022/12/29 01:05] | TRAIN(062): [500/879] Batch: 0.1138 (0.1139) Data: 0.0096 (0.0141) Loss: 0.7073 (0.8665)
[2022/12/29 01:05] | TRAIN(062): [550/879] Batch: 0.1159 (0.1135) Data: 0.0091 (0.0138) Loss: 0.6825 (0.8657)
[2022/12/29 01:05] | TRAIN(062): [600/879] Batch: 0.1011 (0.1134) Data: 0.0090 (0.0135) Loss: 0.5747 (0.8646)
[2022/12/29 01:05] | TRAIN(062): [650/879] Batch: 0.1055 (0.1133) Data: 0.0093 (0.0133) Loss: 0.7308 (0.8666)
[2022/12/29 01:05] | TRAIN(062): [700/879] Batch: 0.1107 (0.1131) Data: 0.0108 (0.0131) Loss: 1.0967 (0.8673)
[2022/12/29 01:05] | TRAIN(062): [750/879] Batch: 0.1001 (0.1126) Data: 0.0111 (0.0129) Loss: 1.0154 (0.8665)
[2022/12/29 01:06] | TRAIN(062): [800/879] Batch: 0.1016 (0.1123) Data: 0.0088 (0.0127) Loss: 1.0295 (0.8667)
[2022/12/29 01:06] | TRAIN(062): [850/879] Batch: 0.1104 (0.1122) Data: 0.0096 (0.0126) Loss: 0.5445 (0.8687)
[2022/12/29 01:06] | ------------------------------------------------------------
[2022/12/29 01:06] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:06] | ------------------------------------------------------------
[2022/12/29 01:06] |    TRAIN(62)     0:01:38     0:00:10     0:01:27      0.8679
[2022/12/29 01:06] | ------------------------------------------------------------
[2022/12/29 01:06] | VALID(062): [ 50/220] Batch: 0.0390 (0.0709) Data: 0.0269 (0.0575) Loss: 0.7816 (0.8465)
[2022/12/29 01:06] | VALID(062): [100/220] Batch: 0.0401 (0.0545) Data: 0.0275 (0.0417) Loss: 1.1096 (0.8694)
[2022/12/29 01:06] | VALID(062): [150/220] Batch: 0.0379 (0.0491) Data: 0.0206 (0.0355) Loss: 0.7574 (0.8612)
[2022/12/29 01:06] | VALID(062): [200/220] Batch: 0.0376 (0.0463) Data: 0.0258 (0.0325) Loss: 0.5094 (0.8689)
[2022/12/29 01:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:06] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:06] |    VALID(62)      0.8685      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:06] | ####################################################################################################
[2022/12/29 01:06] | TRAIN(063): [ 50/879] Batch: 0.1081 (0.1438) Data: 0.0096 (0.0442) Loss: 0.9956 (0.8951)
[2022/12/29 01:06] | TRAIN(063): [100/879] Batch: 0.1114 (0.1283) Data: 0.0123 (0.0280) Loss: 1.2363 (0.8928)
[2022/12/29 01:06] | TRAIN(063): [150/879] Batch: 0.1241 (0.1238) Data: 0.0116 (0.0226) Loss: 1.0299 (0.8901)
[2022/12/29 01:06] | TRAIN(063): [200/879] Batch: 0.1244 (0.1209) Data: 0.0117 (0.0197) Loss: 0.9312 (0.9017)
[2022/12/29 01:06] | TRAIN(063): [250/879] Batch: 0.1063 (0.1192) Data: 0.0092 (0.0180) Loss: 0.8882 (0.8930)
[2022/12/29 01:06] | TRAIN(063): [300/879] Batch: 0.1015 (0.1179) Data: 0.0123 (0.0168) Loss: 0.9368 (0.8865)
[2022/12/29 01:07] | TRAIN(063): [350/879] Batch: 0.1161 (0.1169) Data: 0.0151 (0.0159) Loss: 1.3519 (0.8859)
[2022/12/29 01:07] | TRAIN(063): [400/879] Batch: 0.1183 (0.1165) Data: 0.0105 (0.0154) Loss: 1.1657 (0.8831)
[2022/12/29 01:07] | TRAIN(063): [450/879] Batch: 0.1129 (0.1160) Data: 0.0109 (0.0149) Loss: 0.8489 (0.8789)
[2022/12/29 01:07] | TRAIN(063): [500/879] Batch: 0.1194 (0.1157) Data: 0.0102 (0.0145) Loss: 1.1540 (0.8764)
[2022/12/29 01:07] | TRAIN(063): [550/879] Batch: 0.1003 (0.1153) Data: 0.0112 (0.0142) Loss: 0.9297 (0.8744)
[2022/12/29 01:07] | TRAIN(063): [600/879] Batch: 0.1243 (0.1146) Data: 0.0115 (0.0139) Loss: 0.9965 (0.8748)
[2022/12/29 01:07] | TRAIN(063): [650/879] Batch: 0.1055 (0.1143) Data: 0.0116 (0.0137) Loss: 0.8608 (0.8725)
[2022/12/29 01:07] | TRAIN(063): [700/879] Batch: 0.1005 (0.1140) Data: 0.0168 (0.0135) Loss: 0.6751 (0.8708)
[2022/12/29 01:07] | TRAIN(063): [750/879] Batch: 0.0990 (0.1138) Data: 0.0122 (0.0133) Loss: 1.0077 (0.8693)
[2022/12/29 01:07] | TRAIN(063): [800/879] Batch: 0.1053 (0.1139) Data: 0.0090 (0.0132) Loss: 1.0374 (0.8693)
[2022/12/29 01:07] | TRAIN(063): [850/879] Batch: 0.1183 (0.1136) Data: 0.0115 (0.0131) Loss: 0.5527 (0.8685)
[2022/12/29 01:08] | ------------------------------------------------------------
[2022/12/29 01:08] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:08] | ------------------------------------------------------------
[2022/12/29 01:08] |    TRAIN(63)     0:01:39     0:00:11     0:01:28      0.8680
[2022/12/29 01:08] | ------------------------------------------------------------
[2022/12/29 01:08] | VALID(063): [ 50/220] Batch: 0.0382 (0.0648) Data: 0.0294 (0.0519) Loss: 0.7813 (0.8463)
[2022/12/29 01:08] | VALID(063): [100/220] Batch: 0.0408 (0.0516) Data: 0.0225 (0.0385) Loss: 1.1169 (0.8692)
[2022/12/29 01:08] | VALID(063): [150/220] Batch: 0.0391 (0.0471) Data: 0.0273 (0.0342) Loss: 0.7557 (0.8610)
[2022/12/29 01:08] | VALID(063): [200/220] Batch: 0.0381 (0.0449) Data: 0.0276 (0.0321) Loss: 0.5081 (0.8687)
[2022/12/29 01:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:08] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:08] |    VALID(63)      0.8685      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:08] | ####################################################################################################
[2022/12/29 01:08] | TRAIN(064): [ 50/879] Batch: 0.1105 (0.1482) Data: 0.0122 (0.0473) Loss: 0.6782 (0.8419)
[2022/12/29 01:08] | TRAIN(064): [100/879] Batch: 0.1026 (0.1283) Data: 0.0093 (0.0289) Loss: 0.6348 (0.8575)
[2022/12/29 01:08] | TRAIN(064): [150/879] Batch: 0.1047 (0.1206) Data: 0.0104 (0.0227) Loss: 0.7151 (0.8584)
[2022/12/29 01:08] | TRAIN(064): [200/879] Batch: 0.1141 (0.1182) Data: 0.0106 (0.0198) Loss: 0.9679 (0.8679)
[2022/12/29 01:08] | TRAIN(064): [250/879] Batch: 0.1120 (0.1163) Data: 0.0088 (0.0179) Loss: 0.7420 (0.8649)
[2022/12/29 01:08] | TRAIN(064): [300/879] Batch: 0.1200 (0.1153) Data: 0.0116 (0.0166) Loss: 0.8458 (0.8690)
[2022/12/29 01:08] | TRAIN(064): [350/879] Batch: 0.1000 (0.1144) Data: 0.0097 (0.0157) Loss: 1.1500 (0.8678)
[2022/12/29 01:08] | TRAIN(064): [400/879] Batch: 0.1064 (0.1134) Data: 0.0107 (0.0151) Loss: 1.2896 (0.8696)
[2022/12/29 01:09] | TRAIN(064): [450/879] Batch: 0.1111 (0.1130) Data: 0.0088 (0.0146) Loss: 0.6968 (0.8693)
[2022/12/29 01:09] | TRAIN(064): [500/879] Batch: 0.1019 (0.1125) Data: 0.0112 (0.0141) Loss: 0.5835 (0.8651)
[2022/12/29 01:09] | TRAIN(064): [550/879] Batch: 0.1014 (0.1118) Data: 0.0096 (0.0137) Loss: 1.1445 (0.8673)
[2022/12/29 01:09] | TRAIN(064): [600/879] Batch: 0.1145 (0.1119) Data: 0.0099 (0.0135) Loss: 0.6255 (0.8651)
[2022/12/29 01:09] | TRAIN(064): [650/879] Batch: 0.1044 (0.1115) Data: 0.0096 (0.0133) Loss: 0.7036 (0.8670)
[2022/12/29 01:09] | TRAIN(064): [700/879] Batch: 0.1056 (0.1116) Data: 0.0095 (0.0131) Loss: 1.1337 (0.8680)
[2022/12/29 01:09] | TRAIN(064): [750/879] Batch: 0.0999 (0.1114) Data: 0.0097 (0.0129) Loss: 1.1271 (0.8684)
[2022/12/29 01:09] | TRAIN(064): [800/879] Batch: 0.1183 (0.1113) Data: 0.0117 (0.0128) Loss: 1.2525 (0.8693)
[2022/12/29 01:09] | TRAIN(064): [850/879] Batch: 0.1238 (0.1112) Data: 0.0100 (0.0126) Loss: 0.9389 (0.8692)
[2022/12/29 01:09] | ------------------------------------------------------------
[2022/12/29 01:09] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:09] | ------------------------------------------------------------
[2022/12/29 01:09] |    TRAIN(64)     0:01:37     0:00:11     0:01:26      0.8679
[2022/12/29 01:09] | ------------------------------------------------------------
[2022/12/29 01:09] | VALID(064): [ 50/220] Batch: 0.0395 (0.0677) Data: 0.0253 (0.0541) Loss: 0.7823 (0.8464)
[2022/12/29 01:09] | VALID(064): [100/220] Batch: 0.0356 (0.0530) Data: 0.0226 (0.0387) Loss: 1.1112 (0.8687)
[2022/12/29 01:09] | VALID(064): [150/220] Batch: 0.0405 (0.0482) Data: 0.0259 (0.0344) Loss: 0.7576 (0.8607)
[2022/12/29 01:09] | VALID(064): [200/220] Batch: 0.0425 (0.0458) Data: 0.0233 (0.0318) Loss: 0.5184 (0.8682)
[2022/12/29 01:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:09] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:09] |    VALID(64)      0.8680      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:09] | ####################################################################################################
[2022/12/29 01:10] | TRAIN(065): [ 50/879] Batch: 0.1070 (0.1489) Data: 0.0109 (0.0473) Loss: 0.6602 (0.8773)
[2022/12/29 01:10] | TRAIN(065): [100/879] Batch: 0.1153 (0.1312) Data: 0.0124 (0.0295) Loss: 0.9154 (0.8572)
[2022/12/29 01:10] | TRAIN(065): [150/879] Batch: 0.1125 (0.1253) Data: 0.0102 (0.0235) Loss: 1.1790 (0.8602)
[2022/12/29 01:10] | TRAIN(065): [200/879] Batch: 0.1155 (0.1213) Data: 0.0102 (0.0201) Loss: 0.7357 (0.8642)
[2022/12/29 01:10] | TRAIN(065): [250/879] Batch: 0.1008 (0.1188) Data: 0.0106 (0.0182) Loss: 0.6675 (0.8643)
[2022/12/29 01:10] | TRAIN(065): [300/879] Batch: 0.1058 (0.1180) Data: 0.0106 (0.0171) Loss: 0.8032 (0.8606)
[2022/12/29 01:10] | TRAIN(065): [350/879] Batch: 0.1007 (0.1173) Data: 0.0139 (0.0163) Loss: 1.2671 (0.8671)
[2022/12/29 01:10] | TRAIN(065): [400/879] Batch: 0.1141 (0.1161) Data: 0.0094 (0.0156) Loss: 0.9878 (0.8616)
[2022/12/29 01:10] | TRAIN(065): [450/879] Batch: 0.1026 (0.1157) Data: 0.0095 (0.0151) Loss: 0.9276 (0.8624)
[2022/12/29 01:10] | TRAIN(065): [500/879] Batch: 0.1038 (0.1148) Data: 0.0101 (0.0146) Loss: 0.9137 (0.8629)
[2022/12/29 01:11] | TRAIN(065): [550/879] Batch: 0.1076 (0.1145) Data: 0.0090 (0.0142) Loss: 0.5070 (0.8636)
[2022/12/29 01:11] | TRAIN(065): [600/879] Batch: 0.1030 (0.1140) Data: 0.0102 (0.0139) Loss: 0.7804 (0.8661)
[2022/12/29 01:11] | TRAIN(065): [650/879] Batch: 0.1057 (0.1134) Data: 0.0095 (0.0136) Loss: 1.0294 (0.8661)
[2022/12/29 01:11] | TRAIN(065): [700/879] Batch: 0.1228 (0.1135) Data: 0.0098 (0.0133) Loss: 1.0174 (0.8680)
[2022/12/29 01:11] | TRAIN(065): [750/879] Batch: 0.1093 (0.1135) Data: 0.0096 (0.0131) Loss: 1.1790 (0.8696)
[2022/12/29 01:11] | TRAIN(065): [800/879] Batch: 0.1124 (0.1138) Data: 0.0120 (0.0130) Loss: 1.0019 (0.8700)
[2022/12/29 01:11] | TRAIN(065): [850/879] Batch: 0.1187 (0.1137) Data: 0.0116 (0.0129) Loss: 0.7427 (0.8686)
[2022/12/29 01:11] | ------------------------------------------------------------
[2022/12/29 01:11] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:11] | ------------------------------------------------------------
[2022/12/29 01:11] |    TRAIN(65)     0:01:39     0:00:11     0:01:28      0.8678
[2022/12/29 01:11] | ------------------------------------------------------------
[2022/12/29 01:11] | VALID(065): [ 50/220] Batch: 0.0399 (0.0647) Data: 0.0247 (0.0483) Loss: 0.7820 (0.8461)
[2022/12/29 01:11] | VALID(065): [100/220] Batch: 0.0444 (0.0516) Data: 0.0223 (0.0366) Loss: 1.1104 (0.8684)
[2022/12/29 01:11] | VALID(065): [150/220] Batch: 0.0383 (0.0473) Data: 0.0216 (0.0325) Loss: 0.7580 (0.8605)
[2022/12/29 01:11] | VALID(065): [200/220] Batch: 0.0389 (0.0451) Data: 0.0259 (0.0306) Loss: 0.5163 (0.8680)
[2022/12/29 01:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:11] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:11] |    VALID(65)      0.8678      0.7347      0.4967      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:11] | ####################################################################################################
[2022/12/29 01:11] | TRAIN(066): [ 50/879] Batch: 0.1129 (0.1459) Data: 0.0098 (0.0463) Loss: 1.1723 (0.8548)
[2022/12/29 01:12] | TRAIN(066): [100/879] Batch: 0.1119 (0.1271) Data: 0.0108 (0.0285) Loss: 0.7523 (0.8623)
[2022/12/29 01:12] | TRAIN(066): [150/879] Batch: 0.1066 (0.1201) Data: 0.0103 (0.0224) Loss: 0.9564 (0.8638)
[2022/12/29 01:12] | TRAIN(066): [200/879] Batch: 0.1088 (0.1167) Data: 0.0096 (0.0193) Loss: 1.0403 (0.8572)
[2022/12/29 01:12] | TRAIN(066): [250/879] Batch: 0.1212 (0.1147) Data: 0.0118 (0.0174) Loss: 1.1284 (0.8699)
[2022/12/29 01:12] | TRAIN(066): [300/879] Batch: 0.1067 (0.1138) Data: 0.0108 (0.0162) Loss: 0.8722 (0.8645)
[2022/12/29 01:12] | TRAIN(066): [350/879] Batch: 0.1082 (0.1129) Data: 0.0104 (0.0153) Loss: 0.7013 (0.8668)
[2022/12/29 01:12] | TRAIN(066): [400/879] Batch: 0.1100 (0.1127) Data: 0.0077 (0.0147) Loss: 0.6635 (0.8663)
[2022/12/29 01:12] | TRAIN(066): [450/879] Batch: 0.1005 (0.1123) Data: 0.0100 (0.0142) Loss: 0.6642 (0.8714)
[2022/12/29 01:12] | TRAIN(066): [500/879] Batch: 0.1122 (0.1119) Data: 0.0083 (0.0138) Loss: 1.0368 (0.8722)
[2022/12/29 01:12] | TRAIN(066): [550/879] Batch: 0.1150 (0.1114) Data: 0.0109 (0.0134) Loss: 0.5847 (0.8712)
[2022/12/29 01:12] | TRAIN(066): [600/879] Batch: 0.1104 (0.1113) Data: 0.0098 (0.0132) Loss: 0.8170 (0.8683)
[2022/12/29 01:13] | TRAIN(066): [650/879] Batch: 0.1085 (0.1113) Data: 0.0099 (0.0130) Loss: 1.0133 (0.8696)
[2022/12/29 01:13] | TRAIN(066): [700/879] Batch: 0.1251 (0.1113) Data: 0.0112 (0.0128) Loss: 0.9641 (0.8689)
[2022/12/29 01:13] | TRAIN(066): [750/879] Batch: 0.1003 (0.1113) Data: 0.0103 (0.0127) Loss: 0.6730 (0.8666)
[2022/12/29 01:13] | TRAIN(066): [800/879] Batch: 0.1148 (0.1111) Data: 0.0099 (0.0125) Loss: 0.6594 (0.8673)
[2022/12/29 01:13] | TRAIN(066): [850/879] Batch: 0.1096 (0.1110) Data: 0.0096 (0.0124) Loss: 0.9097 (0.8675)
[2022/12/29 01:13] | ------------------------------------------------------------
[2022/12/29 01:13] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:13] | ------------------------------------------------------------
[2022/12/29 01:13] |    TRAIN(66)     0:01:37     0:00:10     0:01:26      0.8678
[2022/12/29 01:13] | ------------------------------------------------------------
[2022/12/29 01:13] | VALID(066): [ 50/220] Batch: 0.0429 (0.0675) Data: 0.0239 (0.0506) Loss: 0.7829 (0.8463)
[2022/12/29 01:13] | VALID(066): [100/220] Batch: 0.0376 (0.0529) Data: 0.0273 (0.0381) Loss: 1.1054 (0.8684)
[2022/12/29 01:13] | VALID(066): [150/220] Batch: 0.0447 (0.0481) Data: 0.0241 (0.0340) Loss: 0.7598 (0.8606)
[2022/12/29 01:13] | VALID(066): [200/220] Batch: 0.0346 (0.0457) Data: 0.0289 (0.0319) Loss: 0.5191 (0.8681)
[2022/12/29 01:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:13] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:13] |    VALID(66)      0.8678      0.7347      0.4999      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:13] | ####################################################################################################
[2022/12/29 01:13] | TRAIN(067): [ 50/879] Batch: 0.1183 (0.1541) Data: 0.0110 (0.0536) Loss: 0.8560 (0.8754)
[2022/12/29 01:13] | TRAIN(067): [100/879] Batch: 0.1268 (0.1327) Data: 0.0103 (0.0326) Loss: 0.7128 (0.8710)
[2022/12/29 01:13] | TRAIN(067): [150/879] Batch: 0.1048 (0.1268) Data: 0.0089 (0.0258) Loss: 0.8332 (0.8687)
[2022/12/29 01:14] | TRAIN(067): [200/879] Batch: 0.1006 (0.1232) Data: 0.0108 (0.0220) Loss: 0.9687 (0.8686)
[2022/12/29 01:14] | TRAIN(067): [250/879] Batch: 0.1116 (0.1216) Data: 0.0106 (0.0201) Loss: 0.7700 (0.8807)
[2022/12/29 01:14] | TRAIN(067): [300/879] Batch: 0.1121 (0.1197) Data: 0.0094 (0.0186) Loss: 0.8447 (0.8803)
[2022/12/29 01:14] | TRAIN(067): [350/879] Batch: 0.1259 (0.1191) Data: 0.0118 (0.0176) Loss: 0.9597 (0.8800)
[2022/12/29 01:14] | TRAIN(067): [400/879] Batch: 0.1224 (0.1185) Data: 0.0118 (0.0169) Loss: 0.9921 (0.8838)
[2022/12/29 01:14] | TRAIN(067): [450/879] Batch: 0.1187 (0.1176) Data: 0.0092 (0.0162) Loss: 0.4407 (0.8834)
[2022/12/29 01:14] | TRAIN(067): [500/879] Batch: 0.1072 (0.1169) Data: 0.0093 (0.0157) Loss: 0.5654 (0.8786)
[2022/12/29 01:14] | TRAIN(067): [550/879] Batch: 0.1052 (0.1162) Data: 0.0103 (0.0152) Loss: 0.8076 (0.8760)
[2022/12/29 01:14] | TRAIN(067): [600/879] Batch: 0.1162 (0.1158) Data: 0.0114 (0.0148) Loss: 0.5742 (0.8726)
[2022/12/29 01:14] | TRAIN(067): [650/879] Batch: 0.1052 (0.1154) Data: 0.0151 (0.0145) Loss: 0.9544 (0.8689)
[2022/12/29 01:14] | TRAIN(067): [700/879] Batch: 0.1070 (0.1150) Data: 0.0088 (0.0143) Loss: 0.7581 (0.8694)
[2022/12/29 01:15] | TRAIN(067): [750/879] Batch: 0.1003 (0.1146) Data: 0.0095 (0.0140) Loss: 0.9334 (0.8679)
[2022/12/29 01:15] | TRAIN(067): [800/879] Batch: 0.1150 (0.1143) Data: 0.0118 (0.0138) Loss: 1.1229 (0.8669)
[2022/12/29 01:15] | TRAIN(067): [850/879] Batch: 0.1227 (0.1141) Data: 0.0103 (0.0136) Loss: 0.7901 (0.8663)
[2022/12/29 01:15] | ------------------------------------------------------------
[2022/12/29 01:15] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:15] | ------------------------------------------------------------
[2022/12/29 01:15] |    TRAIN(67)     0:01:40     0:00:11     0:01:28      0.8677
[2022/12/29 01:15] | ------------------------------------------------------------
[2022/12/29 01:15] | VALID(067): [ 50/220] Batch: 0.0368 (0.0658) Data: 0.0192 (0.0509) Loss: 0.7843 (0.8465)
[2022/12/29 01:15] | VALID(067): [100/220] Batch: 0.0398 (0.0519) Data: 0.0268 (0.0383) Loss: 1.1061 (0.8685)
[2022/12/29 01:15] | VALID(067): [150/220] Batch: 0.0390 (0.0473) Data: 0.0251 (0.0337) Loss: 0.7600 (0.8607)
[2022/12/29 01:15] | VALID(067): [200/220] Batch: 0.0386 (0.0450) Data: 0.0282 (0.0315) Loss: 0.5199 (0.8682)
[2022/12/29 01:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:15] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:15] |    VALID(67)      0.8680      0.7347      0.4981      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:15] | ####################################################################################################
[2022/12/29 01:15] | TRAIN(068): [ 50/879] Batch: 0.1151 (0.1426) Data: 0.0119 (0.0450) Loss: 0.8029 (0.8442)
[2022/12/29 01:15] | TRAIN(068): [100/879] Batch: 0.1013 (0.1277) Data: 0.0113 (0.0281) Loss: 0.8270 (0.8481)
[2022/12/29 01:15] | TRAIN(068): [150/879] Batch: 0.1163 (0.1208) Data: 0.0104 (0.0222) Loss: 0.8655 (0.8427)
[2022/12/29 01:15] | TRAIN(068): [200/879] Batch: 0.1140 (0.1176) Data: 0.0087 (0.0192) Loss: 0.6756 (0.8535)
[2022/12/29 01:15] | TRAIN(068): [250/879] Batch: 0.0998 (0.1156) Data: 0.0095 (0.0174) Loss: 0.7425 (0.8608)
[2022/12/29 01:16] | TRAIN(068): [300/879] Batch: 0.1011 (0.1145) Data: 0.0114 (0.0162) Loss: 0.6658 (0.8674)
[2022/12/29 01:16] | TRAIN(068): [350/879] Batch: 0.1115 (0.1134) Data: 0.0098 (0.0153) Loss: 0.8164 (0.8685)
[2022/12/29 01:16] | TRAIN(068): [400/879] Batch: 0.1249 (0.1128) Data: 0.0123 (0.0147) Loss: 0.7280 (0.8633)
[2022/12/29 01:16] | TRAIN(068): [450/879] Batch: 0.1102 (0.1124) Data: 0.0093 (0.0142) Loss: 0.9288 (0.8598)
[2022/12/29 01:16] | TRAIN(068): [500/879] Batch: 0.1128 (0.1119) Data: 0.0102 (0.0138) Loss: 1.0751 (0.8614)
[2022/12/29 01:16] | TRAIN(068): [550/879] Batch: 0.1134 (0.1114) Data: 0.0101 (0.0135) Loss: 0.8157 (0.8599)
[2022/12/29 01:16] | TRAIN(068): [600/879] Batch: 0.0993 (0.1120) Data: 0.0094 (0.0133) Loss: 0.9990 (0.8629)
[2022/12/29 01:16] | TRAIN(068): [650/879] Batch: 0.1064 (0.1116) Data: 0.0098 (0.0131) Loss: 1.1897 (0.8668)
[2022/12/29 01:16] | TRAIN(068): [700/879] Batch: 0.1102 (0.1115) Data: 0.0138 (0.0129) Loss: 1.1157 (0.8659)
[2022/12/29 01:16] | TRAIN(068): [750/879] Batch: 0.1112 (0.1113) Data: 0.0118 (0.0127) Loss: 0.8892 (0.8675)
[2022/12/29 01:16] | TRAIN(068): [800/879] Batch: 0.1111 (0.1111) Data: 0.0099 (0.0126) Loss: 0.8007 (0.8672)
[2022/12/29 01:17] | TRAIN(068): [850/879] Batch: 0.1060 (0.1109) Data: 0.0107 (0.0124) Loss: 0.9445 (0.8668)
[2022/12/29 01:17] | ------------------------------------------------------------
[2022/12/29 01:17] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:17] | ------------------------------------------------------------
[2022/12/29 01:17] |    TRAIN(68)     0:01:37     0:00:10     0:01:26      0.8678
[2022/12/29 01:17] | ------------------------------------------------------------
[2022/12/29 01:17] | VALID(068): [ 50/220] Batch: 0.0400 (0.0646) Data: 0.0268 (0.0518) Loss: 0.7857 (0.8469)
[2022/12/29 01:17] | VALID(068): [100/220] Batch: 0.0387 (0.0513) Data: 0.0236 (0.0387) Loss: 1.1074 (0.8685)
[2022/12/29 01:17] | VALID(068): [150/220] Batch: 0.0385 (0.0469) Data: 0.0236 (0.0338) Loss: 0.7604 (0.8608)
[2022/12/29 01:17] | VALID(068): [200/220] Batch: 0.0395 (0.0447) Data: 0.0234 (0.0312) Loss: 0.5258 (0.8681)
[2022/12/29 01:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:17] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:17] |    VALID(68)      0.8680      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:17] | ####################################################################################################
[2022/12/29 01:17] | TRAIN(069): [ 50/879] Batch: 0.1054 (0.1540) Data: 0.0134 (0.0512) Loss: 0.7912 (0.8725)
[2022/12/29 01:17] | TRAIN(069): [100/879] Batch: 0.1138 (0.1320) Data: 0.0097 (0.0311) Loss: 1.1015 (0.8583)
[2022/12/29 01:17] | TRAIN(069): [150/879] Batch: 0.1130 (0.1240) Data: 0.0096 (0.0245) Loss: 0.8718 (0.8436)
[2022/12/29 01:17] | TRAIN(069): [200/879] Batch: 0.1025 (0.1202) Data: 0.0108 (0.0211) Loss: 0.8132 (0.8408)
[2022/12/29 01:17] | TRAIN(069): [250/879] Batch: 0.1013 (0.1182) Data: 0.0099 (0.0190) Loss: 0.9119 (0.8555)
[2022/12/29 01:17] | TRAIN(069): [300/879] Batch: 0.1143 (0.1165) Data: 0.0116 (0.0177) Loss: 0.7924 (0.8641)
[2022/12/29 01:17] | TRAIN(069): [350/879] Batch: 0.1120 (0.1161) Data: 0.0107 (0.0167) Loss: 0.8206 (0.8674)
[2022/12/29 01:18] | TRAIN(069): [400/879] Batch: 0.1001 (0.1152) Data: 0.0096 (0.0159) Loss: 0.9422 (0.8707)
[2022/12/29 01:18] | TRAIN(069): [450/879] Batch: 0.1083 (0.1142) Data: 0.0099 (0.0153) Loss: 0.7494 (0.8686)
[2022/12/29 01:18] | TRAIN(069): [500/879] Batch: 0.1141 (0.1137) Data: 0.0099 (0.0148) Loss: 0.7011 (0.8676)
[2022/12/29 01:18] | TRAIN(069): [550/879] Batch: 0.1123 (0.1132) Data: 0.0119 (0.0144) Loss: 1.2762 (0.8713)
[2022/12/29 01:18] | TRAIN(069): [600/879] Batch: 0.1033 (0.1126) Data: 0.0102 (0.0140) Loss: 0.8956 (0.8728)
[2022/12/29 01:18] | TRAIN(069): [650/879] Batch: 0.1095 (0.1124) Data: 0.0105 (0.0138) Loss: 0.7041 (0.8725)
[2022/12/29 01:18] | TRAIN(069): [700/879] Batch: 0.1079 (0.1122) Data: 0.0130 (0.0136) Loss: 0.7291 (0.8736)
[2022/12/29 01:18] | TRAIN(069): [750/879] Batch: 0.1180 (0.1118) Data: 0.0120 (0.0134) Loss: 0.9030 (0.8723)
[2022/12/29 01:18] | TRAIN(069): [800/879] Batch: 0.1050 (0.1116) Data: 0.0180 (0.0132) Loss: 0.5732 (0.8698)
[2022/12/29 01:18] | TRAIN(069): [850/879] Batch: 0.1097 (0.1116) Data: 0.0141 (0.0131) Loss: 0.9381 (0.8675)
[2022/12/29 01:18] | ------------------------------------------------------------
[2022/12/29 01:18] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:18] | ------------------------------------------------------------
[2022/12/29 01:18] |    TRAIN(69)     0:01:37     0:00:11     0:01:26      0.8677
[2022/12/29 01:18] | ------------------------------------------------------------
[2022/12/29 01:18] | VALID(069): [ 50/220] Batch: 0.0388 (0.0678) Data: 0.0263 (0.0535) Loss: 0.7824 (0.8463)
[2022/12/29 01:18] | VALID(069): [100/220] Batch: 0.0414 (0.0529) Data: 0.0236 (0.0390) Loss: 1.1123 (0.8688)
[2022/12/29 01:18] | VALID(069): [150/220] Batch: 0.0371 (0.0480) Data: 0.0249 (0.0342) Loss: 0.7574 (0.8608)
[2022/12/29 01:19] | VALID(069): [200/220] Batch: 0.0407 (0.0455) Data: 0.0248 (0.0321) Loss: 0.5124 (0.8684)
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] |    VALID(69)      0.8682      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] | ####################################################################################################
[2022/12/29 01:19] | TRAIN(070): [ 50/879] Batch: 0.1013 (0.1474) Data: 0.0098 (0.0513) Loss: 0.6773 (0.8393)
[2022/12/29 01:19] | TRAIN(070): [100/879] Batch: 0.1212 (0.1293) Data: 0.0095 (0.0313) Loss: 0.6144 (0.8495)
[2022/12/29 01:19] | TRAIN(070): [150/879] Batch: 0.1035 (0.1228) Data: 0.0104 (0.0244) Loss: 0.9753 (0.8606)
[2022/12/29 01:19] | TRAIN(070): [200/879] Batch: 0.1253 (0.1203) Data: 0.0121 (0.0210) Loss: 1.0927 (0.8679)
[2022/12/29 01:19] | TRAIN(070): [250/879] Batch: 0.1027 (0.1185) Data: 0.0095 (0.0190) Loss: 0.7422 (0.8648)
[2022/12/29 01:19] | TRAIN(070): [300/879] Batch: 0.1007 (0.1169) Data: 0.0087 (0.0175) Loss: 0.7624 (0.8715)
[2022/12/29 01:19] | TRAIN(070): [350/879] Batch: 0.1141 (0.1160) Data: 0.0118 (0.0166) Loss: 1.1482 (0.8714)
[2022/12/29 01:19] | TRAIN(070): [400/879] Batch: 0.1150 (0.1153) Data: 0.0107 (0.0159) Loss: 0.8967 (0.8707)
[2022/12/29 01:19] | TRAIN(070): [450/879] Batch: 0.1068 (0.1151) Data: 0.0103 (0.0155) Loss: 0.7251 (0.8653)
[2022/12/29 01:19] | TRAIN(070): [500/879] Batch: 0.1153 (0.1150) Data: 0.0116 (0.0151) Loss: 0.7817 (0.8640)
[2022/12/29 01:20] | TRAIN(070): [550/879] Batch: 0.1038 (0.1145) Data: 0.0107 (0.0147) Loss: 0.7167 (0.8644)
[2022/12/29 01:20] | TRAIN(070): [600/879] Batch: 0.1020 (0.1142) Data: 0.0094 (0.0144) Loss: 0.9605 (0.8649)
[2022/12/29 01:20] | TRAIN(070): [650/879] Batch: 0.1109 (0.1139) Data: 0.0091 (0.0141) Loss: 1.1211 (0.8666)
[2022/12/29 01:20] | TRAIN(070): [700/879] Batch: 0.1087 (0.1135) Data: 0.0101 (0.0138) Loss: 0.8884 (0.8665)
[2022/12/29 01:20] | TRAIN(070): [750/879] Batch: 0.1093 (0.1132) Data: 0.0103 (0.0136) Loss: 0.5862 (0.8666)
[2022/12/29 01:20] | TRAIN(070): [800/879] Batch: 0.1062 (0.1128) Data: 0.0100 (0.0134) Loss: 0.5305 (0.8664)
[2022/12/29 01:20] | TRAIN(070): [850/879] Batch: 0.1013 (0.1127) Data: 0.0097 (0.0132) Loss: 0.9504 (0.8681)
[2022/12/29 01:20] | ------------------------------------------------------------
[2022/12/29 01:20] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:20] | ------------------------------------------------------------
[2022/12/29 01:20] |    TRAIN(70)     0:01:38     0:00:11     0:01:27      0.8678
[2022/12/29 01:20] | ------------------------------------------------------------
[2022/12/29 01:20] | VALID(070): [ 50/220] Batch: 0.0394 (0.0650) Data: 0.0256 (0.0519) Loss: 0.7833 (0.8464)
[2022/12/29 01:20] | VALID(070): [100/220] Batch: 0.0398 (0.0516) Data: 0.0266 (0.0391) Loss: 1.1064 (0.8685)
[2022/12/29 01:20] | VALID(070): [150/220] Batch: 0.0403 (0.0472) Data: 0.0266 (0.0347) Loss: 0.7597 (0.8607)
[2022/12/29 01:20] | VALID(070): [200/220] Batch: 0.0385 (0.0449) Data: 0.0272 (0.0326) Loss: 0.5205 (0.8681)
[2022/12/29 01:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:20] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:20] |    VALID(70)      0.8679      0.7347      0.5004      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:20] | ####################################################################################################
[2022/12/29 01:20] | TRAIN(071): [ 50/879] Batch: 0.1010 (0.1482) Data: 0.0103 (0.0474) Loss: 0.8287 (0.8629)
[2022/12/29 01:21] | TRAIN(071): [100/879] Batch: 0.1014 (0.1283) Data: 0.0095 (0.0291) Loss: 0.7685 (0.8693)
[2022/12/29 01:21] | TRAIN(071): [150/879] Batch: 0.1119 (0.1221) Data: 0.0091 (0.0228) Loss: 0.7326 (0.8688)
[2022/12/29 01:21] | TRAIN(071): [200/879] Batch: 0.1062 (0.1179) Data: 0.0086 (0.0196) Loss: 0.8091 (0.8708)
[2022/12/29 01:21] | TRAIN(071): [250/879] Batch: 0.1143 (0.1156) Data: 0.0092 (0.0176) Loss: 1.0788 (0.8664)
[2022/12/29 01:21] | TRAIN(071): [300/879] Batch: 0.1070 (0.1142) Data: 0.0090 (0.0163) Loss: 0.8202 (0.8604)
[2022/12/29 01:21] | TRAIN(071): [350/879] Batch: 0.1026 (0.1129) Data: 0.0128 (0.0153) Loss: 1.2387 (0.8591)
[2022/12/29 01:21] | TRAIN(071): [400/879] Batch: 0.1068 (0.1125) Data: 0.0101 (0.0146) Loss: 0.7555 (0.8630)
[2022/12/29 01:21] | TRAIN(071): [450/879] Batch: 0.1069 (0.1122) Data: 0.0096 (0.0142) Loss: 0.9865 (0.8659)
[2022/12/29 01:21] | TRAIN(071): [500/879] Batch: 0.1006 (0.1119) Data: 0.0099 (0.0138) Loss: 0.9020 (0.8698)
[2022/12/29 01:21] | TRAIN(071): [550/879] Batch: 0.1165 (0.1122) Data: 0.0115 (0.0136) Loss: 1.0880 (0.8710)
[2022/12/29 01:21] | TRAIN(071): [600/879] Batch: 0.1144 (0.1124) Data: 0.0114 (0.0134) Loss: 0.9442 (0.8714)
[2022/12/29 01:22] | TRAIN(071): [650/879] Batch: 0.0996 (0.1123) Data: 0.0103 (0.0132) Loss: 1.0082 (0.8710)
[2022/12/29 01:22] | TRAIN(071): [700/879] Batch: 0.1092 (0.1124) Data: 0.0106 (0.0131) Loss: 0.8270 (0.8723)
[2022/12/29 01:22] | TRAIN(071): [750/879] Batch: 0.1140 (0.1124) Data: 0.0168 (0.0130) Loss: 1.0101 (0.8702)
[2022/12/29 01:22] | TRAIN(071): [800/879] Batch: 0.0994 (0.1125) Data: 0.0094 (0.0129) Loss: 0.9553 (0.8708)
[2022/12/29 01:22] | TRAIN(071): [850/879] Batch: 0.1092 (0.1124) Data: 0.0100 (0.0128) Loss: 0.8889 (0.8694)
[2022/12/29 01:22] | ------------------------------------------------------------
[2022/12/29 01:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:22] | ------------------------------------------------------------
[2022/12/29 01:22] |    TRAIN(71)     0:01:38     0:00:11     0:01:27      0.8678
[2022/12/29 01:22] | ------------------------------------------------------------
[2022/12/29 01:22] | VALID(071): [ 50/220] Batch: 0.0330 (0.0671) Data: 0.0281 (0.0515) Loss: 0.7817 (0.8463)
[2022/12/29 01:22] | VALID(071): [100/220] Batch: 0.0384 (0.0527) Data: 0.0264 (0.0385) Loss: 1.1138 (0.8688)
[2022/12/29 01:22] | VALID(071): [150/220] Batch: 0.0386 (0.0478) Data: 0.0275 (0.0345) Loss: 0.7571 (0.8608)
[2022/12/29 01:22] | VALID(071): [200/220] Batch: 0.0379 (0.0454) Data: 0.0249 (0.0325) Loss: 0.5130 (0.8684)
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] |    VALID(71)      0.8683      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] | ####################################################################################################
[2022/12/29 01:22] | TRAIN(072): [ 50/879] Batch: 0.1144 (0.1462) Data: 0.0144 (0.0465) Loss: 0.8390 (0.8684)
[2022/12/29 01:22] | TRAIN(072): [100/879] Batch: 0.1130 (0.1283) Data: 0.0138 (0.0290) Loss: 0.8161 (0.8655)
[2022/12/29 01:22] | TRAIN(072): [150/879] Batch: 0.1138 (0.1237) Data: 0.0100 (0.0233) Loss: 0.7764 (0.8545)
[2022/12/29 01:23] | TRAIN(072): [200/879] Batch: 0.1102 (0.1196) Data: 0.0115 (0.0201) Loss: 0.8710 (0.8532)
[2022/12/29 01:23] | TRAIN(072): [250/879] Batch: 0.1172 (0.1180) Data: 0.0123 (0.0182) Loss: 0.8211 (0.8616)
[2022/12/29 01:23] | TRAIN(072): [300/879] Batch: 0.1188 (0.1171) Data: 0.0158 (0.0171) Loss: 0.8908 (0.8602)
[2022/12/29 01:23] | TRAIN(072): [350/879] Batch: 0.1103 (0.1158) Data: 0.0093 (0.0161) Loss: 0.6777 (0.8603)
[2022/12/29 01:23] | TRAIN(072): [400/879] Batch: 0.1012 (0.1151) Data: 0.0103 (0.0155) Loss: 1.3050 (0.8596)
[2022/12/29 01:23] | TRAIN(072): [450/879] Batch: 0.0998 (0.1142) Data: 0.0099 (0.0149) Loss: 0.7397 (0.8605)
[2022/12/29 01:23] | TRAIN(072): [500/879] Batch: 0.1205 (0.1139) Data: 0.0114 (0.0145) Loss: 0.6060 (0.8638)
[2022/12/29 01:23] | TRAIN(072): [550/879] Batch: 0.1041 (0.1135) Data: 0.0099 (0.0142) Loss: 1.0638 (0.8649)
[2022/12/29 01:23] | TRAIN(072): [600/879] Batch: 0.1079 (0.1133) Data: 0.0091 (0.0139) Loss: 0.9881 (0.8645)
[2022/12/29 01:23] | TRAIN(072): [650/879] Batch: 0.1002 (0.1130) Data: 0.0076 (0.0136) Loss: 0.6759 (0.8651)
[2022/12/29 01:23] | TRAIN(072): [700/879] Batch: 0.1233 (0.1127) Data: 0.0098 (0.0134) Loss: 0.8412 (0.8671)
[2022/12/29 01:24] | TRAIN(072): [750/879] Batch: 0.1117 (0.1131) Data: 0.0101 (0.0132) Loss: 0.8143 (0.8658)
[2022/12/29 01:24] | TRAIN(072): [800/879] Batch: 0.1006 (0.1127) Data: 0.0094 (0.0130) Loss: 0.5071 (0.8658)
[2022/12/29 01:24] | TRAIN(072): [850/879] Batch: 0.1142 (0.1125) Data: 0.0117 (0.0129) Loss: 0.8620 (0.8686)
[2022/12/29 01:24] | ------------------------------------------------------------
[2022/12/29 01:24] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:24] | ------------------------------------------------------------
[2022/12/29 01:24] |    TRAIN(72)     0:01:38     0:00:11     0:01:27      0.8676
[2022/12/29 01:24] | ------------------------------------------------------------
[2022/12/29 01:24] | VALID(072): [ 50/220] Batch: 0.0347 (0.0657) Data: 0.0241 (0.0509) Loss: 0.7896 (0.8471)
[2022/12/29 01:24] | VALID(072): [100/220] Batch: 0.0349 (0.0519) Data: 0.0235 (0.0381) Loss: 1.1077 (0.8685)
[2022/12/29 01:24] | VALID(072): [150/220] Batch: 0.0378 (0.0474) Data: 0.0225 (0.0336) Loss: 0.7614 (0.8609)
[2022/12/29 01:24] | VALID(072): [200/220] Batch: 0.0418 (0.0451) Data: 0.0258 (0.0317) Loss: 0.5181 (0.8683)
[2022/12/29 01:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:24] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:24] |    VALID(72)      0.8683      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:24] | ####################################################################################################
[2022/12/29 01:24] | TRAIN(073): [ 50/879] Batch: 0.1181 (0.1539) Data: 0.0113 (0.0526) Loss: 0.6286 (0.8549)
[2022/12/29 01:24] | TRAIN(073): [100/879] Batch: 0.1084 (0.1328) Data: 0.0107 (0.0323) Loss: 0.6472 (0.8483)
[2022/12/29 01:24] | TRAIN(073): [150/879] Batch: 0.1067 (0.1250) Data: 0.0094 (0.0252) Loss: 0.9952 (0.8576)
[2022/12/29 01:24] | TRAIN(073): [200/879] Batch: 0.1025 (0.1212) Data: 0.0160 (0.0217) Loss: 0.7119 (0.8496)
[2022/12/29 01:24] | TRAIN(073): [250/879] Batch: 0.1021 (0.1193) Data: 0.0077 (0.0197) Loss: 0.5771 (0.8536)
[2022/12/29 01:25] | TRAIN(073): [300/879] Batch: 0.1042 (0.1180) Data: 0.0082 (0.0183) Loss: 0.7668 (0.8596)
[2022/12/29 01:25] | TRAIN(073): [350/879] Batch: 0.1144 (0.1165) Data: 0.0085 (0.0172) Loss: 0.9082 (0.8635)
[2022/12/29 01:25] | TRAIN(073): [400/879] Batch: 0.1005 (0.1158) Data: 0.0094 (0.0164) Loss: 0.7180 (0.8653)
[2022/12/29 01:25] | TRAIN(073): [450/879] Batch: 0.1188 (0.1155) Data: 0.0148 (0.0158) Loss: 0.7099 (0.8658)
[2022/12/29 01:25] | TRAIN(073): [500/879] Batch: 0.1154 (0.1153) Data: 0.0097 (0.0153) Loss: 0.9153 (0.8612)
[2022/12/29 01:25] | TRAIN(073): [550/879] Batch: 0.1090 (0.1150) Data: 0.0098 (0.0149) Loss: 0.8539 (0.8622)
[2022/12/29 01:25] | TRAIN(073): [600/879] Batch: 0.1160 (0.1145) Data: 0.0098 (0.0145) Loss: 0.7696 (0.8666)
[2022/12/29 01:25] | TRAIN(073): [650/879] Batch: 0.1132 (0.1144) Data: 0.0104 (0.0143) Loss: 1.0037 (0.8662)
[2022/12/29 01:25] | TRAIN(073): [700/879] Batch: 0.1100 (0.1139) Data: 0.0083 (0.0140) Loss: 0.7594 (0.8652)
[2022/12/29 01:25] | TRAIN(073): [750/879] Batch: 0.1147 (0.1136) Data: 0.0101 (0.0138) Loss: 0.8888 (0.8670)
[2022/12/29 01:25] | TRAIN(073): [800/879] Batch: 0.1208 (0.1137) Data: 0.0115 (0.0137) Loss: 1.1906 (0.8658)
[2022/12/29 01:26] | TRAIN(073): [850/879] Batch: 0.1071 (0.1137) Data: 0.0099 (0.0135) Loss: 0.8272 (0.8657)
[2022/12/29 01:26] | ------------------------------------------------------------
[2022/12/29 01:26] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:26] | ------------------------------------------------------------
[2022/12/29 01:26] |    TRAIN(73)     0:01:39     0:00:11     0:01:27      0.8677
[2022/12/29 01:26] | ------------------------------------------------------------
[2022/12/29 01:26] | VALID(073): [ 50/220] Batch: 0.0434 (0.0655) Data: 0.0260 (0.0508) Loss: 0.7840 (0.8465)
[2022/12/29 01:26] | VALID(073): [100/220] Batch: 0.0491 (0.0520) Data: 0.0182 (0.0383) Loss: 1.1058 (0.8684)
[2022/12/29 01:26] | VALID(073): [150/220] Batch: 0.0370 (0.0474) Data: 0.0279 (0.0339) Loss: 0.7600 (0.8606)
[2022/12/29 01:26] | VALID(073): [200/220] Batch: 0.0383 (0.0452) Data: 0.0272 (0.0317) Loss: 0.5230 (0.8680)
[2022/12/29 01:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:26] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:26] |    VALID(73)      0.8678      0.7347      0.4998      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:26] | ####################################################################################################
[2022/12/29 01:26] | TRAIN(074): [ 50/879] Batch: 0.1000 (0.1461) Data: 0.0083 (0.0469) Loss: 1.0652 (0.8616)
[2022/12/29 01:26] | TRAIN(074): [100/879] Batch: 0.1188 (0.1264) Data: 0.0120 (0.0290) Loss: 0.9532 (0.8733)
[2022/12/29 01:26] | TRAIN(074): [150/879] Batch: 0.1160 (0.1207) Data: 0.0058 (0.0228) Loss: 0.8824 (0.8717)
[2022/12/29 01:26] | TRAIN(074): [200/879] Batch: 0.1225 (0.1182) Data: 0.0103 (0.0197) Loss: 0.7680 (0.8669)
[2022/12/29 01:26] | TRAIN(074): [250/879] Batch: 0.1246 (0.1167) Data: 0.0121 (0.0180) Loss: 1.0860 (0.8656)
[2022/12/29 01:26] | TRAIN(074): [300/879] Batch: 0.1001 (0.1157) Data: 0.0091 (0.0168) Loss: 0.7649 (0.8635)
[2022/12/29 01:26] | TRAIN(074): [350/879] Batch: 0.1050 (0.1151) Data: 0.0103 (0.0159) Loss: 0.8638 (0.8696)
[2022/12/29 01:27] | TRAIN(074): [400/879] Batch: 0.1130 (0.1146) Data: 0.0099 (0.0153) Loss: 0.7888 (0.8704)
[2022/12/29 01:27] | TRAIN(074): [450/879] Batch: 0.1090 (0.1138) Data: 0.0098 (0.0147) Loss: 0.8389 (0.8691)
[2022/12/29 01:27] | TRAIN(074): [500/879] Batch: 0.1168 (0.1137) Data: 0.0128 (0.0143) Loss: 0.8478 (0.8677)
[2022/12/29 01:27] | TRAIN(074): [550/879] Batch: 0.1293 (0.1134) Data: 0.0121 (0.0140) Loss: 0.8138 (0.8682)
[2022/12/29 01:27] | TRAIN(074): [600/879] Batch: 0.1266 (0.1134) Data: 0.0110 (0.0138) Loss: 0.8720 (0.8672)
[2022/12/29 01:27] | TRAIN(074): [650/879] Batch: 0.1094 (0.1131) Data: 0.0093 (0.0135) Loss: 1.1100 (0.8681)
[2022/12/29 01:27] | TRAIN(074): [700/879] Batch: 0.1043 (0.1129) Data: 0.0109 (0.0133) Loss: 0.5343 (0.8694)
[2022/12/29 01:27] | TRAIN(074): [750/879] Batch: 0.1185 (0.1128) Data: 0.0152 (0.0131) Loss: 0.6794 (0.8686)
[2022/12/29 01:27] | TRAIN(074): [800/879] Batch: 0.1026 (0.1125) Data: 0.0098 (0.0130) Loss: 1.0360 (0.8685)
[2022/12/29 01:27] | TRAIN(074): [850/879] Batch: 0.0995 (0.1123) Data: 0.0101 (0.0128) Loss: 0.7791 (0.8675)
[2022/12/29 01:27] | ------------------------------------------------------------
[2022/12/29 01:27] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:27] | ------------------------------------------------------------
[2022/12/29 01:27] |    TRAIN(74)     0:01:38     0:00:11     0:01:27      0.8677
[2022/12/29 01:27] | ------------------------------------------------------------
[2022/12/29 01:28] | VALID(074): [ 50/220] Batch: 0.0412 (0.0658) Data: 0.0295 (0.0538) Loss: 0.7829 (0.8462)
[2022/12/29 01:28] | VALID(074): [100/220] Batch: 0.0388 (0.0520) Data: 0.0271 (0.0403) Loss: 1.1079 (0.8684)
[2022/12/29 01:28] | VALID(074): [150/220] Batch: 0.0381 (0.0475) Data: 0.0272 (0.0355) Loss: 0.7588 (0.8605)
[2022/12/29 01:28] | VALID(074): [200/220] Batch: 0.0384 (0.0452) Data: 0.0244 (0.0332) Loss: 0.5178 (0.8680)
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] |    VALID(74)      0.8678      0.7347      0.5001      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] | ####################################################################################################
[2022/12/29 01:28] | TRAIN(075): [ 50/879] Batch: 0.1069 (0.1447) Data: 0.0104 (0.0463) Loss: 0.9033 (0.8634)
[2022/12/29 01:28] | TRAIN(075): [100/879] Batch: 0.1193 (0.1295) Data: 0.0095 (0.0290) Loss: 0.7336 (0.8544)
[2022/12/29 01:28] | TRAIN(075): [150/879] Batch: 0.1221 (0.1226) Data: 0.0099 (0.0228) Loss: 0.8611 (0.8551)
[2022/12/29 01:28] | TRAIN(075): [200/879] Batch: 0.1127 (0.1190) Data: 0.0119 (0.0198) Loss: 1.1825 (0.8628)
[2022/12/29 01:28] | TRAIN(075): [250/879] Batch: 0.1025 (0.1167) Data: 0.0110 (0.0179) Loss: 0.8275 (0.8735)
[2022/12/29 01:28] | TRAIN(075): [300/879] Batch: 0.1186 (0.1155) Data: 0.0119 (0.0167) Loss: 0.9075 (0.8748)
[2022/12/29 01:28] | TRAIN(075): [350/879] Batch: 0.1128 (0.1147) Data: 0.0106 (0.0158) Loss: 0.8839 (0.8713)
[2022/12/29 01:28] | TRAIN(075): [400/879] Batch: 0.1085 (0.1140) Data: 0.0099 (0.0152) Loss: 0.9627 (0.8724)
[2022/12/29 01:28] | TRAIN(075): [450/879] Batch: 0.1015 (0.1138) Data: 0.0095 (0.0147) Loss: 0.6843 (0.8679)
[2022/12/29 01:29] | TRAIN(075): [500/879] Batch: 0.1031 (0.1132) Data: 0.0119 (0.0142) Loss: 0.8793 (0.8649)
[2022/12/29 01:29] | TRAIN(075): [550/879] Batch: 0.1021 (0.1125) Data: 0.0110 (0.0139) Loss: 1.0361 (0.8657)
[2022/12/29 01:29] | TRAIN(075): [600/879] Batch: 0.1088 (0.1121) Data: 0.0096 (0.0136) Loss: 0.7848 (0.8669)
[2022/12/29 01:29] | TRAIN(075): [650/879] Batch: 0.1064 (0.1120) Data: 0.0107 (0.0133) Loss: 0.6536 (0.8660)
[2022/12/29 01:29] | TRAIN(075): [700/879] Batch: 0.1099 (0.1116) Data: 0.0102 (0.0131) Loss: 1.0056 (0.8676)
[2022/12/29 01:29] | TRAIN(075): [750/879] Batch: 0.1002 (0.1112) Data: 0.0095 (0.0129) Loss: 0.7909 (0.8669)
[2022/12/29 01:29] | TRAIN(075): [800/879] Batch: 0.1265 (0.1109) Data: 0.0115 (0.0127) Loss: 0.6785 (0.8662)
[2022/12/29 01:29] | TRAIN(075): [850/879] Batch: 0.1060 (0.1106) Data: 0.0084 (0.0125) Loss: 0.9216 (0.8681)
[2022/12/29 01:29] | ------------------------------------------------------------
[2022/12/29 01:29] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:29] | ------------------------------------------------------------
[2022/12/29 01:29] |    TRAIN(75)     0:01:36     0:00:10     0:01:26      0.8676
[2022/12/29 01:29] | ------------------------------------------------------------
[2022/12/29 01:29] | VALID(075): [ 50/220] Batch: 0.0329 (0.0668) Data: 0.0287 (0.0517) Loss: 0.7835 (0.8464)
[2022/12/29 01:29] | VALID(075): [100/220] Batch: 0.0380 (0.0524) Data: 0.0277 (0.0389) Loss: 1.1075 (0.8684)
[2022/12/29 01:29] | VALID(075): [150/220] Batch: 0.0381 (0.0475) Data: 0.0268 (0.0346) Loss: 0.7592 (0.8606)
[2022/12/29 01:29] | VALID(075): [200/220] Batch: 0.0347 (0.0451) Data: 0.0240 (0.0324) Loss: 0.5190 (0.8681)
[2022/12/29 01:29] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:29] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:29] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:29] |    VALID(75)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:29] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:29] | ####################################################################################################
[2022/12/29 01:30] | TRAIN(076): [ 50/879] Batch: 0.0968 (0.1469) Data: 0.0109 (0.0463) Loss: 0.7040 (0.9096)
[2022/12/29 01:30] | TRAIN(076): [100/879] Batch: 0.0998 (0.1270) Data: 0.0094 (0.0285) Loss: 0.8921 (0.8893)
[2022/12/29 01:30] | TRAIN(076): [150/879] Batch: 0.1044 (0.1200) Data: 0.0098 (0.0224) Loss: 0.9804 (0.8720)
[2022/12/29 01:30] | TRAIN(076): [200/879] Batch: 0.1242 (0.1169) Data: 0.0098 (0.0194) Loss: 1.1776 (0.8748)
[2022/12/29 01:30] | TRAIN(076): [250/879] Batch: 0.1139 (0.1175) Data: 0.0109 (0.0178) Loss: 0.9148 (0.8713)
[2022/12/29 01:30] | TRAIN(076): [300/879] Batch: 0.0998 (0.1157) Data: 0.0095 (0.0166) Loss: 0.7742 (0.8586)
[2022/12/29 01:30] | TRAIN(076): [350/879] Batch: 0.1115 (0.1143) Data: 0.0101 (0.0157) Loss: 1.1031 (0.8594)
[2022/12/29 01:30] | TRAIN(076): [400/879] Batch: 0.0997 (0.1137) Data: 0.0098 (0.0150) Loss: 0.7643 (0.8626)
[2022/12/29 01:30] | TRAIN(076): [450/879] Batch: 0.1045 (0.1129) Data: 0.0101 (0.0145) Loss: 1.0968 (0.8626)
[2022/12/29 01:30] | TRAIN(076): [500/879] Batch: 0.1005 (0.1127) Data: 0.0095 (0.0142) Loss: 0.8820 (0.8617)
[2022/12/29 01:30] | TRAIN(076): [550/879] Batch: 0.1197 (0.1124) Data: 0.0120 (0.0138) Loss: 0.6675 (0.8614)
[2022/12/29 01:31] | TRAIN(076): [600/879] Batch: 0.1204 (0.1126) Data: 0.0179 (0.0136) Loss: 0.8956 (0.8640)
[2022/12/29 01:31] | TRAIN(076): [650/879] Batch: 0.1091 (0.1126) Data: 0.0093 (0.0134) Loss: 0.8174 (0.8657)
[2022/12/29 01:31] | TRAIN(076): [700/879] Batch: 0.1026 (0.1125) Data: 0.0100 (0.0132) Loss: 0.8269 (0.8687)
[2022/12/29 01:31] | TRAIN(076): [750/879] Batch: 0.1230 (0.1124) Data: 0.0108 (0.0131) Loss: 0.9645 (0.8659)
[2022/12/29 01:31] | TRAIN(076): [800/879] Batch: 0.1205 (0.1124) Data: 0.0078 (0.0130) Loss: 1.0037 (0.8677)
[2022/12/29 01:31] | TRAIN(076): [850/879] Batch: 0.1010 (0.1123) Data: 0.0100 (0.0129) Loss: 0.5178 (0.8669)
[2022/12/29 01:31] | ------------------------------------------------------------
[2022/12/29 01:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:31] | ------------------------------------------------------------
[2022/12/29 01:31] |    TRAIN(76)     0:01:38     0:00:11     0:01:27      0.8676
[2022/12/29 01:31] | ------------------------------------------------------------
[2022/12/29 01:31] | VALID(076): [ 50/220] Batch: 0.0397 (0.0715) Data: 0.0277 (0.0588) Loss: 0.7850 (0.8465)
[2022/12/29 01:31] | VALID(076): [100/220] Batch: 0.0359 (0.0546) Data: 0.0277 (0.0423) Loss: 1.1082 (0.8683)
[2022/12/29 01:31] | VALID(076): [150/220] Batch: 0.0377 (0.0490) Data: 0.0235 (0.0366) Loss: 0.7596 (0.8605)
[2022/12/29 01:31] | VALID(076): [200/220] Batch: 0.0426 (0.0462) Data: 0.0220 (0.0334) Loss: 0.5207 (0.8679)
[2022/12/29 01:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:31] |    VALID(76)      0.8678      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:31] | ####################################################################################################
[2022/12/29 01:31] | TRAIN(077): [ 50/879] Batch: 0.1010 (0.1508) Data: 0.0143 (0.0470) Loss: 0.8340 (0.8866)
[2022/12/29 01:31] | TRAIN(077): [100/879] Batch: 0.1283 (0.1325) Data: 0.0106 (0.0297) Loss: 0.8004 (0.8590)
[2022/12/29 01:32] | TRAIN(077): [150/879] Batch: 0.1021 (0.1251) Data: 0.0090 (0.0235) Loss: 0.8653 (0.8646)
[2022/12/29 01:32] | TRAIN(077): [200/879] Batch: 0.1153 (0.1209) Data: 0.0168 (0.0204) Loss: 0.7780 (0.8683)
[2022/12/29 01:32] | TRAIN(077): [250/879] Batch: 0.1181 (0.1199) Data: 0.0092 (0.0185) Loss: 0.9239 (0.8671)
[2022/12/29 01:32] | TRAIN(077): [300/879] Batch: 0.1163 (0.1195) Data: 0.0119 (0.0174) Loss: 0.7034 (0.8666)
[2022/12/29 01:32] | TRAIN(077): [350/879] Batch: 0.1076 (0.1185) Data: 0.0112 (0.0166) Loss: 0.8085 (0.8650)
[2022/12/29 01:32] | TRAIN(077): [400/879] Batch: 0.1299 (0.1182) Data: 0.0119 (0.0159) Loss: 0.8084 (0.8652)
[2022/12/29 01:32] | TRAIN(077): [450/879] Batch: 0.1001 (0.1174) Data: 0.0091 (0.0153) Loss: 0.5926 (0.8626)
[2022/12/29 01:32] | TRAIN(077): [500/879] Batch: 0.1283 (0.1174) Data: 0.0115 (0.0150) Loss: 0.7194 (0.8626)
[2022/12/29 01:32] | TRAIN(077): [550/879] Batch: 0.1067 (0.1166) Data: 0.0127 (0.0146) Loss: 0.6852 (0.8596)
[2022/12/29 01:32] | TRAIN(077): [600/879] Batch: 0.1055 (0.1159) Data: 0.0097 (0.0142) Loss: 0.8132 (0.8622)
[2022/12/29 01:32] | TRAIN(077): [650/879] Batch: 0.1057 (0.1155) Data: 0.0095 (0.0139) Loss: 1.2304 (0.8636)
[2022/12/29 01:33] | TRAIN(077): [700/879] Batch: 0.1113 (0.1150) Data: 0.0095 (0.0137) Loss: 1.0303 (0.8650)
[2022/12/29 01:33] | TRAIN(077): [750/879] Batch: 0.1030 (0.1145) Data: 0.0101 (0.0135) Loss: 0.6917 (0.8645)
[2022/12/29 01:33] | TRAIN(077): [800/879] Batch: 0.1095 (0.1141) Data: 0.0168 (0.0133) Loss: 1.0680 (0.8655)
[2022/12/29 01:33] | TRAIN(077): [850/879] Batch: 0.1021 (0.1141) Data: 0.0100 (0.0131) Loss: 0.7244 (0.8665)
[2022/12/29 01:33] | ------------------------------------------------------------
[2022/12/29 01:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:33] | ------------------------------------------------------------
[2022/12/29 01:33] |    TRAIN(77)     0:01:40     0:00:11     0:01:28      0.8675
[2022/12/29 01:33] | ------------------------------------------------------------
[2022/12/29 01:33] | VALID(077): [ 50/220] Batch: 0.0366 (0.0649) Data: 0.0234 (0.0527) Loss: 0.7848 (0.8468)
[2022/12/29 01:33] | VALID(077): [100/220] Batch: 0.0370 (0.0515) Data: 0.0193 (0.0383) Loss: 1.1069 (0.8685)
[2022/12/29 01:33] | VALID(077): [150/220] Batch: 0.0363 (0.0470) Data: 0.0238 (0.0333) Loss: 0.7603 (0.8608)
[2022/12/29 01:33] | VALID(077): [200/220] Batch: 0.0434 (0.0449) Data: 0.0171 (0.0296) Loss: 0.5243 (0.8681)
[2022/12/29 01:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:33] |    VALID(77)      0.8680      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:33] | ####################################################################################################
[2022/12/29 01:33] | TRAIN(078): [ 50/879] Batch: 0.1197 (0.1555) Data: 0.0096 (0.0480) Loss: 1.2438 (0.8931)
[2022/12/29 01:33] | TRAIN(078): [100/879] Batch: 0.1021 (0.1366) Data: 0.0109 (0.0299) Loss: 1.0535 (0.8817)
[2022/12/29 01:33] | TRAIN(078): [150/879] Batch: 0.1105 (0.1278) Data: 0.0094 (0.0235) Loss: 0.8269 (0.8743)
[2022/12/29 01:33] | TRAIN(078): [200/879] Batch: 0.1091 (0.1226) Data: 0.0097 (0.0202) Loss: 0.9419 (0.8719)
[2022/12/29 01:34] | TRAIN(078): [250/879] Batch: 0.1150 (0.1191) Data: 0.0097 (0.0182) Loss: 0.7295 (0.8729)
[2022/12/29 01:34] | TRAIN(078): [300/879] Batch: 0.1016 (0.1168) Data: 0.0099 (0.0169) Loss: 0.6570 (0.8744)
[2022/12/29 01:34] | TRAIN(078): [350/879] Batch: 0.1035 (0.1154) Data: 0.0099 (0.0160) Loss: 0.7607 (0.8709)
[2022/12/29 01:34] | TRAIN(078): [400/879] Batch: 0.1027 (0.1141) Data: 0.0103 (0.0153) Loss: 0.7928 (0.8730)
[2022/12/29 01:34] | TRAIN(078): [450/879] Batch: 0.1200 (0.1135) Data: 0.0120 (0.0147) Loss: 0.5653 (0.8740)
[2022/12/29 01:34] | TRAIN(078): [500/879] Batch: 0.1026 (0.1134) Data: 0.0103 (0.0143) Loss: 1.1130 (0.8703)
[2022/12/29 01:34] | TRAIN(078): [550/879] Batch: 0.1189 (0.1131) Data: 0.0119 (0.0140) Loss: 1.0445 (0.8707)
[2022/12/29 01:34] | TRAIN(078): [600/879] Batch: 0.1251 (0.1131) Data: 0.0118 (0.0138) Loss: 0.7178 (0.8714)
[2022/12/29 01:34] | TRAIN(078): [650/879] Batch: 0.1022 (0.1128) Data: 0.0095 (0.0136) Loss: 1.3294 (0.8698)
[2022/12/29 01:34] | TRAIN(078): [700/879] Batch: 0.1029 (0.1129) Data: 0.0100 (0.0134) Loss: 0.8135 (0.8660)
[2022/12/29 01:34] | TRAIN(078): [750/879] Batch: 0.1164 (0.1128) Data: 0.0107 (0.0133) Loss: 0.8890 (0.8660)
[2022/12/29 01:35] | TRAIN(078): [800/879] Batch: 0.1084 (0.1125) Data: 0.0100 (0.0131) Loss: 1.3030 (0.8667)
[2022/12/29 01:35] | TRAIN(078): [850/879] Batch: 0.1065 (0.1122) Data: 0.0084 (0.0130) Loss: 0.8514 (0.8676)
[2022/12/29 01:35] | ------------------------------------------------------------
[2022/12/29 01:35] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:35] | ------------------------------------------------------------
[2022/12/29 01:35] |    TRAIN(78)     0:01:38     0:00:11     0:01:27      0.8677
[2022/12/29 01:35] | ------------------------------------------------------------
[2022/12/29 01:35] | VALID(078): [ 50/220] Batch: 0.0396 (0.0655) Data: 0.0263 (0.0526) Loss: 0.7828 (0.8462)
[2022/12/29 01:35] | VALID(078): [100/220] Batch: 0.0383 (0.0518) Data: 0.0274 (0.0385) Loss: 1.1083 (0.8684)
[2022/12/29 01:35] | VALID(078): [150/220] Batch: 0.0342 (0.0472) Data: 0.0273 (0.0343) Loss: 0.7589 (0.8605)
[2022/12/29 01:35] | VALID(078): [200/220] Batch: 0.0388 (0.0450) Data: 0.0258 (0.0322) Loss: 0.5184 (0.8680)
[2022/12/29 01:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:35] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:35] |    VALID(78)      0.8678      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:35] | ####################################################################################################
[2022/12/29 01:35] | TRAIN(079): [ 50/879] Batch: 0.1084 (0.1429) Data: 0.0165 (0.0462) Loss: 0.9408 (0.8704)
[2022/12/29 01:35] | TRAIN(079): [100/879] Batch: 0.1108 (0.1254) Data: 0.0116 (0.0289) Loss: 1.0612 (0.8670)
[2022/12/29 01:35] | TRAIN(079): [150/879] Batch: 0.1052 (0.1204) Data: 0.0096 (0.0230) Loss: 0.5385 (0.8788)
[2022/12/29 01:35] | TRAIN(079): [200/879] Batch: 0.1001 (0.1172) Data: 0.0109 (0.0199) Loss: 0.9708 (0.8784)
[2022/12/29 01:35] | TRAIN(079): [250/879] Batch: 0.1137 (0.1164) Data: 0.0141 (0.0182) Loss: 0.8475 (0.8685)
[2022/12/29 01:35] | TRAIN(079): [300/879] Batch: 0.1185 (0.1156) Data: 0.0119 (0.0170) Loss: 0.5776 (0.8626)
[2022/12/29 01:36] | TRAIN(079): [350/879] Batch: 0.1061 (0.1150) Data: 0.0095 (0.0162) Loss: 1.1115 (0.8599)
[2022/12/29 01:36] | TRAIN(079): [400/879] Batch: 0.1191 (0.1146) Data: 0.0087 (0.0155) Loss: 1.0305 (0.8649)
[2022/12/29 01:36] | TRAIN(079): [450/879] Batch: 0.1009 (0.1141) Data: 0.0104 (0.0149) Loss: 0.8645 (0.8670)
[2022/12/29 01:36] | TRAIN(079): [500/879] Batch: 0.1166 (0.1141) Data: 0.0114 (0.0145) Loss: 0.8727 (0.8654)
[2022/12/29 01:36] | TRAIN(079): [550/879] Batch: 0.1140 (0.1133) Data: 0.0095 (0.0141) Loss: 0.6377 (0.8651)
[2022/12/29 01:36] | TRAIN(079): [600/879] Batch: 0.1129 (0.1133) Data: 0.0081 (0.0139) Loss: 1.0791 (0.8649)
[2022/12/29 01:36] | TRAIN(079): [650/879] Batch: 0.1238 (0.1132) Data: 0.0098 (0.0136) Loss: 0.8161 (0.8663)
[2022/12/29 01:36] | TRAIN(079): [700/879] Batch: 0.1117 (0.1130) Data: 0.0088 (0.0134) Loss: 0.8516 (0.8681)
[2022/12/29 01:36] | TRAIN(079): [750/879] Batch: 0.1026 (0.1127) Data: 0.0091 (0.0132) Loss: 1.3412 (0.8668)
[2022/12/29 01:36] | TRAIN(079): [800/879] Batch: 0.1244 (0.1125) Data: 0.0119 (0.0131) Loss: 0.8725 (0.8665)
[2022/12/29 01:36] | TRAIN(079): [850/879] Batch: 0.1081 (0.1124) Data: 0.0081 (0.0129) Loss: 0.6308 (0.8679)
[2022/12/29 01:37] | ------------------------------------------------------------
[2022/12/29 01:37] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:37] | ------------------------------------------------------------
[2022/12/29 01:37] |    TRAIN(79)     0:01:38     0:00:11     0:01:27      0.8675
[2022/12/29 01:37] | ------------------------------------------------------------
[2022/12/29 01:37] | VALID(079): [ 50/220] Batch: 0.0395 (0.0665) Data: 0.0267 (0.0518) Loss: 0.7842 (0.8467)
[2022/12/29 01:37] | VALID(079): [100/220] Batch: 0.0362 (0.0523) Data: 0.0269 (0.0388) Loss: 1.1068 (0.8685)
[2022/12/29 01:37] | VALID(079): [150/220] Batch: 0.0428 (0.0476) Data: 0.0247 (0.0346) Loss: 0.7601 (0.8608)
[2022/12/29 01:37] | VALID(079): [200/220] Batch: 0.0392 (0.0453) Data: 0.0261 (0.0321) Loss: 0.5221 (0.8682)
[2022/12/29 01:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:37] |    VALID(79)      0.8680      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:37] | ####################################################################################################
[2022/12/29 01:37] | TRAIN(080): [ 50/879] Batch: 0.0998 (0.1448) Data: 0.0092 (0.0464) Loss: 0.9960 (0.9018)
[2022/12/29 01:37] | TRAIN(080): [100/879] Batch: 0.1081 (0.1246) Data: 0.0101 (0.0284) Loss: 0.8025 (0.8877)
[2022/12/29 01:37] | TRAIN(080): [150/879] Batch: 0.0989 (0.1177) Data: 0.0097 (0.0223) Loss: 0.9324 (0.8773)
[2022/12/29 01:37] | TRAIN(080): [200/879] Batch: 0.1074 (0.1145) Data: 0.0077 (0.0192) Loss: 0.8851 (0.8655)
[2022/12/29 01:37] | TRAIN(080): [250/879] Batch: 0.1142 (0.1130) Data: 0.0097 (0.0174) Loss: 0.7762 (0.8594)
[2022/12/29 01:37] | TRAIN(080): [300/879] Batch: 0.1151 (0.1125) Data: 0.0134 (0.0164) Loss: 1.3580 (0.8679)
[2022/12/29 01:37] | TRAIN(080): [350/879] Batch: 0.1135 (0.1127) Data: 0.0105 (0.0156) Loss: 0.9311 (0.8676)
[2022/12/29 01:37] | TRAIN(080): [400/879] Batch: 0.1153 (0.1127) Data: 0.0092 (0.0151) Loss: 0.8338 (0.8671)
[2022/12/29 01:38] | TRAIN(080): [450/879] Batch: 0.0997 (0.1128) Data: 0.0092 (0.0147) Loss: 1.0161 (0.8655)
[2022/12/29 01:38] | TRAIN(080): [500/879] Batch: 0.0994 (0.1128) Data: 0.0102 (0.0144) Loss: 0.7592 (0.8648)
[2022/12/29 01:38] | TRAIN(080): [550/879] Batch: 0.1053 (0.1128) Data: 0.0139 (0.0141) Loss: 1.0872 (0.8646)
[2022/12/29 01:38] | TRAIN(080): [600/879] Batch: 0.1075 (0.1126) Data: 0.0111 (0.0138) Loss: 0.8629 (0.8655)
[2022/12/29 01:38] | TRAIN(080): [650/879] Batch: 0.1089 (0.1125) Data: 0.0105 (0.0136) Loss: 0.7891 (0.8680)
[2022/12/29 01:38] | TRAIN(080): [700/879] Batch: 0.1177 (0.1122) Data: 0.0102 (0.0134) Loss: 0.7925 (0.8685)
[2022/12/29 01:38] | TRAIN(080): [750/879] Batch: 0.1008 (0.1120) Data: 0.0094 (0.0132) Loss: 0.7679 (0.8677)
[2022/12/29 01:38] | TRAIN(080): [800/879] Batch: 0.1226 (0.1120) Data: 0.0173 (0.0131) Loss: 0.9885 (0.8686)
[2022/12/29 01:38] | TRAIN(080): [850/879] Batch: 0.1228 (0.1122) Data: 0.0120 (0.0130) Loss: 0.7268 (0.8674)
[2022/12/29 01:38] | ------------------------------------------------------------
[2022/12/29 01:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:38] | ------------------------------------------------------------
[2022/12/29 01:38] |    TRAIN(80)     0:01:38     0:00:11     0:01:27      0.8675
[2022/12/29 01:38] | ------------------------------------------------------------
[2022/12/29 01:38] | VALID(080): [ 50/220] Batch: 0.0395 (0.0660) Data: 0.0274 (0.0490) Loss: 0.7833 (0.8464)
[2022/12/29 01:38] | VALID(080): [100/220] Batch: 0.0380 (0.0521) Data: 0.0270 (0.0379) Loss: 1.1084 (0.8684)
[2022/12/29 01:38] | VALID(080): [150/220] Batch: 0.0340 (0.0475) Data: 0.0237 (0.0331) Loss: 0.7592 (0.8606)
[2022/12/29 01:38] | VALID(080): [200/220] Batch: 0.0383 (0.0453) Data: 0.0245 (0.0308) Loss: 0.5196 (0.8681)
[2022/12/29 01:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:38] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:38] |    VALID(80)      0.8679      0.7347      0.5005      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:38] | ####################################################################################################
[2022/12/29 01:39] | TRAIN(081): [ 50/879] Batch: 0.1088 (0.1513) Data: 0.0103 (0.0493) Loss: 0.5030 (0.8473)
[2022/12/29 01:39] | TRAIN(081): [100/879] Batch: 0.1224 (0.1317) Data: 0.0117 (0.0303) Loss: 0.9958 (0.8615)
[2022/12/29 01:39] | TRAIN(081): [150/879] Batch: 0.1159 (0.1249) Data: 0.0095 (0.0237) Loss: 1.0868 (0.8511)
[2022/12/29 01:39] | TRAIN(081): [200/879] Batch: 0.1061 (0.1208) Data: 0.0107 (0.0203) Loss: 0.5503 (0.8497)
[2022/12/29 01:39] | TRAIN(081): [250/879] Batch: 0.1268 (0.1179) Data: 0.0115 (0.0183) Loss: 1.2703 (0.8637)
[2022/12/29 01:39] | TRAIN(081): [300/879] Batch: 0.1075 (0.1161) Data: 0.0086 (0.0169) Loss: 0.8775 (0.8617)
[2022/12/29 01:39] | TRAIN(081): [350/879] Batch: 0.1098 (0.1147) Data: 0.0102 (0.0160) Loss: 0.7618 (0.8631)
[2022/12/29 01:39] | TRAIN(081): [400/879] Batch: 0.1063 (0.1141) Data: 0.0114 (0.0153) Loss: 0.9009 (0.8632)
[2022/12/29 01:39] | TRAIN(081): [450/879] Batch: 0.1063 (0.1135) Data: 0.0092 (0.0148) Loss: 1.0772 (0.8635)
[2022/12/29 01:39] | TRAIN(081): [500/879] Batch: 0.1073 (0.1130) Data: 0.0096 (0.0143) Loss: 1.0705 (0.8656)
[2022/12/29 01:40] | TRAIN(081): [550/879] Batch: 0.1273 (0.1128) Data: 0.0115 (0.0140) Loss: 0.8511 (0.8633)
[2022/12/29 01:40] | TRAIN(081): [600/879] Batch: 0.1073 (0.1124) Data: 0.0105 (0.0137) Loss: 0.7527 (0.8643)
[2022/12/29 01:40] | TRAIN(081): [650/879] Batch: 0.1057 (0.1122) Data: 0.0099 (0.0134) Loss: 1.1101 (0.8668)
[2022/12/29 01:40] | TRAIN(081): [700/879] Batch: 0.1214 (0.1121) Data: 0.0117 (0.0132) Loss: 0.9172 (0.8670)
[2022/12/29 01:40] | TRAIN(081): [750/879] Batch: 0.1153 (0.1119) Data: 0.0094 (0.0130) Loss: 1.1257 (0.8661)
[2022/12/29 01:40] | TRAIN(081): [800/879] Batch: 0.1061 (0.1117) Data: 0.0095 (0.0129) Loss: 0.8689 (0.8654)
[2022/12/29 01:40] | TRAIN(081): [850/879] Batch: 0.1100 (0.1115) Data: 0.0101 (0.0127) Loss: 0.5818 (0.8673)
[2022/12/29 01:40] | ------------------------------------------------------------
[2022/12/29 01:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:40] | ------------------------------------------------------------
[2022/12/29 01:40] |    TRAIN(81)     0:01:37     0:00:11     0:01:26      0.8676
[2022/12/29 01:40] | ------------------------------------------------------------
[2022/12/29 01:40] | VALID(081): [ 50/220] Batch: 0.0402 (0.0660) Data: 0.0221 (0.0514) Loss: 0.7836 (0.8464)
[2022/12/29 01:40] | VALID(081): [100/220] Batch: 0.0371 (0.0520) Data: 0.0242 (0.0387) Loss: 1.1066 (0.8684)
[2022/12/29 01:40] | VALID(081): [150/220] Batch: 0.0382 (0.0475) Data: 0.0220 (0.0342) Loss: 0.7597 (0.8606)
[2022/12/29 01:40] | VALID(081): [200/220] Batch: 0.0375 (0.0453) Data: 0.0241 (0.0312) Loss: 0.5211 (0.8680)
[2022/12/29 01:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:40] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:40] |    VALID(81)      0.8679      0.7347      0.5004      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:40] | ####################################################################################################
[2022/12/29 01:40] | TRAIN(082): [ 50/879] Batch: 0.1116 (0.1516) Data: 0.0095 (0.0465) Loss: 0.9013 (0.8466)
[2022/12/29 01:41] | TRAIN(082): [100/879] Batch: 0.1105 (0.1305) Data: 0.0128 (0.0290) Loss: 0.7253 (0.8491)
[2022/12/29 01:41] | TRAIN(082): [150/879] Batch: 0.1138 (0.1239) Data: 0.0100 (0.0230) Loss: 1.1549 (0.8584)
[2022/12/29 01:41] | TRAIN(082): [200/879] Batch: 0.1024 (0.1213) Data: 0.0104 (0.0200) Loss: 0.9383 (0.8578)
[2022/12/29 01:41] | TRAIN(082): [250/879] Batch: 0.1079 (0.1193) Data: 0.0101 (0.0181) Loss: 0.8703 (0.8614)
[2022/12/29 01:41] | TRAIN(082): [300/879] Batch: 0.1140 (0.1174) Data: 0.0111 (0.0168) Loss: 0.6403 (0.8607)
[2022/12/29 01:41] | TRAIN(082): [350/879] Batch: 0.1012 (0.1165) Data: 0.0097 (0.0159) Loss: 0.8586 (0.8635)
[2022/12/29 01:41] | TRAIN(082): [400/879] Batch: 0.1164 (0.1157) Data: 0.0095 (0.0152) Loss: 1.1064 (0.8701)
[2022/12/29 01:41] | TRAIN(082): [450/879] Batch: 0.1039 (0.1150) Data: 0.0098 (0.0147) Loss: 0.6824 (0.8723)
[2022/12/29 01:41] | TRAIN(082): [500/879] Batch: 0.1013 (0.1143) Data: 0.0094 (0.0143) Loss: 0.6046 (0.8683)
[2022/12/29 01:41] | TRAIN(082): [550/879] Batch: 0.1038 (0.1142) Data: 0.0099 (0.0139) Loss: 0.6363 (0.8684)
[2022/12/29 01:41] | TRAIN(082): [600/879] Batch: 0.1199 (0.1141) Data: 0.0119 (0.0137) Loss: 0.8169 (0.8698)
[2022/12/29 01:42] | TRAIN(082): [650/879] Batch: 0.1232 (0.1138) Data: 0.0109 (0.0135) Loss: 0.5780 (0.8685)
[2022/12/29 01:42] | TRAIN(082): [700/879] Batch: 0.1042 (0.1133) Data: 0.0098 (0.0132) Loss: 0.8328 (0.8689)
[2022/12/29 01:42] | TRAIN(082): [750/879] Batch: 0.1161 (0.1129) Data: 0.0105 (0.0130) Loss: 0.9975 (0.8662)
[2022/12/29 01:42] | TRAIN(082): [800/879] Batch: 0.1057 (0.1125) Data: 0.0115 (0.0128) Loss: 1.3979 (0.8661)
[2022/12/29 01:42] | TRAIN(082): [850/879] Batch: 0.1157 (0.1122) Data: 0.0100 (0.0127) Loss: 0.6862 (0.8669)
[2022/12/29 01:42] | ------------------------------------------------------------
[2022/12/29 01:42] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:42] | ------------------------------------------------------------
[2022/12/29 01:42] |    TRAIN(82)     0:01:38     0:00:11     0:01:27      0.8675
[2022/12/29 01:42] | ------------------------------------------------------------
[2022/12/29 01:42] | VALID(082): [ 50/220] Batch: 0.0389 (0.0690) Data: 0.0266 (0.0542) Loss: 0.7833 (0.8465)
[2022/12/29 01:42] | VALID(082): [100/220] Batch: 0.0389 (0.0538) Data: 0.0240 (0.0401) Loss: 1.1083 (0.8686)
[2022/12/29 01:42] | VALID(082): [150/220] Batch: 0.0391 (0.0487) Data: 0.0283 (0.0346) Loss: 0.7593 (0.8608)
[2022/12/29 01:42] | VALID(082): [200/220] Batch: 0.0388 (0.0462) Data: 0.0211 (0.0318) Loss: 0.5199 (0.8682)
[2022/12/29 01:42] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:42] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:42] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:42] |    VALID(82)      0.8681      0.7347      0.4995      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:42] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:42] | ####################################################################################################
[2022/12/29 01:42] | TRAIN(083): [ 50/879] Batch: 0.1048 (0.1483) Data: 0.0154 (0.0461) Loss: 0.6305 (0.8831)
[2022/12/29 01:42] | TRAIN(083): [100/879] Batch: 0.1385 (0.1297) Data: 0.0114 (0.0286) Loss: 0.6547 (0.8703)
[2022/12/29 01:42] | TRAIN(083): [150/879] Batch: 0.1005 (0.1245) Data: 0.0147 (0.0229) Loss: 0.6384 (0.8567)
[2022/12/29 01:43] | TRAIN(083): [200/879] Batch: 0.1158 (0.1210) Data: 0.0093 (0.0199) Loss: 0.8238 (0.8457)
[2022/12/29 01:43] | TRAIN(083): [250/879] Batch: 0.1175 (0.1191) Data: 0.0097 (0.0180) Loss: 1.0158 (0.8465)
[2022/12/29 01:43] | TRAIN(083): [300/879] Batch: 0.1142 (0.1182) Data: 0.0097 (0.0168) Loss: 1.1162 (0.8507)
[2022/12/29 01:43] | TRAIN(083): [350/879] Batch: 0.1219 (0.1174) Data: 0.0100 (0.0160) Loss: 1.0862 (0.8553)
[2022/12/29 01:43] | TRAIN(083): [400/879] Batch: 0.1036 (0.1169) Data: 0.0091 (0.0153) Loss: 0.9556 (0.8600)
[2022/12/29 01:43] | TRAIN(083): [450/879] Batch: 0.1086 (0.1161) Data: 0.0096 (0.0148) Loss: 0.7993 (0.8597)
[2022/12/29 01:43] | TRAIN(083): [500/879] Batch: 0.1032 (0.1151) Data: 0.0100 (0.0144) Loss: 0.6056 (0.8634)
[2022/12/29 01:43] | TRAIN(083): [550/879] Batch: 0.1036 (0.1145) Data: 0.0094 (0.0140) Loss: 0.7999 (0.8621)
[2022/12/29 01:43] | TRAIN(083): [600/879] Batch: 0.1267 (0.1141) Data: 0.0125 (0.0137) Loss: 0.7035 (0.8644)
[2022/12/29 01:43] | TRAIN(083): [650/879] Batch: 0.1099 (0.1139) Data: 0.0100 (0.0135) Loss: 1.0519 (0.8670)
[2022/12/29 01:43] | TRAIN(083): [700/879] Batch: 0.1177 (0.1137) Data: 0.0100 (0.0133) Loss: 0.8905 (0.8683)
[2022/12/29 01:44] | TRAIN(083): [750/879] Batch: 0.1370 (0.1138) Data: 0.0142 (0.0131) Loss: 0.9063 (0.8669)
[2022/12/29 01:44] | TRAIN(083): [800/879] Batch: 0.1104 (0.1137) Data: 0.0089 (0.0130) Loss: 0.4579 (0.8664)
[2022/12/29 01:44] | TRAIN(083): [850/879] Batch: 0.1014 (0.1136) Data: 0.0095 (0.0129) Loss: 0.9000 (0.8665)
[2022/12/29 01:44] | ------------------------------------------------------------
[2022/12/29 01:44] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:44] | ------------------------------------------------------------
[2022/12/29 01:44] |    TRAIN(83)     0:01:39     0:00:11     0:01:28      0.8675
[2022/12/29 01:44] | ------------------------------------------------------------
[2022/12/29 01:44] | VALID(083): [ 50/220] Batch: 0.0358 (0.0649) Data: 0.0218 (0.0490) Loss: 0.7830 (0.8464)
[2022/12/29 01:44] | VALID(083): [100/220] Batch: 0.0340 (0.0512) Data: 0.0307 (0.0360) Loss: 1.1071 (0.8685)
[2022/12/29 01:44] | VALID(083): [150/220] Batch: 0.0416 (0.0469) Data: 0.0187 (0.0306) Loss: 0.7594 (0.8607)
[2022/12/29 01:44] | VALID(083): [200/220] Batch: 0.0358 (0.0446) Data: 0.0263 (0.0281) Loss: 0.5197 (0.8682)
[2022/12/29 01:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:44] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:44] |    VALID(83)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:44] | ####################################################################################################
[2022/12/29 01:44] | TRAIN(084): [ 50/879] Batch: 0.1015 (0.1477) Data: 0.0091 (0.0461) Loss: 1.0778 (0.8781)
[2022/12/29 01:44] | TRAIN(084): [100/879] Batch: 0.0987 (0.1261) Data: 0.0096 (0.0283) Loss: 0.8812 (0.8626)
[2022/12/29 01:44] | TRAIN(084): [150/879] Batch: 0.1112 (0.1202) Data: 0.0100 (0.0223) Loss: 0.5052 (0.8643)
[2022/12/29 01:44] | TRAIN(084): [200/879] Batch: 0.1216 (0.1171) Data: 0.0112 (0.0194) Loss: 0.6548 (0.8605)
[2022/12/29 01:44] | TRAIN(084): [250/879] Batch: 0.1008 (0.1154) Data: 0.0095 (0.0176) Loss: 0.6255 (0.8646)
[2022/12/29 01:45] | TRAIN(084): [300/879] Batch: 0.0997 (0.1132) Data: 0.0094 (0.0163) Loss: 1.1849 (0.8641)
[2022/12/29 01:45] | TRAIN(084): [350/879] Batch: 0.0996 (0.1117) Data: 0.0098 (0.0154) Loss: 0.7028 (0.8653)
[2022/12/29 01:45] | TRAIN(084): [400/879] Batch: 0.1142 (0.1105) Data: 0.0093 (0.0147) Loss: 0.8930 (0.8633)
[2022/12/29 01:45] | TRAIN(084): [450/879] Batch: 0.1132 (0.1103) Data: 0.0080 (0.0142) Loss: 0.8600 (0.8652)
[2022/12/29 01:45] | TRAIN(084): [500/879] Batch: 0.0987 (0.1099) Data: 0.0096 (0.0138) Loss: 0.7626 (0.8661)
[2022/12/29 01:45] | TRAIN(084): [550/879] Batch: 0.1007 (0.1098) Data: 0.0101 (0.0135) Loss: 0.9881 (0.8642)
[2022/12/29 01:45] | TRAIN(084): [600/879] Batch: 0.1103 (0.1093) Data: 0.0090 (0.0132) Loss: 0.6396 (0.8633)
[2022/12/29 01:45] | TRAIN(084): [650/879] Batch: 0.1004 (0.1089) Data: 0.0094 (0.0130) Loss: 0.8205 (0.8638)
[2022/12/29 01:45] | TRAIN(084): [700/879] Batch: 0.1147 (0.1087) Data: 0.0101 (0.0128) Loss: 0.8017 (0.8628)
[2022/12/29 01:45] | TRAIN(084): [750/879] Batch: 0.1003 (0.1086) Data: 0.0101 (0.0126) Loss: 0.7045 (0.8656)
[2022/12/29 01:45] | TRAIN(084): [800/879] Batch: 0.1148 (0.1089) Data: 0.0108 (0.0125) Loss: 0.9670 (0.8651)
[2022/12/29 01:45] | TRAIN(084): [850/879] Batch: 0.1154 (0.1087) Data: 0.0095 (0.0124) Loss: 1.1985 (0.8670)
[2022/12/29 01:46] | ------------------------------------------------------------
[2022/12/29 01:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:46] | ------------------------------------------------------------
[2022/12/29 01:46] |    TRAIN(84)     0:01:35     0:00:10     0:01:24      0.8675
[2022/12/29 01:46] | ------------------------------------------------------------
[2022/12/29 01:46] | VALID(084): [ 50/220] Batch: 0.0334 (0.0676) Data: 0.0261 (0.0545) Loss: 0.7839 (0.8466)
[2022/12/29 01:46] | VALID(084): [100/220] Batch: 0.0340 (0.0527) Data: 0.0275 (0.0388) Loss: 1.1058 (0.8684)
[2022/12/29 01:46] | VALID(084): [150/220] Batch: 0.0368 (0.0478) Data: 0.0210 (0.0341) Loss: 0.7602 (0.8607)
[2022/12/29 01:46] | VALID(084): [200/220] Batch: 0.0472 (0.0453) Data: 0.0167 (0.0316) Loss: 0.5224 (0.8681)
[2022/12/29 01:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:46] |    VALID(84)      0.8679      0.7347      0.5002      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:46] | ####################################################################################################
[2022/12/29 01:46] | TRAIN(085): [ 50/879] Batch: 0.1001 (0.1433) Data: 0.0090 (0.0456) Loss: 0.7231 (0.8744)
[2022/12/29 01:46] | TRAIN(085): [100/879] Batch: 0.1008 (0.1232) Data: 0.0103 (0.0280) Loss: 0.8027 (0.8761)
[2022/12/29 01:46] | TRAIN(085): [150/879] Batch: 0.1098 (0.1173) Data: 0.0105 (0.0221) Loss: 0.6929 (0.8687)
[2022/12/29 01:46] | TRAIN(085): [200/879] Batch: 0.1135 (0.1138) Data: 0.0113 (0.0191) Loss: 0.7283 (0.8680)
[2022/12/29 01:46] | TRAIN(085): [250/879] Batch: 0.1077 (0.1120) Data: 0.0111 (0.0173) Loss: 0.8599 (0.8684)
[2022/12/29 01:46] | TRAIN(085): [300/879] Batch: 0.1012 (0.1112) Data: 0.0101 (0.0161) Loss: 0.7744 (0.8663)
[2022/12/29 01:46] | TRAIN(085): [350/879] Batch: 0.1026 (0.1105) Data: 0.0103 (0.0153) Loss: 0.7353 (0.8707)
[2022/12/29 01:46] | TRAIN(085): [400/879] Batch: 0.1097 (0.1101) Data: 0.0118 (0.0148) Loss: 1.1421 (0.8684)
[2022/12/29 01:47] | TRAIN(085): [450/879] Batch: 0.1160 (0.1109) Data: 0.0088 (0.0145) Loss: 0.5290 (0.8655)
[2022/12/29 01:47] | TRAIN(085): [500/879] Batch: 0.1183 (0.1104) Data: 0.0123 (0.0141) Loss: 0.6602 (0.8675)
[2022/12/29 01:47] | TRAIN(085): [550/879] Batch: 0.1048 (0.1104) Data: 0.0098 (0.0138) Loss: 0.8832 (0.8684)
[2022/12/29 01:47] | TRAIN(085): [600/879] Batch: 0.1251 (0.1100) Data: 0.0106 (0.0135) Loss: 0.6782 (0.8670)
[2022/12/29 01:47] | TRAIN(085): [650/879] Batch: 0.1240 (0.1102) Data: 0.0092 (0.0133) Loss: 1.1472 (0.8658)
[2022/12/29 01:47] | TRAIN(085): [700/879] Batch: 0.1096 (0.1102) Data: 0.0100 (0.0131) Loss: 0.6037 (0.8658)
[2022/12/29 01:47] | TRAIN(085): [750/879] Batch: 0.1120 (0.1100) Data: 0.0100 (0.0129) Loss: 0.9459 (0.8645)
[2022/12/29 01:47] | TRAIN(085): [800/879] Batch: 0.1040 (0.1100) Data: 0.0094 (0.0127) Loss: 0.6359 (0.8666)
[2022/12/29 01:47] | TRAIN(085): [850/879] Batch: 0.1160 (0.1096) Data: 0.0168 (0.0126) Loss: 0.8229 (0.8674)
[2022/12/29 01:47] | ------------------------------------------------------------
[2022/12/29 01:47] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:47] | ------------------------------------------------------------
[2022/12/29 01:47] |    TRAIN(85)     0:01:36     0:00:10     0:01:25      0.8675
[2022/12/29 01:47] | ------------------------------------------------------------
[2022/12/29 01:47] | VALID(085): [ 50/220] Batch: 0.0373 (0.0650) Data: 0.0227 (0.0515) Loss: 0.7833 (0.8464)
[2022/12/29 01:47] | VALID(085): [100/220] Batch: 0.0381 (0.0514) Data: 0.0250 (0.0381) Loss: 1.1075 (0.8685)
[2022/12/29 01:47] | VALID(085): [150/220] Batch: 0.0342 (0.0469) Data: 0.0277 (0.0328) Loss: 0.7594 (0.8607)
[2022/12/29 01:47] | VALID(085): [200/220] Batch: 0.0345 (0.0446) Data: 0.0230 (0.0308) Loss: 0.5195 (0.8681)
[2022/12/29 01:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:47] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:47] |    VALID(85)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:47] | ####################################################################################################
[2022/12/29 01:48] | TRAIN(086): [ 50/879] Batch: 0.1003 (0.1470) Data: 0.0113 (0.0458) Loss: 1.0129 (0.9000)
[2022/12/29 01:48] | TRAIN(086): [100/879] Batch: 0.1144 (0.1284) Data: 0.0133 (0.0286) Loss: 0.8578 (0.8810)
[2022/12/29 01:48] | TRAIN(086): [150/879] Batch: 0.1037 (0.1222) Data: 0.0120 (0.0228) Loss: 0.7271 (0.8596)
[2022/12/29 01:48] | TRAIN(086): [200/879] Batch: 0.1068 (0.1191) Data: 0.0097 (0.0200) Loss: 0.7348 (0.8604)
[2022/12/29 01:48] | TRAIN(086): [250/879] Batch: 0.0991 (0.1167) Data: 0.0096 (0.0182) Loss: 0.7656 (0.8637)
[2022/12/29 01:48] | TRAIN(086): [300/879] Batch: 0.1191 (0.1154) Data: 0.0137 (0.0170) Loss: 0.6137 (0.8700)
[2022/12/29 01:48] | TRAIN(086): [350/879] Batch: 0.1113 (0.1143) Data: 0.0117 (0.0161) Loss: 0.8692 (0.8705)
[2022/12/29 01:48] | TRAIN(086): [400/879] Batch: 0.1111 (0.1137) Data: 0.0094 (0.0154) Loss: 0.8511 (0.8714)
[2022/12/29 01:48] | TRAIN(086): [450/879] Batch: 0.1081 (0.1135) Data: 0.0139 (0.0149) Loss: 0.8625 (0.8691)
[2022/12/29 01:48] | TRAIN(086): [500/879] Batch: 0.1160 (0.1128) Data: 0.0162 (0.0145) Loss: 0.8076 (0.8653)
[2022/12/29 01:48] | TRAIN(086): [550/879] Batch: 0.0994 (0.1126) Data: 0.0091 (0.0142) Loss: 0.9353 (0.8688)
[2022/12/29 01:49] | TRAIN(086): [600/879] Batch: 0.0998 (0.1124) Data: 0.0091 (0.0139) Loss: 0.6063 (0.8682)
[2022/12/29 01:49] | TRAIN(086): [650/879] Batch: 0.1005 (0.1121) Data: 0.0094 (0.0137) Loss: 0.9798 (0.8677)
[2022/12/29 01:49] | TRAIN(086): [700/879] Batch: 0.1121 (0.1121) Data: 0.0077 (0.0134) Loss: 0.7176 (0.8667)
[2022/12/29 01:49] | TRAIN(086): [750/879] Batch: 0.1193 (0.1119) Data: 0.0105 (0.0133) Loss: 0.6856 (0.8660)
[2022/12/29 01:49] | TRAIN(086): [800/879] Batch: 0.1001 (0.1117) Data: 0.0096 (0.0131) Loss: 0.6054 (0.8671)
[2022/12/29 01:49] | TRAIN(086): [850/879] Batch: 0.1006 (0.1115) Data: 0.0156 (0.0130) Loss: 0.5865 (0.8657)
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] |    TRAIN(86)     0:01:37     0:00:11     0:01:26      0.8675
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] | VALID(086): [ 50/220] Batch: 0.0371 (0.0685) Data: 0.0294 (0.0538) Loss: 0.7833 (0.8462)
[2022/12/29 01:49] | VALID(086): [100/220] Batch: 0.0382 (0.0533) Data: 0.0287 (0.0400) Loss: 1.1077 (0.8683)
[2022/12/29 01:49] | VALID(086): [150/220] Batch: 0.0322 (0.0484) Data: 0.0246 (0.0346) Loss: 0.7591 (0.8605)
[2022/12/29 01:49] | VALID(086): [200/220] Batch: 0.0424 (0.0459) Data: 0.0238 (0.0322) Loss: 0.5191 (0.8679)
[2022/12/29 01:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:49] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:49] |    VALID(86)      0.8678      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:49] | ####################################################################################################
[2022/12/29 01:49] | TRAIN(087): [ 50/879] Batch: 0.1163 (0.1571) Data: 0.0127 (0.0521) Loss: 0.7613 (0.8803)
[2022/12/29 01:49] | TRAIN(087): [100/879] Batch: 0.1186 (0.1386) Data: 0.0119 (0.0322) Loss: 0.8045 (0.8675)
[2022/12/29 01:50] | TRAIN(087): [150/879] Batch: 0.1096 (0.1305) Data: 0.0096 (0.0252) Loss: 0.8321 (0.8588)
[2022/12/29 01:50] | TRAIN(087): [200/879] Batch: 0.1010 (0.1261) Data: 0.0076 (0.0217) Loss: 0.7684 (0.8794)
[2022/12/29 01:50] | TRAIN(087): [250/879] Batch: 0.0990 (0.1214) Data: 0.0095 (0.0193) Loss: 0.9374 (0.8767)
[2022/12/29 01:50] | TRAIN(087): [300/879] Batch: 0.1000 (0.1182) Data: 0.0093 (0.0178) Loss: 1.1580 (0.8794)
[2022/12/29 01:50] | TRAIN(087): [350/879] Batch: 0.1095 (0.1162) Data: 0.0112 (0.0166) Loss: 1.2124 (0.8730)
[2022/12/29 01:50] | TRAIN(087): [400/879] Batch: 0.1012 (0.1150) Data: 0.0097 (0.0158) Loss: 0.6696 (0.8709)
[2022/12/29 01:50] | TRAIN(087): [450/879] Batch: 0.1017 (0.1141) Data: 0.0103 (0.0152) Loss: 0.8588 (0.8673)
[2022/12/29 01:50] | TRAIN(087): [500/879] Batch: 0.0996 (0.1133) Data: 0.0099 (0.0147) Loss: 0.7810 (0.8682)
[2022/12/29 01:50] | TRAIN(087): [550/879] Batch: 0.1002 (0.1123) Data: 0.0095 (0.0143) Loss: 0.7656 (0.8661)
[2022/12/29 01:50] | TRAIN(087): [600/879] Batch: 0.1188 (0.1117) Data: 0.0094 (0.0140) Loss: 0.7587 (0.8653)
[2022/12/29 01:50] | TRAIN(087): [650/879] Batch: 0.1036 (0.1115) Data: 0.0102 (0.0137) Loss: 0.7108 (0.8646)
[2022/12/29 01:51] | TRAIN(087): [700/879] Batch: 0.1096 (0.1119) Data: 0.0096 (0.0135) Loss: 0.8328 (0.8621)
[2022/12/29 01:51] | TRAIN(087): [750/879] Batch: 0.1238 (0.1120) Data: 0.0104 (0.0133) Loss: 0.8329 (0.8656)
[2022/12/29 01:51] | TRAIN(087): [800/879] Batch: 0.1165 (0.1116) Data: 0.0105 (0.0131) Loss: 1.0048 (0.8694)
[2022/12/29 01:51] | TRAIN(087): [850/879] Batch: 0.1050 (0.1113) Data: 0.0094 (0.0129) Loss: 0.7778 (0.8682)
[2022/12/29 01:51] | ------------------------------------------------------------
[2022/12/29 01:51] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:51] | ------------------------------------------------------------
[2022/12/29 01:51] |    TRAIN(87)     0:01:37     0:00:11     0:01:26      0.8676
[2022/12/29 01:51] | ------------------------------------------------------------
[2022/12/29 01:51] | VALID(087): [ 50/220] Batch: 0.0380 (0.0650) Data: 0.0250 (0.0514) Loss: 0.7838 (0.8463)
[2022/12/29 01:51] | VALID(087): [100/220] Batch: 0.0388 (0.0517) Data: 0.0247 (0.0374) Loss: 1.1072 (0.8683)
[2022/12/29 01:51] | VALID(087): [150/220] Batch: 0.0390 (0.0473) Data: 0.0235 (0.0327) Loss: 0.7595 (0.8605)
[2022/12/29 01:51] | VALID(087): [200/220] Batch: 0.0353 (0.0450) Data: 0.0310 (0.0306) Loss: 0.5201 (0.8680)
[2022/12/29 01:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:51] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:51] |    VALID(87)      0.8678      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:51] | ####################################################################################################
[2022/12/29 01:51] | TRAIN(088): [ 50/879] Batch: 0.1074 (0.1470) Data: 0.0113 (0.0473) Loss: 0.7241 (0.8670)
[2022/12/29 01:51] | TRAIN(088): [100/879] Batch: 0.1254 (0.1281) Data: 0.0122 (0.0291) Loss: 0.8833 (0.8694)
[2022/12/29 01:51] | TRAIN(088): [150/879] Batch: 0.1155 (0.1235) Data: 0.0076 (0.0233) Loss: 0.8907 (0.8572)
[2022/12/29 01:51] | TRAIN(088): [200/879] Batch: 0.1037 (0.1202) Data: 0.0098 (0.0203) Loss: 0.9471 (0.8531)
[2022/12/29 01:52] | TRAIN(088): [250/879] Batch: 0.1241 (0.1183) Data: 0.0115 (0.0184) Loss: 0.5032 (0.8498)
[2022/12/29 01:52] | TRAIN(088): [300/879] Batch: 0.1217 (0.1171) Data: 0.0120 (0.0172) Loss: 0.9616 (0.8527)
[2022/12/29 01:52] | TRAIN(088): [350/879] Batch: 0.1145 (0.1166) Data: 0.0113 (0.0165) Loss: 0.8659 (0.8601)
[2022/12/29 01:52] | TRAIN(088): [400/879] Batch: 0.1032 (0.1155) Data: 0.0111 (0.0158) Loss: 0.7970 (0.8632)
[2022/12/29 01:52] | TRAIN(088): [450/879] Batch: 0.1285 (0.1150) Data: 0.0122 (0.0153) Loss: 0.7580 (0.8629)
[2022/12/29 01:52] | TRAIN(088): [500/879] Batch: 0.1084 (0.1144) Data: 0.0111 (0.0148) Loss: 0.8891 (0.8652)
[2022/12/29 01:52] | TRAIN(088): [550/879] Batch: 0.1122 (0.1143) Data: 0.0150 (0.0144) Loss: 1.0518 (0.8678)
[2022/12/29 01:52] | TRAIN(088): [600/879] Batch: 0.1003 (0.1139) Data: 0.0103 (0.0141) Loss: 0.6535 (0.8643)
[2022/12/29 01:52] | TRAIN(088): [650/879] Batch: 0.1148 (0.1135) Data: 0.0110 (0.0139) Loss: 0.8151 (0.8663)
[2022/12/29 01:52] | TRAIN(088): [700/879] Batch: 0.1271 (0.1135) Data: 0.0158 (0.0137) Loss: 0.9766 (0.8672)
[2022/12/29 01:52] | TRAIN(088): [750/879] Batch: 0.1007 (0.1132) Data: 0.0102 (0.0136) Loss: 1.1239 (0.8663)
[2022/12/29 01:53] | TRAIN(088): [800/879] Batch: 0.1128 (0.1132) Data: 0.0098 (0.0134) Loss: 0.7042 (0.8683)
[2022/12/29 01:53] | TRAIN(088): [850/879] Batch: 0.1005 (0.1131) Data: 0.0097 (0.0133) Loss: 0.5942 (0.8673)
[2022/12/29 01:53] | ------------------------------------------------------------
[2022/12/29 01:53] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:53] | ------------------------------------------------------------
[2022/12/29 01:53] |    TRAIN(88)     0:01:39     0:00:11     0:01:27      0.8675
[2022/12/29 01:53] | ------------------------------------------------------------
[2022/12/29 01:53] | VALID(088): [ 50/220] Batch: 0.0401 (0.0665) Data: 0.0242 (0.0522) Loss: 0.7837 (0.8464)
[2022/12/29 01:53] | VALID(088): [100/220] Batch: 0.0350 (0.0522) Data: 0.0287 (0.0388) Loss: 1.1075 (0.8685)
[2022/12/29 01:53] | VALID(088): [150/220] Batch: 0.0379 (0.0476) Data: 0.0220 (0.0341) Loss: 0.7595 (0.8606)
[2022/12/29 01:53] | VALID(088): [200/220] Batch: 0.0382 (0.0453) Data: 0.0287 (0.0319) Loss: 0.5197 (0.8681)
[2022/12/29 01:53] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:53] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:53] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:53] |    VALID(88)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:53] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:53] | ####################################################################################################
[2022/12/29 01:53] | TRAIN(089): [ 50/879] Batch: 0.0998 (0.1490) Data: 0.0095 (0.0520) Loss: 1.0753 (0.8452)
[2022/12/29 01:53] | TRAIN(089): [100/879] Batch: 0.1057 (0.1290) Data: 0.0102 (0.0315) Loss: 1.1715 (0.8623)
[2022/12/29 01:53] | TRAIN(089): [150/879] Batch: 0.1025 (0.1210) Data: 0.0112 (0.0243) Loss: 0.9528 (0.8640)
[2022/12/29 01:53] | TRAIN(089): [200/879] Batch: 0.1119 (0.1177) Data: 0.0085 (0.0208) Loss: 1.1495 (0.8720)
[2022/12/29 01:53] | TRAIN(089): [250/879] Batch: 0.1117 (0.1161) Data: 0.0102 (0.0188) Loss: 0.4829 (0.8698)
[2022/12/29 01:53] | TRAIN(089): [300/879] Batch: 0.1278 (0.1151) Data: 0.0115 (0.0175) Loss: 1.0113 (0.8741)
[2022/12/29 01:54] | TRAIN(089): [350/879] Batch: 0.1055 (0.1142) Data: 0.0101 (0.0165) Loss: 0.7255 (0.8693)
[2022/12/29 01:54] | TRAIN(089): [400/879] Batch: 0.1210 (0.1132) Data: 0.0144 (0.0157) Loss: 0.6792 (0.8692)
[2022/12/29 01:54] | TRAIN(089): [450/879] Batch: 0.1067 (0.1129) Data: 0.0110 (0.0151) Loss: 0.9557 (0.8702)
[2022/12/29 01:54] | TRAIN(089): [500/879] Batch: 0.1235 (0.1128) Data: 0.0123 (0.0147) Loss: 1.2346 (0.8678)
[2022/12/29 01:54] | TRAIN(089): [550/879] Batch: 0.1033 (0.1129) Data: 0.0099 (0.0144) Loss: 0.8552 (0.8678)
[2022/12/29 01:54] | TRAIN(089): [600/879] Batch: 0.1116 (0.1125) Data: 0.0161 (0.0141) Loss: 0.9635 (0.8681)
[2022/12/29 01:54] | TRAIN(089): [650/879] Batch: 0.1011 (0.1122) Data: 0.0091 (0.0138) Loss: 0.7022 (0.8656)
[2022/12/29 01:54] | TRAIN(089): [700/879] Batch: 0.1097 (0.1123) Data: 0.0102 (0.0136) Loss: 1.0206 (0.8673)
[2022/12/29 01:54] | TRAIN(089): [750/879] Batch: 0.1262 (0.1123) Data: 0.0115 (0.0134) Loss: 1.0342 (0.8677)
[2022/12/29 01:54] | TRAIN(089): [800/879] Batch: 0.1117 (0.1124) Data: 0.0149 (0.0132) Loss: 1.0376 (0.8684)
[2022/12/29 01:54] | TRAIN(089): [850/879] Batch: 0.1169 (0.1125) Data: 0.0115 (0.0131) Loss: 0.7408 (0.8676)
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] |    TRAIN(89)     0:01:38     0:00:11     0:01:27      0.8675
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] | VALID(089): [ 50/220] Batch: 0.0392 (0.0666) Data: 0.0277 (0.0541) Loss: 0.7833 (0.8463)
[2022/12/29 01:55] | VALID(089): [100/220] Batch: 0.0436 (0.0525) Data: 0.0235 (0.0398) Loss: 1.1077 (0.8684)
[2022/12/29 01:55] | VALID(089): [150/220] Batch: 0.0331 (0.0477) Data: 0.0315 (0.0351) Loss: 0.7592 (0.8605)
[2022/12/29 01:55] | VALID(089): [200/220] Batch: 0.0369 (0.0454) Data: 0.0286 (0.0327) Loss: 0.5189 (0.8680)
[2022/12/29 01:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:55] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:55] |    VALID(89)      0.8678      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:55] | ####################################################################################################
[2022/12/29 01:55] | TRAIN(090): [ 50/879] Batch: 0.1054 (0.1520) Data: 0.0103 (0.0523) Loss: 0.6049 (0.8830)
[2022/12/29 01:55] | TRAIN(090): [100/879] Batch: 0.1153 (0.1305) Data: 0.0105 (0.0317) Loss: 0.9541 (0.8815)
[2022/12/29 01:55] | TRAIN(090): [150/879] Batch: 0.1100 (0.1239) Data: 0.0143 (0.0252) Loss: 1.1533 (0.8780)
[2022/12/29 01:55] | TRAIN(090): [200/879] Batch: 0.1155 (0.1217) Data: 0.0095 (0.0218) Loss: 1.1501 (0.8713)
[2022/12/29 01:55] | TRAIN(090): [250/879] Batch: 0.1010 (0.1191) Data: 0.0101 (0.0197) Loss: 0.8042 (0.8703)
[2022/12/29 01:55] | TRAIN(090): [300/879] Batch: 0.1030 (0.1173) Data: 0.0098 (0.0182) Loss: 0.9036 (0.8724)
[2022/12/29 01:55] | TRAIN(090): [350/879] Batch: 0.1100 (0.1162) Data: 0.0183 (0.0172) Loss: 0.9532 (0.8718)
[2022/12/29 01:55] | TRAIN(090): [400/879] Batch: 0.1012 (0.1153) Data: 0.0097 (0.0165) Loss: 1.1409 (0.8777)
[2022/12/29 01:56] | TRAIN(090): [450/879] Batch: 0.1117 (0.1148) Data: 0.0118 (0.0159) Loss: 0.9955 (0.8741)
[2022/12/29 01:56] | TRAIN(090): [500/879] Batch: 0.1047 (0.1147) Data: 0.0099 (0.0154) Loss: 0.7320 (0.8745)
[2022/12/29 01:56] | TRAIN(090): [550/879] Batch: 0.1130 (0.1139) Data: 0.0097 (0.0149) Loss: 0.8483 (0.8760)
[2022/12/29 01:56] | TRAIN(090): [600/879] Batch: 0.1191 (0.1135) Data: 0.0095 (0.0146) Loss: 0.9549 (0.8743)
[2022/12/29 01:56] | TRAIN(090): [650/879] Batch: 0.1066 (0.1130) Data: 0.0095 (0.0142) Loss: 0.9214 (0.8694)
[2022/12/29 01:56] | TRAIN(090): [700/879] Batch: 0.1049 (0.1128) Data: 0.0097 (0.0140) Loss: 0.7571 (0.8708)
[2022/12/29 01:56] | TRAIN(090): [750/879] Batch: 0.1121 (0.1125) Data: 0.0099 (0.0137) Loss: 1.0160 (0.8709)
[2022/12/29 01:56] | TRAIN(090): [800/879] Batch: 0.1040 (0.1121) Data: 0.0093 (0.0135) Loss: 0.8886 (0.8692)
[2022/12/29 01:56] | TRAIN(090): [850/879] Batch: 0.1003 (0.1118) Data: 0.0097 (0.0133) Loss: 0.7036 (0.8683)
[2022/12/29 01:56] | ------------------------------------------------------------
[2022/12/29 01:56] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:56] | ------------------------------------------------------------
[2022/12/29 01:56] |    TRAIN(90)     0:01:38     0:00:11     0:01:26      0.8675
[2022/12/29 01:56] | ------------------------------------------------------------
[2022/12/29 01:56] | VALID(090): [ 50/220] Batch: 0.0363 (0.0661) Data: 0.0181 (0.0520) Loss: 0.7831 (0.8463)
[2022/12/29 01:56] | VALID(090): [100/220] Batch: 0.0389 (0.0523) Data: 0.0248 (0.0385) Loss: 1.1083 (0.8685)
[2022/12/29 01:56] | VALID(090): [150/220] Batch: 0.0391 (0.0477) Data: 0.0236 (0.0335) Loss: 0.7590 (0.8606)
[2022/12/29 01:56] | VALID(090): [200/220] Batch: 0.0381 (0.0454) Data: 0.0263 (0.0312) Loss: 0.5180 (0.8681)
[2022/12/29 01:57] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:57] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:57] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:57] |    VALID(90)      0.8679      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:57] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:57] | ####################################################################################################
[2022/12/29 01:57] | TRAIN(091): [ 50/879] Batch: 0.1081 (0.1416) Data: 0.0096 (0.0457) Loss: 0.9633 (0.8882)
[2022/12/29 01:57] | TRAIN(091): [100/879] Batch: 0.1013 (0.1223) Data: 0.0104 (0.0280) Loss: 0.6277 (0.8817)
[2022/12/29 01:57] | TRAIN(091): [150/879] Batch: 0.1157 (0.1178) Data: 0.0098 (0.0221) Loss: 1.1506 (0.8775)
[2022/12/29 01:57] | TRAIN(091): [200/879] Batch: 0.1016 (0.1160) Data: 0.0098 (0.0193) Loss: 0.9806 (0.8705)
[2022/12/29 01:57] | TRAIN(091): [250/879] Batch: 0.1099 (0.1146) Data: 0.0102 (0.0175) Loss: 0.8085 (0.8587)
[2022/12/29 01:57] | TRAIN(091): [300/879] Batch: 0.1008 (0.1134) Data: 0.0100 (0.0162) Loss: 0.5545 (0.8673)
[2022/12/29 01:57] | TRAIN(091): [350/879] Batch: 0.1050 (0.1123) Data: 0.0093 (0.0153) Loss: 0.6036 (0.8692)
[2022/12/29 01:57] | TRAIN(091): [400/879] Batch: 0.1121 (0.1117) Data: 0.0098 (0.0146) Loss: 0.9527 (0.8657)
[2022/12/29 01:57] | TRAIN(091): [450/879] Batch: 0.1217 (0.1116) Data: 0.0114 (0.0142) Loss: 1.0631 (0.8650)
[2022/12/29 01:57] | TRAIN(091): [500/879] Batch: 0.1148 (0.1115) Data: 0.0104 (0.0138) Loss: 0.9144 (0.8641)
[2022/12/29 01:58] | TRAIN(091): [550/879] Batch: 0.1125 (0.1109) Data: 0.0114 (0.0134) Loss: 0.7354 (0.8664)
[2022/12/29 01:58] | TRAIN(091): [600/879] Batch: 0.1000 (0.1108) Data: 0.0095 (0.0132) Loss: 0.9261 (0.8665)
[2022/12/29 01:58] | TRAIN(091): [650/879] Batch: 0.1178 (0.1109) Data: 0.0119 (0.0130) Loss: 1.0318 (0.8681)
[2022/12/29 01:58] | TRAIN(091): [700/879] Batch: 0.1178 (0.1107) Data: 0.0122 (0.0128) Loss: 0.8240 (0.8691)
[2022/12/29 01:58] | TRAIN(091): [750/879] Batch: 0.1015 (0.1105) Data: 0.0096 (0.0127) Loss: 0.9321 (0.8681)
[2022/12/29 01:58] | TRAIN(091): [800/879] Batch: 0.1022 (0.1103) Data: 0.0102 (0.0125) Loss: 0.6184 (0.8681)
[2022/12/29 01:58] | TRAIN(091): [850/879] Batch: 0.1001 (0.1100) Data: 0.0092 (0.0124) Loss: 0.7149 (0.8681)
[2022/12/29 01:58] | ------------------------------------------------------------
[2022/12/29 01:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:58] | ------------------------------------------------------------
[2022/12/29 01:58] |    TRAIN(91)     0:01:36     0:00:10     0:01:25      0.8674
[2022/12/29 01:58] | ------------------------------------------------------------
[2022/12/29 01:58] | VALID(091): [ 50/220] Batch: 0.0314 (0.0636) Data: 0.0326 (0.0496) Loss: 0.7831 (0.8462)
[2022/12/29 01:58] | VALID(091): [100/220] Batch: 0.0373 (0.0510) Data: 0.0269 (0.0374) Loss: 1.1077 (0.8683)
[2022/12/29 01:58] | VALID(091): [150/220] Batch: 0.0385 (0.0468) Data: 0.0264 (0.0336) Loss: 0.7590 (0.8605)
[2022/12/29 01:58] | VALID(091): [200/220] Batch: 0.0424 (0.0446) Data: 0.0263 (0.0313) Loss: 0.5186 (0.8680)
[2022/12/29 01:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:58] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:58] |    VALID(91)      0.8678      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:58] | ####################################################################################################
[2022/12/29 01:58] | TRAIN(092): [ 50/879] Batch: 0.1098 (0.1477) Data: 0.0094 (0.0462) Loss: 0.8101 (0.9086)
[2022/12/29 01:58] | TRAIN(092): [100/879] Batch: 0.1069 (0.1267) Data: 0.0101 (0.0282) Loss: 0.8163 (0.9069)
[2022/12/29 01:59] | TRAIN(092): [150/879] Batch: 0.1127 (0.1224) Data: 0.0105 (0.0226) Loss: 1.1141 (0.8885)
[2022/12/29 01:59] | TRAIN(092): [200/879] Batch: 0.0997 (0.1181) Data: 0.0088 (0.0195) Loss: 0.7487 (0.8864)
[2022/12/29 01:59] | TRAIN(092): [250/879] Batch: 0.1074 (0.1155) Data: 0.0099 (0.0177) Loss: 0.7762 (0.8818)
[2022/12/29 01:59] | TRAIN(092): [300/879] Batch: 0.1010 (0.1141) Data: 0.0101 (0.0164) Loss: 0.9592 (0.8810)
[2022/12/29 01:59] | TRAIN(092): [350/879] Batch: 0.1018 (0.1130) Data: 0.0098 (0.0155) Loss: 0.8816 (0.8817)
[2022/12/29 01:59] | TRAIN(092): [400/879] Batch: 0.1001 (0.1124) Data: 0.0096 (0.0148) Loss: 0.7121 (0.8754)
[2022/12/29 01:59] | TRAIN(092): [450/879] Batch: 0.1031 (0.1118) Data: 0.0097 (0.0143) Loss: 0.7274 (0.8742)
[2022/12/29 01:59] | TRAIN(092): [500/879] Batch: 0.1252 (0.1115) Data: 0.0116 (0.0139) Loss: 1.0380 (0.8756)
[2022/12/29 01:59] | TRAIN(092): [550/879] Batch: 0.1145 (0.1116) Data: 0.0087 (0.0137) Loss: 0.8026 (0.8730)
[2022/12/29 01:59] | TRAIN(092): [600/879] Batch: 0.1121 (0.1112) Data: 0.0100 (0.0134) Loss: 0.8114 (0.8712)
[2022/12/29 01:59] | TRAIN(092): [650/879] Batch: 0.1059 (0.1109) Data: 0.0106 (0.0131) Loss: 0.8650 (0.8729)
[2022/12/29 02:00] | TRAIN(092): [700/879] Batch: 0.1242 (0.1113) Data: 0.0121 (0.0130) Loss: 0.6426 (0.8707)
[2022/12/29 02:00] | TRAIN(092): [750/879] Batch: 0.1195 (0.1114) Data: 0.0120 (0.0129) Loss: 0.7844 (0.8701)
[2022/12/29 02:00] | TRAIN(092): [800/879] Batch: 0.1002 (0.1111) Data: 0.0101 (0.0127) Loss: 0.6675 (0.8683)
[2022/12/29 02:00] | TRAIN(092): [850/879] Batch: 0.1092 (0.1109) Data: 0.0092 (0.0125) Loss: 0.9475 (0.8682)
[2022/12/29 02:00] | ------------------------------------------------------------
[2022/12/29 02:00] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:00] | ------------------------------------------------------------
[2022/12/29 02:00] |    TRAIN(92)     0:01:37     0:00:10     0:01:26      0.8675
[2022/12/29 02:00] | ------------------------------------------------------------
[2022/12/29 02:00] | VALID(092): [ 50/220] Batch: 0.0396 (0.0649) Data: 0.0281 (0.0515) Loss: 0.7832 (0.8463)
[2022/12/29 02:00] | VALID(092): [100/220] Batch: 0.0399 (0.0514) Data: 0.0264 (0.0387) Loss: 1.1079 (0.8684)
[2022/12/29 02:00] | VALID(092): [150/220] Batch: 0.0380 (0.0469) Data: 0.0276 (0.0341) Loss: 0.7590 (0.8605)
[2022/12/29 02:00] | VALID(092): [200/220] Batch: 0.0389 (0.0447) Data: 0.0280 (0.0318) Loss: 0.5185 (0.8680)
[2022/12/29 02:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:00] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:00] |    VALID(92)      0.8678      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:00] | ####################################################################################################
[2022/12/29 02:00] | TRAIN(093): [ 50/879] Batch: 0.1058 (0.1450) Data: 0.0096 (0.0434) Loss: 0.7418 (0.8878)
[2022/12/29 02:00] | TRAIN(093): [100/879] Batch: 0.1001 (0.1283) Data: 0.0107 (0.0273) Loss: 1.0364 (0.8762)
[2022/12/29 02:00] | TRAIN(093): [150/879] Batch: 0.1053 (0.1224) Data: 0.0099 (0.0218) Loss: 0.6614 (0.8640)
[2022/12/29 02:00] | TRAIN(093): [200/879] Batch: 0.1000 (0.1180) Data: 0.0103 (0.0189) Loss: 0.9462 (0.8685)
[2022/12/29 02:01] | TRAIN(093): [250/879] Batch: 0.0980 (0.1156) Data: 0.0094 (0.0172) Loss: 1.1854 (0.8701)
[2022/12/29 02:01] | TRAIN(093): [300/879] Batch: 0.1118 (0.1139) Data: 0.0106 (0.0160) Loss: 0.5752 (0.8603)
[2022/12/29 02:01] | TRAIN(093): [350/879] Batch: 0.1094 (0.1133) Data: 0.0098 (0.0152) Loss: 0.9984 (0.8642)
[2022/12/29 02:01] | TRAIN(093): [400/879] Batch: 0.1076 (0.1125) Data: 0.0110 (0.0146) Loss: 0.8792 (0.8648)
[2022/12/29 02:01] | TRAIN(093): [450/879] Batch: 0.0996 (0.1121) Data: 0.0103 (0.0141) Loss: 0.8142 (0.8656)
[2022/12/29 02:01] | TRAIN(093): [500/879] Batch: 0.1003 (0.1113) Data: 0.0101 (0.0137) Loss: 1.1185 (0.8708)
[2022/12/29 02:01] | TRAIN(093): [550/879] Batch: 0.1081 (0.1110) Data: 0.0095 (0.0134) Loss: 0.7809 (0.8669)
[2022/12/29 02:01] | TRAIN(093): [600/879] Batch: 0.1002 (0.1104) Data: 0.0095 (0.0131) Loss: 0.9539 (0.8645)
[2022/12/29 02:01] | TRAIN(093): [650/879] Batch: 0.1001 (0.1101) Data: 0.0099 (0.0128) Loss: 1.0541 (0.8660)
[2022/12/29 02:01] | TRAIN(093): [700/879] Batch: 0.1063 (0.1100) Data: 0.0101 (0.0127) Loss: 0.9205 (0.8666)
[2022/12/29 02:01] | TRAIN(093): [750/879] Batch: 0.1100 (0.1104) Data: 0.0126 (0.0126) Loss: 1.2235 (0.8673)
[2022/12/29 02:02] | TRAIN(093): [800/879] Batch: 0.1032 (0.1104) Data: 0.0105 (0.0125) Loss: 0.8156 (0.8671)
[2022/12/29 02:02] | TRAIN(093): [850/879] Batch: 0.1172 (0.1103) Data: 0.0142 (0.0124) Loss: 0.6552 (0.8677)
[2022/12/29 02:02] | ------------------------------------------------------------
[2022/12/29 02:02] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:02] | ------------------------------------------------------------
[2022/12/29 02:02] |    TRAIN(93)     0:01:36     0:00:10     0:01:26      0.8675
[2022/12/29 02:02] | ------------------------------------------------------------
[2022/12/29 02:02] | VALID(093): [ 50/220] Batch: 0.0376 (0.0647) Data: 0.0270 (0.0510) Loss: 0.7834 (0.8464)
[2022/12/29 02:02] | VALID(093): [100/220] Batch: 0.0392 (0.0513) Data: 0.0259 (0.0383) Loss: 1.1077 (0.8685)
[2022/12/29 02:02] | VALID(093): [150/220] Batch: 0.0387 (0.0469) Data: 0.0272 (0.0341) Loss: 0.7593 (0.8606)
[2022/12/29 02:02] | VALID(093): [200/220] Batch: 0.0380 (0.0446) Data: 0.0288 (0.0320) Loss: 0.5192 (0.8681)
[2022/12/29 02:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:02] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:02] |    VALID(93)      0.8679      0.7347      0.4972      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:02] | ####################################################################################################
[2022/12/29 02:02] | TRAIN(094): [ 50/879] Batch: 0.1029 (0.1462) Data: 0.0150 (0.0488) Loss: 0.7420 (0.8366)
[2022/12/29 02:02] | TRAIN(094): [100/879] Batch: 0.1115 (0.1279) Data: 0.0099 (0.0301) Loss: 0.7028 (0.8401)
[2022/12/29 02:02] | TRAIN(094): [150/879] Batch: 0.0987 (0.1215) Data: 0.0096 (0.0237) Loss: 0.8721 (0.8469)
[2022/12/29 02:02] | TRAIN(094): [200/879] Batch: 0.1061 (0.1185) Data: 0.0104 (0.0205) Loss: 0.7771 (0.8515)
[2022/12/29 02:02] | TRAIN(094): [250/879] Batch: 0.1155 (0.1167) Data: 0.0083 (0.0185) Loss: 0.6033 (0.8548)
[2022/12/29 02:02] | TRAIN(094): [300/879] Batch: 0.1001 (0.1147) Data: 0.0095 (0.0171) Loss: 0.9285 (0.8556)
[2022/12/29 02:03] | TRAIN(094): [350/879] Batch: 0.1250 (0.1141) Data: 0.0119 (0.0162) Loss: 0.8582 (0.8535)
[2022/12/29 02:03] | TRAIN(094): [400/879] Batch: 0.1122 (0.1139) Data: 0.0087 (0.0156) Loss: 1.0239 (0.8556)
[2022/12/29 02:03] | TRAIN(094): [450/879] Batch: 0.0998 (0.1131) Data: 0.0114 (0.0149) Loss: 0.8601 (0.8554)
[2022/12/29 02:03] | TRAIN(094): [500/879] Batch: 0.1033 (0.1128) Data: 0.0095 (0.0145) Loss: 0.7966 (0.8600)
[2022/12/29 02:03] | TRAIN(094): [550/879] Batch: 0.1010 (0.1128) Data: 0.0140 (0.0142) Loss: 0.7809 (0.8597)
[2022/12/29 02:03] | TRAIN(094): [600/879] Batch: 0.1009 (0.1124) Data: 0.0087 (0.0138) Loss: 0.9136 (0.8619)
[2022/12/29 02:03] | TRAIN(094): [650/879] Batch: 0.1060 (0.1120) Data: 0.0085 (0.0136) Loss: 0.8546 (0.8608)
[2022/12/29 02:03] | TRAIN(094): [700/879] Batch: 0.1291 (0.1120) Data: 0.0095 (0.0134) Loss: 0.9776 (0.8642)
[2022/12/29 02:03] | TRAIN(094): [750/879] Batch: 0.1199 (0.1120) Data: 0.0123 (0.0132) Loss: 0.8157 (0.8659)
[2022/12/29 02:03] | TRAIN(094): [800/879] Batch: 0.1144 (0.1118) Data: 0.0085 (0.0130) Loss: 0.8396 (0.8669)
[2022/12/29 02:03] | TRAIN(094): [850/879] Batch: 0.1108 (0.1116) Data: 0.0074 (0.0128) Loss: 1.0775 (0.8674)
[2022/12/29 02:03] | ------------------------------------------------------------
[2022/12/29 02:03] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:03] | ------------------------------------------------------------
[2022/12/29 02:03] |    TRAIN(94)     0:01:38     0:00:11     0:01:26      0.8675
[2022/12/29 02:03] | ------------------------------------------------------------
[2022/12/29 02:04] | VALID(094): [ 50/220] Batch: 0.0397 (0.0648) Data: 0.0266 (0.0505) Loss: 0.7833 (0.8463)
[2022/12/29 02:04] | VALID(094): [100/220] Batch: 0.0396 (0.0515) Data: 0.0255 (0.0371) Loss: 1.1077 (0.8684)
[2022/12/29 02:04] | VALID(094): [150/220] Batch: 0.0338 (0.0470) Data: 0.0272 (0.0331) Loss: 0.7592 (0.8605)
[2022/12/29 02:04] | VALID(094): [200/220] Batch: 0.0373 (0.0448) Data: 0.0272 (0.0312) Loss: 0.5192 (0.8680)
[2022/12/29 02:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:04] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:04] |    VALID(94)      0.8678      0.7347      0.4998      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:04] | ####################################################################################################
[2022/12/29 02:04] | TRAIN(095): [ 50/879] Batch: 0.1072 (0.1436) Data: 0.0098 (0.0460) Loss: 0.8913 (0.8275)
[2022/12/29 02:04] | TRAIN(095): [100/879] Batch: 0.1144 (0.1260) Data: 0.0101 (0.0284) Loss: 0.9750 (0.8449)
[2022/12/29 02:04] | TRAIN(095): [150/879] Batch: 0.1100 (0.1207) Data: 0.0093 (0.0225) Loss: 1.0129 (0.8571)
[2022/12/29 02:04] | TRAIN(095): [200/879] Batch: 0.1165 (0.1175) Data: 0.0093 (0.0194) Loss: 1.0375 (0.8596)
[2022/12/29 02:04] | TRAIN(095): [250/879] Batch: 0.1021 (0.1156) Data: 0.0100 (0.0176) Loss: 0.9805 (0.8694)
[2022/12/29 02:04] | TRAIN(095): [300/879] Batch: 0.1116 (0.1141) Data: 0.0097 (0.0164) Loss: 0.6188 (0.8664)
[2022/12/29 02:04] | TRAIN(095): [350/879] Batch: 0.1000 (0.1128) Data: 0.0100 (0.0155) Loss: 0.8779 (0.8705)
[2022/12/29 02:04] | TRAIN(095): [400/879] Batch: 0.1129 (0.1129) Data: 0.0124 (0.0149) Loss: 0.9131 (0.8705)
[2022/12/29 02:04] | TRAIN(095): [450/879] Batch: 0.1130 (0.1125) Data: 0.0108 (0.0145) Loss: 0.6853 (0.8661)
[2022/12/29 02:05] | TRAIN(095): [500/879] Batch: 0.1063 (0.1120) Data: 0.0097 (0.0141) Loss: 0.4070 (0.8644)
[2022/12/29 02:05] | TRAIN(095): [550/879] Batch: 0.1059 (0.1122) Data: 0.0101 (0.0138) Loss: 0.8264 (0.8651)
[2022/12/29 02:05] | TRAIN(095): [600/879] Batch: 0.0993 (0.1117) Data: 0.0095 (0.0135) Loss: 0.9135 (0.8640)
[2022/12/29 02:05] | TRAIN(095): [650/879] Batch: 0.1163 (0.1113) Data: 0.0113 (0.0133) Loss: 1.0246 (0.8683)
[2022/12/29 02:05] | TRAIN(095): [700/879] Batch: 0.1096 (0.1112) Data: 0.0097 (0.0131) Loss: 1.0120 (0.8686)
[2022/12/29 02:05] | TRAIN(095): [750/879] Batch: 0.1104 (0.1109) Data: 0.0098 (0.0129) Loss: 1.3768 (0.8693)
[2022/12/29 02:05] | TRAIN(095): [800/879] Batch: 0.1113 (0.1105) Data: 0.0094 (0.0127) Loss: 0.9471 (0.8687)
[2022/12/29 02:05] | TRAIN(095): [850/879] Batch: 0.1134 (0.1102) Data: 0.0109 (0.0125) Loss: 0.9774 (0.8680)
[2022/12/29 02:05] | ------------------------------------------------------------
[2022/12/29 02:05] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:05] | ------------------------------------------------------------
[2022/12/29 02:05] |    TRAIN(95)     0:01:36     0:00:10     0:01:25      0.8674
[2022/12/29 02:05] | ------------------------------------------------------------
[2022/12/29 02:05] | VALID(095): [ 50/220] Batch: 0.0341 (0.0657) Data: 0.0314 (0.0530) Loss: 0.7833 (0.8463)
[2022/12/29 02:05] | VALID(095): [100/220] Batch: 0.0383 (0.0520) Data: 0.0273 (0.0396) Loss: 1.1075 (0.8683)
[2022/12/29 02:05] | VALID(095): [150/220] Batch: 0.0376 (0.0473) Data: 0.0271 (0.0350) Loss: 0.7592 (0.8605)
[2022/12/29 02:05] | VALID(095): [200/220] Batch: 0.0396 (0.0450) Data: 0.0265 (0.0327) Loss: 0.5191 (0.8680)
[2022/12/29 02:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:05] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:05] |    VALID(95)      0.8678      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:05] | ####################################################################################################
[2022/12/29 02:06] | TRAIN(096): [ 50/879] Batch: 0.1182 (0.1543) Data: 0.0108 (0.0531) Loss: 0.9649 (0.8763)
[2022/12/29 02:06] | TRAIN(096): [100/879] Batch: 0.1244 (0.1344) Data: 0.0129 (0.0326) Loss: 0.7280 (0.8629)
[2022/12/29 02:06] | TRAIN(096): [150/879] Batch: 0.1221 (0.1270) Data: 0.0081 (0.0254) Loss: 0.6282 (0.8734)
[2022/12/29 02:06] | TRAIN(096): [200/879] Batch: 0.1082 (0.1223) Data: 0.0093 (0.0218) Loss: 0.8788 (0.8792)
[2022/12/29 02:06] | TRAIN(096): [250/879] Batch: 0.1059 (0.1195) Data: 0.0098 (0.0196) Loss: 0.6818 (0.8742)
[2022/12/29 02:06] | TRAIN(096): [300/879] Batch: 0.0997 (0.1181) Data: 0.0126 (0.0181) Loss: 0.9124 (0.8719)
[2022/12/29 02:06] | TRAIN(096): [350/879] Batch: 0.1227 (0.1170) Data: 0.0119 (0.0171) Loss: 0.8956 (0.8697)
[2022/12/29 02:06] | TRAIN(096): [400/879] Batch: 0.1042 (0.1163) Data: 0.0100 (0.0162) Loss: 0.9991 (0.8761)
[2022/12/29 02:06] | TRAIN(096): [450/879] Batch: 0.1069 (0.1156) Data: 0.0107 (0.0156) Loss: 0.8909 (0.8749)
[2022/12/29 02:06] | TRAIN(096): [500/879] Batch: 0.1147 (0.1142) Data: 0.0092 (0.0151) Loss: 0.9864 (0.8743)
[2022/12/29 02:06] | TRAIN(096): [550/879] Batch: 0.1053 (0.1138) Data: 0.0094 (0.0146) Loss: 0.8723 (0.8739)
[2022/12/29 02:07] | TRAIN(096): [600/879] Batch: 0.1085 (0.1132) Data: 0.0104 (0.0143) Loss: 0.9251 (0.8727)
[2022/12/29 02:07] | TRAIN(096): [650/879] Batch: 0.1246 (0.1128) Data: 0.0117 (0.0140) Loss: 0.7589 (0.8713)
[2022/12/29 02:07] | TRAIN(096): [700/879] Batch: 0.1133 (0.1126) Data: 0.0096 (0.0137) Loss: 0.9372 (0.8726)
[2022/12/29 02:07] | TRAIN(096): [750/879] Batch: 0.1008 (0.1126) Data: 0.0101 (0.0135) Loss: 0.7835 (0.8698)
[2022/12/29 02:07] | TRAIN(096): [800/879] Batch: 0.1004 (0.1121) Data: 0.0109 (0.0133) Loss: 0.7527 (0.8678)
[2022/12/29 02:07] | TRAIN(096): [850/879] Batch: 0.1151 (0.1117) Data: 0.0099 (0.0131) Loss: 0.8522 (0.8677)
[2022/12/29 02:07] | ------------------------------------------------------------
[2022/12/29 02:07] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:07] | ------------------------------------------------------------
[2022/12/29 02:07] |    TRAIN(96)     0:01:37     0:00:11     0:01:26      0.8674
[2022/12/29 02:07] | ------------------------------------------------------------
[2022/12/29 02:07] | VALID(096): [ 50/220] Batch: 0.0351 (0.0665) Data: 0.0232 (0.0516) Loss: 0.7834 (0.8463)
[2022/12/29 02:07] | VALID(096): [100/220] Batch: 0.0377 (0.0522) Data: 0.0249 (0.0379) Loss: 1.1075 (0.8684)
[2022/12/29 02:07] | VALID(096): [150/220] Batch: 0.0388 (0.0475) Data: 0.0287 (0.0329) Loss: 0.7592 (0.8605)
[2022/12/29 02:07] | VALID(096): [200/220] Batch: 0.0380 (0.0451) Data: 0.0281 (0.0306) Loss: 0.5193 (0.8680)
[2022/12/29 02:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:07] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:07] |    VALID(96)      0.8678      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:07] | ####################################################################################################
[2022/12/29 02:07] | TRAIN(097): [ 50/879] Batch: 0.1007 (0.1435) Data: 0.0091 (0.0462) Loss: 0.5621 (0.8563)
[2022/12/29 02:07] | TRAIN(097): [100/879] Batch: 0.1223 (0.1235) Data: 0.0102 (0.0282) Loss: 0.9714 (0.8609)
[2022/12/29 02:08] | TRAIN(097): [150/879] Batch: 0.1141 (0.1202) Data: 0.0100 (0.0226) Loss: 0.9136 (0.8547)
[2022/12/29 02:08] | TRAIN(097): [200/879] Batch: 0.1173 (0.1165) Data: 0.0093 (0.0196) Loss: 0.9222 (0.8538)
[2022/12/29 02:08] | TRAIN(097): [250/879] Batch: 0.1002 (0.1151) Data: 0.0118 (0.0179) Loss: 0.8235 (0.8587)
[2022/12/29 02:08] | TRAIN(097): [300/879] Batch: 0.1074 (0.1138) Data: 0.0099 (0.0166) Loss: 0.5792 (0.8594)
[2022/12/29 02:08] | TRAIN(097): [350/879] Batch: 0.1013 (0.1132) Data: 0.0117 (0.0156) Loss: 0.6996 (0.8602)
[2022/12/29 02:08] | TRAIN(097): [400/879] Batch: 0.1001 (0.1125) Data: 0.0106 (0.0149) Loss: 0.5064 (0.8567)
[2022/12/29 02:08] | TRAIN(097): [450/879] Batch: 0.1061 (0.1123) Data: 0.0100 (0.0145) Loss: 0.7662 (0.8625)
[2022/12/29 02:08] | TRAIN(097): [500/879] Batch: 0.1182 (0.1118) Data: 0.0091 (0.0141) Loss: 0.7975 (0.8584)
[2022/12/29 02:08] | TRAIN(097): [550/879] Batch: 0.1006 (0.1119) Data: 0.0098 (0.0138) Loss: 1.0137 (0.8602)
[2022/12/29 02:08] | TRAIN(097): [600/879] Batch: 0.1245 (0.1116) Data: 0.0116 (0.0135) Loss: 0.8012 (0.8607)
[2022/12/29 02:08] | TRAIN(097): [650/879] Batch: 0.1100 (0.1115) Data: 0.0101 (0.0132) Loss: 1.2113 (0.8604)
[2022/12/29 02:09] | TRAIN(097): [700/879] Batch: 0.1011 (0.1116) Data: 0.0098 (0.0131) Loss: 1.1648 (0.8618)
[2022/12/29 02:09] | TRAIN(097): [750/879] Batch: 0.1074 (0.1111) Data: 0.0096 (0.0129) Loss: 1.0071 (0.8629)
[2022/12/29 02:09] | TRAIN(097): [800/879] Batch: 0.1262 (0.1110) Data: 0.0115 (0.0127) Loss: 0.9069 (0.8642)
[2022/12/29 02:09] | TRAIN(097): [850/879] Batch: 0.1085 (0.1109) Data: 0.0109 (0.0126) Loss: 0.7161 (0.8668)
[2022/12/29 02:09] | ------------------------------------------------------------
[2022/12/29 02:09] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:09] | ------------------------------------------------------------
[2022/12/29 02:09] |    TRAIN(97)     0:01:37     0:00:10     0:01:26      0.8674
[2022/12/29 02:09] | ------------------------------------------------------------
[2022/12/29 02:09] | VALID(097): [ 50/220] Batch: 0.0391 (0.0657) Data: 0.0281 (0.0513) Loss: 0.7833 (0.8463)
[2022/12/29 02:09] | VALID(097): [100/220] Batch: 0.0384 (0.0519) Data: 0.0253 (0.0379) Loss: 1.1077 (0.8684)
[2022/12/29 02:09] | VALID(097): [150/220] Batch: 0.0396 (0.0474) Data: 0.0267 (0.0334) Loss: 0.7592 (0.8605)
[2022/12/29 02:09] | VALID(097): [200/220] Batch: 0.0408 (0.0450) Data: 0.0232 (0.0310) Loss: 0.5189 (0.8680)
[2022/12/29 02:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:09] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:09] |    VALID(97)      0.8678      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:09] | ####################################################################################################
[2022/12/29 02:09] | TRAIN(098): [ 50/879] Batch: 0.1185 (0.1481) Data: 0.0131 (0.0501) Loss: 1.1688 (0.8906)
[2022/12/29 02:09] | TRAIN(098): [100/879] Batch: 0.1103 (0.1266) Data: 0.0098 (0.0303) Loss: 1.0023 (0.8875)
[2022/12/29 02:09] | TRAIN(098): [150/879] Batch: 0.1021 (0.1204) Data: 0.0115 (0.0237) Loss: 0.7730 (0.8852)
[2022/12/29 02:09] | TRAIN(098): [200/879] Batch: 0.1005 (0.1173) Data: 0.0090 (0.0204) Loss: 0.8970 (0.8918)
[2022/12/29 02:10] | TRAIN(098): [250/879] Batch: 0.1001 (0.1152) Data: 0.0093 (0.0184) Loss: 0.9564 (0.8792)
[2022/12/29 02:10] | TRAIN(098): [300/879] Batch: 0.1238 (0.1143) Data: 0.0110 (0.0171) Loss: 0.8630 (0.8805)
[2022/12/29 02:10] | TRAIN(098): [350/879] Batch: 0.1116 (0.1132) Data: 0.0093 (0.0161) Loss: 0.9011 (0.8791)
[2022/12/29 02:10] | TRAIN(098): [400/879] Batch: 0.0994 (0.1120) Data: 0.0097 (0.0153) Loss: 0.9496 (0.8729)
[2022/12/29 02:10] | TRAIN(098): [450/879] Batch: 0.1052 (0.1107) Data: 0.0098 (0.0146) Loss: 0.8642 (0.8689)
[2022/12/29 02:10] | TRAIN(098): [500/879] Batch: 0.1005 (0.1101) Data: 0.0095 (0.0142) Loss: 0.7527 (0.8666)
[2022/12/29 02:10] | TRAIN(098): [550/879] Batch: 0.1024 (0.1096) Data: 0.0093 (0.0138) Loss: 0.9129 (0.8641)
[2022/12/29 02:10] | TRAIN(098): [600/879] Batch: 0.1088 (0.1091) Data: 0.0092 (0.0134) Loss: 1.0175 (0.8654)
[2022/12/29 02:10] | TRAIN(098): [650/879] Batch: 0.1114 (0.1092) Data: 0.0089 (0.0132) Loss: 0.8645 (0.8685)
[2022/12/29 02:10] | TRAIN(098): [700/879] Batch: 0.1069 (0.1092) Data: 0.0095 (0.0130) Loss: 0.9205 (0.8721)
[2022/12/29 02:10] | TRAIN(098): [750/879] Batch: 0.1092 (0.1092) Data: 0.0090 (0.0128) Loss: 0.7097 (0.8699)
[2022/12/29 02:10] | TRAIN(098): [800/879] Batch: 0.1221 (0.1094) Data: 0.0129 (0.0127) Loss: 0.6282 (0.8696)
[2022/12/29 02:11] | TRAIN(098): [850/879] Batch: 0.1014 (0.1094) Data: 0.0098 (0.0125) Loss: 0.7267 (0.8679)
[2022/12/29 02:11] | ------------------------------------------------------------
[2022/12/29 02:11] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:11] | ------------------------------------------------------------
[2022/12/29 02:11] |    TRAIN(98)     0:01:35     0:00:10     0:01:25      0.8674
[2022/12/29 02:11] | ------------------------------------------------------------
[2022/12/29 02:11] | VALID(098): [ 50/220] Batch: 0.0408 (0.0635) Data: 0.0253 (0.0495) Loss: 0.7833 (0.8463)
[2022/12/29 02:11] | VALID(098): [100/220] Batch: 0.0391 (0.0510) Data: 0.0257 (0.0363) Loss: 1.1075 (0.8683)
[2022/12/29 02:11] | VALID(098): [150/220] Batch: 0.0377 (0.0467) Data: 0.0267 (0.0326) Loss: 0.7592 (0.8605)
[2022/12/29 02:11] | VALID(098): [200/220] Batch: 0.0394 (0.0447) Data: 0.0273 (0.0309) Loss: 0.5191 (0.8680)
[2022/12/29 02:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:11] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:11] |    VALID(98)      0.8678      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:11] | ####################################################################################################
[2022/12/29 02:11] | TRAIN(099): [ 50/879] Batch: 0.1046 (0.1420) Data: 0.0100 (0.0454) Loss: 1.0089 (0.8705)
[2022/12/29 02:11] | TRAIN(099): [100/879] Batch: 0.1201 (0.1267) Data: 0.0118 (0.0283) Loss: 1.1437 (0.8595)
[2022/12/29 02:11] | TRAIN(099): [150/879] Batch: 0.1074 (0.1223) Data: 0.0103 (0.0227) Loss: 1.0885 (0.8710)
[2022/12/29 02:11] | TRAIN(099): [200/879] Batch: 0.1131 (0.1186) Data: 0.0097 (0.0196) Loss: 0.7424 (0.8747)
[2022/12/29 02:11] | TRAIN(099): [250/879] Batch: 0.1076 (0.1173) Data: 0.0099 (0.0179) Loss: 0.6038 (0.8714)
[2022/12/29 02:11] | TRAIN(099): [300/879] Batch: 0.1149 (0.1152) Data: 0.0097 (0.0165) Loss: 0.6911 (0.8750)
[2022/12/29 02:11] | TRAIN(099): [350/879] Batch: 0.1175 (0.1148) Data: 0.0124 (0.0157) Loss: 0.9710 (0.8680)
[2022/12/29 02:12] | TRAIN(099): [400/879] Batch: 0.1018 (0.1138) Data: 0.0100 (0.0150) Loss: 1.1786 (0.8680)
[2022/12/29 02:12] | TRAIN(099): [450/879] Batch: 0.1063 (0.1129) Data: 0.0097 (0.0144) Loss: 0.7846 (0.8689)
[2022/12/29 02:12] | TRAIN(099): [500/879] Batch: 0.1008 (0.1126) Data: 0.0099 (0.0140) Loss: 1.0795 (0.8630)
[2022/12/29 02:12] | TRAIN(099): [550/879] Batch: 0.1003 (0.1123) Data: 0.0086 (0.0137) Loss: 1.1930 (0.8655)
[2022/12/29 02:12] | TRAIN(099): [600/879] Batch: 0.1080 (0.1116) Data: 0.0093 (0.0133) Loss: 0.9246 (0.8663)
[2022/12/29 02:12] | TRAIN(099): [650/879] Batch: 0.1003 (0.1113) Data: 0.0093 (0.0131) Loss: 1.1014 (0.8689)
[2022/12/29 02:12] | TRAIN(099): [700/879] Batch: 0.1140 (0.1109) Data: 0.0107 (0.0129) Loss: 1.0394 (0.8671)
[2022/12/29 02:12] | TRAIN(099): [750/879] Batch: 0.1130 (0.1106) Data: 0.0096 (0.0127) Loss: 0.9604 (0.8672)
[2022/12/29 02:12] | TRAIN(099): [800/879] Batch: 0.1252 (0.1104) Data: 0.0119 (0.0125) Loss: 0.6290 (0.8670)
[2022/12/29 02:12] | TRAIN(099): [850/879] Batch: 0.1262 (0.1104) Data: 0.0115 (0.0124) Loss: 0.7341 (0.8678)
[2022/12/29 02:12] | ------------------------------------------------------------
[2022/12/29 02:12] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:12] | ------------------------------------------------------------
[2022/12/29 02:12] |    TRAIN(99)     0:01:37     0:00:10     0:01:26      0.8674
[2022/12/29 02:12] | ------------------------------------------------------------
[2022/12/29 02:12] | VALID(099): [ 50/220] Batch: 0.0393 (0.0670) Data: 0.0257 (0.0529) Loss: 0.7835 (0.8464)
[2022/12/29 02:12] | VALID(099): [100/220] Batch: 0.0343 (0.0525) Data: 0.0283 (0.0394) Loss: 1.1077 (0.8685)
[2022/12/29 02:13] | VALID(099): [150/220] Batch: 0.0389 (0.0478) Data: 0.0267 (0.0348) Loss: 0.7593 (0.8606)
[2022/12/29 02:13] | VALID(099): [200/220] Batch: 0.0409 (0.0454) Data: 0.0257 (0.0325) Loss: 0.5191 (0.8681)
[2022/12/29 02:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:13] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:13] |    VALID(99)      0.8679      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:13] | ####################################################################################################
