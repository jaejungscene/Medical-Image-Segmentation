[2022/12/28 23:12] | Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/densenet121_ra-50efcf5c.pth)
[2022/12/28 23:12] | ---------------------------------------------------------------------------------
[2022/12/28 23:12] |                                    INFORMATION
[2022/12/28 23:12] | ---------------------------------------------------------------------------------
[2022/12/28 23:12] | Project Name              | MECLA
[2022/12/28 23:12] | Project Administrator     | jaejung
[2022/12/28 23:12] | Experiment Name           | eyepacs_v1_densenet121_v3
[2022/12/28 23:12] | Experiment Start Time     | 2022-12-28 23:12:39
[2022/12/28 23:12] | Experiment Model Name     | densenet121
[2022/12/28 23:12] | Experiment Log Directory  | log/eyepacs_v1_densenet121_v3
[2022/12/28 23:12] | ---------------------------------------------------------------------------------
[2022/12/28 23:12] |                                 EXPERIMENT SETUP
[2022/12/28 23:12] | ---------------------------------------------------------------------------------
[2022/12/28 23:12] | train_size                | (224, 224)
[2022/12/28 23:12] | test_size                 | (224, 224)
[2022/12/28 23:12] | center_crop_ptr           | 0.875
[2022/12/28 23:12] | interpolation             | bicubic
[2022/12/28 23:12] | mean                      | (0.485, 0.456, 0.406)
[2022/12/28 23:12] | std                       | (0.229, 0.224, 0.225)
[2022/12/28 23:12] | hflip                     | 0.5
[2022/12/28 23:12] | auto_aug                  | False
[2022/12/28 23:12] | cutmix                    | None
[2022/12/28 23:12] | mixup                     | None
[2022/12/28 23:12] | remode                    | 0.2
[2022/12/28 23:12] | model_name                | densenet121
[2022/12/28 23:12] | lr                        | 0.001
[2022/12/28 23:12] | epoch                     | 100
[2022/12/28 23:12] | criterion                 | ce
[2022/12/28 23:12] | optimizer                 | adamw
[2022/12/28 23:12] | weight_decay              | 0.0001
[2022/12/28 23:12] | scheduler                 | cosine
[2022/12/28 23:12] | warmup_epoch              | 1
[2022/12/28 23:12] | batch_size                | 32
[2022/12/28 23:12] | ---------------------------------------------------------------------------------
[2022/12/28 23:12] |                                   DATA & MODEL
[2022/12/28 23:12] | ---------------------------------------------------------------------------------
[2022/12/28 23:12] | Model Parameters(M)       | 6958981
[2022/12/28 23:12] | Number of Train Examples  | 28100
[2022/12/28 23:12] | Number of Valid Examples  | 7026
[2022/12/28 23:12] | Number of Class           | 5
[2022/12/28 23:12] | Task                      | multiclass
[2022/12/28 23:12] | ---------------------------------------------------------------------------------
[2022/12/28 23:12] | TRAIN(000): [ 50/879] Batch: 0.1829 (0.2426) Data: 0.0104 (0.0572) Loss: 1.2152 (1.3413)
[2022/12/28 23:13] | TRAIN(000): [100/879] Batch: 0.2004 (0.2191) Data: 0.0109 (0.0349) Loss: 1.1551 (1.1227)
[2022/12/28 23:13] | TRAIN(000): [150/879] Batch: 0.1932 (0.2108) Data: 0.0124 (0.0273) Loss: 0.6998 (1.0341)
[2022/12/28 23:13] | TRAIN(000): [200/879] Batch: 0.2010 (0.2082) Data: 0.0109 (0.0236) Loss: 0.6314 (0.9807)
[2022/12/28 23:13] | TRAIN(000): [250/879] Batch: 0.1916 (0.2060) Data: 0.0118 (0.0214) Loss: 0.9705 (0.9547)
[2022/12/28 23:13] | TRAIN(000): [300/879] Batch: 0.1913 (0.2046) Data: 0.0113 (0.0198) Loss: 1.0093 (0.9270)
[2022/12/28 23:13] | TRAIN(000): [350/879] Batch: 0.2107 (0.2034) Data: 0.0105 (0.0187) Loss: 0.6298 (0.9044)
[2022/12/28 23:14] | TRAIN(000): [400/879] Batch: 0.1968 (0.2024) Data: 0.0133 (0.0179) Loss: 0.6662 (0.8895)
[2022/12/28 23:14] | TRAIN(000): [450/879] Batch: 0.1774 (0.2014) Data: 0.0112 (0.0172) Loss: 0.8793 (0.8828)
[2022/12/28 23:14] | TRAIN(000): [500/879] Batch: 0.1860 (0.1989) Data: 0.0105 (0.0167) Loss: 0.6962 (0.8762)
[2022/12/28 23:14] | TRAIN(000): [550/879] Batch: 0.1848 (0.1973) Data: 0.0121 (0.0162) Loss: 0.8870 (0.8681)
[2022/12/28 23:14] | TRAIN(000): [600/879] Batch: 0.1881 (0.1972) Data: 0.0119 (0.0159) Loss: 0.6894 (0.8605)
[2022/12/28 23:14] | TRAIN(000): [650/879] Batch: 0.2031 (0.1971) Data: 0.0090 (0.0156) Loss: 0.8858 (0.8564)
[2022/12/28 23:15] | TRAIN(000): [700/879] Batch: 0.1968 (0.1968) Data: 0.0127 (0.0153) Loss: 0.7787 (0.8499)
[2022/12/28 23:15] | TRAIN(000): [750/879] Batch: 0.1877 (0.1969) Data: 0.0125 (0.0152) Loss: 0.7109 (0.8491)
[2022/12/28 23:15] | TRAIN(000): [800/879] Batch: 0.1875 (0.1965) Data: 0.0127 (0.0150) Loss: 1.3129 (0.8436)
[2022/12/28 23:15] | TRAIN(000): [850/879] Batch: 0.2004 (0.1965) Data: 0.0110 (0.0148) Loss: 0.9945 (0.8409)
[2022/12/28 23:15] | ------------------------------------------------------------
[2022/12/28 23:15] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:15] | ------------------------------------------------------------
[2022/12/28 23:15] |     TRAIN(0)     0:02:54     0:00:12     0:02:41      0.8389
[2022/12/28 23:15] | ------------------------------------------------------------
[2022/12/28 23:15] | VALID(000): [ 50/220] Batch: 0.0588 (0.0841) Data: 0.0167 (0.0572) Loss: 0.8878 (0.8247)
[2022/12/28 23:15] | VALID(000): [100/220] Batch: 0.0633 (0.0700) Data: 0.0377 (0.0454) Loss: 0.9887 (0.8476)
[2022/12/28 23:15] | VALID(000): [150/220] Batch: 0.0484 (0.0650) Data: 0.0379 (0.0409) Loss: 0.6284 (0.8343)
[2022/12/28 23:15] | VALID(000): [200/220] Batch: 0.0635 (0.0630) Data: 0.0252 (0.0395) Loss: 0.4025 (0.8362)
[2022/12/28 23:15] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:15] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:15] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:15] |     VALID(0)      0.8356      0.7235      0.7403      0.7235      0.7235      0.7235      0.9309
[2022/12/28 23:15] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:15] | ####################################################################################################
[2022/12/28 23:16] | TRAIN(001): [ 50/879] Batch: 0.2033 (0.2471) Data: 0.0112 (0.0575) Loss: 0.7036 (0.8395)
[2022/12/28 23:16] | TRAIN(001): [100/879] Batch: 0.1908 (0.2202) Data: 0.0129 (0.0351) Loss: 0.5901 (0.8056)
[2022/12/28 23:16] | TRAIN(001): [150/879] Batch: 0.1888 (0.2102) Data: 0.0108 (0.0277) Loss: 0.7749 (0.8039)
[2022/12/28 23:16] | TRAIN(001): [200/879] Batch: 0.2161 (0.2032) Data: 0.0151 (0.0239) Loss: 0.7649 (0.8020)
[2022/12/28 23:16] | TRAIN(001): [250/879] Batch: 0.2189 (0.2018) Data: 0.0129 (0.0216) Loss: 0.8295 (0.8004)
[2022/12/28 23:16] | TRAIN(001): [300/879] Batch: 0.1849 (0.2005) Data: 0.0121 (0.0200) Loss: 0.6469 (0.7868)
[2022/12/28 23:17] | TRAIN(001): [350/879] Batch: 0.1944 (0.1994) Data: 0.0129 (0.0189) Loss: 0.7071 (0.7904)
[2022/12/28 23:17] | TRAIN(001): [400/879] Batch: 0.1916 (0.1980) Data: 0.0116 (0.0180) Loss: 0.6534 (0.7934)
[2022/12/28 23:17] | TRAIN(001): [450/879] Batch: 0.2062 (0.1975) Data: 0.0113 (0.0173) Loss: 0.5543 (0.7871)
[2022/12/28 23:17] | TRAIN(001): [500/879] Batch: 0.1863 (0.1970) Data: 0.0126 (0.0167) Loss: 0.5413 (0.7856)
[2022/12/28 23:17] | TRAIN(001): [550/879] Batch: 0.1891 (0.1962) Data: 0.0112 (0.0163) Loss: 0.8295 (0.7825)
[2022/12/28 23:17] | TRAIN(001): [600/879] Batch: 0.1904 (0.1960) Data: 0.0120 (0.0159) Loss: 0.6889 (0.7795)
[2022/12/28 23:18] | TRAIN(001): [650/879] Batch: 0.2169 (0.1959) Data: 0.0117 (0.0156) Loss: 0.8470 (0.7786)
[2022/12/28 23:18] | TRAIN(001): [700/879] Batch: 0.1895 (0.1959) Data: 0.0111 (0.0153) Loss: 0.6711 (0.7786)
[2022/12/28 23:18] | TRAIN(001): [750/879] Batch: 0.2083 (0.1958) Data: 0.0122 (0.0151) Loss: 0.5335 (0.7766)
[2022/12/28 23:18] | TRAIN(001): [800/879] Batch: 0.1519 (0.1951) Data: 0.0114 (0.0149) Loss: 0.6537 (0.7759)
[2022/12/28 23:18] | TRAIN(001): [850/879] Batch: 0.1845 (0.1946) Data: 0.0118 (0.0147) Loss: 0.6727 (0.7753)
[2022/12/28 23:18] | ------------------------------------------------------------
[2022/12/28 23:18] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:18] | ------------------------------------------------------------
[2022/12/28 23:18] |     TRAIN(1)     0:02:50     0:00:12     0:02:37      0.7740
[2022/12/28 23:18] | ------------------------------------------------------------
[2022/12/28 23:18] | VALID(001): [ 50/220] Batch: 0.0586 (0.0811) Data: 0.0284 (0.0547) Loss: 0.7121 (0.7503)
[2022/12/28 23:18] | VALID(001): [100/220] Batch: 0.0591 (0.0681) Data: 0.0174 (0.0434) Loss: 1.0246 (0.7782)
[2022/12/28 23:18] | VALID(001): [150/220] Batch: 0.0481 (0.0636) Data: 0.0438 (0.0389) Loss: 0.5732 (0.7633)
[2022/12/28 23:18] | VALID(001): [200/220] Batch: 0.0598 (0.0615) Data: 0.0164 (0.0360) Loss: 0.3426 (0.7658)
[2022/12/28 23:19] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:19] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:19] |     VALID(1)      0.7641      0.7586      0.7847      0.7586      0.7586      0.7586      0.9397
[2022/12/28 23:19] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:19] | ####################################################################################################
[2022/12/28 23:19] | TRAIN(002): [ 50/879] Batch: 0.1952 (0.2290) Data: 0.0110 (0.0511) Loss: 0.6040 (0.7690)
[2022/12/28 23:19] | TRAIN(002): [100/879] Batch: 0.1963 (0.2097) Data: 0.0112 (0.0314) Loss: 1.3810 (0.7654)
[2022/12/28 23:19] | TRAIN(002): [150/879] Batch: 0.1959 (0.2042) Data: 0.0127 (0.0249) Loss: 0.7278 (0.7639)
[2022/12/28 23:19] | TRAIN(002): [200/879] Batch: 0.1813 (0.2002) Data: 0.0123 (0.0216) Loss: 0.6551 (0.7607)
[2022/12/28 23:19] | TRAIN(002): [250/879] Batch: 0.1992 (0.1988) Data: 0.0138 (0.0196) Loss: 0.6613 (0.7582)
[2022/12/28 23:20] | TRAIN(002): [300/879] Batch: 0.1939 (0.1984) Data: 0.0105 (0.0183) Loss: 1.1148 (0.7549)
[2022/12/28 23:20] | TRAIN(002): [350/879] Batch: 0.2059 (0.1980) Data: 0.0107 (0.0174) Loss: 0.9197 (0.7523)
[2022/12/28 23:20] | TRAIN(002): [400/879] Batch: 0.1906 (0.1976) Data: 0.0110 (0.0167) Loss: 1.0069 (0.7453)
[2022/12/28 23:20] | TRAIN(002): [450/879] Batch: 0.2097 (0.1973) Data: 0.0195 (0.0162) Loss: 0.5750 (0.7454)
[2022/12/28 23:20] | TRAIN(002): [500/879] Batch: 0.1874 (0.1961) Data: 0.0107 (0.0158) Loss: 0.9362 (0.7432)
[2022/12/28 23:20] | TRAIN(002): [550/879] Batch: 0.1340 (0.1954) Data: 0.0103 (0.0155) Loss: 0.7010 (0.7425)
[2022/12/28 23:20] | TRAIN(002): [600/879] Batch: 0.1969 (0.1945) Data: 0.0124 (0.0152) Loss: 1.1482 (0.7439)
[2022/12/28 23:21] | TRAIN(002): [650/879] Batch: 0.1905 (0.1945) Data: 0.0114 (0.0150) Loss: 0.5072 (0.7448)
[2022/12/28 23:21] | TRAIN(002): [700/879] Batch: 0.2028 (0.1941) Data: 0.0114 (0.0148) Loss: 0.5361 (0.7453)
[2022/12/28 23:21] | TRAIN(002): [750/879] Batch: 0.1926 (0.1937) Data: 0.0108 (0.0146) Loss: 0.6871 (0.7433)
[2022/12/28 23:21] | TRAIN(002): [800/879] Batch: 0.2018 (0.1933) Data: 0.0117 (0.0143) Loss: 0.6446 (0.7429)
[2022/12/28 23:21] | TRAIN(002): [850/879] Batch: 0.1678 (0.1931) Data: 0.0125 (0.0142) Loss: 0.4157 (0.7427)
[2022/12/28 23:21] | ------------------------------------------------------------
[2022/12/28 23:21] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:21] | ------------------------------------------------------------
[2022/12/28 23:21] |     TRAIN(2)     0:02:49     0:00:12     0:02:37      0.7429
[2022/12/28 23:21] | ------------------------------------------------------------
[2022/12/28 23:21] | VALID(002): [ 50/220] Batch: 0.0413 (0.0858) Data: 0.0347 (0.0587) Loss: 0.5865 (0.7152)
[2022/12/28 23:21] | VALID(002): [100/220] Batch: 0.0602 (0.0704) Data: 0.0152 (0.0452) Loss: 0.9459 (0.7424)
[2022/12/28 23:22] | VALID(002): [150/220] Batch: 0.0597 (0.0655) Data: 0.0397 (0.0413) Loss: 0.5878 (0.7266)
[2022/12/28 23:22] | VALID(002): [200/220] Batch: 0.0550 (0.0630) Data: 0.0147 (0.0387) Loss: 0.4588 (0.7354)
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] |     VALID(2)      0.7315      0.7609      0.7980      0.7609      0.7609      0.7609      0.9402
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] | ####################################################################################################
[2022/12/28 23:22] | TRAIN(003): [ 50/879] Batch: 0.1829 (0.2334) Data: 0.0102 (0.0477) Loss: 0.7650 (0.7974)
[2022/12/28 23:22] | TRAIN(003): [100/879] Batch: 0.1935 (0.2112) Data: 0.0100 (0.0295) Loss: 0.7668 (0.7564)
[2022/12/28 23:22] | TRAIN(003): [150/879] Batch: 0.1999 (0.2047) Data: 0.0106 (0.0232) Loss: 0.9484 (0.7644)
[2022/12/28 23:22] | TRAIN(003): [200/879] Batch: 0.1269 (0.1989) Data: 0.0143 (0.0201) Loss: 0.4184 (0.7568)
[2022/12/28 23:22] | TRAIN(003): [250/879] Batch: 0.1876 (0.1957) Data: 0.0091 (0.0183) Loss: 0.5736 (0.7450)
[2022/12/28 23:23] | TRAIN(003): [300/879] Batch: 0.2044 (0.1921) Data: 0.0116 (0.0170) Loss: 0.4576 (0.7420)
[2022/12/28 23:23] | TRAIN(003): [350/879] Batch: 0.2027 (0.1921) Data: 0.0111 (0.0161) Loss: 0.8454 (0.7365)
[2022/12/28 23:23] | TRAIN(003): [400/879] Batch: 0.1889 (0.1921) Data: 0.0129 (0.0154) Loss: 0.5014 (0.7355)
[2022/12/28 23:23] | TRAIN(003): [450/879] Batch: 0.2007 (0.1920) Data: 0.0118 (0.0150) Loss: 0.9054 (0.7346)
[2022/12/28 23:23] | TRAIN(003): [500/879] Batch: 0.1929 (0.1919) Data: 0.0107 (0.0145) Loss: 0.8451 (0.7339)
[2022/12/28 23:23] | TRAIN(003): [550/879] Batch: 0.1939 (0.1921) Data: 0.0098 (0.0142) Loss: 0.9555 (0.7298)
[2022/12/28 23:23] | TRAIN(003): [600/879] Batch: 0.1935 (0.1923) Data: 0.0108 (0.0139) Loss: 0.9823 (0.7316)
[2022/12/28 23:24] | TRAIN(003): [650/879] Batch: 0.1840 (0.1922) Data: 0.0098 (0.0137) Loss: 0.6729 (0.7308)
[2022/12/28 23:24] | TRAIN(003): [700/879] Batch: 0.1762 (0.1921) Data: 0.0112 (0.0135) Loss: 0.6849 (0.7296)
[2022/12/28 23:24] | TRAIN(003): [750/879] Batch: 0.2035 (0.1920) Data: 0.0089 (0.0133) Loss: 0.9438 (0.7288)
[2022/12/28 23:24] | TRAIN(003): [800/879] Batch: 0.1929 (0.1921) Data: 0.0112 (0.0131) Loss: 0.6754 (0.7285)
[2022/12/28 23:24] | TRAIN(003): [850/879] Batch: 0.1933 (0.1923) Data: 0.0109 (0.0130) Loss: 0.8085 (0.7308)
[2022/12/28 23:24] | ------------------------------------------------------------
[2022/12/28 23:24] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:24] | ------------------------------------------------------------
[2022/12/28 23:24] |     TRAIN(3)     0:02:48     0:00:11     0:02:37      0.7316
[2022/12/28 23:24] | ------------------------------------------------------------
[2022/12/28 23:24] | VALID(003): [ 50/220] Batch: 0.0504 (0.0819) Data: 0.0282 (0.0578) Loss: 0.5705 (0.6659)
[2022/12/28 23:24] | VALID(003): [100/220] Batch: 0.0545 (0.0677) Data: 0.0228 (0.0429) Loss: 0.8166 (0.6799)
[2022/12/28 23:25] | VALID(003): [150/220] Batch: 0.0395 (0.0619) Data: 0.0109 (0.0356) Loss: 0.6361 (0.6756)
[2022/12/28 23:25] | VALID(003): [200/220] Batch: 0.0508 (0.0576) Data: 0.0359 (0.0303) Loss: 0.4176 (0.6799)
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] |     VALID(3)      0.6775      0.7738      0.8117      0.7738      0.7738      0.7738      0.9435
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] | ####################################################################################################
[2022/12/28 23:25] | TRAIN(004): [ 50/879] Batch: 0.1867 (0.2352) Data: 0.0109 (0.0509) Loss: 0.7287 (0.7437)
[2022/12/28 23:25] | TRAIN(004): [100/879] Batch: 0.1969 (0.2136) Data: 0.0110 (0.0319) Loss: 0.6112 (0.7396)
[2022/12/28 23:25] | TRAIN(004): [150/879] Batch: 0.2121 (0.2075) Data: 0.0176 (0.0253) Loss: 0.5237 (0.7418)
[2022/12/28 23:25] | TRAIN(004): [200/879] Batch: 0.2013 (0.2050) Data: 0.0160 (0.0223) Loss: 0.9204 (0.7384)
[2022/12/28 23:25] | TRAIN(004): [250/879] Batch: 0.1822 (0.2034) Data: 0.0142 (0.0204) Loss: 0.7602 (0.7300)
[2022/12/28 23:26] | TRAIN(004): [300/879] Batch: 0.1885 (0.2017) Data: 0.0110 (0.0190) Loss: 0.5593 (0.7334)
[2022/12/28 23:26] | TRAIN(004): [350/879] Batch: 0.1909 (0.2007) Data: 0.0123 (0.0181) Loss: 0.7255 (0.7295)
[2022/12/28 23:26] | TRAIN(004): [400/879] Batch: 0.1891 (0.2006) Data: 0.0104 (0.0174) Loss: 0.6869 (0.7279)
[2022/12/28 23:26] | TRAIN(004): [450/879] Batch: 0.2036 (0.2004) Data: 0.0118 (0.0168) Loss: 0.7132 (0.7264)
[2022/12/28 23:26] | TRAIN(004): [500/879] Batch: 0.1920 (0.1996) Data: 0.0118 (0.0163) Loss: 0.5802 (0.7247)
[2022/12/28 23:26] | TRAIN(004): [550/879] Batch: 0.2088 (0.1994) Data: 0.0175 (0.0159) Loss: 1.0248 (0.7228)
[2022/12/28 23:27] | TRAIN(004): [600/879] Batch: 0.1933 (0.1982) Data: 0.0086 (0.0156) Loss: 0.4104 (0.7214)
[2022/12/28 23:27] | TRAIN(004): [650/879] Batch: 0.1850 (0.1970) Data: 0.0110 (0.0153) Loss: 0.7202 (0.7231)
[2022/12/28 23:27] | TRAIN(004): [700/879] Batch: 0.1838 (0.1968) Data: 0.0133 (0.0151) Loss: 1.1460 (0.7204)
[2022/12/28 23:27] | TRAIN(004): [750/879] Batch: 0.2004 (0.1968) Data: 0.0131 (0.0150) Loss: 0.5873 (0.7202)
[2022/12/28 23:27] | TRAIN(004): [800/879] Batch: 0.1976 (0.1967) Data: 0.0126 (0.0148) Loss: 0.7744 (0.7216)
[2022/12/28 23:27] | TRAIN(004): [850/879] Batch: 0.1788 (0.1963) Data: 0.0108 (0.0146) Loss: 0.6500 (0.7218)
[2022/12/28 23:27] | ------------------------------------------------------------
[2022/12/28 23:27] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:27] | ------------------------------------------------------------
[2022/12/28 23:27] |     TRAIN(4)     0:02:52     0:00:12     0:02:39      0.7209
[2022/12/28 23:27] | ------------------------------------------------------------
[2022/12/28 23:28] | VALID(004): [ 50/220] Batch: 0.0455 (0.0821) Data: 0.0381 (0.0598) Loss: 0.5279 (0.6960)
[2022/12/28 23:28] | VALID(004): [100/220] Batch: 0.0604 (0.0691) Data: 0.0409 (0.0473) Loss: 0.9365 (0.7142)
[2022/12/28 23:28] | VALID(004): [150/220] Batch: 0.0590 (0.0649) Data: 0.0262 (0.0431) Loss: 0.6682 (0.7017)
[2022/12/28 23:28] | VALID(004): [200/220] Batch: 0.0590 (0.0629) Data: 0.0321 (0.0399) Loss: 0.4049 (0.7074)
[2022/12/28 23:28] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:28] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:28] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:28] |     VALID(4)      0.7029      0.7714      0.8052      0.7714      0.7714      0.7714      0.9429
[2022/12/28 23:28] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:28] | ####################################################################################################
[2022/12/28 23:28] | TRAIN(005): [ 50/879] Batch: 0.2018 (0.2360) Data: 0.0102 (0.0503) Loss: 0.6875 (0.7584)
[2022/12/28 23:28] | TRAIN(005): [100/879] Batch: 0.1929 (0.2119) Data: 0.0108 (0.0310) Loss: 0.5758 (0.7426)
[2022/12/28 23:28] | TRAIN(005): [150/879] Batch: 0.1934 (0.2060) Data: 0.0109 (0.0248) Loss: 0.5602 (0.7405)
[2022/12/28 23:28] | TRAIN(005): [200/879] Batch: 0.1955 (0.2030) Data: 0.0127 (0.0215) Loss: 0.8849 (0.7375)
[2022/12/28 23:29] | TRAIN(005): [250/879] Batch: 0.2025 (0.2011) Data: 0.0129 (0.0197) Loss: 0.6589 (0.7374)
[2022/12/28 23:29] | TRAIN(005): [300/879] Batch: 0.1875 (0.1966) Data: 0.0188 (0.0183) Loss: 0.6192 (0.7316)
[2022/12/28 23:29] | TRAIN(005): [350/879] Batch: 0.1889 (0.1933) Data: 0.0093 (0.0174) Loss: 0.8803 (0.7224)
[2022/12/28 23:29] | TRAIN(005): [400/879] Batch: 0.1883 (0.1933) Data: 0.0117 (0.0166) Loss: 0.9266 (0.7214)
[2022/12/28 23:29] | TRAIN(005): [450/879] Batch: 0.2019 (0.1934) Data: 0.0137 (0.0161) Loss: 0.6872 (0.7215)
[2022/12/28 23:29] | TRAIN(005): [500/879] Batch: 0.1952 (0.1934) Data: 0.0118 (0.0157) Loss: 0.8989 (0.7205)
[2022/12/28 23:29] | TRAIN(005): [550/879] Batch: 0.1790 (0.1934) Data: 0.0112 (0.0154) Loss: 1.0126 (0.7171)
[2022/12/28 23:30] | TRAIN(005): [600/879] Batch: 0.2001 (0.1933) Data: 0.0101 (0.0150) Loss: 0.5832 (0.7186)
[2022/12/28 23:30] | TRAIN(005): [650/879] Batch: 0.1994 (0.1934) Data: 0.0108 (0.0147) Loss: 0.6352 (0.7149)
[2022/12/28 23:30] | TRAIN(005): [700/879] Batch: 0.1997 (0.1938) Data: 0.0109 (0.0145) Loss: 0.4816 (0.7145)
[2022/12/28 23:30] | TRAIN(005): [750/879] Batch: 0.1889 (0.1938) Data: 0.0113 (0.0144) Loss: 0.9661 (0.7175)
[2022/12/28 23:30] | TRAIN(005): [800/879] Batch: 0.1819 (0.1939) Data: 0.0117 (0.0142) Loss: 0.7663 (0.7173)
[2022/12/28 23:30] | TRAIN(005): [850/879] Batch: 0.1951 (0.1940) Data: 0.0165 (0.0140) Loss: 0.8255 (0.7150)
[2022/12/28 23:31] | ------------------------------------------------------------
[2022/12/28 23:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:31] | ------------------------------------------------------------
[2022/12/28 23:31] |     TRAIN(5)     0:02:50     0:00:12     0:02:38      0.7143
[2022/12/28 23:31] | ------------------------------------------------------------
[2022/12/28 23:31] | VALID(005): [ 50/220] Batch: 0.0432 (0.0827) Data: 0.0268 (0.0541) Loss: 0.5709 (0.6886)
[2022/12/28 23:31] | VALID(005): [100/220] Batch: 0.0591 (0.0696) Data: 0.0408 (0.0438) Loss: 0.9137 (0.7249)
[2022/12/28 23:31] | VALID(005): [150/220] Batch: 0.0366 (0.0611) Data: 0.0114 (0.0353) Loss: 0.5995 (0.7125)
[2022/12/28 23:31] | VALID(005): [200/220] Batch: 0.0551 (0.0588) Data: 0.0347 (0.0335) Loss: 0.3157 (0.7221)
[2022/12/28 23:31] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:31] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:31] |     VALID(5)      0.7217      0.7677      0.8018      0.7677      0.7677      0.7677      0.9419
[2022/12/28 23:31] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:31] | ####################################################################################################
[2022/12/28 23:31] | TRAIN(006): [ 50/879] Batch: 0.1890 (0.2272) Data: 0.0109 (0.0504) Loss: 0.7499 (0.7128)
[2022/12/28 23:31] | TRAIN(006): [100/879] Batch: 0.1902 (0.2106) Data: 0.0122 (0.0314) Loss: 0.7375 (0.7098)
[2022/12/28 23:31] | TRAIN(006): [150/879] Batch: 0.2021 (0.2063) Data: 0.0105 (0.0254) Loss: 0.8500 (0.7040)
[2022/12/28 23:31] | TRAIN(006): [200/879] Batch: 0.2071 (0.2031) Data: 0.0114 (0.0223) Loss: 0.6866 (0.7200)
[2022/12/28 23:32] | TRAIN(006): [250/879] Batch: 0.1876 (0.2021) Data: 0.0121 (0.0204) Loss: 0.9753 (0.7248)
[2022/12/28 23:32] | TRAIN(006): [300/879] Batch: 0.2076 (0.2017) Data: 0.0108 (0.0191) Loss: 0.6273 (0.7179)
[2022/12/28 23:32] | TRAIN(006): [350/879] Batch: 0.1899 (0.2009) Data: 0.0107 (0.0182) Loss: 0.5719 (0.7143)
[2022/12/28 23:32] | TRAIN(006): [400/879] Batch: 0.1908 (0.2005) Data: 0.0120 (0.0175) Loss: 0.7468 (0.7125)
[2022/12/28 23:32] | TRAIN(006): [450/879] Batch: 0.2044 (0.2003) Data: 0.0188 (0.0170) Loss: 0.5698 (0.7154)
[2022/12/28 23:32] | TRAIN(006): [500/879] Batch: 0.1791 (0.1999) Data: 0.0111 (0.0165) Loss: 0.6474 (0.7170)
[2022/12/28 23:33] | TRAIN(006): [550/879] Batch: 0.1892 (0.1992) Data: 0.0131 (0.0162) Loss: 0.6586 (0.7145)
[2022/12/28 23:33] | TRAIN(006): [600/879] Batch: 0.1860 (0.1985) Data: 0.0119 (0.0159) Loss: 0.7103 (0.7143)
[2022/12/28 23:33] | TRAIN(006): [650/879] Batch: 0.1831 (0.1973) Data: 0.0102 (0.0156) Loss: 0.7509 (0.7116)
[2022/12/28 23:33] | TRAIN(006): [700/879] Batch: 0.1933 (0.1961) Data: 0.0107 (0.0153) Loss: 0.9228 (0.7103)
[2022/12/28 23:33] | TRAIN(006): [750/879] Batch: 0.2026 (0.1959) Data: 0.0110 (0.0151) Loss: 0.6797 (0.7082)
[2022/12/28 23:33] | TRAIN(006): [800/879] Batch: 0.1936 (0.1960) Data: 0.0109 (0.0150) Loss: 0.8081 (0.7076)
[2022/12/28 23:34] | TRAIN(006): [850/879] Batch: 0.2046 (0.1959) Data: 0.0121 (0.0148) Loss: 0.7637 (0.7076)
[2022/12/28 23:34] | ------------------------------------------------------------
[2022/12/28 23:34] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:34] | ------------------------------------------------------------
[2022/12/28 23:34] |     TRAIN(6)     0:02:52     0:00:12     0:02:39      0.7076
[2022/12/28 23:34] | ------------------------------------------------------------
[2022/12/28 23:34] | VALID(006): [ 50/220] Batch: 0.0526 (0.0848) Data: 0.0285 (0.0559) Loss: 0.5743 (0.6556)
[2022/12/28 23:34] | VALID(006): [100/220] Batch: 0.0503 (0.0701) Data: 0.0406 (0.0429) Loss: 0.8964 (0.6906)
[2022/12/28 23:34] | VALID(006): [150/220] Batch: 0.0584 (0.0655) Data: 0.0224 (0.0395) Loss: 0.5851 (0.6766)
[2022/12/28 23:34] | VALID(006): [200/220] Batch: 0.0594 (0.0631) Data: 0.0145 (0.0370) Loss: 0.3570 (0.6833)
[2022/12/28 23:34] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:34] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:34] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:34] |     VALID(6)      0.6790      0.7764      0.8214      0.7764      0.7764      0.7764      0.9441
[2022/12/28 23:34] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:34] | ####################################################################################################
[2022/12/28 23:34] | TRAIN(007): [ 50/879] Batch: 0.2104 (0.2364) Data: 0.0177 (0.0529) Loss: 0.6852 (0.6769)
[2022/12/28 23:34] | TRAIN(007): [100/879] Batch: 0.1852 (0.2144) Data: 0.0109 (0.0328) Loss: 0.6580 (0.6703)
[2022/12/28 23:34] | TRAIN(007): [150/879] Batch: 0.2001 (0.2090) Data: 0.0121 (0.0262) Loss: 0.6490 (0.6692)
[2022/12/28 23:35] | TRAIN(007): [200/879] Batch: 0.2114 (0.2073) Data: 0.0139 (0.0231) Loss: 0.4731 (0.6704)
[2022/12/28 23:35] | TRAIN(007): [250/879] Batch: 0.1899 (0.2045) Data: 0.0152 (0.0209) Loss: 1.0055 (0.6868)
[2022/12/28 23:35] | TRAIN(007): [300/879] Batch: 0.1425 (0.2011) Data: 0.0110 (0.0194) Loss: 0.7289 (0.6906)
[2022/12/28 23:35] | TRAIN(007): [350/879] Batch: 0.1261 (0.1994) Data: 0.0122 (0.0185) Loss: 0.6589 (0.6921)
[2022/12/28 23:35] | TRAIN(007): [400/879] Batch: 0.1961 (0.1977) Data: 0.0111 (0.0178) Loss: 0.5840 (0.6943)
[2022/12/28 23:35] | TRAIN(007): [450/879] Batch: 0.2040 (0.1974) Data: 0.0165 (0.0172) Loss: 1.0558 (0.6968)
[2022/12/28 23:36] | TRAIN(007): [500/879] Batch: 0.2272 (0.1974) Data: 0.0130 (0.0168) Loss: 0.6323 (0.6960)
[2022/12/28 23:36] | TRAIN(007): [550/879] Batch: 0.1865 (0.1972) Data: 0.0120 (0.0163) Loss: 0.7812 (0.6938)
[2022/12/28 23:36] | TRAIN(007): [600/879] Batch: 0.1996 (0.1971) Data: 0.0158 (0.0160) Loss: 0.9679 (0.6946)
[2022/12/28 23:36] | TRAIN(007): [650/879] Batch: 0.1825 (0.1968) Data: 0.0090 (0.0157) Loss: 0.4642 (0.6943)
[2022/12/28 23:36] | TRAIN(007): [700/879] Batch: 0.1955 (0.1963) Data: 0.0102 (0.0155) Loss: 0.5592 (0.6959)
[2022/12/28 23:36] | TRAIN(007): [750/879] Batch: 0.2012 (0.1962) Data: 0.0122 (0.0153) Loss: 0.6730 (0.6964)
[2022/12/28 23:36] | TRAIN(007): [800/879] Batch: 0.2115 (0.1963) Data: 0.0143 (0.0151) Loss: 0.5804 (0.6968)
[2022/12/28 23:37] | TRAIN(007): [850/879] Batch: 0.1760 (0.1961) Data: 0.0192 (0.0150) Loss: 0.7579 (0.6957)
[2022/12/28 23:37] | ------------------------------------------------------------
[2022/12/28 23:37] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:37] | ------------------------------------------------------------
[2022/12/28 23:37] |     TRAIN(7)     0:02:52     0:00:13     0:02:39      0.6980
[2022/12/28 23:37] | ------------------------------------------------------------
[2022/12/28 23:37] | VALID(007): [ 50/220] Batch: 0.0413 (0.0817) Data: 0.0374 (0.0594) Loss: 0.5953 (0.6437)
[2022/12/28 23:37] | VALID(007): [100/220] Batch: 0.0451 (0.0674) Data: 0.0381 (0.0455) Loss: 0.8657 (0.6596)
[2022/12/28 23:37] | VALID(007): [150/220] Batch: 0.0590 (0.0638) Data: 0.0349 (0.0416) Loss: 0.5571 (0.6456)
[2022/12/28 23:37] | VALID(007): [200/220] Batch: 0.0590 (0.0621) Data: 0.0338 (0.0399) Loss: 0.3600 (0.6491)
[2022/12/28 23:37] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:37] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:37] |     VALID(7)      0.6475      0.7822      0.8258      0.7822      0.7822      0.7822      0.9456
[2022/12/28 23:37] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:37] | ####################################################################################################
[2022/12/28 23:37] | TRAIN(008): [ 50/879] Batch: 0.1714 (0.2241) Data: 0.0123 (0.0511) Loss: 0.4538 (0.7092)
[2022/12/28 23:37] | TRAIN(008): [100/879] Batch: 0.1974 (0.2086) Data: 0.0102 (0.0312) Loss: 0.9027 (0.7030)
[2022/12/28 23:37] | TRAIN(008): [150/879] Batch: 0.1891 (0.2031) Data: 0.0087 (0.0245) Loss: 0.8743 (0.6962)
[2022/12/28 23:38] | TRAIN(008): [200/879] Batch: 0.1877 (0.1996) Data: 0.0106 (0.0211) Loss: 0.6034 (0.6970)
[2022/12/28 23:38] | TRAIN(008): [250/879] Batch: 0.1904 (0.1978) Data: 0.0117 (0.0191) Loss: 0.7966 (0.6993)
[2022/12/28 23:38] | TRAIN(008): [300/879] Batch: 0.1938 (0.1967) Data: 0.0103 (0.0176) Loss: 0.5454 (0.6895)
[2022/12/28 23:38] | TRAIN(008): [350/879] Batch: 0.1874 (0.1955) Data: 0.0105 (0.0167) Loss: 0.7288 (0.6912)
[2022/12/28 23:38] | TRAIN(008): [400/879] Batch: 0.2288 (0.1951) Data: 0.0122 (0.0159) Loss: 0.9468 (0.6972)
[2022/12/28 23:38] | TRAIN(008): [450/879] Batch: 0.1806 (0.1938) Data: 0.0103 (0.0153) Loss: 0.7844 (0.6943)
[2022/12/28 23:39] | TRAIN(008): [500/879] Batch: 0.1897 (0.1933) Data: 0.0091 (0.0149) Loss: 0.7401 (0.6992)
[2022/12/28 23:39] | TRAIN(008): [550/879] Batch: 0.1963 (0.1932) Data: 0.0114 (0.0145) Loss: 0.6606 (0.6995)
[2022/12/28 23:39] | TRAIN(008): [600/879] Batch: 0.1887 (0.1933) Data: 0.0119 (0.0142) Loss: 0.9972 (0.6981)
[2022/12/28 23:39] | TRAIN(008): [650/879] Batch: 0.1808 (0.1933) Data: 0.0112 (0.0139) Loss: 0.9084 (0.7004)
[2022/12/28 23:39] | TRAIN(008): [700/879] Batch: 0.2056 (0.1921) Data: 0.0113 (0.0137) Loss: 0.5740 (0.6978)
[2022/12/28 23:39] | TRAIN(008): [750/879] Batch: 0.1884 (0.1907) Data: 0.0108 (0.0135) Loss: 0.8851 (0.6959)
[2022/12/28 23:40] | TRAIN(008): [800/879] Batch: 0.1906 (0.1906) Data: 0.0105 (0.0133) Loss: 0.6314 (0.6944)
[2022/12/28 23:40] | TRAIN(008): [850/879] Batch: 0.1881 (0.1901) Data: 0.0108 (0.0131) Loss: 0.9515 (0.6944)
[2022/12/28 23:40] | ------------------------------------------------------------
[2022/12/28 23:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:40] | ------------------------------------------------------------
[2022/12/28 23:40] |     TRAIN(8)     0:02:46     0:00:11     0:02:35      0.6941
[2022/12/28 23:40] | ------------------------------------------------------------
[2022/12/28 23:40] | VALID(008): [ 50/220] Batch: 0.0427 (0.0827) Data: 0.0329 (0.0577) Loss: 0.4894 (0.6163)
[2022/12/28 23:40] | VALID(008): [100/220] Batch: 0.0579 (0.0690) Data: 0.0392 (0.0445) Loss: 0.8965 (0.6415)
[2022/12/28 23:40] | VALID(008): [150/220] Batch: 0.0599 (0.0646) Data: 0.0374 (0.0410) Loss: 0.6375 (0.6362)
[2022/12/28 23:40] | VALID(008): [200/220] Batch: 0.0630 (0.0623) Data: 0.0307 (0.0392) Loss: 0.2915 (0.6379)
[2022/12/28 23:40] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:40] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:40] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:40] |     VALID(8)      0.6369      0.7902      0.8329      0.7902      0.7902      0.7902      0.9476
[2022/12/28 23:40] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:40] | ####################################################################################################
[2022/12/28 23:40] | TRAIN(009): [ 50/879] Batch: 0.1900 (0.2358) Data: 0.0086 (0.0557) Loss: 0.6152 (0.6800)
[2022/12/28 23:40] | TRAIN(009): [100/879] Batch: 0.1820 (0.2126) Data: 0.0109 (0.0329) Loss: 0.4884 (0.6756)
[2022/12/28 23:40] | TRAIN(009): [150/879] Batch: 0.1897 (0.2046) Data: 0.0084 (0.0253) Loss: 0.6347 (0.6891)
[2022/12/28 23:41] | TRAIN(009): [200/879] Batch: 0.1933 (0.1999) Data: 0.0090 (0.0214) Loss: 0.7421 (0.6829)
[2022/12/28 23:41] | TRAIN(009): [250/879] Batch: 0.2051 (0.1981) Data: 0.0116 (0.0192) Loss: 0.5715 (0.6782)
[2022/12/28 23:41] | TRAIN(009): [300/879] Batch: 0.2037 (0.1964) Data: 0.0098 (0.0176) Loss: 1.2048 (0.6810)
[2022/12/28 23:41] | TRAIN(009): [350/879] Batch: 0.2052 (0.1957) Data: 0.0102 (0.0166) Loss: 1.1129 (0.6876)
[2022/12/28 23:41] | TRAIN(009): [400/879] Batch: 0.1848 (0.1939) Data: 0.0098 (0.0158) Loss: 0.7291 (0.6879)
[2022/12/28 23:41] | TRAIN(009): [450/879] Batch: 0.1204 (0.1920) Data: 0.0086 (0.0152) Loss: 0.2972 (0.6882)
[2022/12/28 23:42] | TRAIN(009): [500/879] Batch: 0.1851 (0.1908) Data: 0.0094 (0.0147) Loss: 0.6086 (0.6876)
[2022/12/28 23:42] | TRAIN(009): [550/879] Batch: 0.1863 (0.1905) Data: 0.0089 (0.0142) Loss: 0.8397 (0.6837)
[2022/12/28 23:42] | TRAIN(009): [600/879] Batch: 0.1842 (0.1900) Data: 0.0100 (0.0139) Loss: 0.4696 (0.6856)
[2022/12/28 23:42] | TRAIN(009): [650/879] Batch: 0.1909 (0.1900) Data: 0.0106 (0.0136) Loss: 0.7843 (0.6880)
[2022/12/28 23:42] | TRAIN(009): [700/879] Batch: 0.1829 (0.1896) Data: 0.0096 (0.0133) Loss: 0.4696 (0.6866)
[2022/12/28 23:42] | TRAIN(009): [750/879] Batch: 0.1803 (0.1893) Data: 0.0110 (0.0131) Loss: 0.5672 (0.6900)
[2022/12/28 23:43] | TRAIN(009): [800/879] Batch: 0.1938 (0.1891) Data: 0.0100 (0.0129) Loss: 0.7905 (0.6919)
[2022/12/28 23:43] | TRAIN(009): [850/879] Batch: 0.1968 (0.1893) Data: 0.0096 (0.0128) Loss: 0.5045 (0.6890)
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] |     TRAIN(9)     0:02:46     0:00:11     0:02:35      0.6886
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] | VALID(009): [ 50/220] Batch: 0.0468 (0.0832) Data: 0.0395 (0.0592) Loss: 0.6111 (0.6464)
[2022/12/28 23:43] | VALID(009): [100/220] Batch: 0.0369 (0.0689) Data: 0.0383 (0.0461) Loss: 0.9067 (0.6470)
[2022/12/28 23:43] | VALID(009): [150/220] Batch: 0.0588 (0.0643) Data: 0.0280 (0.0417) Loss: 0.5405 (0.6450)
[2022/12/28 23:43] | VALID(009): [200/220] Batch: 0.0562 (0.0616) Data: 0.0379 (0.0392) Loss: 0.4027 (0.6467)
[2022/12/28 23:43] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:43] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:43] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:43] |     VALID(9)      0.6451      0.7838      0.8378      0.7838      0.7838      0.7838      0.9460
[2022/12/28 23:43] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:43] | ####################################################################################################
[2022/12/28 23:43] | TRAIN(010): [ 50/879] Batch: 0.1899 (0.2325) Data: 0.0098 (0.0496) Loss: 0.6828 (0.6808)
[2022/12/28 23:43] | TRAIN(010): [100/879] Batch: 0.1816 (0.2101) Data: 0.0109 (0.0301) Loss: 0.7744 (0.6765)
[2022/12/28 23:43] | TRAIN(010): [150/879] Batch: 0.2055 (0.1996) Data: 0.0112 (0.0236) Loss: 0.7237 (0.6773)
[2022/12/28 23:44] | TRAIN(010): [200/879] Batch: 0.1892 (0.1946) Data: 0.0109 (0.0205) Loss: 0.6908 (0.6788)
[2022/12/28 23:44] | TRAIN(010): [250/879] Batch: 0.1880 (0.1934) Data: 0.0096 (0.0185) Loss: 0.7025 (0.6846)
[2022/12/28 23:44] | TRAIN(010): [300/879] Batch: 0.2014 (0.1930) Data: 0.0087 (0.0171) Loss: 0.6676 (0.6846)
[2022/12/28 23:44] | TRAIN(010): [350/879] Batch: 0.1830 (0.1928) Data: 0.0109 (0.0161) Loss: 0.8564 (0.6778)
[2022/12/28 23:44] | TRAIN(010): [400/879] Batch: 0.1822 (0.1924) Data: 0.0097 (0.0154) Loss: 0.6597 (0.6779)
[2022/12/28 23:44] | TRAIN(010): [450/879] Batch: 0.1957 (0.1923) Data: 0.0090 (0.0149) Loss: 0.7029 (0.6785)
[2022/12/28 23:45] | TRAIN(010): [500/879] Batch: 0.1796 (0.1921) Data: 0.0112 (0.0144) Loss: 0.5794 (0.6779)
[2022/12/28 23:45] | TRAIN(010): [550/879] Batch: 0.1803 (0.1921) Data: 0.0086 (0.0140) Loss: 0.6348 (0.6793)
[2022/12/28 23:45] | TRAIN(010): [600/879] Batch: 0.1778 (0.1915) Data: 0.0108 (0.0137) Loss: 0.9239 (0.6831)
[2022/12/28 23:45] | TRAIN(010): [650/879] Batch: 0.1830 (0.1910) Data: 0.0108 (0.0134) Loss: 0.7232 (0.6815)
[2022/12/28 23:45] | TRAIN(010): [700/879] Batch: 0.1875 (0.1907) Data: 0.0108 (0.0132) Loss: 0.8850 (0.6801)
[2022/12/28 23:45] | TRAIN(010): [750/879] Batch: 0.1927 (0.1906) Data: 0.0098 (0.0130) Loss: 0.7306 (0.6798)
[2022/12/28 23:46] | TRAIN(010): [800/879] Batch: 0.1859 (0.1905) Data: 0.0117 (0.0128) Loss: 0.9387 (0.6754)
[2022/12/28 23:46] | TRAIN(010): [850/879] Batch: 0.1905 (0.1896) Data: 0.0107 (0.0126) Loss: 0.4752 (0.6761)
[2022/12/28 23:46] | ------------------------------------------------------------
[2022/12/28 23:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:46] | ------------------------------------------------------------
[2022/12/28 23:46] |    TRAIN(10)     0:02:46     0:00:11     0:02:35      0.6751
[2022/12/28 23:46] | ------------------------------------------------------------
[2022/12/28 23:46] | VALID(010): [ 50/220] Batch: 0.0612 (0.0845) Data: 0.0340 (0.0608) Loss: 0.5728 (0.6019)
[2022/12/28 23:46] | VALID(010): [100/220] Batch: 0.0581 (0.0689) Data: 0.0121 (0.0454) Loss: 0.7864 (0.6205)
[2022/12/28 23:46] | VALID(010): [150/220] Batch: 0.0588 (0.0642) Data: 0.0401 (0.0409) Loss: 0.5131 (0.6145)
[2022/12/28 23:46] | VALID(010): [200/220] Batch: 0.0480 (0.0621) Data: 0.0314 (0.0390) Loss: 0.2975 (0.6129)
[2022/12/28 23:46] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:46] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:46] |    VALID(10)      0.6117      0.7965      0.8466      0.7965      0.7965      0.7965      0.9491
[2022/12/28 23:46] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:46] | ####################################################################################################
[2022/12/28 23:46] | TRAIN(011): [ 50/879] Batch: 0.1790 (0.2271) Data: 0.0097 (0.0474) Loss: 0.6602 (0.6763)
[2022/12/28 23:46] | TRAIN(011): [100/879] Batch: 0.1895 (0.2081) Data: 0.0088 (0.0290) Loss: 1.0110 (0.6809)
[2022/12/28 23:46] | TRAIN(011): [150/879] Batch: 0.1994 (0.2022) Data: 0.0105 (0.0229) Loss: 0.8077 (0.6872)
[2022/12/28 23:47] | TRAIN(011): [200/879] Batch: 0.1896 (0.1989) Data: 0.0095 (0.0198) Loss: 0.9744 (0.6807)
[2022/12/28 23:47] | TRAIN(011): [250/879] Batch: 0.1805 (0.1970) Data: 0.0095 (0.0179) Loss: 0.4444 (0.6833)
[2022/12/28 23:47] | TRAIN(011): [300/879] Batch: 0.1925 (0.1956) Data: 0.0107 (0.0166) Loss: 0.5125 (0.6831)
[2022/12/28 23:47] | TRAIN(011): [350/879] Batch: 0.1804 (0.1945) Data: 0.0091 (0.0156) Loss: 0.9234 (0.6715)
[2022/12/28 23:47] | TRAIN(011): [400/879] Batch: 0.1878 (0.1939) Data: 0.0089 (0.0149) Loss: 0.7066 (0.6719)
[2022/12/28 23:47] | TRAIN(011): [450/879] Batch: 0.1829 (0.1934) Data: 0.0098 (0.0144) Loss: 0.6503 (0.6754)
[2022/12/28 23:48] | TRAIN(011): [500/879] Batch: 0.2020 (0.1931) Data: 0.0114 (0.0140) Loss: 0.6671 (0.6710)
[2022/12/28 23:48] | TRAIN(011): [550/879] Batch: 0.1364 (0.1919) Data: 0.0091 (0.0137) Loss: 0.7980 (0.6735)
[2022/12/28 23:48] | TRAIN(011): [600/879] Batch: 0.1763 (0.1908) Data: 0.0115 (0.0134) Loss: 1.0575 (0.6758)
[2022/12/28 23:48] | TRAIN(011): [650/879] Batch: 0.1799 (0.1892) Data: 0.0102 (0.0132) Loss: 0.5397 (0.6755)
[2022/12/28 23:48] | TRAIN(011): [700/879] Batch: 0.1880 (0.1888) Data: 0.0107 (0.0130) Loss: 0.6730 (0.6759)
[2022/12/28 23:48] | TRAIN(011): [750/879] Batch: 0.1831 (0.1887) Data: 0.0098 (0.0128) Loss: 0.6338 (0.6748)
[2022/12/28 23:48] | TRAIN(011): [800/879] Batch: 0.1694 (0.1884) Data: 0.0091 (0.0126) Loss: 0.6692 (0.6724)
[2022/12/28 23:49] | TRAIN(011): [850/879] Batch: 0.1768 (0.1880) Data: 0.0090 (0.0124) Loss: 0.9183 (0.6737)
[2022/12/28 23:49] | ------------------------------------------------------------
[2022/12/28 23:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:49] | ------------------------------------------------------------
[2022/12/28 23:49] |    TRAIN(11)     0:02:45     0:00:10     0:02:34      0.6734
[2022/12/28 23:49] | ------------------------------------------------------------
[2022/12/28 23:49] | VALID(011): [ 50/220] Batch: 0.0593 (0.0839) Data: 0.0213 (0.0606) Loss: 0.5112 (0.6165)
[2022/12/28 23:49] | VALID(011): [100/220] Batch: 0.0586 (0.0700) Data: 0.0387 (0.0476) Loss: 0.8249 (0.6275)
[2022/12/28 23:49] | VALID(011): [150/220] Batch: 0.0511 (0.0648) Data: 0.0388 (0.0426) Loss: 0.5454 (0.6248)
[2022/12/28 23:49] | VALID(011): [200/220] Batch: 0.0470 (0.0625) Data: 0.0345 (0.0405) Loss: 0.3170 (0.6307)
[2022/12/28 23:49] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:49] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:49] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:49] |    VALID(11)      0.6280      0.7958      0.8375      0.7958      0.7958      0.7958      0.9489
[2022/12/28 23:49] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:49] | ####################################################################################################
[2022/12/28 23:49] | TRAIN(012): [ 50/879] Batch: 0.2108 (0.2288) Data: 0.0108 (0.0513) Loss: 0.6574 (0.6934)
[2022/12/28 23:49] | TRAIN(012): [100/879] Batch: 0.1915 (0.2119) Data: 0.0086 (0.0311) Loss: 0.8528 (0.6785)
[2022/12/28 23:49] | TRAIN(012): [150/879] Batch: 0.1779 (0.2042) Data: 0.0105 (0.0242) Loss: 0.4805 (0.6900)
[2022/12/28 23:50] | TRAIN(012): [200/879] Batch: 0.1928 (0.2002) Data: 0.0107 (0.0207) Loss: 0.6616 (0.6840)
[2022/12/28 23:50] | TRAIN(012): [250/879] Batch: 0.1999 (0.1984) Data: 0.0108 (0.0186) Loss: 0.6102 (0.6766)
[2022/12/28 23:50] | TRAIN(012): [300/879] Batch: 0.1864 (0.1947) Data: 0.0081 (0.0172) Loss: 0.7516 (0.6727)
[2022/12/28 23:50] | TRAIN(012): [350/879] Batch: 0.1210 (0.1912) Data: 0.0092 (0.0161) Loss: 0.7264 (0.6772)
[2022/12/28 23:50] | TRAIN(012): [400/879] Batch: 0.1759 (0.1898) Data: 0.0094 (0.0153) Loss: 0.6531 (0.6757)
[2022/12/28 23:50] | TRAIN(012): [450/879] Batch: 0.1829 (0.1891) Data: 0.0105 (0.0147) Loss: 0.6691 (0.6787)
[2022/12/28 23:51] | TRAIN(012): [500/879] Batch: 0.1996 (0.1891) Data: 0.0090 (0.0142) Loss: 0.5127 (0.6770)
[2022/12/28 23:51] | TRAIN(012): [550/879] Batch: 0.1784 (0.1890) Data: 0.0101 (0.0138) Loss: 0.4568 (0.6764)
[2022/12/28 23:51] | TRAIN(012): [600/879] Batch: 0.1892 (0.1890) Data: 0.0091 (0.0135) Loss: 0.5827 (0.6744)
[2022/12/28 23:51] | TRAIN(012): [650/879] Batch: 0.1821 (0.1892) Data: 0.0112 (0.0133) Loss: 0.4388 (0.6744)
[2022/12/28 23:51] | TRAIN(012): [700/879] Batch: 0.2007 (0.1892) Data: 0.0109 (0.0130) Loss: 0.4762 (0.6716)
[2022/12/28 23:51] | TRAIN(012): [750/879] Batch: 0.1867 (0.1892) Data: 0.0108 (0.0128) Loss: 0.6386 (0.6709)
[2022/12/28 23:51] | TRAIN(012): [800/879] Batch: 0.1837 (0.1892) Data: 0.0091 (0.0127) Loss: 0.5464 (0.6719)
[2022/12/28 23:52] | TRAIN(012): [850/879] Batch: 0.1817 (0.1888) Data: 0.0103 (0.0125) Loss: 1.0286 (0.6722)
[2022/12/28 23:52] | ------------------------------------------------------------
[2022/12/28 23:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:52] | ------------------------------------------------------------
[2022/12/28 23:52] |    TRAIN(12)     0:02:45     0:00:10     0:02:34      0.6714
[2022/12/28 23:52] | ------------------------------------------------------------
[2022/12/28 23:52] | VALID(012): [ 50/220] Batch: 0.0589 (0.0801) Data: 0.0163 (0.0572) Loss: 0.5249 (0.6110)
[2022/12/28 23:52] | VALID(012): [100/220] Batch: 0.0398 (0.0660) Data: 0.0353 (0.0437) Loss: 0.9091 (0.6346)
[2022/12/28 23:52] | VALID(012): [150/220] Batch: 0.0548 (0.0612) Data: 0.0332 (0.0390) Loss: 0.5079 (0.6251)
[2022/12/28 23:52] | VALID(012): [200/220] Batch: 0.0406 (0.0594) Data: 0.0381 (0.0373) Loss: 0.3223 (0.6299)
[2022/12/28 23:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:52] |    VALID(12)      0.6283      0.7906      0.8406      0.7906      0.7906      0.7906      0.9477
[2022/12/28 23:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:52] | ####################################################################################################
[2022/12/28 23:52] | TRAIN(013): [ 50/879] Batch: 0.1795 (0.2195) Data: 0.0101 (0.0474) Loss: 0.7741 (0.6625)
[2022/12/28 23:52] | TRAIN(013): [100/879] Batch: 0.1824 (0.1986) Data: 0.0095 (0.0288) Loss: 0.9628 (0.6644)
[2022/12/28 23:52] | TRAIN(013): [150/879] Batch: 0.1896 (0.1947) Data: 0.0089 (0.0226) Loss: 0.6932 (0.6510)
[2022/12/28 23:53] | TRAIN(013): [200/879] Batch: 0.1924 (0.1936) Data: 0.0112 (0.0194) Loss: 0.7331 (0.6455)
[2022/12/28 23:53] | TRAIN(013): [250/879] Batch: 0.1786 (0.1930) Data: 0.0110 (0.0176) Loss: 0.7260 (0.6450)
[2022/12/28 23:53] | TRAIN(013): [300/879] Batch: 0.1785 (0.1920) Data: 0.0107 (0.0164) Loss: 0.5899 (0.6514)
[2022/12/28 23:53] | TRAIN(013): [350/879] Batch: 0.1728 (0.1920) Data: 0.0106 (0.0156) Loss: 0.4196 (0.6480)
[2022/12/28 23:53] | TRAIN(013): [400/879] Batch: 0.1921 (0.1920) Data: 0.0092 (0.0149) Loss: 0.8750 (0.6482)
[2022/12/28 23:53] | TRAIN(013): [450/879] Batch: 0.1879 (0.1918) Data: 0.0094 (0.0144) Loss: 0.6096 (0.6523)
[2022/12/28 23:54] | TRAIN(013): [500/879] Batch: 0.1969 (0.1910) Data: 0.0084 (0.0140) Loss: 0.9817 (0.6564)
[2022/12/28 23:54] | TRAIN(013): [550/879] Batch: 0.1906 (0.1908) Data: 0.0102 (0.0136) Loss: 0.7388 (0.6585)
[2022/12/28 23:54] | TRAIN(013): [600/879] Batch: 0.1870 (0.1903) Data: 0.0091 (0.0133) Loss: 0.5311 (0.6580)
[2022/12/28 23:54] | TRAIN(013): [650/879] Batch: 0.1835 (0.1902) Data: 0.0111 (0.0131) Loss: 0.5306 (0.6579)
[2022/12/28 23:54] | TRAIN(013): [700/879] Batch: 0.1961 (0.1900) Data: 0.0090 (0.0129) Loss: 0.5885 (0.6583)
[2022/12/28 23:54] | TRAIN(013): [750/879] Batch: 0.1764 (0.1891) Data: 0.0086 (0.0127) Loss: 0.3950 (0.6610)
[2022/12/28 23:54] | TRAIN(013): [800/879] Batch: 0.1956 (0.1880) Data: 0.0081 (0.0125) Loss: 0.5507 (0.6634)
[2022/12/28 23:55] | TRAIN(013): [850/879] Batch: 0.1806 (0.1880) Data: 0.0121 (0.0124) Loss: 0.6544 (0.6640)
[2022/12/28 23:55] | ------------------------------------------------------------
[2022/12/28 23:55] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:55] | ------------------------------------------------------------
[2022/12/28 23:55] |    TRAIN(13)     0:02:45     0:00:10     0:02:34      0.6642
[2022/12/28 23:55] | ------------------------------------------------------------
[2022/12/28 23:55] | VALID(013): [ 50/220] Batch: 0.0627 (0.0815) Data: 0.0402 (0.0580) Loss: 0.5944 (0.6057)
[2022/12/28 23:55] | VALID(013): [100/220] Batch: 0.0544 (0.0685) Data: 0.0385 (0.0461) Loss: 0.8690 (0.6293)
[2022/12/28 23:55] | VALID(013): [150/220] Batch: 0.0418 (0.0642) Data: 0.0384 (0.0422) Loss: 0.4692 (0.6206)
[2022/12/28 23:55] | VALID(013): [200/220] Batch: 0.0610 (0.0622) Data: 0.0391 (0.0403) Loss: 0.2718 (0.6230)
[2022/12/28 23:55] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:55] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:55] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:55] |    VALID(13)      0.6203      0.7939      0.8446      0.7939      0.7939      0.7939      0.9485
[2022/12/28 23:55] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:55] | ####################################################################################################
[2022/12/28 23:55] | TRAIN(014): [ 50/879] Batch: 0.1929 (0.2271) Data: 0.0110 (0.0458) Loss: 0.4203 (0.6875)
[2022/12/28 23:55] | TRAIN(014): [100/879] Batch: 0.1750 (0.2071) Data: 0.0103 (0.0282) Loss: 0.7972 (0.6636)
[2022/12/28 23:55] | TRAIN(014): [150/879] Batch: 0.1913 (0.2007) Data: 0.0105 (0.0221) Loss: 0.6442 (0.6540)
[2022/12/28 23:56] | TRAIN(014): [200/879] Batch: 0.1958 (0.1975) Data: 0.0086 (0.0191) Loss: 0.6910 (0.6529)
[2022/12/28 23:56] | TRAIN(014): [250/879] Batch: 0.1881 (0.1961) Data: 0.0096 (0.0173) Loss: 0.5261 (0.6538)
[2022/12/28 23:56] | TRAIN(014): [300/879] Batch: 0.2011 (0.1951) Data: 0.0087 (0.0161) Loss: 0.9044 (0.6589)
[2022/12/28 23:56] | TRAIN(014): [350/879] Batch: 0.1842 (0.1945) Data: 0.0113 (0.0152) Loss: 1.0534 (0.6617)
[2022/12/28 23:56] | TRAIN(014): [400/879] Batch: 0.1785 (0.1936) Data: 0.0094 (0.0146) Loss: 0.5896 (0.6553)
[2022/12/28 23:56] | TRAIN(014): [450/879] Batch: 0.1819 (0.1915) Data: 0.0086 (0.0141) Loss: 0.4198 (0.6565)
[2022/12/28 23:57] | TRAIN(014): [500/879] Batch: 0.1807 (0.1906) Data: 0.0111 (0.0137) Loss: 0.5609 (0.6595)
[2022/12/28 23:57] | TRAIN(014): [550/879] Batch: 0.1924 (0.1888) Data: 0.0084 (0.0133) Loss: 0.8316 (0.6578)
[2022/12/28 23:57] | TRAIN(014): [600/879] Batch: 0.1833 (0.1890) Data: 0.0096 (0.0131) Loss: 0.7191 (0.6593)
[2022/12/28 23:57] | TRAIN(014): [650/879] Batch: 0.1774 (0.1891) Data: 0.0103 (0.0129) Loss: 0.5959 (0.6597)
[2022/12/28 23:57] | TRAIN(014): [700/879] Batch: 0.1884 (0.1889) Data: 0.0110 (0.0127) Loss: 0.5983 (0.6590)
[2022/12/28 23:57] | TRAIN(014): [750/879] Batch: 0.1919 (0.1894) Data: 0.0103 (0.0126) Loss: 0.8321 (0.6599)
[2022/12/28 23:57] | TRAIN(014): [800/879] Batch: 0.1981 (0.1895) Data: 0.0106 (0.0124) Loss: 0.7067 (0.6604)
[2022/12/28 23:58] | TRAIN(014): [850/879] Batch: 0.1939 (0.1895) Data: 0.0095 (0.0123) Loss: 0.6763 (0.6607)
[2022/12/28 23:58] | ------------------------------------------------------------
[2022/12/28 23:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:58] | ------------------------------------------------------------
[2022/12/28 23:58] |    TRAIN(14)     0:02:46     0:00:10     0:02:35      0.6604
[2022/12/28 23:58] | ------------------------------------------------------------
[2022/12/28 23:58] | VALID(014): [ 50/220] Batch: 0.0583 (0.0811) Data: 0.0221 (0.0562) Loss: 0.5791 (0.6052)
[2022/12/28 23:58] | VALID(014): [100/220] Batch: 0.0588 (0.0683) Data: 0.0203 (0.0429) Loss: 0.8344 (0.6253)
[2022/12/28 23:58] | VALID(014): [150/220] Batch: 0.0584 (0.0639) Data: 0.0334 (0.0399) Loss: 0.5302 (0.6158)
[2022/12/28 23:58] | VALID(014): [200/220] Batch: 0.0579 (0.0616) Data: 0.0375 (0.0382) Loss: 0.2950 (0.6220)
[2022/12/28 23:58] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:58] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:58] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:58] |    VALID(14)      0.6197      0.7983      0.8420      0.7983      0.7983      0.7983      0.9496
[2022/12/28 23:58] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:58] | ####################################################################################################
[2022/12/28 23:58] | TRAIN(015): [ 50/879] Batch: 0.1915 (0.2245) Data: 0.0106 (0.0452) Loss: 0.7790 (0.6835)
[2022/12/28 23:58] | TRAIN(015): [100/879] Batch: 0.1915 (0.2061) Data: 0.0088 (0.0278) Loss: 0.5851 (0.6824)
[2022/12/28 23:58] | TRAIN(015): [150/879] Batch: 0.1841 (0.2009) Data: 0.0089 (0.0220) Loss: 0.6337 (0.6626)
[2022/12/28 23:59] | TRAIN(015): [200/879] Batch: 0.1854 (0.1939) Data: 0.0099 (0.0190) Loss: 0.6575 (0.6632)
[2022/12/28 23:59] | TRAIN(015): [250/879] Batch: 0.1894 (0.1892) Data: 0.0108 (0.0172) Loss: 0.6331 (0.6626)
[2022/12/28 23:59] | TRAIN(015): [300/879] Batch: 0.1900 (0.1894) Data: 0.0115 (0.0160) Loss: 0.9408 (0.6659)
[2022/12/28 23:59] | TRAIN(015): [350/879] Batch: 0.1983 (0.1895) Data: 0.0097 (0.0152) Loss: 0.7055 (0.6617)
[2022/12/28 23:59] | TRAIN(015): [400/879] Batch: 0.1822 (0.1892) Data: 0.0092 (0.0146) Loss: 0.4587 (0.6595)
[2022/12/28 23:59] | TRAIN(015): [450/879] Batch: 0.1858 (0.1888) Data: 0.0097 (0.0141) Loss: 0.9499 (0.6557)
[2022/12/29 00:00] | TRAIN(015): [500/879] Batch: 0.1807 (0.1887) Data: 0.0107 (0.0136) Loss: 0.7484 (0.6575)
[2022/12/29 00:00] | TRAIN(015): [550/879] Batch: 0.1979 (0.1883) Data: 0.0116 (0.0133) Loss: 0.5656 (0.6582)
[2022/12/29 00:00] | TRAIN(015): [600/879] Batch: 0.1903 (0.1881) Data: 0.0096 (0.0131) Loss: 0.7449 (0.6603)
[2022/12/29 00:00] | TRAIN(015): [650/879] Batch: 0.2181 (0.1882) Data: 0.0121 (0.0129) Loss: 0.4667 (0.6591)
[2022/12/29 00:00] | TRAIN(015): [700/879] Batch: 0.1898 (0.1885) Data: 0.0092 (0.0127) Loss: 0.7899 (0.6607)
[2022/12/29 00:00] | TRAIN(015): [750/879] Batch: 0.1886 (0.1885) Data: 0.0109 (0.0125) Loss: 0.3871 (0.6614)
[2022/12/29 00:00] | TRAIN(015): [800/879] Batch: 0.1958 (0.1886) Data: 0.0083 (0.0123) Loss: 0.5395 (0.6604)
[2022/12/29 00:01] | TRAIN(015): [850/879] Batch: 0.1954 (0.1887) Data: 0.0092 (0.0122) Loss: 0.5402 (0.6565)
[2022/12/29 00:01] | ------------------------------------------------------------
[2022/12/29 00:01] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:01] | ------------------------------------------------------------
[2022/12/29 00:01] |    TRAIN(15)     0:02:45     0:00:10     0:02:34      0.6566
[2022/12/29 00:01] | ------------------------------------------------------------
[2022/12/29 00:01] | VALID(015): [ 50/220] Batch: 0.0518 (0.0818) Data: 0.0299 (0.0575) Loss: 0.6325 (0.5920)
[2022/12/29 00:01] | VALID(015): [100/220] Batch: 0.0526 (0.0673) Data: 0.0355 (0.0443) Loss: 0.7885 (0.6141)
[2022/12/29 00:01] | VALID(015): [150/220] Batch: 0.0296 (0.0569) Data: 0.0151 (0.0336) Loss: 0.5292 (0.6070)
[2022/12/29 00:01] | VALID(015): [200/220] Batch: 0.0580 (0.0543) Data: 0.0120 (0.0300) Loss: 0.3601 (0.6100)
[2022/12/29 00:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:01] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:01] |    VALID(15)      0.6096      0.7955      0.8500      0.7955      0.7955      0.7955      0.9489
[2022/12/29 00:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:01] | ####################################################################################################
[2022/12/29 00:01] | TRAIN(016): [ 50/879] Batch: 0.2022 (0.2250) Data: 0.0102 (0.0459) Loss: 0.8159 (0.6632)
[2022/12/29 00:01] | TRAIN(016): [100/879] Batch: 0.1886 (0.2063) Data: 0.0113 (0.0283) Loss: 0.6602 (0.6467)
[2022/12/29 00:01] | TRAIN(016): [150/879] Batch: 0.1825 (0.1995) Data: 0.0089 (0.0223) Loss: 0.6153 (0.6501)
[2022/12/29 00:02] | TRAIN(016): [200/879] Batch: 0.1771 (0.1974) Data: 0.0114 (0.0193) Loss: 0.5732 (0.6442)
[2022/12/29 00:02] | TRAIN(016): [250/879] Batch: 0.1727 (0.1956) Data: 0.0103 (0.0175) Loss: 0.9251 (0.6451)
[2022/12/29 00:02] | TRAIN(016): [300/879] Batch: 0.1930 (0.1943) Data: 0.0094 (0.0163) Loss: 0.5956 (0.6432)
[2022/12/29 00:02] | TRAIN(016): [350/879] Batch: 0.2058 (0.1937) Data: 0.0105 (0.0154) Loss: 0.4812 (0.6415)
[2022/12/29 00:02] | TRAIN(016): [400/879] Batch: 0.1863 (0.1933) Data: 0.0102 (0.0148) Loss: 0.7861 (0.6448)
[2022/12/29 00:02] | TRAIN(016): [450/879] Batch: 0.1826 (0.1931) Data: 0.0112 (0.0143) Loss: 0.6860 (0.6487)
[2022/12/29 00:03] | TRAIN(016): [500/879] Batch: 0.1972 (0.1929) Data: 0.0095 (0.0139) Loss: 0.4481 (0.6488)
[2022/12/29 00:03] | TRAIN(016): [550/879] Batch: 0.2037 (0.1925) Data: 0.0100 (0.0135) Loss: 0.7230 (0.6481)
[2022/12/29 00:03] | TRAIN(016): [600/879] Batch: 0.1809 (0.1913) Data: 0.0096 (0.0133) Loss: 0.7986 (0.6492)
[2022/12/29 00:03] | TRAIN(016): [650/879] Batch: 0.1902 (0.1906) Data: 0.0118 (0.0130) Loss: 0.4988 (0.6478)
[2022/12/29 00:03] | TRAIN(016): [700/879] Batch: 0.2026 (0.1895) Data: 0.0097 (0.0128) Loss: 0.8962 (0.6476)
[2022/12/29 00:03] | TRAIN(016): [750/879] Batch: 0.1961 (0.1896) Data: 0.0103 (0.0126) Loss: 0.7819 (0.6505)
[2022/12/29 00:03] | TRAIN(016): [800/879] Batch: 0.1811 (0.1895) Data: 0.0106 (0.0125) Loss: 0.7027 (0.6489)
[2022/12/29 00:04] | TRAIN(016): [850/879] Batch: 0.1863 (0.1893) Data: 0.0089 (0.0123) Loss: 0.6593 (0.6488)
[2022/12/29 00:04] | ------------------------------------------------------------
[2022/12/29 00:04] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:04] | ------------------------------------------------------------
[2022/12/29 00:04] |    TRAIN(16)     0:02:46     0:00:10     0:02:35      0.6496
[2022/12/29 00:04] | ------------------------------------------------------------
[2022/12/29 00:04] | VALID(016): [ 50/220] Batch: 0.0594 (0.0825) Data: 0.0252 (0.0592) Loss: 0.5017 (0.5858)
[2022/12/29 00:04] | VALID(016): [100/220] Batch: 0.0609 (0.0691) Data: 0.0176 (0.0467) Loss: 0.8670 (0.6073)
[2022/12/29 00:04] | VALID(016): [150/220] Batch: 0.0589 (0.0647) Data: 0.0240 (0.0424) Loss: 0.5658 (0.6008)
[2022/12/29 00:04] | VALID(016): [200/220] Batch: 0.0733 (0.0621) Data: 0.0255 (0.0395) Loss: 0.2891 (0.6024)
[2022/12/29 00:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:04] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:04] |    VALID(16)      0.6000      0.8007      0.8525      0.8007      0.8007      0.8007      0.9502
[2022/12/29 00:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:04] | ####################################################################################################
[2022/12/29 00:04] | TRAIN(017): [ 50/879] Batch: 0.1932 (0.2277) Data: 0.0112 (0.0477) Loss: 0.8524 (0.6209)
[2022/12/29 00:04] | TRAIN(017): [100/879] Batch: 0.1928 (0.2080) Data: 0.0110 (0.0292) Loss: 0.6400 (0.6272)
[2022/12/29 00:04] | TRAIN(017): [150/879] Batch: 0.1881 (0.2025) Data: 0.0114 (0.0230) Loss: 0.7228 (0.6408)
[2022/12/29 00:05] | TRAIN(017): [200/879] Batch: 0.1920 (0.1997) Data: 0.0104 (0.0199) Loss: 1.0262 (0.6443)
[2022/12/29 00:05] | TRAIN(017): [250/879] Batch: 0.1830 (0.1978) Data: 0.0111 (0.0179) Loss: 0.6636 (0.6401)
[2022/12/29 00:05] | TRAIN(017): [300/879] Batch: 0.1892 (0.1965) Data: 0.0093 (0.0166) Loss: 0.6384 (0.6388)
[2022/12/29 00:05] | TRAIN(017): [350/879] Batch: 0.1793 (0.1935) Data: 0.0085 (0.0157) Loss: 0.6111 (0.6413)
[2022/12/29 00:05] | TRAIN(017): [400/879] Batch: 0.1725 (0.1906) Data: 0.0082 (0.0149) Loss: 0.6313 (0.6384)
[2022/12/29 00:05] | TRAIN(017): [450/879] Batch: 0.1934 (0.1905) Data: 0.0094 (0.0144) Loss: 0.5774 (0.6413)
[2022/12/29 00:05] | TRAIN(017): [500/879] Batch: 0.1757 (0.1897) Data: 0.0111 (0.0140) Loss: 0.4349 (0.6417)
[2022/12/29 00:06] | TRAIN(017): [550/879] Batch: 0.2014 (0.1896) Data: 0.0087 (0.0136) Loss: 0.5023 (0.6410)
[2022/12/29 00:06] | TRAIN(017): [600/879] Batch: 0.1932 (0.1899) Data: 0.0085 (0.0133) Loss: 0.3405 (0.6408)
[2022/12/29 00:06] | TRAIN(017): [650/879] Batch: 0.1833 (0.1901) Data: 0.0100 (0.0131) Loss: 0.4479 (0.6409)
[2022/12/29 00:06] | TRAIN(017): [700/879] Batch: 0.1905 (0.1899) Data: 0.0102 (0.0128) Loss: 0.5245 (0.6424)
[2022/12/29 00:06] | TRAIN(017): [750/879] Batch: 0.1922 (0.1900) Data: 0.0094 (0.0127) Loss: 0.9971 (0.6424)
[2022/12/29 00:06] | TRAIN(017): [800/879] Batch: 0.1984 (0.1900) Data: 0.0099 (0.0125) Loss: 0.5429 (0.6447)
[2022/12/29 00:07] | TRAIN(017): [850/879] Batch: 0.2028 (0.1901) Data: 0.0104 (0.0124) Loss: 0.7996 (0.6452)
[2022/12/29 00:07] | ------------------------------------------------------------
[2022/12/29 00:07] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:07] | ------------------------------------------------------------
[2022/12/29 00:07] |    TRAIN(17)     0:02:46     0:00:10     0:02:36      0.6470
[2022/12/29 00:07] | ------------------------------------------------------------
[2022/12/29 00:07] | VALID(017): [ 50/220] Batch: 0.0537 (0.0801) Data: 0.0389 (0.0537) Loss: 0.5192 (0.5998)
[2022/12/29 00:07] | VALID(017): [100/220] Batch: 0.0591 (0.0663) Data: 0.0170 (0.0423) Loss: 0.8317 (0.6178)
[2022/12/29 00:07] | VALID(017): [150/220] Batch: 0.0351 (0.0623) Data: 0.0393 (0.0393) Loss: 0.5144 (0.6133)
[2022/12/29 00:07] | VALID(017): [200/220] Batch: 0.0562 (0.0605) Data: 0.0381 (0.0378) Loss: 0.2837 (0.6159)
[2022/12/29 00:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:07] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:07] |    VALID(17)      0.6133      0.7929      0.8533      0.7929      0.7929      0.7929      0.9482
[2022/12/29 00:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:07] | ####################################################################################################
[2022/12/29 00:07] | TRAIN(018): [ 50/879] Batch: 0.1675 (0.2135) Data: 0.0109 (0.0487) Loss: 0.6049 (0.6482)
[2022/12/29 00:07] | TRAIN(018): [100/879] Batch: 0.1904 (0.1990) Data: 0.0120 (0.0298) Loss: 0.4670 (0.6414)
[2022/12/29 00:07] | TRAIN(018): [150/879] Batch: 0.1896 (0.1919) Data: 0.0104 (0.0235) Loss: 0.5369 (0.6368)
[2022/12/29 00:08] | TRAIN(018): [200/879] Batch: 0.1861 (0.1915) Data: 0.0091 (0.0202) Loss: 0.4035 (0.6350)
[2022/12/29 00:08] | TRAIN(018): [250/879] Batch: 0.1806 (0.1907) Data: 0.0112 (0.0182) Loss: 0.7391 (0.6336)
[2022/12/29 00:08] | TRAIN(018): [300/879] Batch: 0.1865 (0.1899) Data: 0.0123 (0.0169) Loss: 0.5001 (0.6309)
[2022/12/29 00:08] | TRAIN(018): [350/879] Batch: 0.1953 (0.1894) Data: 0.0089 (0.0159) Loss: 0.3491 (0.6331)
[2022/12/29 00:08] | TRAIN(018): [400/879] Batch: 0.1793 (0.1894) Data: 0.0092 (0.0152) Loss: 0.7899 (0.6340)
[2022/12/29 00:08] | TRAIN(018): [450/879] Batch: 0.1960 (0.1896) Data: 0.0103 (0.0147) Loss: 0.5337 (0.6344)
[2022/12/29 00:08] | TRAIN(018): [500/879] Batch: 0.1828 (0.1897) Data: 0.0088 (0.0142) Loss: 0.5786 (0.6376)
[2022/12/29 00:09] | TRAIN(018): [550/879] Batch: 0.1886 (0.1897) Data: 0.0109 (0.0138) Loss: 0.5967 (0.6380)
[2022/12/29 00:09] | TRAIN(018): [600/879] Batch: 0.1876 (0.1894) Data: 0.0089 (0.0135) Loss: 0.6430 (0.6405)
[2022/12/29 00:09] | TRAIN(018): [650/879] Batch: 0.1819 (0.1889) Data: 0.0103 (0.0133) Loss: 0.6928 (0.6402)
[2022/12/29 00:09] | TRAIN(018): [700/879] Batch: 0.1873 (0.1888) Data: 0.0113 (0.0130) Loss: 0.5595 (0.6415)
[2022/12/29 00:09] | TRAIN(018): [750/879] Batch: 0.1320 (0.1884) Data: 0.0096 (0.0129) Loss: 0.8374 (0.6421)
[2022/12/29 00:09] | TRAIN(018): [800/879] Batch: 0.1845 (0.1880) Data: 0.0092 (0.0127) Loss: 0.4531 (0.6421)
[2022/12/29 00:10] | TRAIN(018): [850/879] Batch: 0.1821 (0.1870) Data: 0.0093 (0.0125) Loss: 0.5864 (0.6408)
[2022/12/29 00:10] | ------------------------------------------------------------
[2022/12/29 00:10] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:10] | ------------------------------------------------------------
[2022/12/29 00:10] |    TRAIN(18)     0:02:44     0:00:10     0:02:33      0.6405
[2022/12/29 00:10] | ------------------------------------------------------------
[2022/12/29 00:10] | VALID(018): [ 50/220] Batch: 0.0606 (0.0819) Data: 0.0283 (0.0567) Loss: 0.4725 (0.5985)
[2022/12/29 00:10] | VALID(018): [100/220] Batch: 0.0431 (0.0688) Data: 0.0387 (0.0453) Loss: 0.9548 (0.6230)
[2022/12/29 00:10] | VALID(018): [150/220] Batch: 0.0476 (0.0639) Data: 0.0428 (0.0410) Loss: 0.4754 (0.6158)
[2022/12/29 00:10] | VALID(018): [200/220] Batch: 0.0362 (0.0612) Data: 0.0340 (0.0384) Loss: 0.3326 (0.6129)
[2022/12/29 00:10] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:10] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:10] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:10] |    VALID(18)      0.6120      0.7997      0.8492      0.7997      0.7997      0.7997      0.9499
[2022/12/29 00:10] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:10] | ####################################################################################################
[2022/12/29 00:10] | TRAIN(019): [ 50/879] Batch: 0.2040 (0.2282) Data: 0.0108 (0.0482) Loss: 0.7545 (0.6316)
[2022/12/29 00:10] | TRAIN(019): [100/879] Batch: 0.1809 (0.2060) Data: 0.0093 (0.0295) Loss: 0.4617 (0.6219)
[2022/12/29 00:10] | TRAIN(019): [150/879] Batch: 0.1928 (0.2007) Data: 0.0101 (0.0232) Loss: 0.5814 (0.6255)
[2022/12/29 00:11] | TRAIN(019): [200/879] Batch: 0.1962 (0.1986) Data: 0.0095 (0.0199) Loss: 0.5880 (0.6313)
[2022/12/29 00:11] | TRAIN(019): [250/879] Batch: 0.1914 (0.1973) Data: 0.0086 (0.0181) Loss: 0.7821 (0.6300)
[2022/12/29 00:11] | TRAIN(019): [300/879] Batch: 0.1952 (0.1957) Data: 0.0090 (0.0167) Loss: 0.5138 (0.6402)
[2022/12/29 00:11] | TRAIN(019): [350/879] Batch: 0.1879 (0.1943) Data: 0.0107 (0.0158) Loss: 0.7877 (0.6397)
[2022/12/29 00:11] | TRAIN(019): [400/879] Batch: 0.1795 (0.1938) Data: 0.0107 (0.0151) Loss: 0.5720 (0.6391)
[2022/12/29 00:11] | TRAIN(019): [450/879] Batch: 0.1745 (0.1936) Data: 0.0087 (0.0146) Loss: 0.6796 (0.6387)
[2022/12/29 00:11] | TRAIN(019): [500/879] Batch: 0.1797 (0.1915) Data: 0.0102 (0.0142) Loss: 0.5915 (0.6416)
[2022/12/29 00:12] | TRAIN(019): [550/879] Batch: 0.1402 (0.1898) Data: 0.0088 (0.0138) Loss: 0.3586 (0.6398)
[2022/12/29 00:12] | TRAIN(019): [600/879] Batch: 0.1919 (0.1891) Data: 0.0102 (0.0134) Loss: 0.7097 (0.6386)
[2022/12/29 00:12] | TRAIN(019): [650/879] Batch: 0.2025 (0.1892) Data: 0.0097 (0.0132) Loss: 0.6177 (0.6373)
[2022/12/29 00:12] | TRAIN(019): [700/879] Batch: 0.1858 (0.1893) Data: 0.0096 (0.0130) Loss: 0.8406 (0.6367)
[2022/12/29 00:12] | TRAIN(019): [750/879] Batch: 0.1864 (0.1894) Data: 0.0095 (0.0128) Loss: 0.3536 (0.6365)
[2022/12/29 00:12] | TRAIN(019): [800/879] Batch: 0.1828 (0.1894) Data: 0.0106 (0.0126) Loss: 0.7926 (0.6352)
[2022/12/29 00:13] | TRAIN(019): [850/879] Batch: 0.1983 (0.1895) Data: 0.0090 (0.0125) Loss: 0.3758 (0.6384)
[2022/12/29 00:13] | ------------------------------------------------------------
[2022/12/29 00:13] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:13] | ------------------------------------------------------------
[2022/12/29 00:13] |    TRAIN(19)     0:02:46     0:00:10     0:02:35      0.6373
[2022/12/29 00:13] | ------------------------------------------------------------
[2022/12/29 00:13] | VALID(019): [ 50/220] Batch: 0.0556 (0.0822) Data: 0.0283 (0.0572) Loss: 0.5022 (0.5863)
[2022/12/29 00:13] | VALID(019): [100/220] Batch: 0.0590 (0.0682) Data: 0.0343 (0.0450) Loss: 0.8791 (0.6041)
[2022/12/29 00:13] | VALID(019): [150/220] Batch: 0.0587 (0.0638) Data: 0.0217 (0.0411) Loss: 0.5162 (0.5958)
[2022/12/29 00:13] | VALID(019): [200/220] Batch: 0.0610 (0.0619) Data: 0.0400 (0.0396) Loss: 0.3127 (0.5971)
[2022/12/29 00:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:13] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:13] |    VALID(19)      0.5949      0.8012      0.8585      0.8012      0.8012      0.8012      0.9503
[2022/12/29 00:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:13] | ####################################################################################################
[2022/12/29 00:13] | TRAIN(020): [ 50/879] Batch: 0.1909 (0.2317) Data: 0.0104 (0.0482) Loss: 0.6151 (0.6194)
[2022/12/29 00:13] | TRAIN(020): [100/879] Batch: 0.1909 (0.2118) Data: 0.0110 (0.0295) Loss: 0.7596 (0.6281)
[2022/12/29 00:13] | TRAIN(020): [150/879] Batch: 0.1935 (0.2063) Data: 0.0089 (0.0233) Loss: 0.6638 (0.6306)
[2022/12/29 00:14] | TRAIN(020): [200/879] Batch: 0.1387 (0.2001) Data: 0.0124 (0.0201) Loss: 0.5633 (0.6309)
[2022/12/29 00:14] | TRAIN(020): [250/879] Batch: 0.1857 (0.1970) Data: 0.0089 (0.0182) Loss: 0.4407 (0.6254)
[2022/12/29 00:14] | TRAIN(020): [300/879] Batch: 0.1885 (0.1935) Data: 0.0100 (0.0169) Loss: 0.6670 (0.6316)
[2022/12/29 00:14] | TRAIN(020): [350/879] Batch: 0.1932 (0.1929) Data: 0.0094 (0.0160) Loss: 0.4386 (0.6345)
[2022/12/29 00:14] | TRAIN(020): [400/879] Batch: 0.1982 (0.1924) Data: 0.0105 (0.0153) Loss: 0.8404 (0.6372)
[2022/12/29 00:14] | TRAIN(020): [450/879] Batch: 0.1971 (0.1923) Data: 0.0107 (0.0148) Loss: 0.4297 (0.6371)
[2022/12/29 00:14] | TRAIN(020): [500/879] Batch: 0.1858 (0.1921) Data: 0.0107 (0.0144) Loss: 0.3361 (0.6301)
[2022/12/29 00:15] | TRAIN(020): [550/879] Batch: 0.1778 (0.1915) Data: 0.0132 (0.0140) Loss: 0.3699 (0.6289)
[2022/12/29 00:15] | TRAIN(020): [600/879] Batch: 0.1875 (0.1913) Data: 0.0127 (0.0137) Loss: 0.5357 (0.6307)
[2022/12/29 00:15] | TRAIN(020): [650/879] Batch: 0.2006 (0.1919) Data: 0.0123 (0.0135) Loss: 0.9463 (0.6292)
[2022/12/29 00:15] | TRAIN(020): [700/879] Batch: 0.2011 (0.1922) Data: 0.0106 (0.0134) Loss: 0.6562 (0.6306)
[2022/12/29 00:15] | TRAIN(020): [750/879] Batch: 0.1929 (0.1922) Data: 0.0111 (0.0132) Loss: 0.5228 (0.6317)
[2022/12/29 00:15] | TRAIN(020): [800/879] Batch: 0.1976 (0.1920) Data: 0.0091 (0.0130) Loss: 0.3616 (0.6297)
[2022/12/29 00:16] | TRAIN(020): [850/879] Batch: 0.1985 (0.1919) Data: 0.0108 (0.0128) Loss: 0.3776 (0.6304)
[2022/12/29 00:16] | ------------------------------------------------------------
[2022/12/29 00:16] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:16] | ------------------------------------------------------------
[2022/12/29 00:16] |    TRAIN(20)     0:02:48     0:00:11     0:02:37      0.6313
[2022/12/29 00:16] | ------------------------------------------------------------
[2022/12/29 00:16] | VALID(020): [ 50/220] Batch: 0.0508 (0.0797) Data: 0.0355 (0.0546) Loss: 0.4740 (0.5698)
[2022/12/29 00:16] | VALID(020): [100/220] Batch: 0.0547 (0.0659) Data: 0.0301 (0.0425) Loss: 0.9269 (0.5907)
[2022/12/29 00:16] | VALID(020): [150/220] Batch: 0.0490 (0.0615) Data: 0.0342 (0.0385) Loss: 0.4993 (0.5854)
[2022/12/29 00:16] | VALID(020): [200/220] Batch: 0.0286 (0.0587) Data: 0.0090 (0.0359) Loss: 0.2293 (0.5868)
[2022/12/29 00:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:16] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:16] |    VALID(20)      0.5846      0.8050      0.8606      0.8050      0.8050      0.8050      0.9513
[2022/12/29 00:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:16] | ####################################################################################################
[2022/12/29 00:16] | TRAIN(021): [ 50/879] Batch: 0.1888 (0.2333) Data: 0.0117 (0.0497) Loss: 0.3114 (0.5948)
[2022/12/29 00:16] | TRAIN(021): [100/879] Batch: 0.1888 (0.2130) Data: 0.0137 (0.0308) Loss: 0.6422 (0.6300)
[2022/12/29 00:16] | TRAIN(021): [150/879] Batch: 0.1953 (0.2056) Data: 0.0098 (0.0242) Loss: 0.6579 (0.6244)
[2022/12/29 00:17] | TRAIN(021): [200/879] Batch: 0.1709 (0.2017) Data: 0.0128 (0.0209) Loss: 0.6153 (0.6272)
[2022/12/29 00:17] | TRAIN(021): [250/879] Batch: 0.1991 (0.2002) Data: 0.0112 (0.0190) Loss: 1.1134 (0.6192)
[2022/12/29 00:17] | TRAIN(021): [300/879] Batch: 0.1821 (0.1988) Data: 0.0099 (0.0177) Loss: 0.4854 (0.6257)
[2022/12/29 00:17] | TRAIN(021): [350/879] Batch: 0.1963 (0.1977) Data: 0.0114 (0.0167) Loss: 0.7421 (0.6299)
[2022/12/29 00:17] | TRAIN(021): [400/879] Batch: 0.1949 (0.1971) Data: 0.0102 (0.0160) Loss: 0.3922 (0.6279)
[2022/12/29 00:17] | TRAIN(021): [450/879] Batch: 0.2002 (0.1961) Data: 0.0107 (0.0154) Loss: 0.6263 (0.6282)
[2022/12/29 00:18] | TRAIN(021): [500/879] Batch: 0.1939 (0.1955) Data: 0.0099 (0.0150) Loss: 0.5338 (0.6305)
[2022/12/29 00:18] | TRAIN(021): [550/879] Batch: 0.1918 (0.1950) Data: 0.0107 (0.0146) Loss: 0.4994 (0.6306)
[2022/12/29 00:18] | TRAIN(021): [600/879] Batch: 0.1999 (0.1944) Data: 0.0124 (0.0143) Loss: 1.0547 (0.6287)
[2022/12/29 00:18] | TRAIN(021): [650/879] Batch: 0.1874 (0.1928) Data: 0.0112 (0.0140) Loss: 0.5504 (0.6302)
[2022/12/29 00:18] | TRAIN(021): [700/879] Batch: 0.1969 (0.1912) Data: 0.0105 (0.0137) Loss: 0.7304 (0.6299)
[2022/12/29 00:18] | TRAIN(021): [750/879] Batch: 0.1860 (0.1909) Data: 0.0091 (0.0135) Loss: 0.9078 (0.6299)
[2022/12/29 00:18] | TRAIN(021): [800/879] Batch: 0.1851 (0.1909) Data: 0.0118 (0.0134) Loss: 0.6112 (0.6313)
[2022/12/29 00:19] | TRAIN(021): [850/879] Batch: 0.1896 (0.1908) Data: 0.0121 (0.0132) Loss: 0.2634 (0.6293)
[2022/12/29 00:19] | ------------------------------------------------------------
[2022/12/29 00:19] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:19] | ------------------------------------------------------------
[2022/12/29 00:19] |    TRAIN(21)     0:02:47     0:00:11     0:02:36      0.6284
[2022/12/29 00:19] | ------------------------------------------------------------
[2022/12/29 00:19] | VALID(021): [ 50/220] Batch: 0.0547 (0.0831) Data: 0.0172 (0.0585) Loss: 0.4905 (0.5757)
[2022/12/29 00:19] | VALID(021): [100/220] Batch: 0.0634 (0.0685) Data: 0.0385 (0.0452) Loss: 0.7906 (0.5999)
[2022/12/29 00:19] | VALID(021): [150/220] Batch: 0.0589 (0.0635) Data: 0.0344 (0.0401) Loss: 0.4913 (0.5943)
[2022/12/29 00:19] | VALID(021): [200/220] Batch: 0.0481 (0.0613) Data: 0.0364 (0.0382) Loss: 0.2964 (0.5970)
[2022/12/29 00:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:19] |    VALID(21)      0.5962      0.8019      0.8604      0.8019      0.8019      0.8019      0.9505
[2022/12/29 00:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:19] | ####################################################################################################
[2022/12/29 00:19] | TRAIN(022): [ 50/879] Batch: 0.1723 (0.2326) Data: 0.0109 (0.0516) Loss: 0.4268 (0.6202)
[2022/12/29 00:19] | TRAIN(022): [100/879] Batch: 0.2009 (0.2112) Data: 0.0097 (0.0313) Loss: 0.4757 (0.6304)
[2022/12/29 00:19] | TRAIN(022): [150/879] Batch: 0.1914 (0.2043) Data: 0.0108 (0.0244) Loss: 0.5534 (0.6428)
[2022/12/29 00:20] | TRAIN(022): [200/879] Batch: 0.1790 (0.2003) Data: 0.0172 (0.0210) Loss: 0.6494 (0.6420)
[2022/12/29 00:20] | TRAIN(022): [250/879] Batch: 0.2202 (0.1983) Data: 0.0116 (0.0189) Loss: 0.9154 (0.6368)
[2022/12/29 00:20] | TRAIN(022): [300/879] Batch: 0.1904 (0.1972) Data: 0.0088 (0.0175) Loss: 0.4358 (0.6372)
[2022/12/29 00:20] | TRAIN(022): [350/879] Batch: 0.2015 (0.1939) Data: 0.0147 (0.0165) Loss: 0.4635 (0.6322)
[2022/12/29 00:20] | TRAIN(022): [400/879] Batch: 0.1690 (0.1911) Data: 0.0107 (0.0158) Loss: 0.7032 (0.6297)
[2022/12/29 00:20] | TRAIN(022): [450/879] Batch: 0.2152 (0.1912) Data: 0.0106 (0.0152) Loss: 0.7910 (0.6342)
[2022/12/29 00:21] | TRAIN(022): [500/879] Batch: 0.1920 (0.1911) Data: 0.0096 (0.0148) Loss: 0.5141 (0.6315)
[2022/12/29 00:21] | TRAIN(022): [550/879] Batch: 0.1662 (0.1910) Data: 0.0089 (0.0144) Loss: 0.9280 (0.6326)
[2022/12/29 00:21] | TRAIN(022): [600/879] Batch: 0.2040 (0.1909) Data: 0.0109 (0.0140) Loss: 0.7799 (0.6312)
[2022/12/29 00:21] | TRAIN(022): [650/879] Batch: 0.1834 (0.1911) Data: 0.0108 (0.0137) Loss: 0.7543 (0.6318)
[2022/12/29 00:21] | TRAIN(022): [700/879] Batch: 0.2028 (0.1909) Data: 0.0100 (0.0135) Loss: 0.8193 (0.6297)
[2022/12/29 00:21] | TRAIN(022): [750/879] Batch: 0.1937 (0.1908) Data: 0.0101 (0.0133) Loss: 0.4312 (0.6287)
[2022/12/29 00:21] | TRAIN(022): [800/879] Batch: 0.1857 (0.1909) Data: 0.0091 (0.0131) Loss: 0.4661 (0.6290)
[2022/12/29 00:22] | TRAIN(022): [850/879] Batch: 0.1989 (0.1911) Data: 0.0103 (0.0130) Loss: 0.5817 (0.6280)
[2022/12/29 00:22] | ------------------------------------------------------------
[2022/12/29 00:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:22] | ------------------------------------------------------------
[2022/12/29 00:22] |    TRAIN(22)     0:02:47     0:00:11     0:02:36      0.6270
[2022/12/29 00:22] | ------------------------------------------------------------
[2022/12/29 00:22] | VALID(022): [ 50/220] Batch: 0.0599 (0.0821) Data: 0.0181 (0.0549) Loss: 0.5362 (0.5832)
[2022/12/29 00:22] | VALID(022): [100/220] Batch: 0.0419 (0.0676) Data: 0.0362 (0.0430) Loss: 0.9252 (0.6058)
[2022/12/29 00:22] | VALID(022): [150/220] Batch: 0.0594 (0.0634) Data: 0.0361 (0.0396) Loss: 0.4999 (0.5997)
[2022/12/29 00:22] | VALID(022): [200/220] Batch: 0.0584 (0.0613) Data: 0.0253 (0.0378) Loss: 0.3549 (0.6022)
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] |    VALID(22)      0.5981      0.8032      0.8570      0.8032      0.8032      0.8032      0.9508
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] | ####################################################################################################
[2022/12/29 00:22] | TRAIN(023): [ 50/879] Batch: 0.2134 (0.2224) Data: 0.0110 (0.0474) Loss: 0.7746 (0.6382)
[2022/12/29 00:22] | TRAIN(023): [100/879] Batch: 0.1339 (0.2030) Data: 0.0092 (0.0298) Loss: 0.4556 (0.6263)
[2022/12/29 00:22] | TRAIN(023): [150/879] Batch: 0.1977 (0.1969) Data: 0.0095 (0.0236) Loss: 0.5852 (0.6279)
[2022/12/29 00:23] | TRAIN(023): [200/879] Batch: 0.1815 (0.1954) Data: 0.0169 (0.0206) Loss: 0.7435 (0.6192)
[2022/12/29 00:23] | TRAIN(023): [250/879] Batch: 0.2010 (0.1949) Data: 0.0091 (0.0187) Loss: 0.4394 (0.6245)
[2022/12/29 00:23] | TRAIN(023): [300/879] Batch: 0.2052 (0.1948) Data: 0.0108 (0.0175) Loss: 0.7045 (0.6250)
[2022/12/29 00:23] | TRAIN(023): [350/879] Batch: 0.1806 (0.1946) Data: 0.0094 (0.0166) Loss: 1.0260 (0.6235)
[2022/12/29 00:23] | TRAIN(023): [400/879] Batch: 0.1992 (0.1942) Data: 0.0100 (0.0159) Loss: 0.7274 (0.6195)
[2022/12/29 00:23] | TRAIN(023): [450/879] Batch: 0.2173 (0.1942) Data: 0.0136 (0.0154) Loss: 0.5793 (0.6156)
[2022/12/29 00:24] | TRAIN(023): [500/879] Batch: 0.1935 (0.1941) Data: 0.0096 (0.0150) Loss: 0.4109 (0.6148)
[2022/12/29 00:24] | TRAIN(023): [550/879] Batch: 0.1931 (0.1941) Data: 0.0092 (0.0147) Loss: 0.5146 (0.6204)
[2022/12/29 00:24] | TRAIN(023): [600/879] Batch: 0.2015 (0.1939) Data: 0.0112 (0.0144) Loss: 0.4767 (0.6238)
[2022/12/29 00:24] | TRAIN(023): [650/879] Batch: 0.2161 (0.1940) Data: 0.0098 (0.0142) Loss: 0.4876 (0.6227)
[2022/12/29 00:24] | TRAIN(023): [700/879] Batch: 0.1255 (0.1941) Data: 0.0092 (0.0140) Loss: 0.5508 (0.6222)
[2022/12/29 00:24] | TRAIN(023): [750/879] Batch: 0.1955 (0.1932) Data: 0.0095 (0.0138) Loss: 0.5949 (0.6250)
[2022/12/29 00:25] | TRAIN(023): [800/879] Batch: 0.1997 (0.1924) Data: 0.0108 (0.0137) Loss: 0.5399 (0.6250)
[2022/12/29 00:25] | TRAIN(023): [850/879] Batch: 0.2009 (0.1924) Data: 0.0104 (0.0135) Loss: 0.5705 (0.6235)
[2022/12/29 00:25] | ------------------------------------------------------------
[2022/12/29 00:25] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:25] | ------------------------------------------------------------
[2022/12/29 00:25] |    TRAIN(23)     0:02:49     0:00:11     0:02:37      0.6228
[2022/12/29 00:25] | ------------------------------------------------------------
[2022/12/29 00:25] | VALID(023): [ 50/220] Batch: 0.0406 (0.0804) Data: 0.0434 (0.0573) Loss: 0.5254 (0.5750)
[2022/12/29 00:25] | VALID(023): [100/220] Batch: 0.0480 (0.0669) Data: 0.0348 (0.0445) Loss: 0.8391 (0.6038)
[2022/12/29 00:25] | VALID(023): [150/220] Batch: 0.0402 (0.0624) Data: 0.0390 (0.0403) Loss: 0.5635 (0.6024)
[2022/12/29 00:25] | VALID(023): [200/220] Batch: 0.0584 (0.0604) Data: 0.0244 (0.0383) Loss: 0.3106 (0.5994)
[2022/12/29 00:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:25] |    VALID(23)      0.5973      0.8015      0.8558      0.8015      0.8015      0.8015      0.9504
[2022/12/29 00:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:25] | ####################################################################################################
[2022/12/29 00:25] | TRAIN(024): [ 50/879] Batch: 0.1885 (0.2287) Data: 0.0107 (0.0483) Loss: 0.5378 (0.6122)
[2022/12/29 00:25] | TRAIN(024): [100/879] Batch: 0.1909 (0.2087) Data: 0.0089 (0.0295) Loss: 0.4119 (0.6142)
[2022/12/29 00:26] | TRAIN(024): [150/879] Batch: 0.1810 (0.2034) Data: 0.0107 (0.0233) Loss: 0.8134 (0.5964)
[2022/12/29 00:26] | TRAIN(024): [200/879] Batch: 0.1812 (0.1996) Data: 0.0106 (0.0201) Loss: 0.9600 (0.6047)
[2022/12/29 00:26] | TRAIN(024): [250/879] Batch: 0.1931 (0.1973) Data: 0.0086 (0.0183) Loss: 0.7652 (0.6070)
[2022/12/29 00:26] | TRAIN(024): [300/879] Batch: 0.1913 (0.1957) Data: 0.0091 (0.0170) Loss: 0.6020 (0.6097)
[2022/12/29 00:26] | TRAIN(024): [350/879] Batch: 0.2130 (0.1942) Data: 0.0094 (0.0160) Loss: 0.6012 (0.6110)
[2022/12/29 00:26] | TRAIN(024): [400/879] Batch: 0.1852 (0.1935) Data: 0.0112 (0.0153) Loss: 0.8250 (0.6120)
[2022/12/29 00:26] | TRAIN(024): [450/879] Batch: 0.2029 (0.1923) Data: 0.0116 (0.0149) Loss: 0.5846 (0.6147)
[2022/12/29 00:27] | TRAIN(024): [500/879] Batch: 0.1836 (0.1904) Data: 0.0126 (0.0144) Loss: 0.4362 (0.6179)
[2022/12/29 00:27] | TRAIN(024): [550/879] Batch: 0.1988 (0.1903) Data: 0.0105 (0.0140) Loss: 0.5946 (0.6163)
[2022/12/29 00:27] | TRAIN(024): [600/879] Batch: 0.1919 (0.1904) Data: 0.0114 (0.0137) Loss: 0.6446 (0.6162)
[2022/12/29 00:27] | TRAIN(024): [650/879] Batch: 0.1934 (0.1905) Data: 0.0159 (0.0134) Loss: 0.9964 (0.6198)
[2022/12/29 00:27] | TRAIN(024): [700/879] Batch: 0.1821 (0.1907) Data: 0.0111 (0.0132) Loss: 0.5812 (0.6195)
[2022/12/29 00:27] | TRAIN(024): [750/879] Batch: 0.1949 (0.1910) Data: 0.0112 (0.0131) Loss: 0.6009 (0.6187)
[2022/12/29 00:28] | TRAIN(024): [800/879] Batch: 0.1944 (0.1910) Data: 0.0106 (0.0129) Loss: 0.4079 (0.6203)
[2022/12/29 00:28] | TRAIN(024): [850/879] Batch: 0.1976 (0.1909) Data: 0.0100 (0.0128) Loss: 0.6005 (0.6214)
[2022/12/29 00:28] | ------------------------------------------------------------
[2022/12/29 00:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:28] | ------------------------------------------------------------
[2022/12/29 00:28] |    TRAIN(24)     0:02:47     0:00:11     0:02:36      0.6206
[2022/12/29 00:28] | ------------------------------------------------------------
[2022/12/29 00:28] | VALID(024): [ 50/220] Batch: 0.0581 (0.0835) Data: 0.0391 (0.0613) Loss: 0.6067 (0.5866)
[2022/12/29 00:28] | VALID(024): [100/220] Batch: 0.0460 (0.0703) Data: 0.0384 (0.0483) Loss: 0.7345 (0.5975)
[2022/12/29 00:28] | VALID(024): [150/220] Batch: 0.0451 (0.0661) Data: 0.0379 (0.0441) Loss: 0.4614 (0.5926)
[2022/12/29 00:28] | VALID(024): [200/220] Batch: 0.0475 (0.0637) Data: 0.0375 (0.0416) Loss: 0.3105 (0.5938)
[2022/12/29 00:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:28] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:28] |    VALID(24)      0.5932      0.8056      0.8580      0.8056      0.8056      0.8056      0.9514
[2022/12/29 00:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:28] | ####################################################################################################
[2022/12/29 00:28] | TRAIN(025): [ 50/879] Batch: 0.1929 (0.2295) Data: 0.0090 (0.0474) Loss: 0.5055 (0.5926)
[2022/12/29 00:28] | TRAIN(025): [100/879] Batch: 0.1963 (0.2095) Data: 0.0105 (0.0292) Loss: 0.6414 (0.6191)
[2022/12/29 00:29] | TRAIN(025): [150/879] Batch: 0.1969 (0.1998) Data: 0.0114 (0.0230) Loss: 0.3308 (0.6309)
[2022/12/29 00:29] | TRAIN(025): [200/879] Batch: 0.1834 (0.1937) Data: 0.0100 (0.0198) Loss: 0.4465 (0.6322)
[2022/12/29 00:29] | TRAIN(025): [250/879] Batch: 0.1883 (0.1933) Data: 0.0091 (0.0179) Loss: 0.5964 (0.6337)
[2022/12/29 00:29] | TRAIN(025): [300/879] Batch: 0.2016 (0.1933) Data: 0.0096 (0.0167) Loss: 0.9296 (0.6330)
[2022/12/29 00:29] | TRAIN(025): [350/879] Batch: 0.1925 (0.1933) Data: 0.0100 (0.0158) Loss: 0.7678 (0.6273)
[2022/12/29 00:29] | TRAIN(025): [400/879] Batch: 0.1959 (0.1927) Data: 0.0090 (0.0151) Loss: 0.8316 (0.6248)
[2022/12/29 00:29] | TRAIN(025): [450/879] Batch: 0.1817 (0.1923) Data: 0.0091 (0.0147) Loss: 0.6279 (0.6276)
[2022/12/29 00:30] | TRAIN(025): [500/879] Batch: 0.1940 (0.1926) Data: 0.0089 (0.0143) Loss: 0.5891 (0.6261)
[2022/12/29 00:30] | TRAIN(025): [550/879] Batch: 0.1872 (0.1926) Data: 0.0093 (0.0139) Loss: 0.5239 (0.6210)
[2022/12/29 00:30] | TRAIN(025): [600/879] Batch: 0.1938 (0.1924) Data: 0.0091 (0.0137) Loss: 0.7356 (0.6213)
[2022/12/29 00:30] | TRAIN(025): [650/879] Batch: 0.1933 (0.1923) Data: 0.0089 (0.0134) Loss: 0.4101 (0.6230)
[2022/12/29 00:30] | TRAIN(025): [700/879] Batch: 0.1945 (0.1923) Data: 0.0109 (0.0132) Loss: 0.5714 (0.6222)
[2022/12/29 00:30] | TRAIN(025): [750/879] Batch: 0.1954 (0.1924) Data: 0.0091 (0.0131) Loss: 0.6320 (0.6202)
[2022/12/29 00:31] | TRAIN(025): [800/879] Batch: 0.1339 (0.1921) Data: 0.0164 (0.0129) Loss: 0.6203 (0.6194)
[2022/12/29 00:31] | TRAIN(025): [850/879] Batch: 0.1959 (0.1917) Data: 0.0108 (0.0128) Loss: 0.6676 (0.6184)
[2022/12/29 00:31] | ------------------------------------------------------------
[2022/12/29 00:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:31] | ------------------------------------------------------------
[2022/12/29 00:31] |    TRAIN(25)     0:02:48     0:00:11     0:02:36      0.6187
[2022/12/29 00:31] | ------------------------------------------------------------
[2022/12/29 00:31] | VALID(025): [ 50/220] Batch: 0.0602 (0.0817) Data: 0.0362 (0.0571) Loss: 0.5186 (0.5756)
[2022/12/29 00:31] | VALID(025): [100/220] Batch: 0.0756 (0.0681) Data: 0.0193 (0.0448) Loss: 0.9609 (0.6095)
[2022/12/29 00:31] | VALID(025): [150/220] Batch: 0.0575 (0.0635) Data: 0.0168 (0.0376) Loss: 0.5588 (0.6032)
[2022/12/29 00:31] | VALID(025): [200/220] Batch: 0.0583 (0.0613) Data: 0.0315 (0.0341) Loss: 0.3197 (0.6052)
[2022/12/29 00:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:31] |    VALID(25)      0.6020      0.8073      0.8614      0.8073      0.8073      0.8073      0.9518
[2022/12/29 00:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:31] | ####################################################################################################
[2022/12/29 00:31] | TRAIN(026): [ 50/879] Batch: 0.1920 (0.2252) Data: 0.0128 (0.0457) Loss: 0.8277 (0.6191)
[2022/12/29 00:31] | TRAIN(026): [100/879] Batch: 0.1887 (0.2073) Data: 0.0086 (0.0282) Loss: 0.4518 (0.6139)
[2022/12/29 00:32] | TRAIN(026): [150/879] Batch: 0.1900 (0.2016) Data: 0.0109 (0.0222) Loss: 0.7539 (0.6007)
[2022/12/29 00:32] | TRAIN(026): [200/879] Batch: 0.1886 (0.1983) Data: 0.0096 (0.0193) Loss: 0.5612 (0.6050)
[2022/12/29 00:32] | TRAIN(026): [250/879] Batch: 0.1857 (0.1966) Data: 0.0095 (0.0175) Loss: 0.5449 (0.6098)
[2022/12/29 00:32] | TRAIN(026): [300/879] Batch: 0.1970 (0.1957) Data: 0.0098 (0.0163) Loss: 0.5871 (0.6148)
[2022/12/29 00:32] | TRAIN(026): [350/879] Batch: 0.1726 (0.1949) Data: 0.0100 (0.0154) Loss: 0.4470 (0.6143)
[2022/12/29 00:32] | TRAIN(026): [400/879] Batch: 0.1856 (0.1948) Data: 0.0106 (0.0148) Loss: 0.5836 (0.6191)
[2022/12/29 00:33] | TRAIN(026): [450/879] Batch: 0.1911 (0.1945) Data: 0.0094 (0.0143) Loss: 0.6433 (0.6240)
[2022/12/29 00:33] | TRAIN(026): [500/879] Batch: 0.1837 (0.1942) Data: 0.0105 (0.0140) Loss: 0.4354 (0.6220)
[2022/12/29 00:33] | TRAIN(026): [550/879] Batch: 0.1950 (0.1927) Data: 0.0092 (0.0136) Loss: 0.4103 (0.6201)
[2022/12/29 00:33] | TRAIN(026): [600/879] Batch: 0.1816 (0.1916) Data: 0.0107 (0.0134) Loss: 0.4052 (0.6216)
[2022/12/29 00:33] | TRAIN(026): [650/879] Batch: 0.1878 (0.1914) Data: 0.0110 (0.0132) Loss: 0.6023 (0.6209)
[2022/12/29 00:33] | TRAIN(026): [700/879] Batch: 0.1837 (0.1913) Data: 0.0094 (0.0130) Loss: 0.7233 (0.6183)
[2022/12/29 00:33] | TRAIN(026): [750/879] Batch: 0.1812 (0.1911) Data: 0.0108 (0.0128) Loss: 0.4185 (0.6163)
[2022/12/29 00:34] | TRAIN(026): [800/879] Batch: 0.2014 (0.1910) Data: 0.0108 (0.0127) Loss: 0.4265 (0.6170)
[2022/12/29 00:34] | TRAIN(026): [850/879] Batch: 0.2131 (0.1908) Data: 0.0099 (0.0126) Loss: 0.3162 (0.6167)
[2022/12/29 00:34] | ------------------------------------------------------------
[2022/12/29 00:34] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:34] | ------------------------------------------------------------
[2022/12/29 00:34] |    TRAIN(26)     0:02:47     0:00:10     0:02:36      0.6157
[2022/12/29 00:34] | ------------------------------------------------------------
[2022/12/29 00:34] | VALID(026): [ 50/220] Batch: 0.0587 (0.0833) Data: 0.0206 (0.0612) Loss: 0.5341 (0.5748)
[2022/12/29 00:34] | VALID(026): [100/220] Batch: 0.0490 (0.0690) Data: 0.0389 (0.0474) Loss: 0.8926 (0.5915)
[2022/12/29 00:34] | VALID(026): [150/220] Batch: 0.0578 (0.0648) Data: 0.0338 (0.0433) Loss: 0.5003 (0.5862)
[2022/12/29 00:34] | VALID(026): [200/220] Batch: 0.0706 (0.0628) Data: 0.0239 (0.0409) Loss: 0.2792 (0.5846)
[2022/12/29 00:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:34] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:34] |    VALID(26)      0.5840      0.8104      0.8642      0.8104      0.8104      0.8104      0.9526
[2022/12/29 00:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:34] | ####################################################################################################
[2022/12/29 00:34] | TRAIN(027): [ 50/879] Batch: 0.1884 (0.2312) Data: 0.0095 (0.0472) Loss: 0.3790 (0.5695)
[2022/12/29 00:34] | TRAIN(027): [100/879] Batch: 0.1864 (0.2112) Data: 0.0098 (0.0291) Loss: 0.6581 (0.5911)
[2022/12/29 00:35] | TRAIN(027): [150/879] Batch: 0.1875 (0.2032) Data: 0.0111 (0.0230) Loss: 0.8362 (0.5876)
[2022/12/29 00:35] | TRAIN(027): [200/879] Batch: 0.1949 (0.1996) Data: 0.0099 (0.0199) Loss: 1.0591 (0.5912)
[2022/12/29 00:35] | TRAIN(027): [250/879] Batch: 0.1938 (0.1952) Data: 0.0157 (0.0181) Loss: 0.7724 (0.5886)
[2022/12/29 00:35] | TRAIN(027): [300/879] Batch: 0.1873 (0.1931) Data: 0.0112 (0.0169) Loss: 0.8893 (0.5939)
[2022/12/29 00:35] | TRAIN(027): [350/879] Batch: 0.1881 (0.1925) Data: 0.0099 (0.0160) Loss: 0.3909 (0.5961)
[2022/12/29 00:35] | TRAIN(027): [400/879] Batch: 0.1847 (0.1919) Data: 0.0090 (0.0153) Loss: 1.0046 (0.6007)
[2022/12/29 00:36] | TRAIN(027): [450/879] Batch: 0.1986 (0.1912) Data: 0.0096 (0.0147) Loss: 0.6753 (0.6026)
[2022/12/29 00:36] | TRAIN(027): [500/879] Batch: 0.1872 (0.1913) Data: 0.0094 (0.0143) Loss: 0.5736 (0.6055)
[2022/12/29 00:36] | TRAIN(027): [550/879] Batch: 0.1941 (0.1914) Data: 0.0091 (0.0139) Loss: 0.6801 (0.6061)
[2022/12/29 00:36] | TRAIN(027): [600/879] Batch: 0.1964 (0.1915) Data: 0.0092 (0.0136) Loss: 0.7687 (0.6080)
[2022/12/29 00:36] | TRAIN(027): [650/879] Batch: 0.1931 (0.1916) Data: 0.0108 (0.0134) Loss: 0.8178 (0.6058)
[2022/12/29 00:36] | TRAIN(027): [700/879] Batch: 0.1891 (0.1917) Data: 0.0110 (0.0132) Loss: 0.7008 (0.6073)
[2022/12/29 00:36] | TRAIN(027): [750/879] Batch: 0.1922 (0.1916) Data: 0.0094 (0.0130) Loss: 0.5123 (0.6083)
[2022/12/29 00:37] | TRAIN(027): [800/879] Batch: 0.1844 (0.1911) Data: 0.0084 (0.0128) Loss: 0.3917 (0.6099)
[2022/12/29 00:37] | TRAIN(027): [850/879] Batch: 0.1870 (0.1911) Data: 0.0119 (0.0127) Loss: 0.5691 (0.6087)
[2022/12/29 00:37] | ------------------------------------------------------------
[2022/12/29 00:37] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:37] | ------------------------------------------------------------
[2022/12/29 00:37] |    TRAIN(27)     0:02:48     0:00:11     0:02:36      0.6099
[2022/12/29 00:37] | ------------------------------------------------------------
[2022/12/29 00:37] | VALID(027): [ 50/220] Batch: 0.0420 (0.0791) Data: 0.0111 (0.0523) Loss: 0.4934 (0.5458)
[2022/12/29 00:37] | VALID(027): [100/220] Batch: 0.0529 (0.0626) Data: 0.0266 (0.0345) Loss: 0.9033 (0.5704)
[2022/12/29 00:37] | VALID(027): [150/220] Batch: 0.0532 (0.0604) Data: 0.0355 (0.0319) Loss: 0.5043 (0.5668)
[2022/12/29 00:37] | VALID(027): [200/220] Batch: 0.0570 (0.0592) Data: 0.0373 (0.0324) Loss: 0.3023 (0.5695)
[2022/12/29 00:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:37] |    VALID(27)      0.5681      0.8113      0.8656      0.8113      0.8113      0.8113      0.9528
[2022/12/29 00:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:37] | ####################################################################################################
[2022/12/29 00:37] | TRAIN(028): [ 50/879] Batch: 0.1910 (0.2288) Data: 0.0106 (0.0508) Loss: 0.3402 (0.5484)
[2022/12/29 00:37] | TRAIN(028): [100/879] Batch: 0.1775 (0.2087) Data: 0.0095 (0.0306) Loss: 0.7298 (0.5777)
[2022/12/29 00:38] | TRAIN(028): [150/879] Batch: 0.1805 (0.2028) Data: 0.0104 (0.0239) Loss: 0.7136 (0.5875)
[2022/12/29 00:38] | TRAIN(028): [200/879] Batch: 0.1907 (0.2005) Data: 0.0111 (0.0205) Loss: 0.5621 (0.6045)
[2022/12/29 00:38] | TRAIN(028): [250/879] Batch: 0.1961 (0.1988) Data: 0.0095 (0.0184) Loss: 0.4361 (0.6028)
[2022/12/29 00:38] | TRAIN(028): [300/879] Batch: 0.1860 (0.1981) Data: 0.0105 (0.0172) Loss: 0.7540 (0.6035)
[2022/12/29 00:38] | TRAIN(028): [350/879] Batch: 0.1996 (0.1970) Data: 0.0120 (0.0163) Loss: 0.4333 (0.6093)
[2022/12/29 00:38] | TRAIN(028): [400/879] Batch: 0.1874 (0.1964) Data: 0.0097 (0.0155) Loss: 0.6709 (0.6096)
[2022/12/29 00:39] | TRAIN(028): [450/879] Batch: 0.1900 (0.1962) Data: 0.0108 (0.0150) Loss: 0.5925 (0.6118)
[2022/12/29 00:39] | TRAIN(028): [500/879] Batch: 0.1861 (0.1957) Data: 0.0094 (0.0146) Loss: 0.6566 (0.6095)
[2022/12/29 00:39] | TRAIN(028): [550/879] Batch: 0.1914 (0.1955) Data: 0.0103 (0.0142) Loss: 0.6478 (0.6039)
[2022/12/29 00:39] | TRAIN(028): [600/879] Batch: 0.1963 (0.1953) Data: 0.0088 (0.0139) Loss: 0.7368 (0.6034)
[2022/12/29 00:39] | TRAIN(028): [650/879] Batch: 0.1864 (0.1939) Data: 0.0106 (0.0137) Loss: 0.6744 (0.6019)
[2022/12/29 00:39] | TRAIN(028): [700/879] Batch: 0.1794 (0.1921) Data: 0.0110 (0.0134) Loss: 0.5386 (0.6039)
[2022/12/29 00:39] | TRAIN(028): [750/879] Batch: 0.1992 (0.1920) Data: 0.0134 (0.0132) Loss: 0.5645 (0.6059)
[2022/12/29 00:40] | TRAIN(028): [800/879] Batch: 0.2006 (0.1919) Data: 0.0125 (0.0130) Loss: 0.6948 (0.6050)
[2022/12/29 00:40] | TRAIN(028): [850/879] Batch: 0.1729 (0.1917) Data: 0.0123 (0.0129) Loss: 0.3395 (0.6061)
[2022/12/29 00:40] | ------------------------------------------------------------
[2022/12/29 00:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:40] | ------------------------------------------------------------
[2022/12/29 00:40] |    TRAIN(28)     0:02:48     0:00:11     0:02:37      0.6065
[2022/12/29 00:40] | ------------------------------------------------------------
[2022/12/29 00:40] | VALID(028): [ 50/220] Batch: 0.0580 (0.0841) Data: 0.0246 (0.0599) Loss: 0.4651 (0.5637)
[2022/12/29 00:40] | VALID(028): [100/220] Batch: 0.0543 (0.0698) Data: 0.0349 (0.0471) Loss: 0.9023 (0.5857)
[2022/12/29 00:40] | VALID(028): [150/220] Batch: 0.0541 (0.0651) Data: 0.0394 (0.0427) Loss: 0.5439 (0.5832)
[2022/12/29 00:40] | VALID(028): [200/220] Batch: 0.0608 (0.0629) Data: 0.0407 (0.0407) Loss: 0.2893 (0.5859)
[2022/12/29 00:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:40] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:40] |    VALID(28)      0.5844      0.8090      0.8575      0.8090      0.8090      0.8090      0.9522
[2022/12/29 00:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:40] | ####################################################################################################
[2022/12/29 00:40] | TRAIN(029): [ 50/879] Batch: 0.2007 (0.2298) Data: 0.0092 (0.0482) Loss: 0.7217 (0.5773)
[2022/12/29 00:40] | TRAIN(029): [100/879] Batch: 0.1855 (0.2106) Data: 0.0090 (0.0298) Loss: 0.4105 (0.5995)
[2022/12/29 00:41] | TRAIN(029): [150/879] Batch: 0.1923 (0.2044) Data: 0.0113 (0.0234) Loss: 0.4960 (0.6047)
[2022/12/29 00:41] | TRAIN(029): [200/879] Batch: 0.1900 (0.2000) Data: 0.0110 (0.0202) Loss: 0.6883 (0.6019)
[2022/12/29 00:41] | TRAIN(029): [250/879] Batch: 0.1930 (0.1985) Data: 0.0110 (0.0182) Loss: 0.6821 (0.6061)
[2022/12/29 00:41] | TRAIN(029): [300/879] Batch: 0.1927 (0.1965) Data: 0.0099 (0.0169) Loss: 0.5504 (0.6049)
[2022/12/29 00:41] | TRAIN(029): [350/879] Batch: 0.1882 (0.1934) Data: 0.0109 (0.0160) Loss: 0.4993 (0.6091)
[2022/12/29 00:41] | TRAIN(029): [400/879] Batch: 0.2003 (0.1911) Data: 0.0107 (0.0153) Loss: 0.8810 (0.6109)
[2022/12/29 00:42] | TRAIN(029): [450/879] Batch: 0.1976 (0.1910) Data: 0.0106 (0.0147) Loss: 0.3010 (0.6101)
[2022/12/29 00:42] | TRAIN(029): [500/879] Batch: 0.1867 (0.1911) Data: 0.0111 (0.0143) Loss: 0.8715 (0.6083)
[2022/12/29 00:42] | TRAIN(029): [550/879] Batch: 0.1907 (0.1913) Data: 0.0110 (0.0140) Loss: 0.5946 (0.6084)
[2022/12/29 00:42] | TRAIN(029): [600/879] Batch: 0.1722 (0.1913) Data: 0.0102 (0.0137) Loss: 0.2866 (0.6050)
[2022/12/29 00:42] | TRAIN(029): [650/879] Batch: 0.1806 (0.1913) Data: 0.0119 (0.0135) Loss: 0.5800 (0.6020)
[2022/12/29 00:42] | TRAIN(029): [700/879] Batch: 0.1907 (0.1913) Data: 0.0129 (0.0133) Loss: 0.4556 (0.6025)
[2022/12/29 00:43] | TRAIN(029): [750/879] Batch: 0.1775 (0.1912) Data: 0.0107 (0.0131) Loss: 0.6549 (0.6015)
[2022/12/29 00:43] | TRAIN(029): [800/879] Batch: 0.1829 (0.1912) Data: 0.0100 (0.0129) Loss: 0.4799 (0.6008)
[2022/12/29 00:43] | TRAIN(029): [850/879] Batch: 0.1993 (0.1913) Data: 0.0102 (0.0127) Loss: 0.9039 (0.6031)
[2022/12/29 00:43] | ------------------------------------------------------------
[2022/12/29 00:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:43] | ------------------------------------------------------------
[2022/12/29 00:43] |    TRAIN(29)     0:02:48     0:00:11     0:02:36      0.6036
[2022/12/29 00:43] | ------------------------------------------------------------
[2022/12/29 00:43] | VALID(029): [ 50/220] Batch: 0.0587 (0.0830) Data: 0.0286 (0.0601) Loss: 0.5402 (0.5479)
[2022/12/29 00:43] | VALID(029): [100/220] Batch: 0.0594 (0.0702) Data: 0.0334 (0.0482) Loss: 0.8554 (0.5688)
[2022/12/29 00:43] | VALID(029): [150/220] Batch: 0.0585 (0.0659) Data: 0.0319 (0.0442) Loss: 0.4553 (0.5689)
[2022/12/29 00:43] | VALID(029): [200/220] Batch: 0.0591 (0.0638) Data: 0.0284 (0.0405) Loss: 0.2678 (0.5697)
[2022/12/29 00:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:43] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:43] |    VALID(29)      0.5679      0.8134      0.8695      0.8134      0.8134      0.8134      0.9534
[2022/12/29 00:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:43] | ####################################################################################################
[2022/12/29 00:43] | TRAIN(030): [ 50/879] Batch: 0.1940 (0.2202) Data: 0.0164 (0.0465) Loss: 0.5842 (0.5806)
[2022/12/29 00:44] | TRAIN(030): [100/879] Batch: 0.1967 (0.1996) Data: 0.0089 (0.0293) Loss: 0.4522 (0.5936)
[2022/12/29 00:44] | TRAIN(030): [150/879] Batch: 0.1890 (0.1955) Data: 0.0109 (0.0235) Loss: 0.2941 (0.5927)
[2022/12/29 00:44] | TRAIN(030): [200/879] Batch: 0.1948 (0.1941) Data: 0.0094 (0.0205) Loss: 0.6263 (0.5974)
[2022/12/29 00:44] | TRAIN(030): [250/879] Batch: 0.1968 (0.1935) Data: 0.0115 (0.0185) Loss: 0.5668 (0.5956)
[2022/12/29 00:44] | TRAIN(030): [300/879] Batch: 0.1672 (0.1936) Data: 0.0116 (0.0173) Loss: 0.5636 (0.5939)
[2022/12/29 00:44] | TRAIN(030): [350/879] Batch: 0.1897 (0.1930) Data: 0.0112 (0.0164) Loss: 0.3483 (0.5942)
[2022/12/29 00:44] | TRAIN(030): [400/879] Batch: 0.1905 (0.1924) Data: 0.0099 (0.0157) Loss: 0.6642 (0.5951)
[2022/12/29 00:45] | TRAIN(030): [450/879] Batch: 0.1933 (0.1920) Data: 0.0086 (0.0151) Loss: 0.6664 (0.5970)
[2022/12/29 00:45] | TRAIN(030): [500/879] Batch: 0.1894 (0.1919) Data: 0.0093 (0.0146) Loss: 0.4960 (0.5964)
[2022/12/29 00:45] | TRAIN(030): [550/879] Batch: 0.1984 (0.1921) Data: 0.0117 (0.0142) Loss: 0.6343 (0.5990)
[2022/12/29 00:45] | TRAIN(030): [600/879] Batch: 0.2007 (0.1921) Data: 0.0111 (0.0139) Loss: 0.5140 (0.5973)
[2022/12/29 00:45] | TRAIN(030): [650/879] Batch: 0.1898 (0.1920) Data: 0.0094 (0.0137) Loss: 0.3166 (0.5980)
[2022/12/29 00:45] | TRAIN(030): [700/879] Batch: 0.1343 (0.1919) Data: 0.0095 (0.0134) Loss: 0.6076 (0.5962)
[2022/12/29 00:46] | TRAIN(030): [750/879] Batch: 0.1930 (0.1914) Data: 0.0093 (0.0133) Loss: 0.4782 (0.5974)
[2022/12/29 00:46] | TRAIN(030): [800/879] Batch: 0.1892 (0.1901) Data: 0.0088 (0.0131) Loss: 0.4916 (0.5979)
[2022/12/29 00:46] | TRAIN(030): [850/879] Batch: 0.1767 (0.1899) Data: 0.0092 (0.0129) Loss: 0.5081 (0.6007)
[2022/12/29 00:46] | ------------------------------------------------------------
[2022/12/29 00:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:46] | ------------------------------------------------------------
[2022/12/29 00:46] |    TRAIN(30)     0:02:46     0:00:11     0:02:35      0.6014
[2022/12/29 00:46] | ------------------------------------------------------------
[2022/12/29 00:46] | VALID(030): [ 50/220] Batch: 0.0563 (0.0815) Data: 0.0370 (0.0547) Loss: 0.5252 (0.5641)
[2022/12/29 00:46] | VALID(030): [100/220] Batch: 0.0563 (0.0680) Data: 0.0207 (0.0434) Loss: 0.8990 (0.5797)
[2022/12/29 00:46] | VALID(030): [150/220] Batch: 0.0459 (0.0635) Data: 0.0350 (0.0399) Loss: 0.5564 (0.5773)
[2022/12/29 00:46] | VALID(030): [200/220] Batch: 0.0590 (0.0615) Data: 0.0376 (0.0385) Loss: 0.3163 (0.5787)
[2022/12/29 00:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:46] |    VALID(30)      0.5800      0.8070      0.8623      0.8070      0.8070      0.8070      0.9518
[2022/12/29 00:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:46] | ####################################################################################################
[2022/12/29 00:46] | TRAIN(031): [ 50/879] Batch: 0.1911 (0.2286) Data: 0.0121 (0.0487) Loss: 0.3980 (0.5634)
[2022/12/29 00:47] | TRAIN(031): [100/879] Batch: 0.1916 (0.2090) Data: 0.0097 (0.0298) Loss: 0.5946 (0.5672)
[2022/12/29 00:47] | TRAIN(031): [150/879] Batch: 0.1785 (0.2028) Data: 0.0106 (0.0234) Loss: 0.2380 (0.5777)
[2022/12/29 00:47] | TRAIN(031): [200/879] Batch: 0.1822 (0.1991) Data: 0.0109 (0.0201) Loss: 0.7023 (0.5768)
[2022/12/29 00:47] | TRAIN(031): [250/879] Batch: 0.1996 (0.1967) Data: 0.0090 (0.0181) Loss: 0.4196 (0.5725)
[2022/12/29 00:47] | TRAIN(031): [300/879] Batch: 0.1924 (0.1953) Data: 0.0098 (0.0168) Loss: 0.5634 (0.5752)
[2022/12/29 00:47] | TRAIN(031): [350/879] Batch: 0.1862 (0.1947) Data: 0.0103 (0.0159) Loss: 0.4419 (0.5783)
[2022/12/29 00:47] | TRAIN(031): [400/879] Batch: 0.1848 (0.1937) Data: 0.0108 (0.0152) Loss: 0.6021 (0.5832)
[2022/12/29 00:48] | TRAIN(031): [450/879] Batch: 0.1835 (0.1922) Data: 0.0087 (0.0146) Loss: 0.7188 (0.5864)
[2022/12/29 00:48] | TRAIN(031): [500/879] Batch: 0.1986 (0.1901) Data: 0.0102 (0.0142) Loss: 0.5681 (0.5918)
[2022/12/29 00:48] | TRAIN(031): [550/879] Batch: 0.1913 (0.1906) Data: 0.0111 (0.0139) Loss: 0.5166 (0.5930)
[2022/12/29 00:48] | TRAIN(031): [600/879] Batch: 0.1892 (0.1905) Data: 0.0098 (0.0136) Loss: 0.8258 (0.5973)
[2022/12/29 00:48] | TRAIN(031): [650/879] Batch: 0.1990 (0.1904) Data: 0.0108 (0.0134) Loss: 0.5552 (0.5986)
[2022/12/29 00:48] | TRAIN(031): [700/879] Batch: 0.1758 (0.1898) Data: 0.0090 (0.0131) Loss: 0.5008 (0.6000)
[2022/12/29 00:49] | TRAIN(031): [750/879] Batch: 0.1906 (0.1897) Data: 0.0089 (0.0129) Loss: 0.5052 (0.5998)
[2022/12/29 00:49] | TRAIN(031): [800/879] Batch: 0.1892 (0.1898) Data: 0.0109 (0.0127) Loss: 0.7520 (0.5995)
[2022/12/29 00:49] | TRAIN(031): [850/879] Batch: 0.1932 (0.1900) Data: 0.0087 (0.0126) Loss: 0.5235 (0.5970)
[2022/12/29 00:49] | ------------------------------------------------------------
[2022/12/29 00:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:49] | ------------------------------------------------------------
[2022/12/29 00:49] |    TRAIN(31)     0:02:46     0:00:10     0:02:35      0.5968
[2022/12/29 00:49] | ------------------------------------------------------------
[2022/12/29 00:49] | VALID(031): [ 50/220] Batch: 0.0505 (0.0865) Data: 0.0353 (0.0614) Loss: 0.5406 (0.5537)
[2022/12/29 00:49] | VALID(031): [100/220] Batch: 0.0569 (0.0712) Data: 0.0142 (0.0456) Loss: 0.8417 (0.5780)
[2022/12/29 00:49] | VALID(031): [150/220] Batch: 0.0418 (0.0651) Data: 0.0378 (0.0399) Loss: 0.4635 (0.5713)
[2022/12/29 00:49] | VALID(031): [200/220] Batch: 0.0636 (0.0622) Data: 0.0364 (0.0378) Loss: 0.2515 (0.5744)
[2022/12/29 00:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:49] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:49] |    VALID(31)      0.5748      0.8131      0.8654      0.8131      0.8131      0.8131      0.9533
[2022/12/29 00:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:49] | ####################################################################################################
[2022/12/29 00:49] | TRAIN(032): [ 50/879] Batch: 0.2252 (0.2304) Data: 0.0144 (0.0476) Loss: 0.5879 (0.5829)
[2022/12/29 00:50] | TRAIN(032): [100/879] Batch: 0.1963 (0.2108) Data: 0.0101 (0.0293) Loss: 0.6755 (0.5827)
[2022/12/29 00:50] | TRAIN(032): [150/879] Batch: 0.1880 (0.1995) Data: 0.0111 (0.0231) Loss: 0.5748 (0.6000)
[2022/12/29 00:50] | TRAIN(032): [200/879] Batch: 0.1668 (0.1949) Data: 0.0096 (0.0198) Loss: 0.6654 (0.5897)
[2022/12/29 00:50] | TRAIN(032): [250/879] Batch: 0.2008 (0.1923) Data: 0.0115 (0.0180) Loss: 0.7816 (0.5894)
[2022/12/29 00:50] | TRAIN(032): [300/879] Batch: 0.1962 (0.1921) Data: 0.0116 (0.0167) Loss: 0.6969 (0.5913)
[2022/12/29 00:50] | TRAIN(032): [350/879] Batch: 0.1853 (0.1916) Data: 0.0109 (0.0159) Loss: 0.7544 (0.5909)
[2022/12/29 00:50] | TRAIN(032): [400/879] Batch: 0.1937 (0.1913) Data: 0.0104 (0.0152) Loss: 0.7227 (0.5928)
[2022/12/29 00:51] | TRAIN(032): [450/879] Batch: 0.2157 (0.1912) Data: 0.0112 (0.0147) Loss: 0.5576 (0.5961)
[2022/12/29 00:51] | TRAIN(032): [500/879] Batch: 0.1833 (0.1909) Data: 0.0109 (0.0142) Loss: 0.6262 (0.5960)
[2022/12/29 00:51] | TRAIN(032): [550/879] Batch: 0.2028 (0.1907) Data: 0.0110 (0.0139) Loss: 0.4058 (0.5956)
[2022/12/29 00:51] | TRAIN(032): [600/879] Batch: 0.1841 (0.1905) Data: 0.0094 (0.0136) Loss: 0.3651 (0.5976)
[2022/12/29 00:51] | TRAIN(032): [650/879] Batch: 0.1892 (0.1906) Data: 0.0107 (0.0133) Loss: 0.9721 (0.5974)
[2022/12/29 00:51] | TRAIN(032): [700/879] Batch: 0.1869 (0.1902) Data: 0.0114 (0.0131) Loss: 0.5369 (0.5981)
[2022/12/29 00:52] | TRAIN(032): [750/879] Batch: 0.1951 (0.1902) Data: 0.0103 (0.0130) Loss: 0.5614 (0.5969)
[2022/12/29 00:52] | TRAIN(032): [800/879] Batch: 0.1866 (0.1903) Data: 0.0090 (0.0128) Loss: 0.9568 (0.5957)
[2022/12/29 00:52] | TRAIN(032): [850/879] Batch: 0.1928 (0.1892) Data: 0.0105 (0.0127) Loss: 0.5142 (0.5950)
[2022/12/29 00:52] | ------------------------------------------------------------
[2022/12/29 00:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:52] | ------------------------------------------------------------
[2022/12/29 00:52] |    TRAIN(32)     0:02:46     0:00:11     0:02:35      0.5956
[2022/12/29 00:52] | ------------------------------------------------------------
[2022/12/29 00:52] | VALID(032): [ 50/220] Batch: 0.0415 (0.0770) Data: 0.0291 (0.0486) Loss: 0.4436 (0.5543)
[2022/12/29 00:52] | VALID(032): [100/220] Batch: 0.0591 (0.0660) Data: 0.0336 (0.0407) Loss: 0.9257 (0.5829)
[2022/12/29 00:52] | VALID(032): [150/220] Batch: 0.0409 (0.0624) Data: 0.0373 (0.0381) Loss: 0.4944 (0.5765)
[2022/12/29 00:52] | VALID(032): [200/220] Batch: 0.0596 (0.0604) Data: 0.0248 (0.0366) Loss: 0.3397 (0.5828)
[2022/12/29 00:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:52] |    VALID(32)      0.5814      0.8093      0.8643      0.8093      0.8093      0.8093      0.9523
[2022/12/29 00:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:52] | ####################################################################################################
[2022/12/29 00:52] | TRAIN(033): [ 50/879] Batch: 0.1873 (0.2245) Data: 0.0107 (0.0463) Loss: 0.4638 (0.5859)
[2022/12/29 00:53] | TRAIN(033): [100/879] Batch: 0.1883 (0.2066) Data: 0.0115 (0.0286) Loss: 0.6246 (0.5764)
[2022/12/29 00:53] | TRAIN(033): [150/879] Batch: 0.1987 (0.2005) Data: 0.0108 (0.0226) Loss: 1.0035 (0.5846)
[2022/12/29 00:53] | TRAIN(033): [200/879] Batch: 0.1825 (0.1973) Data: 0.0091 (0.0195) Loss: 0.4884 (0.5788)
[2022/12/29 00:53] | TRAIN(033): [250/879] Batch: 0.1838 (0.1954) Data: 0.0113 (0.0178) Loss: 0.5608 (0.5782)
[2022/12/29 00:53] | TRAIN(033): [300/879] Batch: 0.1836 (0.1942) Data: 0.0109 (0.0165) Loss: 0.6434 (0.5794)
[2022/12/29 00:53] | TRAIN(033): [350/879] Batch: 0.1935 (0.1931) Data: 0.0115 (0.0157) Loss: 0.4628 (0.5779)
[2022/12/29 00:53] | TRAIN(033): [400/879] Batch: 0.1751 (0.1925) Data: 0.0102 (0.0150) Loss: 0.6644 (0.5831)
[2022/12/29 00:54] | TRAIN(033): [450/879] Batch: 0.1872 (0.1921) Data: 0.0109 (0.0145) Loss: 0.8369 (0.5874)
[2022/12/29 00:54] | TRAIN(033): [500/879] Batch: 0.1892 (0.1925) Data: 0.0108 (0.0141) Loss: 0.3061 (0.5867)
[2022/12/29 00:54] | TRAIN(033): [550/879] Batch: 0.1406 (0.1922) Data: 0.0092 (0.0138) Loss: 0.6075 (0.5869)
[2022/12/29 00:54] | TRAIN(033): [600/879] Batch: 0.1854 (0.1913) Data: 0.0107 (0.0135) Loss: 0.5767 (0.5878)
[2022/12/29 00:54] | TRAIN(033): [650/879] Batch: 0.1885 (0.1898) Data: 0.0088 (0.0133) Loss: 0.6407 (0.5914)
[2022/12/29 00:54] | TRAIN(033): [700/879] Batch: 0.1910 (0.1894) Data: 0.0102 (0.0130) Loss: 0.4459 (0.5907)
[2022/12/29 00:55] | TRAIN(033): [750/879] Batch: 0.1955 (0.1893) Data: 0.0087 (0.0129) Loss: 0.4763 (0.5927)
[2022/12/29 00:55] | TRAIN(033): [800/879] Batch: 0.1751 (0.1894) Data: 0.0094 (0.0127) Loss: 0.7101 (0.5913)
[2022/12/29 00:55] | TRAIN(033): [850/879] Batch: 0.2370 (0.1896) Data: 0.0149 (0.0126) Loss: 0.8752 (0.5918)
[2022/12/29 00:55] | ------------------------------------------------------------
[2022/12/29 00:55] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:55] | ------------------------------------------------------------
[2022/12/29 00:55] |    TRAIN(33)     0:02:46     0:00:11     0:02:35      0.5921
[2022/12/29 00:55] | ------------------------------------------------------------
[2022/12/29 00:55] | VALID(033): [ 50/220] Batch: 0.0592 (0.0823) Data: 0.0357 (0.0563) Loss: 0.4354 (0.5412)
[2022/12/29 00:55] | VALID(033): [100/220] Batch: 0.0611 (0.0694) Data: 0.0390 (0.0435) Loss: 0.9300 (0.5639)
[2022/12/29 00:55] | VALID(033): [150/220] Batch: 0.0628 (0.0653) Data: 0.0201 (0.0397) Loss: 0.4542 (0.5630)
[2022/12/29 00:55] | VALID(033): [200/220] Batch: 0.0566 (0.0623) Data: 0.0222 (0.0377) Loss: 0.3342 (0.5649)
[2022/12/29 00:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:55] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:55] |    VALID(33)      0.5625      0.8144      0.8723      0.8144      0.8144      0.8144      0.9536
[2022/12/29 00:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:55] | ####################################################################################################
[2022/12/29 00:55] | TRAIN(034): [ 50/879] Batch: 0.1790 (0.2287) Data: 0.0096 (0.0472) Loss: 0.4458 (0.5750)
[2022/12/29 00:56] | TRAIN(034): [100/879] Batch: 0.2007 (0.2102) Data: 0.0109 (0.0289) Loss: 0.8306 (0.5830)
[2022/12/29 00:56] | TRAIN(034): [150/879] Batch: 0.1907 (0.2043) Data: 0.0104 (0.0228) Loss: 0.7623 (0.5884)
[2022/12/29 00:56] | TRAIN(034): [200/879] Batch: 0.1916 (0.2014) Data: 0.0111 (0.0198) Loss: 0.5758 (0.5891)
[2022/12/29 00:56] | TRAIN(034): [250/879] Batch: 0.1916 (0.1997) Data: 0.0110 (0.0180) Loss: 0.5498 (0.5907)
[2022/12/29 00:56] | TRAIN(034): [300/879] Batch: 0.1891 (0.1961) Data: 0.0096 (0.0167) Loss: 0.7294 (0.5987)
[2022/12/29 00:56] | TRAIN(034): [350/879] Batch: 0.1952 (0.1942) Data: 0.0090 (0.0158) Loss: 0.5767 (0.5890)
[2022/12/29 00:56] | TRAIN(034): [400/879] Batch: 0.1783 (0.1938) Data: 0.0098 (0.0151) Loss: 0.9090 (0.5880)
[2022/12/29 00:57] | TRAIN(034): [450/879] Batch: 0.1814 (0.1931) Data: 0.0096 (0.0145) Loss: 0.4023 (0.5920)
[2022/12/29 00:57] | TRAIN(034): [500/879] Batch: 0.1972 (0.1934) Data: 0.0086 (0.0141) Loss: 0.6059 (0.5861)
[2022/12/29 00:57] | TRAIN(034): [550/879] Batch: 0.1949 (0.1933) Data: 0.0110 (0.0138) Loss: 0.2666 (0.5884)
[2022/12/29 00:57] | TRAIN(034): [600/879] Batch: 0.1874 (0.1933) Data: 0.0101 (0.0135) Loss: 0.6318 (0.5856)
[2022/12/29 00:57] | TRAIN(034): [650/879] Batch: 0.1899 (0.1933) Data: 0.0114 (0.0133) Loss: 0.3993 (0.5847)
[2022/12/29 00:57] | TRAIN(034): [700/879] Batch: 0.1847 (0.1931) Data: 0.0099 (0.0131) Loss: 0.6505 (0.5858)
[2022/12/29 00:58] | TRAIN(034): [750/879] Batch: 0.1913 (0.1932) Data: 0.0111 (0.0129) Loss: 0.5506 (0.5852)
[2022/12/29 00:58] | TRAIN(034): [800/879] Batch: 0.1932 (0.1930) Data: 0.0105 (0.0128) Loss: 0.5353 (0.5863)
[2022/12/29 00:58] | TRAIN(034): [850/879] Batch: 0.1888 (0.1927) Data: 0.0109 (0.0126) Loss: 0.7097 (0.5875)
[2022/12/29 00:58] | ------------------------------------------------------------
[2022/12/29 00:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:58] | ------------------------------------------------------------
[2022/12/29 00:58] |    TRAIN(34)     0:02:49     0:00:11     0:02:38      0.5876
[2022/12/29 00:58] | ------------------------------------------------------------
[2022/12/29 00:58] | VALID(034): [ 50/220] Batch: 0.0604 (0.0806) Data: 0.0373 (0.0552) Loss: 0.4413 (0.5456)
[2022/12/29 00:58] | VALID(034): [100/220] Batch: 0.0557 (0.0684) Data: 0.0221 (0.0443) Loss: 0.8715 (0.5738)
[2022/12/29 00:58] | VALID(034): [150/220] Batch: 0.0433 (0.0603) Data: 0.0092 (0.0355) Loss: 0.4350 (0.5695)
[2022/12/29 00:58] | VALID(034): [200/220] Batch: 0.0532 (0.0575) Data: 0.0350 (0.0333) Loss: 0.2397 (0.5683)
[2022/12/29 00:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:58] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:58] |    VALID(34)      0.5669      0.8150      0.8716      0.8150      0.8150      0.8150      0.9537
[2022/12/29 00:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:58] | ####################################################################################################
[2022/12/29 00:58] | TRAIN(035): [ 50/879] Batch: 0.1890 (0.2180) Data: 0.0087 (0.0482) Loss: 0.5616 (0.6269)
[2022/12/29 00:59] | TRAIN(035): [100/879] Batch: 0.1880 (0.2045) Data: 0.0121 (0.0298) Loss: 0.6536 (0.5959)
[2022/12/29 00:59] | TRAIN(035): [150/879] Batch: 0.1835 (0.1995) Data: 0.0107 (0.0235) Loss: 0.6543 (0.5872)
[2022/12/29 00:59] | TRAIN(035): [200/879] Batch: 0.1946 (0.1981) Data: 0.0113 (0.0202) Loss: 0.3425 (0.5894)
[2022/12/29 00:59] | TRAIN(035): [250/879] Batch: 0.1778 (0.1960) Data: 0.0099 (0.0183) Loss: 0.6124 (0.5909)
[2022/12/29 00:59] | TRAIN(035): [300/879] Batch: 0.1844 (0.1944) Data: 0.0108 (0.0169) Loss: 0.5974 (0.5921)
[2022/12/29 00:59] | TRAIN(035): [350/879] Batch: 0.1934 (0.1938) Data: 0.0109 (0.0159) Loss: 0.5500 (0.5914)
[2022/12/29 01:00] | TRAIN(035): [400/879] Batch: 0.1921 (0.1935) Data: 0.0088 (0.0152) Loss: 0.8259 (0.5919)
[2022/12/29 01:00] | TRAIN(035): [450/879] Batch: 0.1890 (0.1934) Data: 0.0100 (0.0147) Loss: 0.7580 (0.5902)
[2022/12/29 01:00] | TRAIN(035): [500/879] Batch: 0.1878 (0.1933) Data: 0.0114 (0.0143) Loss: 0.7972 (0.5905)
[2022/12/29 01:00] | TRAIN(035): [550/879] Batch: 0.1888 (0.1927) Data: 0.0095 (0.0139) Loss: 0.4873 (0.5897)
[2022/12/29 01:00] | TRAIN(035): [600/879] Batch: 0.1744 (0.1927) Data: 0.0103 (0.0136) Loss: 0.5665 (0.5866)
[2022/12/29 01:00] | TRAIN(035): [650/879] Batch: 0.2131 (0.1921) Data: 0.0122 (0.0134) Loss: 0.8958 (0.5885)
[2022/12/29 01:00] | TRAIN(035): [700/879] Batch: 0.1663 (0.1915) Data: 0.0094 (0.0132) Loss: 0.4524 (0.5898)
[2022/12/29 01:01] | TRAIN(035): [750/879] Batch: 0.1836 (0.1910) Data: 0.0106 (0.0130) Loss: 0.4940 (0.5891)
[2022/12/29 01:01] | TRAIN(035): [800/879] Batch: 0.1873 (0.1911) Data: 0.0091 (0.0128) Loss: 0.4962 (0.5879)
[2022/12/29 01:01] | TRAIN(035): [850/879] Batch: 0.2027 (0.1911) Data: 0.0116 (0.0127) Loss: 0.4078 (0.5883)
[2022/12/29 01:01] | ------------------------------------------------------------
[2022/12/29 01:01] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:01] | ------------------------------------------------------------
[2022/12/29 01:01] |    TRAIN(35)     0:02:47     0:00:11     0:02:36      0.5862
[2022/12/29 01:01] | ------------------------------------------------------------
[2022/12/29 01:01] | VALID(035): [ 50/220] Batch: 0.0604 (0.0825) Data: 0.0316 (0.0588) Loss: 0.4525 (0.5674)
[2022/12/29 01:01] | VALID(035): [100/220] Batch: 0.0592 (0.0688) Data: 0.0269 (0.0456) Loss: 0.9047 (0.5984)
[2022/12/29 01:01] | VALID(035): [150/220] Batch: 0.0587 (0.0635) Data: 0.0220 (0.0407) Loss: 0.5403 (0.5921)
[2022/12/29 01:01] | VALID(035): [200/220] Batch: 0.0450 (0.0617) Data: 0.0274 (0.0367) Loss: 0.2383 (0.5962)
[2022/12/29 01:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:01] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:01] |    VALID(35)      0.5934      0.8080      0.8657      0.8080      0.8080      0.8080      0.9520
[2022/12/29 01:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:01] | ####################################################################################################
[2022/12/29 01:01] | TRAIN(036): [ 50/879] Batch: 0.2200 (0.2326) Data: 0.0133 (0.0501) Loss: 0.4723 (0.5688)
[2022/12/29 01:02] | TRAIN(036): [100/879] Batch: 0.1893 (0.2104) Data: 0.0093 (0.0305) Loss: 0.6071 (0.5726)
[2022/12/29 01:02] | TRAIN(036): [150/879] Batch: 0.1873 (0.2028) Data: 0.0112 (0.0239) Loss: 0.4333 (0.5646)
[2022/12/29 01:02] | TRAIN(036): [200/879] Batch: 0.1809 (0.1984) Data: 0.0087 (0.0205) Loss: 0.3431 (0.5657)
[2022/12/29 01:02] | TRAIN(036): [250/879] Batch: 0.1994 (0.1957) Data: 0.0167 (0.0185) Loss: 0.3728 (0.5643)
[2022/12/29 01:02] | TRAIN(036): [300/879] Batch: 0.1900 (0.1949) Data: 0.0102 (0.0172) Loss: 0.3511 (0.5639)
[2022/12/29 01:02] | TRAIN(036): [350/879] Batch: 0.1482 (0.1937) Data: 0.0112 (0.0163) Loss: 0.6457 (0.5706)
[2022/12/29 01:03] | TRAIN(036): [400/879] Batch: 0.1911 (0.1917) Data: 0.0098 (0.0156) Loss: 0.6514 (0.5705)
[2022/12/29 01:03] | TRAIN(036): [450/879] Batch: 0.1952 (0.1889) Data: 0.0094 (0.0150) Loss: 0.5918 (0.5726)
[2022/12/29 01:03] | TRAIN(036): [500/879] Batch: 0.1858 (0.1893) Data: 0.0100 (0.0145) Loss: 0.4259 (0.5745)
[2022/12/29 01:03] | TRAIN(036): [550/879] Batch: 0.1838 (0.1894) Data: 0.0104 (0.0142) Loss: 0.5361 (0.5771)
[2022/12/29 01:03] | TRAIN(036): [600/879] Batch: 0.1865 (0.1894) Data: 0.0112 (0.0139) Loss: 0.4994 (0.5792)
[2022/12/29 01:03] | TRAIN(036): [650/879] Batch: 0.2064 (0.1895) Data: 0.0138 (0.0136) Loss: 0.5360 (0.5806)
[2022/12/29 01:03] | TRAIN(036): [700/879] Batch: 0.1976 (0.1895) Data: 0.0104 (0.0134) Loss: 0.5256 (0.5798)
[2022/12/29 01:04] | TRAIN(036): [750/879] Batch: 0.1907 (0.1897) Data: 0.0106 (0.0132) Loss: 0.5360 (0.5795)
[2022/12/29 01:04] | TRAIN(036): [800/879] Batch: 0.1867 (0.1898) Data: 0.0105 (0.0130) Loss: 0.5733 (0.5805)
[2022/12/29 01:04] | TRAIN(036): [850/879] Batch: 0.1903 (0.1900) Data: 0.0098 (0.0129) Loss: 0.4988 (0.5819)
[2022/12/29 01:04] | ------------------------------------------------------------
[2022/12/29 01:04] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:04] | ------------------------------------------------------------
[2022/12/29 01:04] |    TRAIN(36)     0:02:46     0:00:11     0:02:35      0.5797
[2022/12/29 01:04] | ------------------------------------------------------------
[2022/12/29 01:04] | VALID(036): [ 50/220] Batch: 0.0472 (0.0822) Data: 0.0346 (0.0562) Loss: 0.4756 (0.5612)
[2022/12/29 01:04] | VALID(036): [100/220] Batch: 0.0581 (0.0690) Data: 0.0293 (0.0446) Loss: 0.9192 (0.5865)
[2022/12/29 01:04] | VALID(036): [150/220] Batch: 0.0583 (0.0648) Data: 0.0439 (0.0415) Loss: 0.5167 (0.5780)
[2022/12/29 01:04] | VALID(036): [200/220] Batch: 0.0482 (0.0627) Data: 0.0382 (0.0399) Loss: 0.2122 (0.5823)
[2022/12/29 01:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:04] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:04] |    VALID(36)      0.5808      0.8108      0.8647      0.8108      0.8108      0.8108      0.9527
[2022/12/29 01:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:04] | ####################################################################################################
[2022/12/29 01:04] | TRAIN(037): [ 50/879] Batch: 0.1851 (0.2295) Data: 0.0094 (0.0485) Loss: 0.3370 (0.5803)
[2022/12/29 01:05] | TRAIN(037): [100/879] Batch: 0.1871 (0.2026) Data: 0.0113 (0.0295) Loss: 0.5572 (0.5805)
[2022/12/29 01:05] | TRAIN(037): [150/879] Batch: 0.1975 (0.1931) Data: 0.0109 (0.0230) Loss: 0.6827 (0.5823)
[2022/12/29 01:05] | TRAIN(037): [200/879] Batch: 0.1816 (0.1929) Data: 0.0111 (0.0199) Loss: 0.4839 (0.5836)
[2022/12/29 01:05] | TRAIN(037): [250/879] Batch: 0.1934 (0.1924) Data: 0.0091 (0.0180) Loss: 0.6155 (0.5820)
[2022/12/29 01:05] | TRAIN(037): [300/879] Batch: 0.2047 (0.1922) Data: 0.0107 (0.0167) Loss: 0.6042 (0.5792)
[2022/12/29 01:05] | TRAIN(037): [350/879] Batch: 0.1927 (0.1920) Data: 0.0108 (0.0158) Loss: 0.7589 (0.5798)
[2022/12/29 01:06] | TRAIN(037): [400/879] Batch: 0.1975 (0.1919) Data: 0.0092 (0.0151) Loss: 0.4867 (0.5793)
[2022/12/29 01:06] | TRAIN(037): [450/879] Batch: 0.2023 (0.1924) Data: 0.0107 (0.0146) Loss: 0.5506 (0.5773)
[2022/12/29 01:06] | TRAIN(037): [500/879] Batch: 0.2032 (0.1924) Data: 0.0120 (0.0142) Loss: 0.2276 (0.5740)
[2022/12/29 01:06] | TRAIN(037): [550/879] Batch: 0.1904 (0.1924) Data: 0.0097 (0.0138) Loss: 0.4150 (0.5747)
[2022/12/29 01:06] | TRAIN(037): [600/879] Batch: 0.2021 (0.1922) Data: 0.0115 (0.0136) Loss: 0.4450 (0.5738)
[2022/12/29 01:06] | TRAIN(037): [650/879] Batch: 0.1930 (0.1920) Data: 0.0115 (0.0133) Loss: 0.6664 (0.5731)
[2022/12/29 01:07] | TRAIN(037): [700/879] Batch: 0.1926 (0.1920) Data: 0.0113 (0.0131) Loss: 0.3118 (0.5722)
[2022/12/29 01:07] | TRAIN(037): [750/879] Batch: 0.1840 (0.1913) Data: 0.0101 (0.0130) Loss: 0.5369 (0.5760)
[2022/12/29 01:07] | TRAIN(037): [800/879] Batch: 0.1196 (0.1903) Data: 0.0091 (0.0128) Loss: 0.5125 (0.5761)
[2022/12/29 01:07] | TRAIN(037): [850/879] Batch: 0.1921 (0.1903) Data: 0.0110 (0.0127) Loss: 0.5448 (0.5758)
[2022/12/29 01:07] | ------------------------------------------------------------
[2022/12/29 01:07] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:07] | ------------------------------------------------------------
[2022/12/29 01:07] |    TRAIN(37)     0:02:47     0:00:11     0:02:36      0.5764
[2022/12/29 01:07] | ------------------------------------------------------------
[2022/12/29 01:07] | VALID(037): [ 50/220] Batch: 0.0430 (0.0798) Data: 0.0365 (0.0566) Loss: 0.4400 (0.5610)
[2022/12/29 01:07] | VALID(037): [100/220] Batch: 0.0509 (0.0664) Data: 0.0386 (0.0443) Loss: 0.8766 (0.5789)
[2022/12/29 01:07] | VALID(037): [150/220] Batch: 0.0600 (0.0620) Data: 0.0390 (0.0402) Loss: 0.4960 (0.5796)
[2022/12/29 01:07] | VALID(037): [200/220] Batch: 0.0429 (0.0599) Data: 0.0384 (0.0382) Loss: 0.2832 (0.5813)
[2022/12/29 01:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:07] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:07] |    VALID(37)      0.5803      0.8108      0.8618      0.8108      0.8108      0.8108      0.9527
[2022/12/29 01:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:07] | ####################################################################################################
[2022/12/29 01:07] | TRAIN(038): [ 50/879] Batch: 0.1969 (0.2329) Data: 0.0089 (0.0484) Loss: 0.7794 (0.5923)
[2022/12/29 01:08] | TRAIN(038): [100/879] Batch: 0.1942 (0.2116) Data: 0.0112 (0.0295) Loss: 0.5974 (0.5882)
[2022/12/29 01:08] | TRAIN(038): [150/879] Batch: 0.1816 (0.2043) Data: 0.0105 (0.0232) Loss: 0.4601 (0.5780)
[2022/12/29 01:08] | TRAIN(038): [200/879] Batch: 0.1755 (0.2006) Data: 0.0086 (0.0200) Loss: 0.3445 (0.5872)
[2022/12/29 01:08] | TRAIN(038): [250/879] Batch: 0.1794 (0.1980) Data: 0.0088 (0.0180) Loss: 0.4152 (0.5851)
[2022/12/29 01:08] | TRAIN(038): [300/879] Batch: 0.1976 (0.1964) Data: 0.0111 (0.0168) Loss: 0.3417 (0.5823)
[2022/12/29 01:08] | TRAIN(038): [350/879] Batch: 0.1811 (0.1949) Data: 0.0106 (0.0159) Loss: 0.2416 (0.5788)
[2022/12/29 01:09] | TRAIN(038): [400/879] Batch: 0.1944 (0.1940) Data: 0.0100 (0.0152) Loss: 0.3856 (0.5803)
[2022/12/29 01:09] | TRAIN(038): [450/879] Batch: 0.1407 (0.1932) Data: 0.0097 (0.0146) Loss: 0.6252 (0.5804)
[2022/12/29 01:09] | TRAIN(038): [500/879] Batch: 0.1814 (0.1924) Data: 0.0108 (0.0143) Loss: 0.7510 (0.5809)
[2022/12/29 01:09] | TRAIN(038): [550/879] Batch: 0.1895 (0.1905) Data: 0.0112 (0.0139) Loss: 0.4481 (0.5797)
[2022/12/29 01:09] | TRAIN(038): [600/879] Batch: 0.1813 (0.1905) Data: 0.0103 (0.0136) Loss: 0.5414 (0.5763)
[2022/12/29 01:09] | TRAIN(038): [650/879] Batch: 0.2015 (0.1907) Data: 0.0098 (0.0134) Loss: 0.4702 (0.5756)
[2022/12/29 01:10] | TRAIN(038): [700/879] Batch: 0.1985 (0.1906) Data: 0.0110 (0.0132) Loss: 0.5149 (0.5773)
[2022/12/29 01:10] | TRAIN(038): [750/879] Batch: 0.1852 (0.1903) Data: 0.0105 (0.0130) Loss: 0.5447 (0.5789)
[2022/12/29 01:10] | TRAIN(038): [800/879] Batch: 0.2027 (0.1903) Data: 0.0104 (0.0128) Loss: 0.8210 (0.5795)
[2022/12/29 01:10] | TRAIN(038): [850/879] Batch: 0.1865 (0.1904) Data: 0.0093 (0.0127) Loss: 0.4035 (0.5784)
[2022/12/29 01:10] | ------------------------------------------------------------
[2022/12/29 01:10] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:10] | ------------------------------------------------------------
[2022/12/29 01:10] |    TRAIN(38)     0:02:47     0:00:11     0:02:35      0.5784
[2022/12/29 01:10] | ------------------------------------------------------------
[2022/12/29 01:10] | VALID(038): [ 50/220] Batch: 0.0484 (0.0829) Data: 0.0385 (0.0596) Loss: 0.4831 (0.5534)
[2022/12/29 01:10] | VALID(038): [100/220] Batch: 0.0547 (0.0694) Data: 0.0262 (0.0462) Loss: 0.8977 (0.5857)
[2022/12/29 01:10] | VALID(038): [150/220] Batch: 0.0484 (0.0653) Data: 0.0331 (0.0396) Loss: 0.5381 (0.5823)
[2022/12/29 01:10] | VALID(038): [200/220] Batch: 0.0601 (0.0631) Data: 0.0345 (0.0375) Loss: 0.2728 (0.5826)
[2022/12/29 01:10] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:10] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:10] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:10] |    VALID(38)      0.5836      0.8106      0.8700      0.8106      0.8106      0.8106      0.9526
[2022/12/29 01:10] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:10] | ####################################################################################################
[2022/12/29 01:10] | TRAIN(039): [ 50/879] Batch: 0.1914 (0.2324) Data: 0.0102 (0.0487) Loss: 0.4920 (0.5297)
[2022/12/29 01:11] | TRAIN(039): [100/879] Batch: 0.1970 (0.2119) Data: 0.0098 (0.0299) Loss: 0.5462 (0.5692)
[2022/12/29 01:11] | TRAIN(039): [150/879] Batch: 0.1943 (0.2034) Data: 0.0077 (0.0235) Loss: 0.7961 (0.5875)
[2022/12/29 01:11] | TRAIN(039): [200/879] Batch: 0.1842 (0.1979) Data: 0.0108 (0.0205) Loss: 0.6568 (0.5830)
[2022/12/29 01:11] | TRAIN(039): [250/879] Batch: 0.1747 (0.1926) Data: 0.0101 (0.0184) Loss: 0.2375 (0.5765)
[2022/12/29 01:11] | TRAIN(039): [300/879] Batch: 0.1731 (0.1922) Data: 0.0102 (0.0171) Loss: 0.7543 (0.5747)
[2022/12/29 01:11] | TRAIN(039): [350/879] Batch: 0.1940 (0.1916) Data: 0.0098 (0.0161) Loss: 0.6806 (0.5766)
[2022/12/29 01:12] | TRAIN(039): [400/879] Batch: 0.1920 (0.1914) Data: 0.0115 (0.0154) Loss: 0.6087 (0.5719)
[2022/12/29 01:12] | TRAIN(039): [450/879] Batch: 0.1919 (0.1915) Data: 0.0090 (0.0148) Loss: 0.5649 (0.5753)
[2022/12/29 01:12] | TRAIN(039): [500/879] Batch: 0.1992 (0.1912) Data: 0.0099 (0.0144) Loss: 0.6949 (0.5762)
[2022/12/29 01:12] | TRAIN(039): [550/879] Batch: 0.1936 (0.1912) Data: 0.0091 (0.0140) Loss: 0.5280 (0.5762)
[2022/12/29 01:12] | TRAIN(039): [600/879] Batch: 0.1872 (0.1914) Data: 0.0106 (0.0137) Loss: 0.5396 (0.5753)
[2022/12/29 01:12] | TRAIN(039): [650/879] Batch: 0.1985 (0.1914) Data: 0.0110 (0.0135) Loss: 0.5282 (0.5759)
[2022/12/29 01:13] | TRAIN(039): [700/879] Batch: 0.1980 (0.1915) Data: 0.0136 (0.0133) Loss: 0.5703 (0.5759)
[2022/12/29 01:13] | TRAIN(039): [750/879] Batch: 0.1849 (0.1916) Data: 0.0102 (0.0131) Loss: 0.4283 (0.5752)
[2022/12/29 01:13] | TRAIN(039): [800/879] Batch: 0.1975 (0.1917) Data: 0.0106 (0.0129) Loss: 0.6245 (0.5751)
[2022/12/29 01:13] | TRAIN(039): [850/879] Batch: 0.1892 (0.1912) Data: 0.0082 (0.0128) Loss: 0.7191 (0.5745)
[2022/12/29 01:13] | ------------------------------------------------------------
[2022/12/29 01:13] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:13] | ------------------------------------------------------------
[2022/12/29 01:13] |    TRAIN(39)     0:02:47     0:00:11     0:02:36      0.5748
[2022/12/29 01:13] | ------------------------------------------------------------
[2022/12/29 01:13] | VALID(039): [ 50/220] Batch: 0.0387 (0.0721) Data: 0.0078 (0.0405) Loss: 0.5124 (0.5450)
[2022/12/29 01:13] | VALID(039): [100/220] Batch: 0.0593 (0.0620) Data: 0.0314 (0.0352) Loss: 0.9382 (0.5806)
[2022/12/29 01:13] | VALID(039): [150/220] Batch: 0.0571 (0.0601) Data: 0.0142 (0.0344) Loss: 0.4542 (0.5785)
[2022/12/29 01:13] | VALID(039): [200/220] Batch: 0.0615 (0.0588) Data: 0.0310 (0.0341) Loss: 0.2966 (0.5832)
[2022/12/29 01:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:13] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:13] |    VALID(39)      0.5810      0.8140      0.8670      0.8140      0.8140      0.8140      0.9535
[2022/12/29 01:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:13] | ####################################################################################################
[2022/12/29 01:14] | TRAIN(040): [ 50/879] Batch: 0.1808 (0.2261) Data: 0.0106 (0.0459) Loss: 0.3836 (0.5450)
[2022/12/29 01:14] | TRAIN(040): [100/879] Batch: 0.1871 (0.2075) Data: 0.0115 (0.0282) Loss: 0.9354 (0.5720)
[2022/12/29 01:14] | TRAIN(040): [150/879] Batch: 0.1842 (0.2032) Data: 0.0103 (0.0224) Loss: 0.4889 (0.5709)
[2022/12/29 01:14] | TRAIN(040): [200/879] Batch: 0.1947 (0.2009) Data: 0.0097 (0.0196) Loss: 0.9658 (0.5757)
[2022/12/29 01:14] | TRAIN(040): [250/879] Batch: 0.2028 (0.1994) Data: 0.0117 (0.0178) Loss: 0.7443 (0.5728)
[2022/12/29 01:14] | TRAIN(040): [300/879] Batch: 0.1827 (0.1976) Data: 0.0108 (0.0165) Loss: 0.6526 (0.5780)
[2022/12/29 01:14] | TRAIN(040): [350/879] Batch: 0.1892 (0.1966) Data: 0.0109 (0.0156) Loss: 0.3423 (0.5714)
[2022/12/29 01:15] | TRAIN(040): [400/879] Batch: 0.1844 (0.1959) Data: 0.0113 (0.0150) Loss: 0.7186 (0.5670)
[2022/12/29 01:15] | TRAIN(040): [450/879] Batch: 0.1990 (0.1954) Data: 0.0107 (0.0145) Loss: 0.5618 (0.5696)
[2022/12/29 01:15] | TRAIN(040): [500/879] Batch: 0.1938 (0.1949) Data: 0.0102 (0.0141) Loss: 0.5633 (0.5691)
[2022/12/29 01:15] | TRAIN(040): [550/879] Batch: 0.1834 (0.1933) Data: 0.0106 (0.0137) Loss: 0.4719 (0.5723)
[2022/12/29 01:15] | TRAIN(040): [600/879] Batch: 0.1265 (0.1924) Data: 0.0091 (0.0134) Loss: 0.4927 (0.5718)
[2022/12/29 01:15] | TRAIN(040): [650/879] Batch: 0.1934 (0.1911) Data: 0.0103 (0.0132) Loss: 0.5400 (0.5732)
[2022/12/29 01:16] | TRAIN(040): [700/879] Batch: 0.1929 (0.1910) Data: 0.0101 (0.0130) Loss: 0.6222 (0.5701)
[2022/12/29 01:16] | TRAIN(040): [750/879] Batch: 0.1808 (0.1908) Data: 0.0093 (0.0128) Loss: 0.5913 (0.5707)
[2022/12/29 01:16] | TRAIN(040): [800/879] Batch: 0.1774 (0.1905) Data: 0.0099 (0.0126) Loss: 0.5594 (0.5715)
[2022/12/29 01:16] | TRAIN(040): [850/879] Batch: 0.1902 (0.1903) Data: 0.0095 (0.0125) Loss: 0.8939 (0.5701)
[2022/12/29 01:16] | ------------------------------------------------------------
[2022/12/29 01:16] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:16] | ------------------------------------------------------------
[2022/12/29 01:16] |    TRAIN(40)     0:02:47     0:00:10     0:02:36      0.5702
[2022/12/29 01:16] | ------------------------------------------------------------
[2022/12/29 01:16] | VALID(040): [ 50/220] Batch: 0.0591 (0.0826) Data: 0.0282 (0.0607) Loss: 0.5045 (0.5348)
[2022/12/29 01:16] | VALID(040): [100/220] Batch: 0.0608 (0.0694) Data: 0.0384 (0.0480) Loss: 0.8162 (0.5661)
[2022/12/29 01:16] | VALID(040): [150/220] Batch: 0.0462 (0.0647) Data: 0.0383 (0.0432) Loss: 0.5428 (0.5585)
[2022/12/29 01:16] | VALID(040): [200/220] Batch: 0.0390 (0.0620) Data: 0.0357 (0.0406) Loss: 0.2408 (0.5675)
[2022/12/29 01:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:16] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:16] |    VALID(40)      0.5661      0.8177      0.8727      0.8177      0.8177      0.8177      0.9544
[2022/12/29 01:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:16] | ####################################################################################################
[2022/12/29 01:17] | TRAIN(041): [ 50/879] Batch: 0.1959 (0.2334) Data: 0.0164 (0.0499) Loss: 0.5361 (0.5742)
[2022/12/29 01:17] | TRAIN(041): [100/879] Batch: 0.1935 (0.2145) Data: 0.0117 (0.0308) Loss: 0.8944 (0.5665)
[2022/12/29 01:17] | TRAIN(041): [150/879] Batch: 0.1942 (0.2074) Data: 0.0092 (0.0241) Loss: 0.5951 (0.5566)
[2022/12/29 01:17] | TRAIN(041): [200/879] Batch: 0.1935 (0.2033) Data: 0.0091 (0.0207) Loss: 0.6861 (0.5614)
[2022/12/29 01:17] | TRAIN(041): [250/879] Batch: 0.1812 (0.2004) Data: 0.0111 (0.0187) Loss: 0.5476 (0.5573)
[2022/12/29 01:17] | TRAIN(041): [300/879] Batch: 0.1916 (0.1975) Data: 0.0107 (0.0173) Loss: 0.5713 (0.5557)
[2022/12/29 01:17] | TRAIN(041): [350/879] Batch: 0.1899 (0.1938) Data: 0.0114 (0.0163) Loss: 0.8715 (0.5616)
[2022/12/29 01:18] | TRAIN(041): [400/879] Batch: 0.1870 (0.1930) Data: 0.0111 (0.0156) Loss: 0.5237 (0.5631)
[2022/12/29 01:18] | TRAIN(041): [450/879] Batch: 0.1837 (0.1924) Data: 0.0109 (0.0150) Loss: 0.9144 (0.5644)
[2022/12/29 01:18] | TRAIN(041): [500/879] Batch: 0.2079 (0.1916) Data: 0.0105 (0.0146) Loss: 0.2801 (0.5635)
[2022/12/29 01:18] | TRAIN(041): [550/879] Batch: 0.1840 (0.1913) Data: 0.0108 (0.0142) Loss: 0.2759 (0.5661)
[2022/12/29 01:18] | TRAIN(041): [600/879] Batch: 0.1952 (0.1906) Data: 0.0100 (0.0139) Loss: 0.3182 (0.5643)
[2022/12/29 01:18] | TRAIN(041): [650/879] Batch: 0.2119 (0.1908) Data: 0.0107 (0.0137) Loss: 0.2636 (0.5630)
[2022/12/29 01:19] | TRAIN(041): [700/879] Batch: 0.1991 (0.1908) Data: 0.0106 (0.0135) Loss: 0.9549 (0.5670)
[2022/12/29 01:19] | TRAIN(041): [750/879] Batch: 0.1878 (0.1909) Data: 0.0109 (0.0133) Loss: 0.7340 (0.5658)
[2022/12/29 01:19] | TRAIN(041): [800/879] Batch: 0.1943 (0.1907) Data: 0.0097 (0.0131) Loss: 0.4226 (0.5667)
[2022/12/29 01:19] | TRAIN(041): [850/879] Batch: 0.1777 (0.1905) Data: 0.0106 (0.0129) Loss: 0.2854 (0.5663)
[2022/12/29 01:19] | ------------------------------------------------------------
[2022/12/29 01:19] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:19] | ------------------------------------------------------------
[2022/12/29 01:19] |    TRAIN(41)     0:02:47     0:00:11     0:02:36      0.5651
[2022/12/29 01:19] | ------------------------------------------------------------
[2022/12/29 01:19] | VALID(041): [ 50/220] Batch: 0.0598 (0.0835) Data: 0.0317 (0.0587) Loss: 0.4962 (0.5434)
[2022/12/29 01:19] | VALID(041): [100/220] Batch: 0.0530 (0.0699) Data: 0.0368 (0.0464) Loss: 0.8922 (0.5717)
[2022/12/29 01:19] | VALID(041): [150/220] Batch: 0.0484 (0.0655) Data: 0.0109 (0.0419) Loss: 0.4377 (0.5667)
[2022/12/29 01:19] | VALID(041): [200/220] Batch: 0.0531 (0.0602) Data: 0.0223 (0.0347) Loss: 0.2376 (0.5725)
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] |    VALID(41)      0.5686      0.8187      0.8741      0.8187      0.8187      0.8187      0.9547
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] | ####################################################################################################
[2022/12/29 01:20] | TRAIN(042): [ 50/879] Batch: 0.1957 (0.2131) Data: 0.0084 (0.0467) Loss: 0.6902 (0.5703)
[2022/12/29 01:20] | TRAIN(042): [100/879] Batch: 0.1948 (0.2011) Data: 0.0104 (0.0287) Loss: 0.5045 (0.5626)
[2022/12/29 01:20] | TRAIN(042): [150/879] Batch: 0.1937 (0.1982) Data: 0.0100 (0.0228) Loss: 0.3598 (0.5575)
[2022/12/29 01:20] | TRAIN(042): [200/879] Batch: 0.1771 (0.1956) Data: 0.0106 (0.0197) Loss: 0.7898 (0.5675)
[2022/12/29 01:20] | TRAIN(042): [250/879] Batch: 0.1927 (0.1946) Data: 0.0104 (0.0179) Loss: 0.5673 (0.5639)
[2022/12/29 01:20] | TRAIN(042): [300/879] Batch: 0.1879 (0.1945) Data: 0.0116 (0.0168) Loss: 0.7563 (0.5651)
[2022/12/29 01:20] | TRAIN(042): [350/879] Batch: 0.1978 (0.1939) Data: 0.0102 (0.0160) Loss: 0.7046 (0.5653)
[2022/12/29 01:21] | TRAIN(042): [400/879] Batch: 0.1890 (0.1937) Data: 0.0109 (0.0153) Loss: 0.6395 (0.5670)
[2022/12/29 01:21] | TRAIN(042): [450/879] Batch: 0.1982 (0.1936) Data: 0.0096 (0.0148) Loss: 0.5936 (0.5673)
[2022/12/29 01:21] | TRAIN(042): [500/879] Batch: 0.1941 (0.1935) Data: 0.0108 (0.0144) Loss: 0.4355 (0.5646)
[2022/12/29 01:21] | TRAIN(042): [550/879] Batch: 0.1980 (0.1934) Data: 0.0103 (0.0140) Loss: 0.4513 (0.5623)
[2022/12/29 01:21] | TRAIN(042): [600/879] Batch: 0.1839 (0.1934) Data: 0.0098 (0.0138) Loss: 0.4742 (0.5624)
[2022/12/29 01:21] | TRAIN(042): [650/879] Batch: 0.1877 (0.1923) Data: 0.0101 (0.0135) Loss: 0.4463 (0.5609)
[2022/12/29 01:22] | TRAIN(042): [700/879] Batch: 0.1291 (0.1918) Data: 0.0087 (0.0133) Loss: 0.5976 (0.5623)
[2022/12/29 01:22] | TRAIN(042): [750/879] Batch: 0.1847 (0.1907) Data: 0.0109 (0.0131) Loss: 0.7039 (0.5618)
[2022/12/29 01:22] | TRAIN(042): [800/879] Batch: 0.1850 (0.1905) Data: 0.0110 (0.0129) Loss: 0.8524 (0.5605)
[2022/12/29 01:22] | TRAIN(042): [850/879] Batch: 0.1840 (0.1907) Data: 0.0115 (0.0128) Loss: 0.4779 (0.5631)
[2022/12/29 01:22] | ------------------------------------------------------------
[2022/12/29 01:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:22] | ------------------------------------------------------------
[2022/12/29 01:22] |    TRAIN(42)     0:02:47     0:00:11     0:02:36      0.5629
[2022/12/29 01:22] | ------------------------------------------------------------
[2022/12/29 01:22] | VALID(042): [ 50/220] Batch: 0.0553 (0.0797) Data: 0.0372 (0.0570) Loss: 0.4974 (0.5419)
[2022/12/29 01:22] | VALID(042): [100/220] Batch: 0.0426 (0.0668) Data: 0.0363 (0.0406) Loss: 0.9613 (0.5737)
[2022/12/29 01:22] | VALID(042): [150/220] Batch: 0.0599 (0.0632) Data: 0.0377 (0.0386) Loss: 0.5733 (0.5640)
[2022/12/29 01:22] | VALID(042): [200/220] Batch: 0.0652 (0.0615) Data: 0.0385 (0.0377) Loss: 0.2757 (0.5649)
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] |    VALID(42)      0.5615      0.8200      0.8744      0.8200      0.8200      0.8200      0.9550
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] | ####################################################################################################
[2022/12/29 01:23] | TRAIN(043): [ 50/879] Batch: 0.1896 (0.2258) Data: 0.0115 (0.0454) Loss: 0.5086 (0.5363)
[2022/12/29 01:23] | TRAIN(043): [100/879] Batch: 0.1914 (0.2091) Data: 0.0089 (0.0281) Loss: 0.5262 (0.5363)
[2022/12/29 01:23] | TRAIN(043): [150/879] Batch: 0.1938 (0.2038) Data: 0.0093 (0.0224) Loss: 0.3213 (0.5405)
[2022/12/29 01:23] | TRAIN(043): [200/879] Batch: 0.1753 (0.1994) Data: 0.0119 (0.0193) Loss: 0.5080 (0.5473)
[2022/12/29 01:23] | TRAIN(043): [250/879] Batch: 0.1918 (0.1969) Data: 0.0088 (0.0175) Loss: 0.6242 (0.5468)
[2022/12/29 01:23] | TRAIN(043): [300/879] Batch: 0.1826 (0.1952) Data: 0.0103 (0.0164) Loss: 0.6009 (0.5475)
[2022/12/29 01:23] | TRAIN(043): [350/879] Batch: 0.1790 (0.1935) Data: 0.0096 (0.0155) Loss: 0.5683 (0.5447)
[2022/12/29 01:24] | TRAIN(043): [400/879] Batch: 0.1846 (0.1910) Data: 0.0092 (0.0148) Loss: 0.7607 (0.5476)
[2022/12/29 01:24] | TRAIN(043): [450/879] Batch: 0.1751 (0.1892) Data: 0.0090 (0.0143) Loss: 0.6225 (0.5491)
[2022/12/29 01:24] | TRAIN(043): [500/879] Batch: 0.1940 (0.1891) Data: 0.0090 (0.0139) Loss: 0.5163 (0.5509)
[2022/12/29 01:24] | TRAIN(043): [550/879] Batch: 0.1896 (0.1890) Data: 0.0097 (0.0136) Loss: 0.4227 (0.5515)
[2022/12/29 01:24] | TRAIN(043): [600/879] Batch: 0.1845 (0.1893) Data: 0.0105 (0.0134) Loss: 0.5644 (0.5529)
[2022/12/29 01:24] | TRAIN(043): [650/879] Batch: 0.1867 (0.1894) Data: 0.0117 (0.0132) Loss: 0.6534 (0.5548)
[2022/12/29 01:25] | TRAIN(043): [700/879] Batch: 0.1932 (0.1894) Data: 0.0110 (0.0130) Loss: 0.6725 (0.5573)
[2022/12/29 01:25] | TRAIN(043): [750/879] Batch: 0.1857 (0.1893) Data: 0.0099 (0.0128) Loss: 0.4273 (0.5564)
[2022/12/29 01:25] | TRAIN(043): [800/879] Batch: 0.1961 (0.1896) Data: 0.0109 (0.0127) Loss: 0.5236 (0.5568)
[2022/12/29 01:25] | TRAIN(043): [850/879] Batch: 0.1901 (0.1897) Data: 0.0095 (0.0126) Loss: 0.5854 (0.5580)
[2022/12/29 01:25] | ------------------------------------------------------------
[2022/12/29 01:25] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:25] | ------------------------------------------------------------
[2022/12/29 01:25] |    TRAIN(43)     0:02:46     0:00:10     0:02:35      0.5585
[2022/12/29 01:25] | ------------------------------------------------------------
[2022/12/29 01:25] | VALID(043): [ 50/220] Batch: 0.0401 (0.0840) Data: 0.0373 (0.0592) Loss: 0.4522 (0.5365)
[2022/12/29 01:25] | VALID(043): [100/220] Batch: 0.0520 (0.0694) Data: 0.0379 (0.0461) Loss: 0.9352 (0.5661)
[2022/12/29 01:25] | VALID(043): [150/220] Batch: 0.0587 (0.0652) Data: 0.0263 (0.0424) Loss: 0.4702 (0.5576)
[2022/12/29 01:25] | VALID(043): [200/220] Batch: 0.0589 (0.0631) Data: 0.0370 (0.0406) Loss: 0.3016 (0.5616)
[2022/12/29 01:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:25] |    VALID(43)      0.5596      0.8204      0.8734      0.8204      0.8204      0.8204      0.9551
[2022/12/29 01:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:25] | ####################################################################################################
[2022/12/29 01:26] | TRAIN(044): [ 50/879] Batch: 0.1937 (0.2321) Data: 0.0088 (0.0481) Loss: 0.5579 (0.5243)
[2022/12/29 01:26] | TRAIN(044): [100/879] Batch: 0.1865 (0.2044) Data: 0.0101 (0.0296) Loss: 0.6466 (0.5462)
[2022/12/29 01:26] | TRAIN(044): [150/879] Batch: 0.1810 (0.1948) Data: 0.0117 (0.0234) Loss: 0.5622 (0.5437)
[2022/12/29 01:26] | TRAIN(044): [200/879] Batch: 0.1796 (0.1930) Data: 0.0104 (0.0201) Loss: 0.5386 (0.5519)
[2022/12/29 01:26] | TRAIN(044): [250/879] Batch: 0.1868 (0.1921) Data: 0.0096 (0.0182) Loss: 0.2487 (0.5484)
[2022/12/29 01:26] | TRAIN(044): [300/879] Batch: 0.1904 (0.1913) Data: 0.0109 (0.0169) Loss: 0.4484 (0.5465)
[2022/12/29 01:26] | TRAIN(044): [350/879] Batch: 0.1974 (0.1911) Data: 0.0106 (0.0160) Loss: 0.7299 (0.5512)
[2022/12/29 01:27] | TRAIN(044): [400/879] Batch: 0.1905 (0.1910) Data: 0.0096 (0.0153) Loss: 0.3895 (0.5502)
[2022/12/29 01:27] | TRAIN(044): [450/879] Batch: 0.1917 (0.1908) Data: 0.0098 (0.0148) Loss: 0.3585 (0.5504)
[2022/12/29 01:27] | TRAIN(044): [500/879] Batch: 0.1867 (0.1906) Data: 0.0096 (0.0143) Loss: 0.4109 (0.5525)
[2022/12/29 01:27] | TRAIN(044): [550/879] Batch: 0.1911 (0.1906) Data: 0.0116 (0.0139) Loss: 0.4796 (0.5497)
[2022/12/29 01:27] | TRAIN(044): [600/879] Batch: 0.1913 (0.1906) Data: 0.0092 (0.0136) Loss: 0.4771 (0.5516)
[2022/12/29 01:27] | TRAIN(044): [650/879] Batch: 0.1987 (0.1908) Data: 0.0100 (0.0134) Loss: 0.5717 (0.5500)
[2022/12/29 01:28] | TRAIN(044): [700/879] Batch: 0.1920 (0.1908) Data: 0.0114 (0.0132) Loss: 0.3686 (0.5493)
[2022/12/29 01:28] | TRAIN(044): [750/879] Batch: 0.1949 (0.1908) Data: 0.0099 (0.0130) Loss: 0.7431 (0.5507)
[2022/12/29 01:28] | TRAIN(044): [800/879] Batch: 0.2124 (0.1900) Data: 0.0130 (0.0129) Loss: 1.3335 (0.5519)
[2022/12/29 01:28] | TRAIN(044): [850/879] Batch: 0.1960 (0.1892) Data: 0.0106 (0.0127) Loss: 0.5766 (0.5526)
[2022/12/29 01:28] | ------------------------------------------------------------
[2022/12/29 01:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:28] | ------------------------------------------------------------
[2022/12/29 01:28] |    TRAIN(44)     0:02:46     0:00:11     0:02:35      0.5518
[2022/12/29 01:28] | ------------------------------------------------------------
[2022/12/29 01:28] | VALID(044): [ 50/220] Batch: 0.0564 (0.0822) Data: 0.0246 (0.0571) Loss: 0.4883 (0.5414)
[2022/12/29 01:28] | VALID(044): [100/220] Batch: 0.0403 (0.0677) Data: 0.0369 (0.0442) Loss: 0.8639 (0.5744)
[2022/12/29 01:28] | VALID(044): [150/220] Batch: 0.0575 (0.0629) Data: 0.0347 (0.0401) Loss: 0.4863 (0.5730)
[2022/12/29 01:28] | VALID(044): [200/220] Batch: 0.0484 (0.0606) Data: 0.0351 (0.0382) Loss: 0.2522 (0.5759)
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] |    VALID(44)      0.5757      0.8133      0.8696      0.8133      0.8133      0.8133      0.9533
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] | ####################################################################################################
[2022/12/29 01:29] | TRAIN(045): [ 50/879] Batch: 0.1943 (0.2303) Data: 0.0103 (0.0476) Loss: 0.5190 (0.5507)
[2022/12/29 01:29] | TRAIN(045): [100/879] Batch: 0.1864 (0.2103) Data: 0.0129 (0.0292) Loss: 0.3186 (0.5452)
[2022/12/29 01:29] | TRAIN(045): [150/879] Batch: 0.1953 (0.2047) Data: 0.0108 (0.0230) Loss: 0.4472 (0.5439)
[2022/12/29 01:29] | TRAIN(045): [200/879] Batch: 0.1770 (0.2000) Data: 0.0095 (0.0198) Loss: 0.3297 (0.5391)
[2022/12/29 01:29] | TRAIN(045): [250/879] Batch: 0.1799 (0.1972) Data: 0.0109 (0.0179) Loss: 0.5500 (0.5357)
[2022/12/29 01:29] | TRAIN(045): [300/879] Batch: 0.1868 (0.1960) Data: 0.0104 (0.0167) Loss: 0.5931 (0.5460)
[2022/12/29 01:30] | TRAIN(045): [350/879] Batch: 0.1943 (0.1948) Data: 0.0096 (0.0158) Loss: 0.8300 (0.5499)
[2022/12/29 01:30] | TRAIN(045): [400/879] Batch: 0.1899 (0.1939) Data: 0.0109 (0.0152) Loss: 0.6241 (0.5498)
[2022/12/29 01:30] | TRAIN(045): [450/879] Batch: 0.1931 (0.1937) Data: 0.0108 (0.0147) Loss: 0.3672 (0.5496)
[2022/12/29 01:30] | TRAIN(045): [500/879] Batch: 0.1884 (0.1920) Data: 0.0108 (0.0142) Loss: 0.7718 (0.5510)
[2022/12/29 01:30] | TRAIN(045): [550/879] Batch: 0.1568 (0.1903) Data: 0.0080 (0.0139) Loss: 0.5947 (0.5512)
[2022/12/29 01:30] | TRAIN(045): [600/879] Batch: 0.2091 (0.1903) Data: 0.0120 (0.0136) Loss: 0.5032 (0.5502)
[2022/12/29 01:30] | TRAIN(045): [650/879] Batch: 0.1920 (0.1902) Data: 0.0117 (0.0134) Loss: 0.3338 (0.5529)
[2022/12/29 01:31] | TRAIN(045): [700/879] Batch: 0.1987 (0.1904) Data: 0.0105 (0.0132) Loss: 0.7320 (0.5521)
[2022/12/29 01:31] | TRAIN(045): [750/879] Batch: 0.1950 (0.1904) Data: 0.0096 (0.0130) Loss: 0.4812 (0.5521)
[2022/12/29 01:31] | TRAIN(045): [800/879] Batch: 0.1882 (0.1903) Data: 0.0094 (0.0128) Loss: 0.6055 (0.5525)
[2022/12/29 01:31] | TRAIN(045): [850/879] Batch: 0.2175 (0.1904) Data: 0.0114 (0.0127) Loss: 0.5690 (0.5515)
[2022/12/29 01:31] | ------------------------------------------------------------
[2022/12/29 01:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:31] | ------------------------------------------------------------
[2022/12/29 01:31] |    TRAIN(45)     0:02:47     0:00:11     0:02:36      0.5539
[2022/12/29 01:31] | ------------------------------------------------------------
[2022/12/29 01:31] | VALID(045): [ 50/220] Batch: 0.0605 (0.0825) Data: 0.0293 (0.0563) Loss: 0.5017 (0.5526)
[2022/12/29 01:31] | VALID(045): [100/220] Batch: 0.0608 (0.0695) Data: 0.0126 (0.0404) Loss: 0.8818 (0.5781)
[2022/12/29 01:31] | VALID(045): [150/220] Batch: 0.0582 (0.0649) Data: 0.0374 (0.0382) Loss: 0.4968 (0.5735)
[2022/12/29 01:31] | VALID(045): [200/220] Batch: 0.0526 (0.0623) Data: 0.0391 (0.0369) Loss: 0.2928 (0.5708)
[2022/12/29 01:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:31] |    VALID(45)      0.5691      0.8096      0.8741      0.8096      0.8096      0.8096      0.9524
[2022/12/29 01:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:31] | ####################################################################################################
[2022/12/29 01:32] | TRAIN(046): [ 50/879] Batch: 0.1811 (0.2276) Data: 0.0091 (0.0466) Loss: 0.5145 (0.5385)
[2022/12/29 01:32] | TRAIN(046): [100/879] Batch: 0.1917 (0.2091) Data: 0.0109 (0.0286) Loss: 0.2989 (0.5371)
[2022/12/29 01:32] | TRAIN(046): [150/879] Batch: 0.1920 (0.2033) Data: 0.0113 (0.0227) Loss: 0.5615 (0.5495)
[2022/12/29 01:32] | TRAIN(046): [200/879] Batch: 0.1929 (0.1971) Data: 0.0101 (0.0197) Loss: 0.5079 (0.5563)
[2022/12/29 01:32] | TRAIN(046): [250/879] Batch: 0.1591 (0.1933) Data: 0.0120 (0.0178) Loss: 0.6447 (0.5523)
[2022/12/29 01:32] | TRAIN(046): [300/879] Batch: 0.1963 (0.1925) Data: 0.0103 (0.0166) Loss: 0.7137 (0.5538)
[2022/12/29 01:33] | TRAIN(046): [350/879] Batch: 0.1891 (0.1916) Data: 0.0106 (0.0157) Loss: 0.2961 (0.5534)
[2022/12/29 01:33] | TRAIN(046): [400/879] Batch: 0.2027 (0.1917) Data: 0.0108 (0.0150) Loss: 0.4782 (0.5510)
[2022/12/29 01:33] | TRAIN(046): [450/879] Batch: 0.1918 (0.1917) Data: 0.0101 (0.0145) Loss: 0.3523 (0.5541)
[2022/12/29 01:33] | TRAIN(046): [500/879] Batch: 0.1808 (0.1917) Data: 0.0099 (0.0142) Loss: 0.6016 (0.5522)
[2022/12/29 01:33] | TRAIN(046): [550/879] Batch: 0.2056 (0.1919) Data: 0.0098 (0.0139) Loss: 0.5184 (0.5512)
[2022/12/29 01:33] | TRAIN(046): [600/879] Batch: 0.1835 (0.1919) Data: 0.0109 (0.0136) Loss: 0.4873 (0.5526)
[2022/12/29 01:33] | TRAIN(046): [650/879] Batch: 0.1975 (0.1919) Data: 0.0108 (0.0134) Loss: 0.3971 (0.5513)
[2022/12/29 01:34] | TRAIN(046): [700/879] Batch: 0.2023 (0.1917) Data: 0.0111 (0.0132) Loss: 0.6869 (0.5508)
[2022/12/29 01:34] | TRAIN(046): [750/879] Batch: 0.1981 (0.1919) Data: 0.0122 (0.0130) Loss: 0.3435 (0.5501)
[2022/12/29 01:34] | TRAIN(046): [800/879] Batch: 0.2008 (0.1919) Data: 0.0115 (0.0129) Loss: 0.7592 (0.5521)
[2022/12/29 01:34] | TRAIN(046): [850/879] Batch: 0.1971 (0.1917) Data: 0.0115 (0.0127) Loss: 0.4568 (0.5537)
[2022/12/29 01:34] | ------------------------------------------------------------
[2022/12/29 01:34] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:34] | ------------------------------------------------------------
[2022/12/29 01:34] |    TRAIN(46)     0:02:47     0:00:11     0:02:36      0.5534
[2022/12/29 01:34] | ------------------------------------------------------------
[2022/12/29 01:34] | VALID(046): [ 50/220] Batch: 0.0587 (0.0838) Data: 0.0347 (0.0598) Loss: 0.5065 (0.5381)
[2022/12/29 01:34] | VALID(046): [100/220] Batch: 0.0425 (0.0649) Data: 0.0109 (0.0398) Loss: 0.9724 (0.5701)
[2022/12/29 01:34] | VALID(046): [150/220] Batch: 0.0591 (0.0600) Data: 0.0250 (0.0329) Loss: 0.6206 (0.5679)
[2022/12/29 01:34] | VALID(046): [200/220] Batch: 0.0537 (0.0591) Data: 0.0103 (0.0307) Loss: 0.2918 (0.5727)
[2022/12/29 01:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:34] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:34] |    VALID(46)      0.5705      0.8164      0.8745      0.8164      0.8164      0.8164      0.9541
[2022/12/29 01:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:34] | ####################################################################################################
[2022/12/29 01:35] | TRAIN(047): [ 50/879] Batch: 0.1922 (0.2263) Data: 0.0118 (0.0487) Loss: 0.5099 (0.5392)
[2022/12/29 01:35] | TRAIN(047): [100/879] Batch: 0.1990 (0.2081) Data: 0.0096 (0.0299) Loss: 0.5261 (0.5433)
[2022/12/29 01:35] | TRAIN(047): [150/879] Batch: 0.1951 (0.2026) Data: 0.0118 (0.0236) Loss: 0.4963 (0.5438)
[2022/12/29 01:35] | TRAIN(047): [200/879] Batch: 0.1825 (0.1996) Data: 0.0113 (0.0203) Loss: 0.3566 (0.5516)
[2022/12/29 01:35] | TRAIN(047): [250/879] Batch: 0.1965 (0.1980) Data: 0.0114 (0.0184) Loss: 0.4468 (0.5489)
[2022/12/29 01:35] | TRAIN(047): [300/879] Batch: 0.1872 (0.1970) Data: 0.0112 (0.0171) Loss: 0.4804 (0.5460)
[2022/12/29 01:36] | TRAIN(047): [350/879] Batch: 0.1963 (0.1961) Data: 0.0108 (0.0161) Loss: 0.4002 (0.5455)
[2022/12/29 01:36] | TRAIN(047): [400/879] Batch: 0.1984 (0.1954) Data: 0.0109 (0.0154) Loss: 0.8378 (0.5522)
[2022/12/29 01:36] | TRAIN(047): [450/879] Batch: 0.1878 (0.1945) Data: 0.0097 (0.0148) Loss: 0.7050 (0.5489)
[2022/12/29 01:36] | TRAIN(047): [500/879] Batch: 0.1956 (0.1938) Data: 0.0096 (0.0143) Loss: 0.4223 (0.5469)
[2022/12/29 01:36] | TRAIN(047): [550/879] Batch: 0.1712 (0.1934) Data: 0.0111 (0.0140) Loss: 0.5932 (0.5519)
[2022/12/29 01:36] | TRAIN(047): [600/879] Batch: 0.1942 (0.1922) Data: 0.0090 (0.0137) Loss: 0.2417 (0.5498)
[2022/12/29 01:36] | TRAIN(047): [650/879] Batch: 0.1970 (0.1903) Data: 0.0092 (0.0134) Loss: 0.5839 (0.5488)
[2022/12/29 01:37] | TRAIN(047): [700/879] Batch: 0.1922 (0.1900) Data: 0.0097 (0.0132) Loss: 0.6436 (0.5486)
[2022/12/29 01:37] | TRAIN(047): [750/879] Batch: 0.1973 (0.1900) Data: 0.0092 (0.0130) Loss: 0.5205 (0.5476)
[2022/12/29 01:37] | TRAIN(047): [800/879] Batch: 0.1966 (0.1900) Data: 0.0107 (0.0128) Loss: 0.6976 (0.5495)
[2022/12/29 01:37] | TRAIN(047): [850/879] Batch: 0.1947 (0.1901) Data: 0.0095 (0.0127) Loss: 0.3989 (0.5486)
[2022/12/29 01:37] | ------------------------------------------------------------
[2022/12/29 01:37] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:37] | ------------------------------------------------------------
[2022/12/29 01:37] |    TRAIN(47)     0:02:47     0:00:11     0:02:36      0.5497
[2022/12/29 01:37] | ------------------------------------------------------------
[2022/12/29 01:37] | VALID(047): [ 50/220] Batch: 0.0575 (0.0823) Data: 0.0239 (0.0590) Loss: 0.4772 (0.5288)
[2022/12/29 01:37] | VALID(047): [100/220] Batch: 0.0449 (0.0693) Data: 0.0377 (0.0471) Loss: 0.9952 (0.5563)
[2022/12/29 01:37] | VALID(047): [150/220] Batch: 0.0595 (0.0651) Data: 0.0255 (0.0431) Loss: 0.3941 (0.5580)
[2022/12/29 01:37] | VALID(047): [200/220] Batch: 0.0592 (0.0629) Data: 0.0427 (0.0411) Loss: 0.2338 (0.5601)
[2022/12/29 01:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:37] |    VALID(47)      0.5589      0.8171      0.8796      0.8171      0.8171      0.8171      0.9543
[2022/12/29 01:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:37] | ####################################################################################################
[2022/12/29 01:38] | TRAIN(048): [ 50/879] Batch: 0.1938 (0.2269) Data: 0.0110 (0.0478) Loss: 0.6416 (0.5263)
[2022/12/29 01:38] | TRAIN(048): [100/879] Batch: 0.1957 (0.2095) Data: 0.0089 (0.0294) Loss: 0.4655 (0.5579)
[2022/12/29 01:38] | TRAIN(048): [150/879] Batch: 0.2081 (0.2033) Data: 0.0122 (0.0232) Loss: 0.6513 (0.5594)
[2022/12/29 01:38] | TRAIN(048): [200/879] Batch: 0.1927 (0.2004) Data: 0.0108 (0.0201) Loss: 0.4728 (0.5618)
[2022/12/29 01:38] | TRAIN(048): [250/879] Batch: 0.2009 (0.1985) Data: 0.0088 (0.0182) Loss: 0.7702 (0.5540)
[2022/12/29 01:38] | TRAIN(048): [300/879] Batch: 0.1875 (0.1969) Data: 0.0094 (0.0171) Loss: 0.3747 (0.5446)
[2022/12/29 01:39] | TRAIN(048): [350/879] Batch: 0.2051 (0.1957) Data: 0.0114 (0.0163) Loss: 0.7214 (0.5408)
[2022/12/29 01:39] | TRAIN(048): [400/879] Batch: 0.1819 (0.1950) Data: 0.0105 (0.0156) Loss: 0.8151 (0.5439)
[2022/12/29 01:39] | TRAIN(048): [450/879] Batch: 0.1909 (0.1946) Data: 0.0107 (0.0150) Loss: 0.3790 (0.5447)
[2022/12/29 01:39] | TRAIN(048): [500/879] Batch: 0.1989 (0.1944) Data: 0.0129 (0.0145) Loss: 0.3781 (0.5432)
[2022/12/29 01:39] | TRAIN(048): [550/879] Batch: 0.1895 (0.1938) Data: 0.0091 (0.0141) Loss: 0.6191 (0.5420)
[2022/12/29 01:39] | TRAIN(048): [600/879] Batch: 0.1872 (0.1933) Data: 0.0111 (0.0138) Loss: 0.5987 (0.5416)
[2022/12/29 01:40] | TRAIN(048): [650/879] Batch: 0.1886 (0.1933) Data: 0.0099 (0.0136) Loss: 0.4298 (0.5451)
[2022/12/29 01:40] | TRAIN(048): [700/879] Batch: 0.1996 (0.1930) Data: 0.0103 (0.0134) Loss: 0.4104 (0.5441)
[2022/12/29 01:40] | TRAIN(048): [750/879] Batch: 0.1882 (0.1927) Data: 0.0096 (0.0132) Loss: 0.3603 (0.5439)
[2022/12/29 01:40] | TRAIN(048): [800/879] Batch: 0.1853 (0.1922) Data: 0.0127 (0.0130) Loss: 0.6018 (0.5439)
[2022/12/29 01:40] | TRAIN(048): [850/879] Batch: 0.2023 (0.1922) Data: 0.0098 (0.0128) Loss: 0.5140 (0.5456)
[2022/12/29 01:40] | ------------------------------------------------------------
[2022/12/29 01:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:40] | ------------------------------------------------------------
[2022/12/29 01:40] |    TRAIN(48)     0:02:48     0:00:11     0:02:37      0.5462
[2022/12/29 01:40] | ------------------------------------------------------------
[2022/12/29 01:40] | VALID(048): [ 50/220] Batch: 0.0601 (0.0854) Data: 0.0281 (0.0630) Loss: 0.4077 (0.5317)
[2022/12/29 01:40] | VALID(048): [100/220] Batch: 0.0599 (0.0710) Data: 0.0294 (0.0446) Loss: 0.9342 (0.5633)
[2022/12/29 01:40] | VALID(048): [150/220] Batch: 0.0426 (0.0652) Data: 0.0278 (0.0371) Loss: 0.4417 (0.5639)
[2022/12/29 01:40] | VALID(048): [200/220] Batch: 0.0594 (0.0630) Data: 0.0309 (0.0340) Loss: 0.2381 (0.5674)
[2022/12/29 01:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:40] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:40] |    VALID(48)      0.5645      0.8167      0.8748      0.8167      0.8167      0.8167      0.9542
[2022/12/29 01:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:40] | ####################################################################################################
[2022/12/29 01:41] | TRAIN(049): [ 50/879] Batch: 0.1647 (0.2227) Data: 0.0109 (0.0496) Loss: 0.4917 (0.5527)
[2022/12/29 01:41] | TRAIN(049): [100/879] Batch: 0.1912 (0.2057) Data: 0.0111 (0.0304) Loss: 0.6721 (0.5397)
[2022/12/29 01:41] | TRAIN(049): [150/879] Batch: 0.1920 (0.2005) Data: 0.0091 (0.0236) Loss: 0.5263 (0.5335)
[2022/12/29 01:41] | TRAIN(049): [200/879] Batch: 0.1849 (0.1974) Data: 0.0103 (0.0203) Loss: 0.4738 (0.5338)
[2022/12/29 01:41] | TRAIN(049): [250/879] Batch: 0.1942 (0.1954) Data: 0.0093 (0.0183) Loss: 0.6672 (0.5327)
[2022/12/29 01:41] | TRAIN(049): [300/879] Batch: 0.1990 (0.1944) Data: 0.0101 (0.0170) Loss: 0.7604 (0.5274)
[2022/12/29 01:42] | TRAIN(049): [350/879] Batch: 0.2024 (0.1936) Data: 0.0090 (0.0160) Loss: 0.4599 (0.5247)
[2022/12/29 01:42] | TRAIN(049): [400/879] Batch: 0.1880 (0.1931) Data: 0.0113 (0.0153) Loss: 1.0265 (0.5287)
[2022/12/29 01:42] | TRAIN(049): [450/879] Batch: 0.1870 (0.1927) Data: 0.0085 (0.0148) Loss: 0.4035 (0.5311)
[2022/12/29 01:42] | TRAIN(049): [500/879] Batch: 0.1861 (0.1925) Data: 0.0097 (0.0144) Loss: 0.5744 (0.5339)
[2022/12/29 01:42] | TRAIN(049): [550/879] Batch: 0.1788 (0.1928) Data: 0.0105 (0.0141) Loss: 0.3625 (0.5356)
[2022/12/29 01:42] | TRAIN(049): [600/879] Batch: 0.1981 (0.1927) Data: 0.0087 (0.0137) Loss: 0.5853 (0.5395)
[2022/12/29 01:43] | TRAIN(049): [650/879] Batch: 0.1845 (0.1921) Data: 0.0093 (0.0134) Loss: 0.4845 (0.5400)
[2022/12/29 01:43] | TRAIN(049): [700/879] Batch: 0.1854 (0.1913) Data: 0.0090 (0.0132) Loss: 0.5736 (0.5405)
[2022/12/29 01:43] | TRAIN(049): [750/879] Batch: 0.1956 (0.1897) Data: 0.0116 (0.0130) Loss: 0.8238 (0.5394)
[2022/12/29 01:43] | TRAIN(049): [800/879] Batch: 0.1975 (0.1899) Data: 0.0089 (0.0128) Loss: 0.4847 (0.5402)
[2022/12/29 01:43] | TRAIN(049): [850/879] Batch: 0.1857 (0.1901) Data: 0.0097 (0.0127) Loss: 0.3692 (0.5406)
[2022/12/29 01:43] | ------------------------------------------------------------
[2022/12/29 01:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:43] | ------------------------------------------------------------
[2022/12/29 01:43] |    TRAIN(49)     0:02:46     0:00:11     0:02:35      0.5401
[2022/12/29 01:43] | ------------------------------------------------------------
[2022/12/29 01:43] | VALID(049): [ 50/220] Batch: 0.0444 (0.0814) Data: 0.0376 (0.0580) Loss: 0.4296 (0.5410)
[2022/12/29 01:43] | VALID(049): [100/220] Batch: 0.0599 (0.0673) Data: 0.0372 (0.0447) Loss: 0.9452 (0.5724)
[2022/12/29 01:43] | VALID(049): [150/220] Batch: 0.0509 (0.0629) Data: 0.0378 (0.0406) Loss: 0.4894 (0.5680)
[2022/12/29 01:43] | VALID(049): [200/220] Batch: 0.0593 (0.0611) Data: 0.0337 (0.0389) Loss: 0.1852 (0.5747)
[2022/12/29 01:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:43] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:43] |    VALID(49)      0.5710      0.8178      0.8716      0.8178      0.8178      0.8178      0.9545
[2022/12/29 01:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:43] | ####################################################################################################
[2022/12/29 01:44] | TRAIN(050): [ 50/879] Batch: 0.1948 (0.2325) Data: 0.0095 (0.0491) Loss: 0.2525 (0.5140)
[2022/12/29 01:44] | TRAIN(050): [100/879] Batch: 0.1906 (0.2132) Data: 0.0091 (0.0301) Loss: 0.7431 (0.5462)
[2022/12/29 01:44] | TRAIN(050): [150/879] Batch: 0.1875 (0.2070) Data: 0.0104 (0.0238) Loss: 0.2932 (0.5361)
[2022/12/29 01:44] | TRAIN(050): [200/879] Batch: 0.1879 (0.2026) Data: 0.0097 (0.0206) Loss: 0.5905 (0.5312)
[2022/12/29 01:44] | TRAIN(050): [250/879] Batch: 0.1910 (0.2001) Data: 0.0089 (0.0186) Loss: 0.6342 (0.5401)
[2022/12/29 01:44] | TRAIN(050): [300/879] Batch: 0.2009 (0.1988) Data: 0.0101 (0.0173) Loss: 0.4732 (0.5455)
[2022/12/29 01:45] | TRAIN(050): [350/879] Batch: 0.1989 (0.1974) Data: 0.0094 (0.0163) Loss: 0.6394 (0.5479)
[2022/12/29 01:45] | TRAIN(050): [400/879] Batch: 0.1884 (0.1953) Data: 0.0093 (0.0156) Loss: 0.2802 (0.5481)
[2022/12/29 01:45] | TRAIN(050): [450/879] Batch: 0.1988 (0.1929) Data: 0.0089 (0.0151) Loss: 0.3508 (0.5450)
[2022/12/29 01:45] | TRAIN(050): [500/879] Batch: 0.1895 (0.1925) Data: 0.0095 (0.0146) Loss: 0.4038 (0.5446)
[2022/12/29 01:45] | TRAIN(050): [550/879] Batch: 0.1775 (0.1921) Data: 0.0103 (0.0142) Loss: 0.3219 (0.5405)
[2022/12/29 01:45] | TRAIN(050): [600/879] Batch: 0.1924 (0.1917) Data: 0.0114 (0.0138) Loss: 0.7287 (0.5416)
[2022/12/29 01:46] | TRAIN(050): [650/879] Batch: 0.1851 (0.1912) Data: 0.0108 (0.0136) Loss: 0.5311 (0.5409)
[2022/12/29 01:46] | TRAIN(050): [700/879] Batch: 0.1949 (0.1912) Data: 0.0097 (0.0134) Loss: 0.6375 (0.5396)
[2022/12/29 01:46] | TRAIN(050): [750/879] Batch: 0.1801 (0.1907) Data: 0.0110 (0.0132) Loss: 0.3751 (0.5384)
[2022/12/29 01:46] | TRAIN(050): [800/879] Batch: 0.1742 (0.1905) Data: 0.0109 (0.0130) Loss: 0.2327 (0.5352)
[2022/12/29 01:46] | TRAIN(050): [850/879] Batch: 0.1873 (0.1902) Data: 0.0098 (0.0128) Loss: 0.4697 (0.5354)
[2022/12/29 01:46] | ------------------------------------------------------------
[2022/12/29 01:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:46] | ------------------------------------------------------------
[2022/12/29 01:46] |    TRAIN(50)     0:02:46     0:00:11     0:02:35      0.5365
[2022/12/29 01:46] | ------------------------------------------------------------
[2022/12/29 01:46] | VALID(050): [ 50/220] Batch: 0.0588 (0.0842) Data: 0.0401 (0.0610) Loss: 0.3902 (0.5434)
[2022/12/29 01:46] | VALID(050): [100/220] Batch: 0.0466 (0.0675) Data: 0.0323 (0.0448) Loss: 0.9252 (0.5682)
[2022/12/29 01:46] | VALID(050): [150/220] Batch: 0.0539 (0.0627) Data: 0.0382 (0.0403) Loss: 0.4998 (0.5647)
[2022/12/29 01:46] | VALID(050): [200/220] Batch: 0.0587 (0.0607) Data: 0.0352 (0.0380) Loss: 0.1917 (0.5706)
[2022/12/29 01:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:46] |    VALID(50)      0.5670      0.8187      0.8745      0.8187      0.8187      0.8187      0.9547
[2022/12/29 01:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:46] | ####################################################################################################
[2022/12/29 01:47] | TRAIN(051): [ 50/879] Batch: 0.2028 (0.2333) Data: 0.0100 (0.0510) Loss: 0.2756 (0.5196)
[2022/12/29 01:47] | TRAIN(051): [100/879] Batch: 0.1431 (0.2131) Data: 0.0091 (0.0314) Loss: 0.7537 (0.5381)
[2022/12/29 01:47] | TRAIN(051): [150/879] Batch: 0.1830 (0.2014) Data: 0.0158 (0.0245) Loss: 0.6057 (0.5448)
[2022/12/29 01:47] | TRAIN(051): [200/879] Batch: 0.1847 (0.1962) Data: 0.0099 (0.0210) Loss: 0.4405 (0.5486)
[2022/12/29 01:47] | TRAIN(051): [250/879] Batch: 0.1787 (0.1940) Data: 0.0176 (0.0190) Loss: 0.9222 (0.5395)
[2022/12/29 01:47] | TRAIN(051): [300/879] Batch: 0.1785 (0.1933) Data: 0.0163 (0.0176) Loss: 0.7071 (0.5353)
[2022/12/29 01:48] | TRAIN(051): [350/879] Batch: 0.1996 (0.1935) Data: 0.0098 (0.0165) Loss: 0.6312 (0.5353)
[2022/12/29 01:48] | TRAIN(051): [400/879] Batch: 0.1846 (0.1933) Data: 0.0105 (0.0158) Loss: 0.3565 (0.5353)
[2022/12/29 01:48] | TRAIN(051): [450/879] Batch: 0.1802 (0.1933) Data: 0.0094 (0.0152) Loss: 0.5803 (0.5363)
[2022/12/29 01:48] | TRAIN(051): [500/879] Batch: 0.1981 (0.1924) Data: 0.0111 (0.0147) Loss: 0.3424 (0.5359)
[2022/12/29 01:48] | TRAIN(051): [550/879] Batch: 0.1867 (0.1922) Data: 0.0103 (0.0143) Loss: 0.7443 (0.5342)
[2022/12/29 01:48] | TRAIN(051): [600/879] Batch: 0.1921 (0.1919) Data: 0.0095 (0.0139) Loss: 0.5577 (0.5350)
[2022/12/29 01:49] | TRAIN(051): [650/879] Batch: 0.1979 (0.1917) Data: 0.0108 (0.0137) Loss: 0.4828 (0.5319)
[2022/12/29 01:49] | TRAIN(051): [700/879] Batch: 0.1934 (0.1914) Data: 0.0110 (0.0135) Loss: 0.5771 (0.5315)
[2022/12/29 01:49] | TRAIN(051): [750/879] Batch: 0.1934 (0.1914) Data: 0.0085 (0.0133) Loss: 0.6820 (0.5322)
[2022/12/29 01:49] | TRAIN(051): [800/879] Batch: 0.1846 (0.1907) Data: 0.0128 (0.0131) Loss: 0.6465 (0.5312)
[2022/12/29 01:49] | TRAIN(051): [850/879] Batch: 0.1617 (0.1901) Data: 0.0118 (0.0129) Loss: 0.5930 (0.5337)
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] |    TRAIN(51)     0:02:46     0:00:11     0:02:35      0.5340
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] | VALID(051): [ 50/220] Batch: 0.0472 (0.0820) Data: 0.0279 (0.0566) Loss: 0.4186 (0.5336)
[2022/12/29 01:49] | VALID(051): [100/220] Batch: 0.0572 (0.0680) Data: 0.0163 (0.0439) Loss: 0.9206 (0.5641)
[2022/12/29 01:49] | VALID(051): [150/220] Batch: 0.0584 (0.0634) Data: 0.0214 (0.0384) Loss: 0.5488 (0.5640)
[2022/12/29 01:49] | VALID(051): [200/220] Batch: 0.0587 (0.0614) Data: 0.0319 (0.0347) Loss: 0.2461 (0.5721)
[2022/12/29 01:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:50] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:50] |    VALID(51)      0.5708      0.8174      0.8736      0.8174      0.8174      0.8174      0.9543
[2022/12/29 01:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:50] | ####################################################################################################
[2022/12/29 01:50] | TRAIN(052): [ 50/879] Batch: 0.1900 (0.2302) Data: 0.0117 (0.0488) Loss: 0.5015 (0.5312)
[2022/12/29 01:50] | TRAIN(052): [100/879] Batch: 0.1905 (0.2097) Data: 0.0100 (0.0300) Loss: 0.8426 (0.5421)
[2022/12/29 01:50] | TRAIN(052): [150/879] Batch: 0.1973 (0.2037) Data: 0.0105 (0.0236) Loss: 0.3144 (0.5323)
[2022/12/29 01:50] | TRAIN(052): [200/879] Batch: 0.1842 (0.1996) Data: 0.0109 (0.0204) Loss: 0.4639 (0.5283)
[2022/12/29 01:50] | TRAIN(052): [250/879] Batch: 0.1914 (0.1982) Data: 0.0111 (0.0185) Loss: 0.6262 (0.5254)
[2022/12/29 01:50] | TRAIN(052): [300/879] Batch: 0.1920 (0.1966) Data: 0.0093 (0.0171) Loss: 0.9982 (0.5254)
[2022/12/29 01:51] | TRAIN(052): [350/879] Batch: 0.1909 (0.1953) Data: 0.0117 (0.0162) Loss: 0.5947 (0.5248)
[2022/12/29 01:51] | TRAIN(052): [400/879] Batch: 0.1922 (0.1948) Data: 0.0090 (0.0155) Loss: 0.3961 (0.5261)
[2022/12/29 01:51] | TRAIN(052): [450/879] Batch: 0.1933 (0.1937) Data: 0.0095 (0.0149) Loss: 0.6009 (0.5301)
[2022/12/29 01:51] | TRAIN(052): [500/879] Batch: 0.1252 (0.1928) Data: 0.0101 (0.0145) Loss: 0.5590 (0.5288)
[2022/12/29 01:51] | TRAIN(052): [550/879] Batch: 0.1828 (0.1913) Data: 0.0111 (0.0141) Loss: 0.5579 (0.5285)
[2022/12/29 01:51] | TRAIN(052): [600/879] Batch: 0.1884 (0.1899) Data: 0.0113 (0.0138) Loss: 0.6223 (0.5300)
[2022/12/29 01:52] | TRAIN(052): [650/879] Batch: 0.1937 (0.1895) Data: 0.0098 (0.0135) Loss: 0.5567 (0.5301)
[2022/12/29 01:52] | TRAIN(052): [700/879] Batch: 0.1813 (0.1893) Data: 0.0109 (0.0133) Loss: 0.4888 (0.5289)
[2022/12/29 01:52] | TRAIN(052): [750/879] Batch: 0.1887 (0.1891) Data: 0.0092 (0.0131) Loss: 0.3922 (0.5291)
[2022/12/29 01:52] | TRAIN(052): [800/879] Batch: 0.1771 (0.1887) Data: 0.0100 (0.0129) Loss: 0.6143 (0.5273)
[2022/12/29 01:52] | TRAIN(052): [850/879] Batch: 0.1855 (0.1886) Data: 0.0112 (0.0128) Loss: 0.3822 (0.5268)
[2022/12/29 01:52] | ------------------------------------------------------------
[2022/12/29 01:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:52] | ------------------------------------------------------------
[2022/12/29 01:52] |    TRAIN(52)     0:02:45     0:00:11     0:02:34      0.5265
[2022/12/29 01:52] | ------------------------------------------------------------
[2022/12/29 01:52] | VALID(052): [ 50/220] Batch: 0.0600 (0.0804) Data: 0.0167 (0.0513) Loss: 0.4469 (0.5429)
[2022/12/29 01:52] | VALID(052): [100/220] Batch: 0.0572 (0.0679) Data: 0.0276 (0.0418) Loss: 1.0016 (0.5688)
[2022/12/29 01:52] | VALID(052): [150/220] Batch: 0.0511 (0.0635) Data: 0.0367 (0.0386) Loss: 0.5274 (0.5663)
[2022/12/29 01:52] | VALID(052): [200/220] Batch: 0.0593 (0.0609) Data: 0.0335 (0.0369) Loss: 0.2142 (0.5702)
[2022/12/29 01:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:52] |    VALID(52)      0.5678      0.8175      0.8761      0.8175      0.8175      0.8175      0.9544
[2022/12/29 01:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:52] | ####################################################################################################
[2022/12/29 01:53] | TRAIN(053): [ 50/879] Batch: 0.1845 (0.2231) Data: 0.0122 (0.0471) Loss: 0.4437 (0.5139)
[2022/12/29 01:53] | TRAIN(053): [100/879] Batch: 0.1866 (0.2083) Data: 0.0121 (0.0292) Loss: 0.4990 (0.5181)
[2022/12/29 01:53] | TRAIN(053): [150/879] Batch: 0.1920 (0.2034) Data: 0.0109 (0.0231) Loss: 0.9191 (0.5147)
[2022/12/29 01:53] | TRAIN(053): [200/879] Batch: 0.2026 (0.2008) Data: 0.0122 (0.0199) Loss: 0.6020 (0.5144)
[2022/12/29 01:53] | TRAIN(053): [250/879] Batch: 0.1836 (0.1960) Data: 0.0086 (0.0180) Loss: 0.5580 (0.5172)
[2022/12/29 01:53] | TRAIN(053): [300/879] Batch: 0.1225 (0.1918) Data: 0.0108 (0.0166) Loss: 0.6341 (0.5209)
[2022/12/29 01:54] | TRAIN(053): [350/879] Batch: 0.1895 (0.1920) Data: 0.0111 (0.0158) Loss: 0.5665 (0.5231)
[2022/12/29 01:54] | TRAIN(053): [400/879] Batch: 0.1972 (0.1915) Data: 0.0102 (0.0151) Loss: 0.4522 (0.5217)
[2022/12/29 01:54] | TRAIN(053): [450/879] Batch: 0.1953 (0.1916) Data: 0.0097 (0.0146) Loss: 0.5529 (0.5189)
[2022/12/29 01:54] | TRAIN(053): [500/879] Batch: 0.1934 (0.1917) Data: 0.0115 (0.0141) Loss: 0.5366 (0.5204)
[2022/12/29 01:54] | TRAIN(053): [550/879] Batch: 0.1907 (0.1917) Data: 0.0118 (0.0138) Loss: 0.6335 (0.5219)
[2022/12/29 01:54] | TRAIN(053): [600/879] Batch: 0.1888 (0.1915) Data: 0.0112 (0.0135) Loss: 0.4243 (0.5234)
[2022/12/29 01:55] | TRAIN(053): [650/879] Batch: 0.1928 (0.1913) Data: 0.0108 (0.0133) Loss: 0.3752 (0.5220)
[2022/12/29 01:55] | TRAIN(053): [700/879] Batch: 0.1956 (0.1913) Data: 0.0094 (0.0131) Loss: 0.8935 (0.5225)
[2022/12/29 01:55] | TRAIN(053): [750/879] Batch: 0.1869 (0.1914) Data: 0.0099 (0.0129) Loss: 0.5284 (0.5235)
[2022/12/29 01:55] | TRAIN(053): [800/879] Batch: 0.1946 (0.1914) Data: 0.0107 (0.0128) Loss: 0.3497 (0.5229)
[2022/12/29 01:55] | TRAIN(053): [850/879] Batch: 0.1798 (0.1914) Data: 0.0098 (0.0126) Loss: 0.3140 (0.5239)
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] |    TRAIN(53)     0:02:48     0:00:11     0:02:37      0.5252
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] | VALID(053): [ 50/220] Batch: 0.0440 (0.0770) Data: 0.0116 (0.0529) Loss: 0.4953 (0.5486)
[2022/12/29 01:55] | VALID(053): [100/220] Batch: 0.0531 (0.0617) Data: 0.0308 (0.0364) Loss: 0.9908 (0.5838)
[2022/12/29 01:55] | VALID(053): [150/220] Batch: 0.0605 (0.0596) Data: 0.0315 (0.0351) Loss: 0.5295 (0.5788)
[2022/12/29 01:55] | VALID(053): [200/220] Batch: 0.0572 (0.0587) Data: 0.0244 (0.0323) Loss: 0.2646 (0.5814)
[2022/12/29 01:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:56] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:56] |    VALID(53)      0.5764      0.8113      0.8739      0.8113      0.8113      0.8113      0.9528
[2022/12/29 01:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:56] | ####################################################################################################
[2022/12/29 01:56] | TRAIN(054): [ 50/879] Batch: 0.1791 (0.2189) Data: 0.0100 (0.0487) Loss: 0.4657 (0.5149)
[2022/12/29 01:56] | TRAIN(054): [100/879] Batch: 0.1986 (0.2062) Data: 0.0093 (0.0299) Loss: 0.6811 (0.5327)
[2022/12/29 01:56] | TRAIN(054): [150/879] Batch: 0.1841 (0.2015) Data: 0.0118 (0.0236) Loss: 0.3819 (0.5174)
[2022/12/29 01:56] | TRAIN(054): [200/879] Batch: 0.1798 (0.1972) Data: 0.0108 (0.0203) Loss: 0.3838 (0.5143)
[2022/12/29 01:56] | TRAIN(054): [250/879] Batch: 0.1777 (0.1946) Data: 0.0100 (0.0184) Loss: 0.5785 (0.5171)
[2022/12/29 01:56] | TRAIN(054): [300/879] Batch: 0.2016 (0.1946) Data: 0.0091 (0.0171) Loss: 0.5580 (0.5181)
[2022/12/29 01:57] | TRAIN(054): [350/879] Batch: 0.1804 (0.1951) Data: 0.0123 (0.0163) Loss: 0.5820 (0.5134)
[2022/12/29 01:57] | TRAIN(054): [400/879] Batch: 0.1874 (0.1944) Data: 0.0097 (0.0155) Loss: 0.3992 (0.5197)
[2022/12/29 01:57] | TRAIN(054): [450/879] Batch: 0.1851 (0.1940) Data: 0.0109 (0.0149) Loss: 0.6363 (0.5136)
[2022/12/29 01:57] | TRAIN(054): [500/879] Batch: 0.1958 (0.1939) Data: 0.0110 (0.0145) Loss: 0.2957 (0.5120)
[2022/12/29 01:57] | TRAIN(054): [550/879] Batch: 0.1928 (0.1935) Data: 0.0099 (0.0141) Loss: 0.6164 (0.5093)
[2022/12/29 01:57] | TRAIN(054): [600/879] Batch: 0.2032 (0.1932) Data: 0.0093 (0.0138) Loss: 0.4826 (0.5116)
[2022/12/29 01:58] | TRAIN(054): [650/879] Batch: 0.1909 (0.1919) Data: 0.0116 (0.0135) Loss: 0.4226 (0.5096)
[2022/12/29 01:58] | TRAIN(054): [700/879] Batch: 0.2015 (0.1912) Data: 0.0109 (0.0134) Loss: 0.4739 (0.5125)
[2022/12/29 01:58] | TRAIN(054): [750/879] Batch: 0.1750 (0.1913) Data: 0.0109 (0.0132) Loss: 0.7010 (0.5116)
[2022/12/29 01:58] | TRAIN(054): [800/879] Batch: 0.1917 (0.1911) Data: 0.0106 (0.0130) Loss: 0.5546 (0.5115)
[2022/12/29 01:58] | TRAIN(054): [850/879] Batch: 0.1956 (0.1911) Data: 0.0119 (0.0128) Loss: 0.7995 (0.5135)
[2022/12/29 01:58] | ------------------------------------------------------------
[2022/12/29 01:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:58] | ------------------------------------------------------------
[2022/12/29 01:58] |    TRAIN(54)     0:02:47     0:00:11     0:02:36      0.5140
[2022/12/29 01:58] | ------------------------------------------------------------
[2022/12/29 01:58] | VALID(054): [ 50/220] Batch: 0.0590 (0.0829) Data: 0.0293 (0.0574) Loss: 0.4689 (0.5445)
[2022/12/29 01:58] | VALID(054): [100/220] Batch: 0.0590 (0.0699) Data: 0.0384 (0.0463) Loss: 0.8756 (0.5742)
[2022/12/29 01:58] | VALID(054): [150/220] Batch: 0.0481 (0.0654) Data: 0.0384 (0.0426) Loss: 0.5248 (0.5704)
[2022/12/29 01:59] | VALID(054): [200/220] Batch: 0.0597 (0.0630) Data: 0.0403 (0.0405) Loss: 0.1894 (0.5748)
[2022/12/29 01:59] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:59] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:59] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:59] |    VALID(54)      0.5742      0.8195      0.8743      0.8195      0.8195      0.8195      0.9549
[2022/12/29 01:59] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:59] | ####################################################################################################
[2022/12/29 01:59] | TRAIN(055): [ 50/879] Batch: 0.1746 (0.2271) Data: 0.0106 (0.0495) Loss: 0.5816 (0.4929)
[2022/12/29 01:59] | TRAIN(055): [100/879] Batch: 0.1893 (0.2099) Data: 0.0109 (0.0302) Loss: 0.3595 (0.4920)
[2022/12/29 01:59] | TRAIN(055): [150/879] Batch: 0.1862 (0.2040) Data: 0.0119 (0.0237) Loss: 0.6392 (0.5124)
[2022/12/29 01:59] | TRAIN(055): [200/879] Batch: 0.1826 (0.2006) Data: 0.0108 (0.0204) Loss: 0.4451 (0.5125)
[2022/12/29 01:59] | TRAIN(055): [250/879] Batch: 0.1738 (0.1985) Data: 0.0107 (0.0185) Loss: 0.8055 (0.5144)
[2022/12/29 02:00] | TRAIN(055): [300/879] Batch: 0.1901 (0.1972) Data: 0.0095 (0.0172) Loss: 0.6268 (0.5167)
[2022/12/29 02:00] | TRAIN(055): [350/879] Batch: 0.1869 (0.1949) Data: 0.0101 (0.0162) Loss: 0.4709 (0.5148)
[2022/12/29 02:00] | TRAIN(055): [400/879] Batch: 0.1939 (0.1925) Data: 0.0093 (0.0155) Loss: 0.4904 (0.5188)
[2022/12/29 02:00] | TRAIN(055): [450/879] Batch: 0.1888 (0.1924) Data: 0.0091 (0.0150) Loss: 0.4200 (0.5160)
[2022/12/29 02:00] | TRAIN(055): [500/879] Batch: 0.1937 (0.1922) Data: 0.0101 (0.0145) Loss: 0.5971 (0.5195)
[2022/12/29 02:00] | TRAIN(055): [550/879] Batch: 0.2078 (0.1922) Data: 0.0089 (0.0142) Loss: 0.4476 (0.5206)
[2022/12/29 02:00] | TRAIN(055): [600/879] Batch: 0.1997 (0.1921) Data: 0.0093 (0.0139) Loss: 0.4414 (0.5216)
[2022/12/29 02:01] | TRAIN(055): [650/879] Batch: 0.1980 (0.1924) Data: 0.0090 (0.0137) Loss: 0.8556 (0.5220)
[2022/12/29 02:01] | TRAIN(055): [700/879] Batch: 0.1976 (0.1923) Data: 0.0107 (0.0134) Loss: 0.5090 (0.5194)
[2022/12/29 02:01] | TRAIN(055): [750/879] Batch: 0.1896 (0.1922) Data: 0.0109 (0.0133) Loss: 0.3974 (0.5186)
[2022/12/29 02:01] | TRAIN(055): [800/879] Batch: 0.2052 (0.1922) Data: 0.0127 (0.0131) Loss: 0.4186 (0.5178)
[2022/12/29 02:01] | TRAIN(055): [850/879] Batch: 0.1889 (0.1921) Data: 0.0116 (0.0129) Loss: 0.3659 (0.5167)
[2022/12/29 02:01] | ------------------------------------------------------------
[2022/12/29 02:01] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:01] | ------------------------------------------------------------
[2022/12/29 02:01] |    TRAIN(55)     0:02:48     0:00:11     0:02:37      0.5168
[2022/12/29 02:01] | ------------------------------------------------------------
[2022/12/29 02:01] | VALID(055): [ 50/220] Batch: 0.0598 (0.0845) Data: 0.0350 (0.0623) Loss: 0.4950 (0.5477)
[2022/12/29 02:01] | VALID(055): [100/220] Batch: 0.0416 (0.0699) Data: 0.0374 (0.0481) Loss: 1.1227 (0.5813)
[2022/12/29 02:02] | VALID(055): [150/220] Batch: 0.0374 (0.0646) Data: 0.0383 (0.0428) Loss: 0.5145 (0.5845)
[2022/12/29 02:02] | VALID(055): [200/220] Batch: 0.0397 (0.0617) Data: 0.0380 (0.0399) Loss: 0.1683 (0.5854)
[2022/12/29 02:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:02] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:02] |    VALID(55)      0.5829      0.8158      0.8750      0.8158      0.8158      0.8158      0.9540
[2022/12/29 02:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:02] | ####################################################################################################
[2022/12/29 02:02] | TRAIN(056): [ 50/879] Batch: 0.1906 (0.2175) Data: 0.0170 (0.0495) Loss: 0.4305 (0.5111)
[2022/12/29 02:02] | TRAIN(056): [100/879] Batch: 0.1964 (0.1970) Data: 0.0088 (0.0307) Loss: 0.4166 (0.5072)
[2022/12/29 02:02] | TRAIN(056): [150/879] Batch: 0.1944 (0.1956) Data: 0.0092 (0.0240) Loss: 0.6046 (0.5030)
[2022/12/29 02:02] | TRAIN(056): [200/879] Batch: 0.1789 (0.1936) Data: 0.0108 (0.0206) Loss: 0.4415 (0.5120)
[2022/12/29 02:02] | TRAIN(056): [250/879] Batch: 0.1924 (0.1922) Data: 0.0109 (0.0186) Loss: 0.4951 (0.5085)
[2022/12/29 02:03] | TRAIN(056): [300/879] Batch: 0.1932 (0.1913) Data: 0.0095 (0.0172) Loss: 0.5883 (0.5149)
[2022/12/29 02:03] | TRAIN(056): [350/879] Batch: 0.2016 (0.1914) Data: 0.0108 (0.0162) Loss: 0.2356 (0.5134)
[2022/12/29 02:03] | TRAIN(056): [400/879] Batch: 0.1864 (0.1915) Data: 0.0107 (0.0156) Loss: 0.6666 (0.5115)
[2022/12/29 02:03] | TRAIN(056): [450/879] Batch: 0.1915 (0.1914) Data: 0.0113 (0.0150) Loss: 0.4598 (0.5121)
[2022/12/29 02:03] | TRAIN(056): [500/879] Batch: 0.1975 (0.1910) Data: 0.0093 (0.0145) Loss: 0.9231 (0.5099)
[2022/12/29 02:03] | TRAIN(056): [550/879] Batch: 0.1885 (0.1911) Data: 0.0108 (0.0141) Loss: 0.7443 (0.5132)
[2022/12/29 02:04] | TRAIN(056): [600/879] Batch: 0.2017 (0.1914) Data: 0.0134 (0.0138) Loss: 0.2887 (0.5115)
[2022/12/29 02:04] | TRAIN(056): [650/879] Batch: 0.1980 (0.1916) Data: 0.0094 (0.0136) Loss: 0.3054 (0.5105)
[2022/12/29 02:04] | TRAIN(056): [700/879] Batch: 0.1427 (0.1911) Data: 0.0090 (0.0134) Loss: 0.6465 (0.5117)
[2022/12/29 02:04] | TRAIN(056): [750/879] Batch: 0.1926 (0.1905) Data: 0.0110 (0.0132) Loss: 0.4502 (0.5117)
[2022/12/29 02:04] | TRAIN(056): [800/879] Batch: 0.2001 (0.1894) Data: 0.0103 (0.0130) Loss: 0.3582 (0.5140)
[2022/12/29 02:04] | TRAIN(056): [850/879] Batch: 0.1925 (0.1895) Data: 0.0194 (0.0129) Loss: 0.4565 (0.5131)
[2022/12/29 02:04] | ------------------------------------------------------------
[2022/12/29 02:04] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:04] | ------------------------------------------------------------
[2022/12/29 02:04] |    TRAIN(56)     0:02:46     0:00:11     0:02:35      0.5132
[2022/12/29 02:04] | ------------------------------------------------------------
[2022/12/29 02:04] | VALID(056): [ 50/220] Batch: 0.0634 (0.0821) Data: 0.0339 (0.0597) Loss: 0.5294 (0.5441)
[2022/12/29 02:04] | VALID(056): [100/220] Batch: 0.0587 (0.0688) Data: 0.0320 (0.0469) Loss: 0.9998 (0.5748)
[2022/12/29 02:05] | VALID(056): [150/220] Batch: 0.0597 (0.0647) Data: 0.0397 (0.0429) Loss: 0.5305 (0.5755)
[2022/12/29 02:05] | VALID(056): [200/220] Batch: 0.0521 (0.0627) Data: 0.0291 (0.0394) Loss: 0.2128 (0.5780)
[2022/12/29 02:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:05] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:05] |    VALID(56)      0.5766      0.8141      0.8741      0.8141      0.8141      0.8141      0.9535
[2022/12/29 02:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:05] | ####################################################################################################
[2022/12/29 02:05] | TRAIN(057): [ 50/879] Batch: 0.1998 (0.2327) Data: 0.0087 (0.0499) Loss: 0.3918 (0.4661)
[2022/12/29 02:05] | TRAIN(057): [100/879] Batch: 0.1871 (0.2104) Data: 0.0113 (0.0303) Loss: 0.6993 (0.4961)
[2022/12/29 02:05] | TRAIN(057): [150/879] Batch: 0.1947 (0.2049) Data: 0.0102 (0.0239) Loss: 0.4611 (0.4911)
[2022/12/29 02:05] | TRAIN(057): [200/879] Batch: 0.1993 (0.2013) Data: 0.0104 (0.0206) Loss: 0.5225 (0.5021)
[2022/12/29 02:05] | TRAIN(057): [250/879] Batch: 0.1941 (0.2001) Data: 0.0131 (0.0187) Loss: 0.5435 (0.5042)
[2022/12/29 02:06] | TRAIN(057): [300/879] Batch: 0.2171 (0.1989) Data: 0.0116 (0.0174) Loss: 0.2901 (0.5095)
[2022/12/29 02:06] | TRAIN(057): [350/879] Batch: 0.2008 (0.1977) Data: 0.0107 (0.0164) Loss: 0.4899 (0.5071)
[2022/12/29 02:06] | TRAIN(057): [400/879] Batch: 0.1673 (0.1965) Data: 0.0122 (0.0157) Loss: 0.4620 (0.5089)
[2022/12/29 02:06] | TRAIN(057): [450/879] Batch: 0.1929 (0.1955) Data: 0.0096 (0.0152) Loss: 0.4962 (0.5101)
[2022/12/29 02:06] | TRAIN(057): [500/879] Batch: 0.1873 (0.1942) Data: 0.0108 (0.0147) Loss: 0.6029 (0.5085)
[2022/12/29 02:06] | TRAIN(057): [550/879] Batch: 0.1954 (0.1940) Data: 0.0088 (0.0143) Loss: 0.8145 (0.5083)
[2022/12/29 02:07] | TRAIN(057): [600/879] Batch: 0.1876 (0.1941) Data: 0.0111 (0.0140) Loss: 0.3947 (0.5089)
[2022/12/29 02:07] | TRAIN(057): [650/879] Batch: 0.1874 (0.1939) Data: 0.0107 (0.0137) Loss: 0.7589 (0.5094)
[2022/12/29 02:07] | TRAIN(057): [700/879] Batch: 0.1984 (0.1938) Data: 0.0100 (0.0135) Loss: 0.5154 (0.5104)
[2022/12/29 02:07] | TRAIN(057): [750/879] Batch: 0.1831 (0.1936) Data: 0.0114 (0.0133) Loss: 0.4979 (0.5093)
[2022/12/29 02:07] | TRAIN(057): [800/879] Batch: 0.1870 (0.1934) Data: 0.0110 (0.0131) Loss: 0.6805 (0.5107)
[2022/12/29 02:07] | TRAIN(057): [850/879] Batch: 0.1908 (0.1933) Data: 0.0112 (0.0130) Loss: 0.7003 (0.5084)
[2022/12/29 02:07] | ------------------------------------------------------------
[2022/12/29 02:07] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:07] | ------------------------------------------------------------
[2022/12/29 02:07] |    TRAIN(57)     0:02:49     0:00:11     0:02:38      0.5077
[2022/12/29 02:07] | ------------------------------------------------------------
[2022/12/29 02:08] | VALID(057): [ 50/220] Batch: 0.0613 (0.0832) Data: 0.0369 (0.0606) Loss: 0.5080 (0.5467)
[2022/12/29 02:08] | VALID(057): [100/220] Batch: 0.0597 (0.0681) Data: 0.0355 (0.0460) Loss: 1.1139 (0.5762)
[2022/12/29 02:08] | VALID(057): [150/220] Batch: 0.0593 (0.0630) Data: 0.0362 (0.0409) Loss: 0.5516 (0.5770)
[2022/12/29 02:08] | VALID(057): [200/220] Batch: 0.0610 (0.0605) Data: 0.0376 (0.0385) Loss: 0.2425 (0.5773)
[2022/12/29 02:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:08] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:08] |    VALID(57)      0.5764      0.8163      0.8784      0.8163      0.8163      0.8163      0.9541
[2022/12/29 02:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:08] | ####################################################################################################
[2022/12/29 02:08] | TRAIN(058): [ 50/879] Batch: 0.1874 (0.2312) Data: 0.0128 (0.0481) Loss: 0.4296 (0.4972)
[2022/12/29 02:08] | TRAIN(058): [100/879] Batch: 0.1424 (0.2068) Data: 0.0093 (0.0294) Loss: 0.4270 (0.4875)
[2022/12/29 02:08] | TRAIN(058): [150/879] Batch: 0.1895 (0.1991) Data: 0.0101 (0.0231) Loss: 0.7718 (0.4979)
[2022/12/29 02:08] | TRAIN(058): [200/879] Batch: 0.1941 (0.1931) Data: 0.0098 (0.0199) Loss: 0.5540 (0.5070)
[2022/12/29 02:08] | TRAIN(058): [250/879] Batch: 0.1847 (0.1932) Data: 0.0125 (0.0180) Loss: 0.4253 (0.5139)
[2022/12/29 02:09] | TRAIN(058): [300/879] Batch: 0.1896 (0.1936) Data: 0.0101 (0.0168) Loss: 0.4908 (0.5090)
[2022/12/29 02:09] | TRAIN(058): [350/879] Batch: 0.1888 (0.1939) Data: 0.0110 (0.0160) Loss: 0.5919 (0.5099)
[2022/12/29 02:09] | TRAIN(058): [400/879] Batch: 0.1926 (0.1940) Data: 0.0097 (0.0153) Loss: 0.4022 (0.5092)
[2022/12/29 02:09] | TRAIN(058): [450/879] Batch: 0.1991 (0.1938) Data: 0.0101 (0.0148) Loss: 0.7794 (0.5106)
[2022/12/29 02:09] | TRAIN(058): [500/879] Batch: 0.1833 (0.1928) Data: 0.0136 (0.0143) Loss: 0.3581 (0.5106)
[2022/12/29 02:09] | TRAIN(058): [550/879] Batch: 0.1785 (0.1921) Data: 0.0101 (0.0139) Loss: 0.4469 (0.5083)
[2022/12/29 02:10] | TRAIN(058): [600/879] Batch: 0.1988 (0.1916) Data: 0.0096 (0.0136) Loss: 0.2329 (0.5059)
[2022/12/29 02:10] | TRAIN(058): [650/879] Batch: 0.1903 (0.1918) Data: 0.0135 (0.0134) Loss: 0.6828 (0.5055)
[2022/12/29 02:10] | TRAIN(058): [700/879] Batch: 0.1949 (0.1918) Data: 0.0109 (0.0132) Loss: 0.8710 (0.5056)
[2022/12/29 02:10] | TRAIN(058): [750/879] Batch: 0.1929 (0.1916) Data: 0.0109 (0.0130) Loss: 0.5274 (0.5057)
[2022/12/29 02:10] | TRAIN(058): [800/879] Batch: 0.1850 (0.1910) Data: 0.0123 (0.0129) Loss: 0.3857 (0.5054)
[2022/12/29 02:10] | TRAIN(058): [850/879] Batch: 0.1889 (0.1900) Data: 0.0109 (0.0127) Loss: 0.6194 (0.5051)
[2022/12/29 02:10] | ------------------------------------------------------------
[2022/12/29 02:10] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:10] | ------------------------------------------------------------
[2022/12/29 02:10] |    TRAIN(58)     0:02:46     0:00:11     0:02:35      0.5056
[2022/12/29 02:10] | ------------------------------------------------------------
[2022/12/29 02:11] | VALID(058): [ 50/220] Batch: 0.0588 (0.0832) Data: 0.0377 (0.0602) Loss: 0.5120 (0.5485)
[2022/12/29 02:11] | VALID(058): [100/220] Batch: 0.0581 (0.0699) Data: 0.0385 (0.0476) Loss: 1.0989 (0.5797)
[2022/12/29 02:11] | VALID(058): [150/220] Batch: 0.0594 (0.0656) Data: 0.0312 (0.0436) Loss: 0.4726 (0.5816)
[2022/12/29 02:11] | VALID(058): [200/220] Batch: 0.0623 (0.0632) Data: 0.0117 (0.0400) Loss: 0.2116 (0.5843)
[2022/12/29 02:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:11] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:11] |    VALID(58)      0.5822      0.8168      0.8716      0.8168      0.8168      0.8168      0.9542
[2022/12/29 02:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:11] | ####################################################################################################
[2022/12/29 02:11] | TRAIN(059): [ 50/879] Batch: 0.1903 (0.2282) Data: 0.0094 (0.0477) Loss: 0.3740 (0.5044)
[2022/12/29 02:11] | TRAIN(059): [100/879] Batch: 0.1931 (0.2105) Data: 0.0137 (0.0293) Loss: 0.6001 (0.4883)
[2022/12/29 02:11] | TRAIN(059): [150/879] Batch: 0.2053 (0.2048) Data: 0.0111 (0.0232) Loss: 0.4895 (0.5069)
[2022/12/29 02:11] | TRAIN(059): [200/879] Batch: 0.1888 (0.2013) Data: 0.0110 (0.0201) Loss: 0.4197 (0.5014)
[2022/12/29 02:12] | TRAIN(059): [250/879] Batch: 0.1896 (0.2002) Data: 0.0109 (0.0183) Loss: 0.3340 (0.4974)
[2022/12/29 02:12] | TRAIN(059): [300/879] Batch: 0.1949 (0.1982) Data: 0.0091 (0.0170) Loss: 0.3561 (0.5038)
[2022/12/29 02:12] | TRAIN(059): [350/879] Batch: 0.1748 (0.1969) Data: 0.0093 (0.0161) Loss: 0.5432 (0.5058)
[2022/12/29 02:12] | TRAIN(059): [400/879] Batch: 0.1893 (0.1954) Data: 0.0093 (0.0153) Loss: 0.4645 (0.5073)
[2022/12/29 02:12] | TRAIN(059): [450/879] Batch: 0.1873 (0.1947) Data: 0.0111 (0.0148) Loss: 0.5036 (0.5052)
[2022/12/29 02:12] | TRAIN(059): [500/879] Batch: 0.1896 (0.1933) Data: 0.0097 (0.0144) Loss: 0.2350 (0.5009)
[2022/12/29 02:12] | TRAIN(059): [550/879] Batch: 0.1321 (0.1921) Data: 0.0098 (0.0140) Loss: 0.4046 (0.4993)
[2022/12/29 02:13] | TRAIN(059): [600/879] Batch: 0.1976 (0.1914) Data: 0.0102 (0.0138) Loss: 0.3583 (0.4995)
[2022/12/29 02:13] | TRAIN(059): [650/879] Batch: 0.1948 (0.1913) Data: 0.0095 (0.0135) Loss: 0.4728 (0.4987)
[2022/12/29 02:13] | TRAIN(059): [700/879] Batch: 0.1788 (0.1912) Data: 0.0107 (0.0133) Loss: 0.3926 (0.4969)
[2022/12/29 02:13] | TRAIN(059): [750/879] Batch: 0.1972 (0.1912) Data: 0.0111 (0.0131) Loss: 0.3532 (0.4960)
[2022/12/29 02:13] | TRAIN(059): [800/879] Batch: 0.1918 (0.1912) Data: 0.0110 (0.0130) Loss: 0.4362 (0.4981)
[2022/12/29 02:13] | TRAIN(059): [850/879] Batch: 0.1922 (0.1912) Data: 0.0114 (0.0128) Loss: 0.5461 (0.4985)
[2022/12/29 02:13] | ------------------------------------------------------------
[2022/12/29 02:13] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:13] | ------------------------------------------------------------
[2022/12/29 02:13] |    TRAIN(59)     0:02:47     0:00:11     0:02:36      0.4979
[2022/12/29 02:13] | ------------------------------------------------------------
[2022/12/29 02:14] | VALID(059): [ 50/220] Batch: 0.0571 (0.0831) Data: 0.0210 (0.0599) Loss: 0.4516 (0.5451)
[2022/12/29 02:14] | VALID(059): [100/220] Batch: 0.0537 (0.0694) Data: 0.0373 (0.0475) Loss: 0.9778 (0.5868)
[2022/12/29 02:14] | VALID(059): [150/220] Batch: 0.0586 (0.0639) Data: 0.0212 (0.0423) Loss: 0.5351 (0.5873)
[2022/12/29 02:14] | VALID(059): [200/220] Batch: 0.0548 (0.0616) Data: 0.0382 (0.0401) Loss: 0.1964 (0.5975)
[2022/12/29 02:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:14] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:14] |    VALID(59)      0.5943      0.8134      0.8724      0.8134      0.8134      0.8134      0.9534
[2022/12/29 02:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:14] | ####################################################################################################
[2022/12/29 02:14] | TRAIN(060): [ 50/879] Batch: 0.1755 (0.2193) Data: 0.0115 (0.0472) Loss: 0.5357 (0.4788)
[2022/12/29 02:14] | TRAIN(060): [100/879] Batch: 0.1995 (0.2050) Data: 0.0103 (0.0290) Loss: 0.5296 (0.5014)
[2022/12/29 02:14] | TRAIN(060): [150/879] Batch: 0.1796 (0.1993) Data: 0.0115 (0.0229) Loss: 0.4512 (0.4972)
[2022/12/29 02:14] | TRAIN(060): [200/879] Batch: 0.1800 (0.1949) Data: 0.0125 (0.0198) Loss: 0.5637 (0.5004)
[2022/12/29 02:15] | TRAIN(060): [250/879] Batch: 0.1814 (0.1917) Data: 0.0103 (0.0179) Loss: 0.3238 (0.4939)
[2022/12/29 02:15] | TRAIN(060): [300/879] Batch: 0.1866 (0.1868) Data: 0.0092 (0.0165) Loss: 0.3791 (0.4967)
[2022/12/29 02:15] | TRAIN(060): [350/879] Batch: 0.1812 (0.1861) Data: 0.0104 (0.0157) Loss: 0.6532 (0.4957)
[2022/12/29 02:15] | TRAIN(060): [400/879] Batch: 0.1948 (0.1857) Data: 0.0112 (0.0150) Loss: 0.5289 (0.4993)
[2022/12/29 02:15] | TRAIN(060): [450/879] Batch: 0.1808 (0.1857) Data: 0.0098 (0.0145) Loss: 0.4645 (0.4990)
[2022/12/29 02:15] | TRAIN(060): [500/879] Batch: 0.1715 (0.1853) Data: 0.0099 (0.0140) Loss: 0.6190 (0.4984)
[2022/12/29 02:15] | TRAIN(060): [550/879] Batch: 0.1925 (0.1851) Data: 0.0093 (0.0137) Loss: 0.3171 (0.4975)
[2022/12/29 02:16] | TRAIN(060): [600/879] Batch: 0.1726 (0.1850) Data: 0.0106 (0.0134) Loss: 0.7394 (0.4976)
[2022/12/29 02:16] | TRAIN(060): [650/879] Batch: 0.1818 (0.1849) Data: 0.0117 (0.0132) Loss: 0.5395 (0.4958)
[2022/12/29 02:16] | TRAIN(060): [700/879] Batch: 0.1776 (0.1849) Data: 0.0110 (0.0129) Loss: 0.4128 (0.4946)
[2022/12/29 02:16] | TRAIN(060): [750/879] Batch: 0.1754 (0.1848) Data: 0.0102 (0.0128) Loss: 0.3526 (0.4990)
[2022/12/29 02:16] | TRAIN(060): [800/879] Batch: 0.1741 (0.1846) Data: 0.0103 (0.0126) Loss: 0.7730 (0.4975)
[2022/12/29 02:16] | TRAIN(060): [850/879] Batch: 0.1892 (0.1844) Data: 0.0088 (0.0124) Loss: 0.5465 (0.4985)
[2022/12/29 02:16] | ------------------------------------------------------------
[2022/12/29 02:16] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:16] | ------------------------------------------------------------
[2022/12/29 02:16] |    TRAIN(60)     0:02:41     0:00:10     0:02:31      0.4970
[2022/12/29 02:16] | ------------------------------------------------------------
[2022/12/29 02:16] | VALID(060): [ 50/220] Batch: 0.0412 (0.0813) Data: 0.0360 (0.0549) Loss: 0.4199 (0.5464)
[2022/12/29 02:17] | VALID(060): [100/220] Batch: 0.0558 (0.0678) Data: 0.0383 (0.0440) Loss: 1.0090 (0.5875)
[2022/12/29 02:17] | VALID(060): [150/220] Batch: 0.0281 (0.0594) Data: 0.0079 (0.0345) Loss: 0.5423 (0.5850)
[2022/12/29 02:17] | VALID(060): [200/220] Batch: 0.0508 (0.0570) Data: 0.0287 (0.0332) Loss: 0.1753 (0.5903)
[2022/12/29 02:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:17] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:17] |    VALID(60)      0.5887      0.8168      0.8726      0.8168      0.8168      0.8168      0.9542
[2022/12/29 02:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:17] | ####################################################################################################
[2022/12/29 02:17] | TRAIN(061): [ 50/879] Batch: 0.1849 (0.2087) Data: 0.0100 (0.0472) Loss: 0.7605 (0.5012)
[2022/12/29 02:17] | TRAIN(061): [100/879] Batch: 0.1691 (0.1971) Data: 0.0109 (0.0290) Loss: 0.4739 (0.5058)
[2022/12/29 02:17] | TRAIN(061): [150/879] Batch: 0.1877 (0.1932) Data: 0.0087 (0.0228) Loss: 0.6171 (0.5065)
[2022/12/29 02:17] | TRAIN(061): [200/879] Batch: 0.1771 (0.1915) Data: 0.0102 (0.0196) Loss: 0.5342 (0.5030)
[2022/12/29 02:17] | TRAIN(061): [250/879] Batch: 0.1777 (0.1908) Data: 0.0107 (0.0178) Loss: 0.6090 (0.5008)
[2022/12/29 02:18] | TRAIN(061): [300/879] Batch: 0.1942 (0.1896) Data: 0.0113 (0.0165) Loss: 0.4687 (0.4937)
[2022/12/29 02:18] | TRAIN(061): [350/879] Batch: 0.1809 (0.1890) Data: 0.0130 (0.0156) Loss: 0.5978 (0.4959)
[2022/12/29 02:18] | TRAIN(061): [400/879] Batch: 0.1922 (0.1885) Data: 0.0119 (0.0149) Loss: 0.6447 (0.4975)
[2022/12/29 02:18] | TRAIN(061): [450/879] Batch: 0.1955 (0.1881) Data: 0.0088 (0.0143) Loss: 0.7326 (0.5018)
[2022/12/29 02:18] | TRAIN(061): [500/879] Batch: 0.1895 (0.1881) Data: 0.0116 (0.0139) Loss: 0.2830 (0.5010)
[2022/12/29 02:18] | TRAIN(061): [550/879] Batch: 0.1960 (0.1875) Data: 0.0095 (0.0136) Loss: 0.5110 (0.5005)
[2022/12/29 02:18] | TRAIN(061): [600/879] Batch: 0.1921 (0.1875) Data: 0.0088 (0.0133) Loss: 0.4255 (0.5000)
[2022/12/29 02:19] | TRAIN(061): [650/879] Batch: 0.1929 (0.1876) Data: 0.0094 (0.0131) Loss: 0.3516 (0.4993)
[2022/12/29 02:19] | TRAIN(061): [700/879] Batch: 0.1906 (0.1869) Data: 0.0110 (0.0129) Loss: 0.7064 (0.4958)
[2022/12/29 02:19] | TRAIN(061): [750/879] Batch: 0.1971 (0.1854) Data: 0.0095 (0.0127) Loss: 0.5831 (0.4947)
[2022/12/29 02:19] | TRAIN(061): [800/879] Batch: 0.1931 (0.1857) Data: 0.0088 (0.0125) Loss: 0.3942 (0.4939)
[2022/12/29 02:19] | TRAIN(061): [850/879] Batch: 0.1979 (0.1859) Data: 0.0118 (0.0124) Loss: 0.4945 (0.4959)
[2022/12/29 02:19] | ------------------------------------------------------------
[2022/12/29 02:19] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:19] | ------------------------------------------------------------
[2022/12/29 02:19] |    TRAIN(61)     0:02:43     0:00:10     0:02:32      0.4960
[2022/12/29 02:19] | ------------------------------------------------------------
[2022/12/29 02:19] | VALID(061): [ 50/220] Batch: 0.0460 (0.0833) Data: 0.0398 (0.0601) Loss: 0.4207 (0.5434)
[2022/12/29 02:19] | VALID(061): [100/220] Batch: 0.0586 (0.0701) Data: 0.0233 (0.0473) Loss: 1.0439 (0.5861)
[2022/12/29 02:19] | VALID(061): [150/220] Batch: 0.0402 (0.0650) Data: 0.0424 (0.0426) Loss: 0.6050 (0.5838)
[2022/12/29 02:20] | VALID(061): [200/220] Batch: 0.0586 (0.0624) Data: 0.0376 (0.0394) Loss: 0.2041 (0.5898)
[2022/12/29 02:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:20] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:20] |    VALID(61)      0.5873      0.8147      0.8730      0.8147      0.8147      0.8147      0.9537
[2022/12/29 02:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:20] | ####################################################################################################
[2022/12/29 02:20] | TRAIN(062): [ 50/879] Batch: 0.1900 (0.2254) Data: 0.0108 (0.0481) Loss: 0.4649 (0.4885)
[2022/12/29 02:20] | TRAIN(062): [100/879] Batch: 0.1823 (0.2042) Data: 0.0110 (0.0295) Loss: 0.3883 (0.4989)
[2022/12/29 02:20] | TRAIN(062): [150/879] Batch: 0.1836 (0.1988) Data: 0.0103 (0.0232) Loss: 0.5368 (0.4921)
[2022/12/29 02:20] | TRAIN(062): [200/879] Batch: 0.1771 (0.1948) Data: 0.0107 (0.0200) Loss: 0.5147 (0.4938)
[2022/12/29 02:20] | TRAIN(062): [250/879] Batch: 0.1929 (0.1941) Data: 0.0104 (0.0180) Loss: 0.3722 (0.4952)
[2022/12/29 02:21] | TRAIN(062): [300/879] Batch: 0.1837 (0.1936) Data: 0.0101 (0.0167) Loss: 0.3234 (0.4963)
[2022/12/29 02:21] | TRAIN(062): [350/879] Batch: 0.1898 (0.1936) Data: 0.0100 (0.0158) Loss: 0.5332 (0.4968)
[2022/12/29 02:21] | TRAIN(062): [400/879] Batch: 0.1920 (0.1920) Data: 0.0097 (0.0152) Loss: 0.5200 (0.4933)
[2022/12/29 02:21] | TRAIN(062): [450/879] Batch: 0.1374 (0.1908) Data: 0.0095 (0.0146) Loss: 0.6638 (0.4906)
[2022/12/29 02:21] | TRAIN(062): [500/879] Batch: 0.1927 (0.1896) Data: 0.0096 (0.0142) Loss: 0.5436 (0.4892)
[2022/12/29 02:21] | TRAIN(062): [550/879] Batch: 0.1817 (0.1891) Data: 0.0088 (0.0139) Loss: 0.3943 (0.4883)
[2022/12/29 02:21] | TRAIN(062): [600/879] Batch: 0.1876 (0.1887) Data: 0.0103 (0.0136) Loss: 0.5100 (0.4863)
[2022/12/29 02:22] | TRAIN(062): [650/879] Batch: 0.1981 (0.1889) Data: 0.0094 (0.0133) Loss: 0.5631 (0.4857)
[2022/12/29 02:22] | TRAIN(062): [700/879] Batch: 0.1869 (0.1891) Data: 0.0120 (0.0131) Loss: 0.5337 (0.4855)
[2022/12/29 02:22] | TRAIN(062): [750/879] Batch: 0.2086 (0.1891) Data: 0.0122 (0.0129) Loss: 0.3326 (0.4865)
[2022/12/29 02:22] | TRAIN(062): [800/879] Batch: 0.1839 (0.1892) Data: 0.0112 (0.0128) Loss: 0.3639 (0.4874)
[2022/12/29 02:22] | TRAIN(062): [850/879] Batch: 0.1866 (0.1891) Data: 0.0101 (0.0126) Loss: 0.5030 (0.4898)
[2022/12/29 02:22] | ------------------------------------------------------------
[2022/12/29 02:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:22] | ------------------------------------------------------------
[2022/12/29 02:22] |    TRAIN(62)     0:02:46     0:00:11     0:02:35      0.4891
[2022/12/29 02:22] | ------------------------------------------------------------
[2022/12/29 02:22] | VALID(062): [ 50/220] Batch: 0.0401 (0.0810) Data: 0.0399 (0.0582) Loss: 0.4019 (0.5485)
[2022/12/29 02:22] | VALID(062): [100/220] Batch: 0.0617 (0.0689) Data: 0.0268 (0.0467) Loss: 1.0038 (0.5982)
[2022/12/29 02:22] | VALID(062): [150/220] Batch: 0.0593 (0.0647) Data: 0.0354 (0.0427) Loss: 0.5377 (0.5949)
[2022/12/29 02:23] | VALID(062): [200/220] Batch: 0.0589 (0.0627) Data: 0.0314 (0.0409) Loss: 0.1705 (0.6002)
[2022/12/29 02:23] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:23] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:23] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:23] |    VALID(62)      0.5968      0.8144      0.8728      0.8144      0.8144      0.8144      0.9536
[2022/12/29 02:23] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:23] | ####################################################################################################
[2022/12/29 02:23] | TRAIN(063): [ 50/879] Batch: 0.1608 (0.2233) Data: 0.0104 (0.0482) Loss: 0.6233 (0.4740)
[2022/12/29 02:23] | TRAIN(063): [100/879] Batch: 0.1884 (0.2071) Data: 0.0096 (0.0295) Loss: 0.5290 (0.4942)
[2022/12/29 02:23] | TRAIN(063): [150/879] Batch: 0.1851 (0.1967) Data: 0.0108 (0.0232) Loss: 0.6102 (0.4995)
[2022/12/29 02:23] | TRAIN(063): [200/879] Batch: 0.1836 (0.1900) Data: 0.0094 (0.0200) Loss: 0.5511 (0.4940)
[2022/12/29 02:23] | TRAIN(063): [250/879] Batch: 0.1843 (0.1886) Data: 0.0103 (0.0180) Loss: 0.5188 (0.4935)
[2022/12/29 02:24] | TRAIN(063): [300/879] Batch: 0.1911 (0.1883) Data: 0.0114 (0.0168) Loss: 0.5229 (0.4924)
[2022/12/29 02:24] | TRAIN(063): [350/879] Batch: 0.1910 (0.1899) Data: 0.0098 (0.0160) Loss: 0.7143 (0.4973)
[2022/12/29 02:24] | TRAIN(063): [400/879] Batch: 0.1928 (0.1901) Data: 0.0093 (0.0153) Loss: 0.5800 (0.4976)
[2022/12/29 02:24] | TRAIN(063): [450/879] Batch: 0.1868 (0.1900) Data: 0.0099 (0.0148) Loss: 0.6111 (0.4915)
[2022/12/29 02:24] | TRAIN(063): [500/879] Batch: 0.1869 (0.1899) Data: 0.0105 (0.0144) Loss: 0.7349 (0.4896)
[2022/12/29 02:24] | TRAIN(063): [550/879] Batch: 0.1868 (0.1895) Data: 0.0102 (0.0140) Loss: 0.3449 (0.4881)
[2022/12/29 02:24] | TRAIN(063): [600/879] Batch: 0.2006 (0.1892) Data: 0.0107 (0.0137) Loss: 0.7909 (0.4907)
[2022/12/29 02:25] | TRAIN(063): [650/879] Batch: 0.1859 (0.1895) Data: 0.0108 (0.0135) Loss: 0.7605 (0.4903)
[2022/12/29 02:25] | TRAIN(063): [700/879] Batch: 0.1933 (0.1898) Data: 0.0117 (0.0133) Loss: 0.5471 (0.4885)
[2022/12/29 02:25] | TRAIN(063): [750/879] Batch: 0.1914 (0.1896) Data: 0.0100 (0.0131) Loss: 0.5239 (0.4861)
[2022/12/29 02:25] | TRAIN(063): [800/879] Batch: 0.1382 (0.1884) Data: 0.0092 (0.0129) Loss: 0.7840 (0.4857)
[2022/12/29 02:25] | TRAIN(063): [850/879] Batch: 0.1921 (0.1881) Data: 0.0100 (0.0127) Loss: 0.6012 (0.4857)
[2022/12/29 02:25] | ------------------------------------------------------------
[2022/12/29 02:25] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:25] | ------------------------------------------------------------
[2022/12/29 02:25] |    TRAIN(63)     0:02:44     0:00:11     0:02:33      0.4853
[2022/12/29 02:25] | ------------------------------------------------------------
[2022/12/29 02:25] | VALID(063): [ 50/220] Batch: 0.0371 (0.0822) Data: 0.0376 (0.0590) Loss: 0.4778 (0.5518)
[2022/12/29 02:25] | VALID(063): [100/220] Batch: 0.0421 (0.0683) Data: 0.0382 (0.0458) Loss: 1.0917 (0.5995)
[2022/12/29 02:25] | VALID(063): [150/220] Batch: 0.0612 (0.0644) Data: 0.0260 (0.0404) Loss: 0.6257 (0.5932)
[2022/12/29 02:26] | VALID(063): [200/220] Batch: 0.0591 (0.0625) Data: 0.0382 (0.0393) Loss: 0.2291 (0.6007)
[2022/12/29 02:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:26] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:26] |    VALID(63)      0.5975      0.8153      0.8715      0.8153      0.8153      0.8153      0.9538
[2022/12/29 02:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:26] | ####################################################################################################
[2022/12/29 02:26] | TRAIN(064): [ 50/879] Batch: 0.1764 (0.2222) Data: 0.0095 (0.0479) Loss: 0.3094 (0.4740)
[2022/12/29 02:26] | TRAIN(064): [100/879] Batch: 0.1764 (0.2024) Data: 0.0094 (0.0292) Loss: 0.4513 (0.5030)
[2022/12/29 02:26] | TRAIN(064): [150/879] Batch: 0.1918 (0.1957) Data: 0.0108 (0.0228) Loss: 0.3285 (0.4917)
[2022/12/29 02:26] | TRAIN(064): [200/879] Batch: 0.1762 (0.1924) Data: 0.0104 (0.0196) Loss: 0.8423 (0.4920)
[2022/12/29 02:26] | TRAIN(064): [250/879] Batch: 0.1853 (0.1909) Data: 0.0109 (0.0177) Loss: 0.2335 (0.4901)
[2022/12/29 02:26] | TRAIN(064): [300/879] Batch: 0.1831 (0.1896) Data: 0.0111 (0.0164) Loss: 0.7351 (0.4965)
[2022/12/29 02:27] | TRAIN(064): [350/879] Batch: 0.1883 (0.1888) Data: 0.0094 (0.0155) Loss: 0.7624 (0.4956)
[2022/12/29 02:27] | TRAIN(064): [400/879] Batch: 0.2040 (0.1888) Data: 0.0089 (0.0148) Loss: 0.5327 (0.4968)
[2022/12/29 02:27] | TRAIN(064): [450/879] Batch: 0.1746 (0.1883) Data: 0.0103 (0.0143) Loss: 0.4696 (0.4936)
[2022/12/29 02:27] | TRAIN(064): [500/879] Batch: 0.1826 (0.1884) Data: 0.0098 (0.0138) Loss: 0.6134 (0.4899)
[2022/12/29 02:27] | TRAIN(064): [550/879] Batch: 0.1859 (0.1882) Data: 0.0125 (0.0135) Loss: 0.3350 (0.4856)
[2022/12/29 02:27] | TRAIN(064): [600/879] Batch: 0.1240 (0.1871) Data: 0.0101 (0.0133) Loss: 0.2123 (0.4852)
[2022/12/29 02:28] | TRAIN(064): [650/879] Batch: 0.1860 (0.1862) Data: 0.0095 (0.0130) Loss: 0.3542 (0.4842)
[2022/12/29 02:28] | TRAIN(064): [700/879] Batch: 0.1898 (0.1860) Data: 0.0095 (0.0128) Loss: 0.3259 (0.4803)
[2022/12/29 02:28] | TRAIN(064): [750/879] Batch: 0.1789 (0.1855) Data: 0.0116 (0.0126) Loss: 0.5676 (0.4808)
[2022/12/29 02:28] | TRAIN(064): [800/879] Batch: 0.1895 (0.1851) Data: 0.0088 (0.0125) Loss: 0.2854 (0.4821)
[2022/12/29 02:28] | TRAIN(064): [850/879] Batch: 0.1742 (0.1850) Data: 0.0102 (0.0123) Loss: 0.5636 (0.4816)
[2022/12/29 02:28] | ------------------------------------------------------------
[2022/12/29 02:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:28] | ------------------------------------------------------------
[2022/12/29 02:28] |    TRAIN(64)     0:02:42     0:00:10     0:02:31      0.4822
[2022/12/29 02:28] | ------------------------------------------------------------
[2022/12/29 02:28] | VALID(064): [ 50/220] Batch: 0.0394 (0.0787) Data: 0.0389 (0.0537) Loss: 0.5040 (0.5621)
[2022/12/29 02:28] | VALID(064): [100/220] Batch: 0.0499 (0.0655) Data: 0.0205 (0.0422) Loss: 1.1502 (0.5973)
[2022/12/29 02:28] | VALID(064): [150/220] Batch: 0.0434 (0.0610) Data: 0.0385 (0.0385) Loss: 0.4844 (0.5949)
[2022/12/29 02:28] | VALID(064): [200/220] Batch: 0.0433 (0.0593) Data: 0.0280 (0.0344) Loss: 0.2070 (0.6002)
[2022/12/29 02:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:28] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:28] |    VALID(64)      0.5995      0.8158      0.8751      0.8158      0.8158      0.8158      0.9540
[2022/12/29 02:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:28] | ####################################################################################################
[2022/12/29 02:29] | TRAIN(065): [ 50/879] Batch: 0.1713 (0.2225) Data: 0.0122 (0.0489) Loss: 0.5148 (0.4938)
[2022/12/29 02:29] | TRAIN(065): [100/879] Batch: 0.1868 (0.2041) Data: 0.0104 (0.0298) Loss: 0.3199 (0.4771)
[2022/12/29 02:29] | TRAIN(065): [150/879] Batch: 0.1878 (0.1999) Data: 0.0108 (0.0235) Loss: 0.5109 (0.4692)
[2022/12/29 02:29] | TRAIN(065): [200/879] Batch: 0.1887 (0.1977) Data: 0.0103 (0.0202) Loss: 0.3610 (0.4654)
[2022/12/29 02:29] | TRAIN(065): [250/879] Batch: 0.1794 (0.1964) Data: 0.0097 (0.0182) Loss: 0.5457 (0.4692)
[2022/12/29 02:29] | TRAIN(065): [300/879] Batch: 0.1929 (0.1937) Data: 0.0101 (0.0170) Loss: 0.4060 (0.4746)
[2022/12/29 02:30] | TRAIN(065): [350/879] Batch: 0.1336 (0.1898) Data: 0.0105 (0.0160) Loss: 0.2732 (0.4729)
[2022/12/29 02:30] | TRAIN(065): [400/879] Batch: 0.1911 (0.1889) Data: 0.0093 (0.0153) Loss: 0.5471 (0.4705)
[2022/12/29 02:30] | TRAIN(065): [450/879] Batch: 0.1919 (0.1882) Data: 0.0110 (0.0147) Loss: 0.3652 (0.4689)
[2022/12/29 02:30] | TRAIN(065): [500/879] Batch: 0.1871 (0.1883) Data: 0.0095 (0.0143) Loss: 0.4027 (0.4717)
[2022/12/29 02:30] | TRAIN(065): [550/879] Batch: 0.1941 (0.1891) Data: 0.0117 (0.0141) Loss: 0.4269 (0.4717)
[2022/12/29 02:30] | TRAIN(065): [600/879] Batch: 0.1831 (0.1895) Data: 0.0108 (0.0138) Loss: 0.5375 (0.4705)
[2022/12/29 02:31] | TRAIN(065): [650/879] Batch: 0.1922 (0.1896) Data: 0.0097 (0.0135) Loss: 0.4387 (0.4728)
[2022/12/29 02:31] | TRAIN(065): [700/879] Batch: 0.1812 (0.1898) Data: 0.0106 (0.0133) Loss: 0.6822 (0.4747)
[2022/12/29 02:31] | TRAIN(065): [750/879] Batch: 0.1867 (0.1899) Data: 0.0116 (0.0131) Loss: 0.4475 (0.4733)
[2022/12/29 02:31] | TRAIN(065): [800/879] Batch: 0.2011 (0.1900) Data: 0.0112 (0.0130) Loss: 0.5022 (0.4739)
[2022/12/29 02:31] | TRAIN(065): [850/879] Batch: 0.1734 (0.1900) Data: 0.0114 (0.0128) Loss: 0.5642 (0.4727)
[2022/12/29 02:31] | ------------------------------------------------------------
[2022/12/29 02:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:31] | ------------------------------------------------------------
[2022/12/29 02:31] |    TRAIN(65)     0:02:46     0:00:11     0:02:35      0.4727
[2022/12/29 02:31] | ------------------------------------------------------------
[2022/12/29 02:31] | VALID(065): [ 50/220] Batch: 0.0586 (0.0817) Data: 0.0344 (0.0570) Loss: 0.4853 (0.5528)
[2022/12/29 02:31] | VALID(065): [100/220] Batch: 0.0572 (0.0676) Data: 0.0226 (0.0447) Loss: 1.1574 (0.6018)
[2022/12/29 02:31] | VALID(065): [150/220] Batch: 0.0457 (0.0627) Data: 0.0395 (0.0407) Loss: 0.5350 (0.6048)
[2022/12/29 02:31] | VALID(065): [200/220] Batch: 0.0571 (0.0604) Data: 0.0349 (0.0384) Loss: 0.2309 (0.6091)
[2022/12/29 02:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:31] |    VALID(65)      0.6055      0.8121      0.8726      0.8121      0.8121      0.8121      0.9530
[2022/12/29 02:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:31] | ####################################################################################################
[2022/12/29 02:32] | TRAIN(066): [ 50/879] Batch: 0.1868 (0.2242) Data: 0.0095 (0.0482) Loss: 0.6369 (0.4954)
[2022/12/29 02:32] | TRAIN(066): [100/879] Batch: 0.1890 (0.1982) Data: 0.0087 (0.0296) Loss: 0.4526 (0.4815)
[2022/12/29 02:32] | TRAIN(066): [150/879] Batch: 0.1851 (0.1933) Data: 0.0095 (0.0233) Loss: 0.8361 (0.4865)
[2022/12/29 02:32] | TRAIN(066): [200/879] Batch: 0.1922 (0.1918) Data: 0.0111 (0.0203) Loss: 0.5943 (0.4902)
[2022/12/29 02:32] | TRAIN(066): [250/879] Batch: 0.1887 (0.1921) Data: 0.0115 (0.0184) Loss: 0.4092 (0.4827)
[2022/12/29 02:32] | TRAIN(066): [300/879] Batch: 0.1894 (0.1923) Data: 0.0092 (0.0172) Loss: 0.5519 (0.4810)
[2022/12/29 02:33] | TRAIN(066): [350/879] Batch: 0.1876 (0.1912) Data: 0.0101 (0.0164) Loss: 0.6638 (0.4778)
[2022/12/29 02:33] | TRAIN(066): [400/879] Batch: 0.2009 (0.1904) Data: 0.0124 (0.0157) Loss: 0.5417 (0.4753)
[2022/12/29 02:33] | TRAIN(066): [450/879] Batch: 0.1736 (0.1899) Data: 0.0104 (0.0152) Loss: 0.6370 (0.4758)
[2022/12/29 02:33] | TRAIN(066): [500/879] Batch: 0.1906 (0.1902) Data: 0.0094 (0.0148) Loss: 0.9084 (0.4774)
[2022/12/29 02:33] | TRAIN(066): [550/879] Batch: 0.1831 (0.1898) Data: 0.0091 (0.0144) Loss: 0.4912 (0.4770)
[2022/12/29 02:33] | TRAIN(066): [600/879] Batch: 0.1885 (0.1894) Data: 0.0103 (0.0141) Loss: 0.5542 (0.4738)
[2022/12/29 02:34] | TRAIN(066): [650/879] Batch: 0.1984 (0.1897) Data: 0.0111 (0.0139) Loss: 0.5700 (0.4732)
[2022/12/29 02:34] | TRAIN(066): [700/879] Batch: 0.1613 (0.1889) Data: 0.0112 (0.0137) Loss: 0.4656 (0.4725)
[2022/12/29 02:34] | TRAIN(066): [750/879] Batch: 0.1187 (0.1888) Data: 0.0091 (0.0135) Loss: 0.7337 (0.4709)
[2022/12/29 02:34] | TRAIN(066): [800/879] Batch: 0.1721 (0.1879) Data: 0.0099 (0.0133) Loss: 0.4995 (0.4715)
[2022/12/29 02:34] | TRAIN(066): [850/879] Batch: 0.1764 (0.1878) Data: 0.0110 (0.0131) Loss: 0.5530 (0.4718)
[2022/12/29 02:34] | ------------------------------------------------------------
[2022/12/29 02:34] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:34] | ------------------------------------------------------------
[2022/12/29 02:34] |    TRAIN(66)     0:02:44     0:00:11     0:02:33      0.4703
[2022/12/29 02:34] | ------------------------------------------------------------
[2022/12/29 02:34] | VALID(066): [ 50/220] Batch: 0.0598 (0.0829) Data: 0.0284 (0.0609) Loss: 0.5180 (0.5633)
[2022/12/29 02:34] | VALID(066): [100/220] Batch: 0.0569 (0.0686) Data: 0.0202 (0.0466) Loss: 1.0659 (0.6080)
[2022/12/29 02:34] | VALID(066): [150/220] Batch: 0.0620 (0.0639) Data: 0.0350 (0.0423) Loss: 0.5986 (0.6060)
[2022/12/29 02:34] | VALID(066): [200/220] Batch: 0.0561 (0.0620) Data: 0.0299 (0.0394) Loss: 0.1230 (0.6088)
[2022/12/29 02:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:34] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:34] |    VALID(66)      0.6060      0.8127      0.8723      0.8127      0.8127      0.8127      0.9532
[2022/12/29 02:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:34] | ####################################################################################################
[2022/12/29 02:35] | TRAIN(067): [ 50/879] Batch: 0.2012 (0.2256) Data: 0.0178 (0.0486) Loss: 0.3964 (0.4452)
[2022/12/29 02:35] | TRAIN(067): [100/879] Batch: 0.1840 (0.2069) Data: 0.0113 (0.0304) Loss: 0.6963 (0.4593)
[2022/12/29 02:35] | TRAIN(067): [150/879] Batch: 0.1744 (0.2011) Data: 0.0087 (0.0242) Loss: 0.7323 (0.4528)
[2022/12/29 02:35] | TRAIN(067): [200/879] Batch: 0.1776 (0.1975) Data: 0.0105 (0.0208) Loss: 0.7821 (0.4569)
[2022/12/29 02:35] | TRAIN(067): [250/879] Batch: 0.2075 (0.1971) Data: 0.0162 (0.0191) Loss: 0.7034 (0.4710)
[2022/12/29 02:35] | TRAIN(067): [300/879] Batch: 0.2102 (0.1966) Data: 0.0164 (0.0179) Loss: 0.6007 (0.4743)
[2022/12/29 02:36] | TRAIN(067): [350/879] Batch: 0.1887 (0.1966) Data: 0.0093 (0.0170) Loss: 0.4802 (0.4719)
[2022/12/29 02:36] | TRAIN(067): [400/879] Batch: 0.1857 (0.1962) Data: 0.0183 (0.0163) Loss: 0.5858 (0.4665)
[2022/12/29 02:36] | TRAIN(067): [450/879] Batch: 0.1857 (0.1948) Data: 0.0168 (0.0157) Loss: 0.6373 (0.4654)
[2022/12/29 02:36] | TRAIN(067): [500/879] Batch: 0.1921 (0.1931) Data: 0.0118 (0.0152) Loss: 0.6026 (0.4678)
[2022/12/29 02:36] | TRAIN(067): [550/879] Batch: 0.1928 (0.1928) Data: 0.0101 (0.0148) Loss: 0.4063 (0.4668)
[2022/12/29 02:36] | TRAIN(067): [600/879] Batch: 0.1867 (0.1923) Data: 0.0109 (0.0145) Loss: 0.3981 (0.4659)
[2022/12/29 02:37] | TRAIN(067): [650/879] Batch: 0.1844 (0.1918) Data: 0.0116 (0.0143) Loss: 0.5394 (0.4667)
[2022/12/29 02:37] | TRAIN(067): [700/879] Batch: 0.1839 (0.1914) Data: 0.0091 (0.0140) Loss: 0.4681 (0.4672)
[2022/12/29 02:37] | TRAIN(067): [750/879] Batch: 0.1825 (0.1912) Data: 0.0114 (0.0138) Loss: 0.3442 (0.4671)
[2022/12/29 02:37] | TRAIN(067): [800/879] Batch: 0.1857 (0.1912) Data: 0.0094 (0.0137) Loss: 0.4915 (0.4657)
[2022/12/29 02:37] | TRAIN(067): [850/879] Batch: 0.1892 (0.1909) Data: 0.0096 (0.0135) Loss: 0.1829 (0.4653)
[2022/12/29 02:37] | ------------------------------------------------------------
[2022/12/29 02:37] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:37] | ------------------------------------------------------------
[2022/12/29 02:37] |    TRAIN(67)     0:02:47     0:00:11     0:02:35      0.4673
[2022/12/29 02:37] | ------------------------------------------------------------
[2022/12/29 02:37] | VALID(067): [ 50/220] Batch: 0.0596 (0.0848) Data: 0.0385 (0.0608) Loss: 0.4082 (0.5636)
[2022/12/29 02:37] | VALID(067): [100/220] Batch: 0.0484 (0.0707) Data: 0.0376 (0.0480) Loss: 1.0552 (0.6018)
[2022/12/29 02:37] | VALID(067): [150/220] Batch: 0.0602 (0.0648) Data: 0.0409 (0.0423) Loss: 0.5257 (0.6114)
[2022/12/29 02:37] | VALID(067): [200/220] Batch: 0.0595 (0.0622) Data: 0.0274 (0.0401) Loss: 0.1638 (0.6165)
[2022/12/29 02:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:37] |    VALID(67)      0.6142      0.8090      0.8712      0.8090      0.8090      0.8090      0.9522
[2022/12/29 02:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:37] | ####################################################################################################
[2022/12/29 02:38] | TRAIN(068): [ 50/879] Batch: 0.1929 (0.2262) Data: 0.0095 (0.0471) Loss: 0.3121 (0.4718)
[2022/12/29 02:38] | TRAIN(068): [100/879] Batch: 0.1813 (0.2076) Data: 0.0108 (0.0290) Loss: 0.2692 (0.4491)
[2022/12/29 02:38] | TRAIN(068): [150/879] Batch: 0.1867 (0.1982) Data: 0.0096 (0.0229) Loss: 0.4288 (0.4559)
[2022/12/29 02:38] | TRAIN(068): [200/879] Batch: 0.1303 (0.1935) Data: 0.0092 (0.0198) Loss: 0.3232 (0.4518)
[2022/12/29 02:38] | TRAIN(068): [250/879] Batch: 0.1760 (0.1914) Data: 0.0103 (0.0179) Loss: 0.2872 (0.4553)
[2022/12/29 02:38] | TRAIN(068): [300/879] Batch: 0.1971 (0.1901) Data: 0.0090 (0.0166) Loss: 0.6977 (0.4620)
[2022/12/29 02:39] | TRAIN(068): [350/879] Batch: 0.1884 (0.1892) Data: 0.0097 (0.0157) Loss: 0.5633 (0.4644)
[2022/12/29 02:39] | TRAIN(068): [400/879] Batch: 0.1888 (0.1893) Data: 0.0094 (0.0151) Loss: 0.3707 (0.4675)
[2022/12/29 02:39] | TRAIN(068): [450/879] Batch: 0.1875 (0.1897) Data: 0.0097 (0.0146) Loss: 0.4161 (0.4656)
[2022/12/29 02:39] | TRAIN(068): [500/879] Batch: 0.1787 (0.1894) Data: 0.0102 (0.0142) Loss: 0.7146 (0.4640)
[2022/12/29 02:39] | TRAIN(068): [550/879] Batch: 0.1792 (0.1888) Data: 0.0117 (0.0138) Loss: 0.5439 (0.4638)
[2022/12/29 02:39] | TRAIN(068): [600/879] Batch: 0.1889 (0.1889) Data: 0.0114 (0.0136) Loss: 0.2876 (0.4633)
[2022/12/29 02:40] | TRAIN(068): [650/879] Batch: 0.2010 (0.1892) Data: 0.0113 (0.0134) Loss: 0.3263 (0.4623)
[2022/12/29 02:40] | TRAIN(068): [700/879] Batch: 0.1895 (0.1894) Data: 0.0116 (0.0132) Loss: 0.3907 (0.4632)
[2022/12/29 02:40] | TRAIN(068): [750/879] Batch: 0.1853 (0.1892) Data: 0.0118 (0.0130) Loss: 0.6668 (0.4631)
[2022/12/29 02:40] | TRAIN(068): [800/879] Batch: 0.1883 (0.1895) Data: 0.0113 (0.0128) Loss: 0.2523 (0.4613)
[2022/12/29 02:40] | TRAIN(068): [850/879] Batch: 0.1836 (0.1888) Data: 0.0107 (0.0127) Loss: 0.5779 (0.4611)
[2022/12/29 02:40] | ------------------------------------------------------------
[2022/12/29 02:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:40] | ------------------------------------------------------------
[2022/12/29 02:40] |    TRAIN(68)     0:02:45     0:00:11     0:02:34      0.4599
[2022/12/29 02:40] | ------------------------------------------------------------
[2022/12/29 02:40] | VALID(068): [ 50/220] Batch: 0.0350 (0.0634) Data: 0.0088 (0.0371) Loss: 0.4678 (0.5612)
[2022/12/29 02:40] | VALID(068): [100/220] Batch: 0.0424 (0.0498) Data: 0.0122 (0.0239) Loss: 1.2586 (0.6138)
[2022/12/29 02:40] | VALID(068): [150/220] Batch: 0.0340 (0.0463) Data: 0.0098 (0.0196) Loss: 0.5085 (0.6200)
[2022/12/29 02:40] | VALID(068): [200/220] Batch: 0.0280 (0.0430) Data: 0.0080 (0.0171) Loss: 0.1949 (0.6270)
[2022/12/29 02:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:40] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:40] |    VALID(68)      0.6224      0.8190      0.8721      0.8190      0.8190      0.8190      0.9547
[2022/12/29 02:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:40] | ####################################################################################################
[2022/12/29 02:41] | TRAIN(069): [ 50/879] Batch: 0.1200 (0.1686) Data: 0.0103 (0.0500) Loss: 0.3180 (0.4537)
[2022/12/29 02:41] | TRAIN(069): [100/879] Batch: 0.1507 (0.1462) Data: 0.0126 (0.0305) Loss: 0.4881 (0.4611)
[2022/12/29 02:41] | TRAIN(069): [150/879] Batch: 0.1200 (0.1415) Data: 0.0104 (0.0240) Loss: 0.2498 (0.4523)
[2022/12/29 02:41] | TRAIN(069): [200/879] Batch: 0.1244 (0.1372) Data: 0.0118 (0.0206) Loss: 0.1786 (0.4626)
[2022/12/29 02:41] | TRAIN(069): [250/879] Batch: 0.1204 (0.1343) Data: 0.0105 (0.0186) Loss: 0.3180 (0.4611)
[2022/12/29 02:41] | TRAIN(069): [300/879] Batch: 0.1195 (0.1323) Data: 0.0102 (0.0172) Loss: 0.1562 (0.4587)
[2022/12/29 02:41] | TRAIN(069): [350/879] Batch: 0.1194 (0.1306) Data: 0.0105 (0.0162) Loss: 0.5923 (0.4616)
[2022/12/29 02:41] | TRAIN(069): [400/879] Batch: 0.1484 (0.1316) Data: 0.0115 (0.0156) Loss: 0.4098 (0.4633)
[2022/12/29 02:41] | TRAIN(069): [450/879] Batch: 0.1198 (0.1303) Data: 0.0097 (0.0150) Loss: 0.4391 (0.4638)
[2022/12/29 02:41] | TRAIN(069): [500/879] Batch: 0.1200 (0.1293) Data: 0.0106 (0.0146) Loss: 0.6104 (0.4641)
[2022/12/29 02:42] | TRAIN(069): [550/879] Batch: 0.1187 (0.1285) Data: 0.0102 (0.0142) Loss: 0.3172 (0.4689)
[2022/12/29 02:42] | TRAIN(069): [600/879] Batch: 0.1190 (0.1278) Data: 0.0099 (0.0138) Loss: 0.4028 (0.4674)
[2022/12/29 02:42] | TRAIN(069): [650/879] Batch: 0.1191 (0.1272) Data: 0.0105 (0.0135) Loss: 0.4884 (0.4650)
[2022/12/29 02:42] | TRAIN(069): [700/879] Batch: 0.1185 (0.1267) Data: 0.0100 (0.0133) Loss: 0.5473 (0.4658)
[2022/12/29 02:42] | TRAIN(069): [750/879] Batch: 0.1188 (0.1262) Data: 0.0102 (0.0131) Loss: 0.2291 (0.4633)
[2022/12/29 02:42] | TRAIN(069): [800/879] Batch: 0.1189 (0.1258) Data: 0.0096 (0.0129) Loss: 0.3598 (0.4614)
[2022/12/29 02:42] | TRAIN(069): [850/879] Batch: 0.1280 (0.1259) Data: 0.0102 (0.0128) Loss: 0.4063 (0.4624)
[2022/12/29 02:42] | ------------------------------------------------------------
[2022/12/29 02:42] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:42] | ------------------------------------------------------------
[2022/12/29 02:42] |    TRAIN(69)     0:01:50     0:00:11     0:01:39      0.4631
[2022/12/29 02:42] | ------------------------------------------------------------
[2022/12/29 02:42] | VALID(069): [ 50/220] Batch: 0.0329 (0.0625) Data: 0.0092 (0.0385) Loss: 0.4221 (0.5631)
[2022/12/29 02:42] | VALID(069): [100/220] Batch: 0.0325 (0.0475) Data: 0.0108 (0.0244) Loss: 1.1623 (0.6121)
[2022/12/29 02:42] | VALID(069): [150/220] Batch: 0.0423 (0.0430) Data: 0.0153 (0.0198) Loss: 0.4865 (0.6150)
[2022/12/29 02:42] | VALID(069): [200/220] Batch: 0.0378 (0.0409) Data: 0.0139 (0.0175) Loss: 0.1778 (0.6213)
[2022/12/29 02:42] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:42] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:42] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:42] |    VALID(69)      0.6183      0.8127      0.8722      0.8127      0.8127      0.8127      0.9532
[2022/12/29 02:42] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:42] | ####################################################################################################
[2022/12/29 02:43] | TRAIN(070): [ 50/879] Batch: 0.1192 (0.1695) Data: 0.0108 (0.0508) Loss: 0.3794 (0.4612)
[2022/12/29 02:43] | TRAIN(070): [100/879] Batch: 0.1196 (0.1454) Data: 0.0100 (0.0308) Loss: 0.2997 (0.4700)
[2022/12/29 02:43] | TRAIN(070): [150/879] Batch: 0.1264 (0.1374) Data: 0.0102 (0.0241) Loss: 0.6649 (0.4750)
[2022/12/29 02:43] | TRAIN(070): [200/879] Batch: 0.1185 (0.1347) Data: 0.0102 (0.0207) Loss: 0.5331 (0.4744)
[2022/12/29 02:43] | TRAIN(070): [250/879] Batch: 0.1188 (0.1317) Data: 0.0100 (0.0186) Loss: 0.5913 (0.4673)
[2022/12/29 02:43] | TRAIN(070): [300/879] Batch: 0.1227 (0.1296) Data: 0.0100 (0.0172) Loss: 0.5014 (0.4696)
[2022/12/29 02:43] | TRAIN(070): [350/879] Batch: 0.1197 (0.1295) Data: 0.0105 (0.0163) Loss: 0.5484 (0.4598)
[2022/12/29 02:43] | TRAIN(070): [400/879] Batch: 0.1168 (0.1285) Data: 0.0104 (0.0156) Loss: 0.4486 (0.4573)
[2022/12/29 02:43] | TRAIN(070): [450/879] Batch: 0.1200 (0.1284) Data: 0.0100 (0.0150) Loss: 0.3265 (0.4575)
[2022/12/29 02:43] | TRAIN(070): [500/879] Batch: 0.1220 (0.1274) Data: 0.0104 (0.0146) Loss: 0.3734 (0.4562)
[2022/12/29 02:44] | TRAIN(070): [550/879] Batch: 0.1189 (0.1267) Data: 0.0103 (0.0142) Loss: 0.7013 (0.4564)
[2022/12/29 02:44] | TRAIN(070): [600/879] Batch: 0.1181 (0.1261) Data: 0.0102 (0.0138) Loss: 0.5592 (0.4560)
[2022/12/29 02:44] | TRAIN(070): [650/879] Batch: 0.1148 (0.1256) Data: 0.0104 (0.0136) Loss: 0.5335 (0.4535)
[2022/12/29 02:44] | TRAIN(070): [700/879] Batch: 0.1183 (0.1252) Data: 0.0099 (0.0133) Loss: 0.6297 (0.4536)
[2022/12/29 02:44] | TRAIN(070): [750/879] Batch: 0.1244 (0.1249) Data: 0.0104 (0.0131) Loss: 0.3006 (0.4557)
[2022/12/29 02:44] | TRAIN(070): [800/879] Batch: 0.1151 (0.1245) Data: 0.0098 (0.0129) Loss: 0.4110 (0.4554)
[2022/12/29 02:44] | TRAIN(070): [850/879] Batch: 0.1136 (0.1242) Data: 0.0096 (0.0127) Loss: 0.3093 (0.4558)
[2022/12/29 02:44] | ------------------------------------------------------------
[2022/12/29 02:44] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:44] | ------------------------------------------------------------
[2022/12/29 02:44] |    TRAIN(70)     0:01:49     0:00:11     0:01:37      0.4562
[2022/12/29 02:44] | ------------------------------------------------------------
[2022/12/29 02:44] | VALID(070): [ 50/220] Batch: 0.0408 (0.0675) Data: 0.0117 (0.0395) Loss: 0.4257 (0.5462)
[2022/12/29 02:44] | VALID(070): [100/220] Batch: 0.0413 (0.0546) Data: 0.0114 (0.0258) Loss: 1.0896 (0.6055)
[2022/12/29 02:44] | VALID(070): [150/220] Batch: 0.0319 (0.0486) Data: 0.0097 (0.0208) Loss: 0.5414 (0.6065)
[2022/12/29 02:44] | VALID(070): [200/220] Batch: 0.0339 (0.0444) Data: 0.0112 (0.0181) Loss: 0.1678 (0.6129)
[2022/12/29 02:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:44] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:44] |    VALID(70)      0.6101      0.8178      0.8742      0.8178      0.8178      0.8178      0.9545
[2022/12/29 02:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:44] | ####################################################################################################
[2022/12/29 02:45] | TRAIN(071): [ 50/879] Batch: 0.1184 (0.1606) Data: 0.0115 (0.0497) Loss: 0.2992 (0.4509)
[2022/12/29 02:45] | TRAIN(071): [100/879] Batch: 0.1261 (0.1407) Data: 0.0113 (0.0301) Loss: 0.5480 (0.4419)
[2022/12/29 02:45] | TRAIN(071): [150/879] Batch: 0.1149 (0.1339) Data: 0.0105 (0.0235) Loss: 0.4618 (0.4601)
[2022/12/29 02:45] | TRAIN(071): [200/879] Batch: 0.1233 (0.1310) Data: 0.0123 (0.0202) Loss: 0.4853 (0.4569)
[2022/12/29 02:45] | TRAIN(071): [250/879] Batch: 0.1293 (0.1289) Data: 0.0122 (0.0182) Loss: 0.6042 (0.4605)
[2022/12/29 02:45] | TRAIN(071): [300/879] Batch: 0.1194 (0.1273) Data: 0.0103 (0.0169) Loss: 0.4532 (0.4590)
[2022/12/29 02:45] | TRAIN(071): [350/879] Batch: 0.1184 (0.1262) Data: 0.0103 (0.0159) Loss: 0.5627 (0.4595)
[2022/12/29 02:45] | TRAIN(071): [400/879] Batch: 0.1192 (0.1254) Data: 0.0103 (0.0152) Loss: 0.6816 (0.4589)
[2022/12/29 02:45] | TRAIN(071): [450/879] Batch: 0.1194 (0.1249) Data: 0.0095 (0.0147) Loss: 0.6048 (0.4571)
[2022/12/29 02:45] | TRAIN(071): [500/879] Batch: 0.1194 (0.1244) Data: 0.0103 (0.0143) Loss: 0.3325 (0.4557)
[2022/12/29 02:46] | TRAIN(071): [550/879] Batch: 0.1190 (0.1241) Data: 0.0110 (0.0139) Loss: 0.6225 (0.4546)
[2022/12/29 02:46] | TRAIN(071): [600/879] Batch: 0.1200 (0.1238) Data: 0.0106 (0.0136) Loss: 0.2707 (0.4529)
[2022/12/29 02:46] | TRAIN(071): [650/879] Batch: 0.1188 (0.1234) Data: 0.0100 (0.0133) Loss: 0.6297 (0.4531)
[2022/12/29 02:46] | TRAIN(071): [700/879] Batch: 0.1356 (0.1239) Data: 0.0121 (0.0132) Loss: 0.2741 (0.4548)
[2022/12/29 02:46] | TRAIN(071): [750/879] Batch: 0.1343 (0.1239) Data: 0.0095 (0.0130) Loss: 0.4678 (0.4546)
[2022/12/29 02:46] | TRAIN(071): [800/879] Batch: 0.1261 (0.1240) Data: 0.0113 (0.0128) Loss: 0.5139 (0.4542)
[2022/12/29 02:46] | TRAIN(071): [850/879] Batch: 0.1310 (0.1239) Data: 0.0106 (0.0127) Loss: 0.3730 (0.4521)
[2022/12/29 02:46] | ------------------------------------------------------------
[2022/12/29 02:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:46] | ------------------------------------------------------------
[2022/12/29 02:46] |    TRAIN(71)     0:01:48     0:00:11     0:01:37      0.4520
[2022/12/29 02:46] | ------------------------------------------------------------
[2022/12/29 02:46] | VALID(071): [ 50/220] Batch: 0.0336 (0.0635) Data: 0.0096 (0.0384) Loss: 0.3545 (0.5612)
[2022/12/29 02:46] | VALID(071): [100/220] Batch: 0.0426 (0.0485) Data: 0.0119 (0.0246) Loss: 1.3102 (0.6210)
[2022/12/29 02:46] | VALID(071): [150/220] Batch: 0.0424 (0.0464) Data: 0.0119 (0.0204) Loss: 0.5455 (0.6228)
[2022/12/29 02:46] | VALID(071): [200/220] Batch: 0.0421 (0.0454) Data: 0.0120 (0.0183) Loss: 0.2321 (0.6285)
[2022/12/29 02:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:46] |    VALID(71)      0.6243      0.8154      0.8732      0.8154      0.8154      0.8154      0.9538
[2022/12/29 02:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:46] | ####################################################################################################
[2022/12/29 02:46] | TRAIN(072): [ 50/879] Batch: 0.1199 (0.1631) Data: 0.0104 (0.0479) Loss: 0.5417 (0.4641)
[2022/12/29 02:47] | TRAIN(072): [100/879] Batch: 0.1286 (0.1425) Data: 0.0104 (0.0294) Loss: 0.2929 (0.4448)
[2022/12/29 02:47] | TRAIN(072): [150/879] Batch: 0.1190 (0.1379) Data: 0.0107 (0.0231) Loss: 0.3288 (0.4443)
[2022/12/29 02:47] | TRAIN(072): [200/879] Batch: 0.1247 (0.1342) Data: 0.0104 (0.0200) Loss: 0.4918 (0.4433)
[2022/12/29 02:47] | TRAIN(072): [250/879] Batch: 0.1277 (0.1321) Data: 0.0103 (0.0181) Loss: 0.3346 (0.4374)
[2022/12/29 02:47] | TRAIN(072): [300/879] Batch: 0.1192 (0.1307) Data: 0.0103 (0.0168) Loss: 0.3803 (0.4407)
[2022/12/29 02:47] | TRAIN(072): [350/879] Batch: 0.1200 (0.1297) Data: 0.0104 (0.0159) Loss: 0.3029 (0.4413)
[2022/12/29 02:47] | TRAIN(072): [400/879] Batch: 0.1195 (0.1285) Data: 0.0107 (0.0152) Loss: 0.5632 (0.4460)
[2022/12/29 02:47] | TRAIN(072): [450/879] Batch: 0.1189 (0.1275) Data: 0.0102 (0.0146) Loss: 0.3243 (0.4482)
[2022/12/29 02:47] | TRAIN(072): [500/879] Batch: 0.1337 (0.1270) Data: 0.0100 (0.0142) Loss: 0.2267 (0.4506)
[2022/12/29 02:48] | TRAIN(072): [550/879] Batch: 0.1257 (0.1265) Data: 0.0103 (0.0139) Loss: 0.3273 (0.4463)
[2022/12/29 02:48] | TRAIN(072): [600/879] Batch: 0.1183 (0.1262) Data: 0.0108 (0.0136) Loss: 0.3912 (0.4447)
[2022/12/29 02:48] | TRAIN(072): [650/879] Batch: 0.1196 (0.1260) Data: 0.0100 (0.0134) Loss: 0.4081 (0.4449)
[2022/12/29 02:48] | TRAIN(072): [700/879] Batch: 0.1195 (0.1257) Data: 0.0096 (0.0132) Loss: 0.3674 (0.4455)
[2022/12/29 02:48] | TRAIN(072): [750/879] Batch: 0.1188 (0.1253) Data: 0.0102 (0.0130) Loss: 0.4820 (0.4442)
[2022/12/29 02:48] | TRAIN(072): [800/879] Batch: 0.1186 (0.1249) Data: 0.0096 (0.0128) Loss: 0.3922 (0.4468)
[2022/12/29 02:48] | TRAIN(072): [850/879] Batch: 0.1200 (0.1247) Data: 0.0096 (0.0126) Loss: 0.1785 (0.4483)
[2022/12/29 02:48] | ------------------------------------------------------------
[2022/12/29 02:48] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:48] | ------------------------------------------------------------
[2022/12/29 02:48] |    TRAIN(72)     0:01:49     0:00:11     0:01:38      0.4475
[2022/12/29 02:48] | ------------------------------------------------------------
[2022/12/29 02:48] | VALID(072): [ 50/220] Batch: 0.0448 (0.0683) Data: 0.0105 (0.0401) Loss: 0.4070 (0.5654)
[2022/12/29 02:48] | VALID(072): [100/220] Batch: 0.0427 (0.0550) Data: 0.0121 (0.0261) Loss: 1.2636 (0.6212)
[2022/12/29 02:48] | VALID(072): [150/220] Batch: 0.0423 (0.0507) Data: 0.0182 (0.0214) Loss: 0.4573 (0.6232)
[2022/12/29 02:48] | VALID(072): [200/220] Batch: 0.0417 (0.0479) Data: 0.0122 (0.0189) Loss: 0.2267 (0.6305)
[2022/12/29 02:48] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:48] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:48] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:48] |    VALID(72)      0.6264      0.8104      0.8729      0.8104      0.8104      0.8104      0.9526
[2022/12/29 02:48] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:48] | ####################################################################################################
[2022/12/29 02:48] | TRAIN(073): [ 50/879] Batch: 0.1193 (0.1678) Data: 0.0097 (0.0489) Loss: 0.5698 (0.4392)
[2022/12/29 02:49] | TRAIN(073): [100/879] Batch: 0.1196 (0.1445) Data: 0.0102 (0.0299) Loss: 0.2944 (0.4400)
[2022/12/29 02:49] | TRAIN(073): [150/879] Batch: 0.1212 (0.1384) Data: 0.0110 (0.0235) Loss: 0.4948 (0.4396)
[2022/12/29 02:49] | TRAIN(073): [200/879] Batch: 0.1196 (0.1348) Data: 0.0096 (0.0202) Loss: 0.2779 (0.4338)
[2022/12/29 02:49] | TRAIN(073): [250/879] Batch: 0.1187 (0.1323) Data: 0.0098 (0.0183) Loss: 0.5866 (0.4435)
[2022/12/29 02:49] | TRAIN(073): [300/879] Batch: 0.1238 (0.1308) Data: 0.0103 (0.0170) Loss: 0.2636 (0.4393)
[2022/12/29 02:49] | TRAIN(073): [350/879] Batch: 0.1201 (0.1295) Data: 0.0101 (0.0161) Loss: 0.4323 (0.4321)
[2022/12/29 02:49] | TRAIN(073): [400/879] Batch: 0.1356 (0.1286) Data: 0.0101 (0.0153) Loss: 0.4662 (0.4348)
[2022/12/29 02:49] | TRAIN(073): [450/879] Batch: 0.1191 (0.1280) Data: 0.0107 (0.0148) Loss: 0.4511 (0.4359)
[2022/12/29 02:49] | TRAIN(073): [500/879] Batch: 0.1197 (0.1272) Data: 0.0105 (0.0144) Loss: 0.3632 (0.4382)
[2022/12/29 02:50] | TRAIN(073): [550/879] Batch: 0.1206 (0.1268) Data: 0.0103 (0.0140) Loss: 0.5309 (0.4399)
[2022/12/29 02:50] | TRAIN(073): [600/879] Batch: 0.1299 (0.1266) Data: 0.0118 (0.0137) Loss: 0.3940 (0.4417)
[2022/12/29 02:50] | TRAIN(073): [650/879] Batch: 0.1188 (0.1261) Data: 0.0102 (0.0135) Loss: 0.6964 (0.4419)
[2022/12/29 02:50] | TRAIN(073): [700/879] Batch: 0.1178 (0.1256) Data: 0.0106 (0.0132) Loss: 0.6661 (0.4432)
[2022/12/29 02:50] | TRAIN(073): [750/879] Batch: 0.1195 (0.1252) Data: 0.0095 (0.0130) Loss: 0.4043 (0.4440)
[2022/12/29 02:50] | TRAIN(073): [800/879] Batch: 0.1184 (0.1250) Data: 0.0105 (0.0129) Loss: 0.4984 (0.4444)
[2022/12/29 02:50] | TRAIN(073): [850/879] Batch: 0.1195 (0.1247) Data: 0.0105 (0.0127) Loss: 0.2933 (0.4429)
[2022/12/29 02:50] | ------------------------------------------------------------
[2022/12/29 02:50] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:50] | ------------------------------------------------------------
[2022/12/29 02:50] |    TRAIN(73)     0:01:49     0:00:11     0:01:38      0.4439
[2022/12/29 02:50] | ------------------------------------------------------------
[2022/12/29 02:50] | VALID(073): [ 50/220] Batch: 0.0344 (0.0670) Data: 0.0095 (0.0417) Loss: 0.4589 (0.5610)
[2022/12/29 02:50] | VALID(073): [100/220] Batch: 0.0429 (0.0525) Data: 0.0119 (0.0266) Loss: 1.3265 (0.6254)
[2022/12/29 02:50] | VALID(073): [150/220] Batch: 0.0426 (0.0493) Data: 0.0120 (0.0217) Loss: 0.5486 (0.6333)
[2022/12/29 02:50] | VALID(073): [200/220] Batch: 0.0428 (0.0476) Data: 0.0129 (0.0193) Loss: 0.1623 (0.6413)
[2022/12/29 02:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:50] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:50] |    VALID(73)      0.6384      0.8110      0.8716      0.8110      0.8110      0.8110      0.9527
[2022/12/29 02:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:50] | ####################################################################################################
[2022/12/29 02:51] | TRAIN(074): [ 50/879] Batch: 0.1305 (0.1678) Data: 0.0101 (0.0453) Loss: 0.4148 (0.4750)
[2022/12/29 02:51] | TRAIN(074): [100/879] Batch: 0.1302 (0.1472) Data: 0.0101 (0.0280) Loss: 0.4678 (0.4596)
[2022/12/29 02:51] | TRAIN(074): [150/879] Batch: 0.1190 (0.1408) Data: 0.0101 (0.0222) Loss: 0.3398 (0.4564)
[2022/12/29 02:51] | TRAIN(074): [200/879] Batch: 0.1192 (0.1385) Data: 0.0102 (0.0193) Loss: 0.4795 (0.4553)
[2022/12/29 02:51] | TRAIN(074): [250/879] Batch: 0.1308 (0.1365) Data: 0.0106 (0.0176) Loss: 0.3195 (0.4529)
[2022/12/29 02:51] | TRAIN(074): [300/879] Batch: 0.1160 (0.1351) Data: 0.0105 (0.0164) Loss: 0.5836 (0.4491)
[2022/12/29 02:51] | TRAIN(074): [350/879] Batch: 0.1195 (0.1332) Data: 0.0103 (0.0156) Loss: 0.4229 (0.4449)
[2022/12/29 02:51] | TRAIN(074): [400/879] Batch: 0.1570 (0.1343) Data: 0.0127 (0.0151) Loss: 0.4113 (0.4417)
[2022/12/29 02:51] | TRAIN(074): [450/879] Batch: 0.1227 (0.1339) Data: 0.0097 (0.0145) Loss: 0.5716 (0.4431)
[2022/12/29 02:51] | TRAIN(074): [500/879] Batch: 0.1226 (0.1325) Data: 0.0104 (0.0141) Loss: 0.7514 (0.4433)
[2022/12/29 02:52] | TRAIN(074): [550/879] Batch: 0.1312 (0.1316) Data: 0.0095 (0.0137) Loss: 0.3429 (0.4435)
[2022/12/29 02:52] | TRAIN(074): [600/879] Batch: 0.1582 (0.1310) Data: 0.0122 (0.0134) Loss: 0.2995 (0.4440)
[2022/12/29 02:52] | TRAIN(074): [650/879] Batch: 0.1143 (0.1305) Data: 0.0104 (0.0132) Loss: 0.3403 (0.4442)
[2022/12/29 02:52] | TRAIN(074): [700/879] Batch: 0.1176 (0.1298) Data: 0.0101 (0.0130) Loss: 0.4032 (0.4423)
[2022/12/29 02:52] | TRAIN(074): [750/879] Batch: 0.1260 (0.1292) Data: 0.0104 (0.0128) Loss: 0.3432 (0.4426)
[2022/12/29 02:52] | TRAIN(074): [800/879] Batch: 0.1189 (0.1286) Data: 0.0099 (0.0127) Loss: 0.2298 (0.4426)
[2022/12/29 02:52] | TRAIN(074): [850/879] Batch: 0.1184 (0.1281) Data: 0.0107 (0.0125) Loss: 0.6454 (0.4423)
[2022/12/29 02:52] | ------------------------------------------------------------
[2022/12/29 02:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:52] | ------------------------------------------------------------
[2022/12/29 02:52] |    TRAIN(74)     0:01:52     0:00:10     0:01:41      0.4438
[2022/12/29 02:52] | ------------------------------------------------------------
[2022/12/29 02:52] | VALID(074): [ 50/220] Batch: 0.0287 (0.0619) Data: 0.0096 (0.0381) Loss: 0.5087 (0.5763)
[2022/12/29 02:52] | VALID(074): [100/220] Batch: 0.0455 (0.0509) Data: 0.0118 (0.0247) Loss: 1.2348 (0.6302)
[2022/12/29 02:52] | VALID(074): [150/220] Batch: 0.0423 (0.0465) Data: 0.0117 (0.0201) Loss: 0.4947 (0.6343)
[2022/12/29 02:52] | VALID(074): [200/220] Batch: 0.0433 (0.0433) Data: 0.0120 (0.0177) Loss: 0.1847 (0.6397)
[2022/12/29 02:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:52] |    VALID(74)      0.6365      0.8097      0.8714      0.8097      0.8097      0.8097      0.9524
[2022/12/29 02:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:52] | ####################################################################################################
[2022/12/29 02:53] | TRAIN(075): [ 50/879] Batch: 0.1210 (0.1578) Data: 0.0101 (0.0456) Loss: 0.3791 (0.4545)
[2022/12/29 02:53] | TRAIN(075): [100/879] Batch: 0.1178 (0.1391) Data: 0.0102 (0.0282) Loss: 0.3735 (0.4486)
[2022/12/29 02:53] | TRAIN(075): [150/879] Batch: 0.1187 (0.1326) Data: 0.0102 (0.0223) Loss: 0.4321 (0.4407)
[2022/12/29 02:53] | TRAIN(075): [200/879] Batch: 0.1192 (0.1292) Data: 0.0102 (0.0193) Loss: 0.2935 (0.4312)
[2022/12/29 02:53] | TRAIN(075): [250/879] Batch: 0.1188 (0.1272) Data: 0.0098 (0.0176) Loss: 0.3480 (0.4288)
[2022/12/29 02:53] | TRAIN(075): [300/879] Batch: 0.1186 (0.1260) Data: 0.0107 (0.0164) Loss: 0.4221 (0.4291)
[2022/12/29 02:53] | TRAIN(075): [350/879] Batch: 0.1240 (0.1252) Data: 0.0101 (0.0155) Loss: 0.4317 (0.4313)
[2022/12/29 02:53] | TRAIN(075): [400/879] Batch: 0.1182 (0.1246) Data: 0.0105 (0.0149) Loss: 0.3123 (0.4367)
[2022/12/29 02:53] | TRAIN(075): [450/879] Batch: 0.1364 (0.1251) Data: 0.0101 (0.0144) Loss: 0.6023 (0.4374)
[2022/12/29 02:53] | TRAIN(075): [500/879] Batch: 0.1524 (0.1253) Data: 0.0098 (0.0140) Loss: 0.4162 (0.4388)
[2022/12/29 02:54] | TRAIN(075): [550/879] Batch: 0.1487 (0.1261) Data: 0.0123 (0.0137) Loss: 0.4308 (0.4371)
[2022/12/29 02:54] | TRAIN(075): [600/879] Batch: 0.1204 (0.1262) Data: 0.0108 (0.0134) Loss: 0.6866 (0.4376)
[2022/12/29 02:54] | TRAIN(075): [650/879] Batch: 0.1204 (0.1264) Data: 0.0105 (0.0132) Loss: 0.4637 (0.4394)
[2022/12/29 02:54] | TRAIN(075): [700/879] Batch: 0.1459 (0.1267) Data: 0.0137 (0.0130) Loss: 0.5260 (0.4402)
[2022/12/29 02:54] | TRAIN(075): [750/879] Batch: 0.1195 (0.1268) Data: 0.0106 (0.0128) Loss: 0.2869 (0.4423)
[2022/12/29 02:54] | TRAIN(075): [800/879] Batch: 0.1189 (0.1263) Data: 0.0095 (0.0127) Loss: 0.4250 (0.4421)
[2022/12/29 02:54] | TRAIN(075): [850/879] Batch: 0.1182 (0.1259) Data: 0.0101 (0.0125) Loss: 0.3538 (0.4426)
[2022/12/29 02:54] | ------------------------------------------------------------
[2022/12/29 02:54] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:54] | ------------------------------------------------------------
[2022/12/29 02:54] |    TRAIN(75)     0:01:50     0:00:10     0:01:39      0.4425
[2022/12/29 02:54] | ------------------------------------------------------------
[2022/12/29 02:54] | VALID(075): [ 50/220] Batch: 0.0335 (0.0598) Data: 0.0111 (0.0388) Loss: 0.5182 (0.5667)
[2022/12/29 02:54] | VALID(075): [100/220] Batch: 0.0331 (0.0462) Data: 0.0092 (0.0247) Loss: 1.1127 (0.6221)
[2022/12/29 02:54] | VALID(075): [150/220] Batch: 0.0332 (0.0426) Data: 0.0125 (0.0200) Loss: 0.5775 (0.6257)
[2022/12/29 02:54] | VALID(075): [200/220] Batch: 0.0476 (0.0403) Data: 0.0121 (0.0176) Loss: 0.2025 (0.6288)
[2022/12/29 02:54] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:54] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:54] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:54] |    VALID(75)      0.6271      0.8154      0.8710      0.8154      0.8154      0.8154      0.9538
[2022/12/29 02:54] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:54] | ####################################################################################################
[2022/12/29 02:55] | TRAIN(076): [ 50/879] Batch: 0.1243 (0.1655) Data: 0.0104 (0.0471) Loss: 0.4657 (0.4315)
[2022/12/29 02:55] | TRAIN(076): [100/879] Batch: 0.1185 (0.1424) Data: 0.0102 (0.0289) Loss: 0.5295 (0.4336)
[2022/12/29 02:55] | TRAIN(076): [150/879] Batch: 0.1155 (0.1352) Data: 0.0105 (0.0227) Loss: 0.2639 (0.4360)
[2022/12/29 02:55] | TRAIN(076): [200/879] Batch: 0.1204 (0.1312) Data: 0.0107 (0.0196) Loss: 0.4093 (0.4287)
[2022/12/29 02:55] | TRAIN(076): [250/879] Batch: 0.1198 (0.1290) Data: 0.0104 (0.0178) Loss: 0.2778 (0.4288)
[2022/12/29 02:55] | TRAIN(076): [300/879] Batch: 0.1199 (0.1273) Data: 0.0106 (0.0165) Loss: 0.4926 (0.4309)
[2022/12/29 02:55] | TRAIN(076): [350/879] Batch: 0.1196 (0.1263) Data: 0.0099 (0.0156) Loss: 0.3077 (0.4298)
[2022/12/29 02:55] | TRAIN(076): [400/879] Batch: 0.1200 (0.1255) Data: 0.0103 (0.0150) Loss: 0.2612 (0.4284)
[2022/12/29 02:55] | TRAIN(076): [450/879] Batch: 0.1193 (0.1249) Data: 0.0105 (0.0145) Loss: 0.4197 (0.4322)
[2022/12/29 02:55] | TRAIN(076): [500/879] Batch: 0.1193 (0.1244) Data: 0.0103 (0.0141) Loss: 0.5207 (0.4323)
[2022/12/29 02:56] | TRAIN(076): [550/879] Batch: 0.1416 (0.1251) Data: 0.0112 (0.0137) Loss: 0.5056 (0.4323)
[2022/12/29 02:56] | TRAIN(076): [600/879] Batch: 0.1422 (0.1262) Data: 0.0101 (0.0135) Loss: 0.3095 (0.4316)
[2022/12/29 02:56] | TRAIN(076): [650/879] Batch: 0.1373 (0.1283) Data: 0.0109 (0.0133) Loss: 0.4627 (0.4310)
[2022/12/29 02:56] | TRAIN(076): [700/879] Batch: 0.1370 (0.1289) Data: 0.0107 (0.0131) Loss: 0.4466 (0.4319)
[2022/12/29 02:56] | TRAIN(076): [750/879] Batch: 0.1186 (0.1293) Data: 0.0104 (0.0129) Loss: 0.6175 (0.4322)
[2022/12/29 02:56] | TRAIN(076): [800/879] Batch: 0.1182 (0.1287) Data: 0.0099 (0.0128) Loss: 0.5223 (0.4319)
[2022/12/29 02:56] | TRAIN(076): [850/879] Batch: 0.1163 (0.1282) Data: 0.0102 (0.0126) Loss: 0.6176 (0.4320)
[2022/12/29 02:56] | ------------------------------------------------------------
[2022/12/29 02:56] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:56] | ------------------------------------------------------------
[2022/12/29 02:56] |    TRAIN(76)     0:01:53     0:00:11     0:01:42      0.4314
[2022/12/29 02:56] | ------------------------------------------------------------
[2022/12/29 02:56] | VALID(076): [ 50/220] Batch: 0.0430 (0.0712) Data: 0.0119 (0.0399) Loss: 0.5664 (0.5756)
[2022/12/29 02:56] | VALID(076): [100/220] Batch: 0.0332 (0.0542) Data: 0.0096 (0.0255) Loss: 1.2625 (0.6468)
[2022/12/29 02:56] | VALID(076): [150/220] Batch: 0.0341 (0.0472) Data: 0.0100 (0.0205) Loss: 0.5588 (0.6494)
[2022/12/29 02:56] | VALID(076): [200/220] Batch: 0.0343 (0.0439) Data: 0.0098 (0.0180) Loss: 0.2114 (0.6552)
[2022/12/29 02:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:56] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:56] |    VALID(76)      0.6519      0.8141      0.8703      0.8141      0.8141      0.8141      0.9535
[2022/12/29 02:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:56] | ####################################################################################################
[2022/12/29 02:57] | TRAIN(077): [ 50/879] Batch: 0.1369 (0.1797) Data: 0.0099 (0.0503) Loss: 0.2766 (0.4063)
[2022/12/29 02:57] | TRAIN(077): [100/879] Batch: 0.1188 (0.1528) Data: 0.0101 (0.0304) Loss: 0.5723 (0.4143)
[2022/12/29 02:57] | TRAIN(077): [150/879] Batch: 0.1149 (0.1419) Data: 0.0108 (0.0236) Loss: 0.3782 (0.4251)
[2022/12/29 02:57] | TRAIN(077): [200/879] Batch: 0.1146 (0.1359) Data: 0.0105 (0.0203) Loss: 0.3827 (0.4317)
[2022/12/29 02:57] | TRAIN(077): [250/879] Batch: 0.1186 (0.1329) Data: 0.0105 (0.0183) Loss: 0.2677 (0.4387)
[2022/12/29 02:57] | TRAIN(077): [300/879] Batch: 0.1177 (0.1306) Data: 0.0096 (0.0169) Loss: 0.4111 (0.4375)
[2022/12/29 02:57] | TRAIN(077): [350/879] Batch: 0.1261 (0.1290) Data: 0.0097 (0.0160) Loss: 0.3158 (0.4369)
[2022/12/29 02:57] | TRAIN(077): [400/879] Batch: 0.1189 (0.1278) Data: 0.0105 (0.0152) Loss: 0.2996 (0.4319)
[2022/12/29 02:57] | TRAIN(077): [450/879] Batch: 0.1156 (0.1269) Data: 0.0104 (0.0147) Loss: 0.4888 (0.4321)
[2022/12/29 02:57] | TRAIN(077): [500/879] Batch: 0.1196 (0.1266) Data: 0.0102 (0.0142) Loss: 0.2840 (0.4310)
[2022/12/29 02:58] | TRAIN(077): [550/879] Batch: 0.1174 (0.1260) Data: 0.0095 (0.0139) Loss: 0.5343 (0.4289)
[2022/12/29 02:58] | TRAIN(077): [600/879] Batch: 0.1179 (0.1255) Data: 0.0105 (0.0136) Loss: 0.2867 (0.4289)
[2022/12/29 02:58] | TRAIN(077): [650/879] Batch: 0.1211 (0.1251) Data: 0.0100 (0.0133) Loss: 0.5162 (0.4273)
[2022/12/29 02:58] | TRAIN(077): [700/879] Batch: 0.1195 (0.1253) Data: 0.0106 (0.0131) Loss: 0.4613 (0.4280)
[2022/12/29 02:58] | TRAIN(077): [750/879] Batch: 0.1195 (0.1251) Data: 0.0101 (0.0130) Loss: 0.5598 (0.4292)
[2022/12/29 02:58] | TRAIN(077): [800/879] Batch: 0.1199 (0.1248) Data: 0.0104 (0.0128) Loss: 0.3271 (0.4311)
[2022/12/29 02:58] | TRAIN(077): [850/879] Batch: 0.1204 (0.1245) Data: 0.0097 (0.0126) Loss: 0.2841 (0.4335)
[2022/12/29 02:58] | ------------------------------------------------------------
[2022/12/29 02:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:58] | ------------------------------------------------------------
[2022/12/29 02:58] |    TRAIN(77)     0:01:49     0:00:11     0:01:38      0.4325
[2022/12/29 02:58] | ------------------------------------------------------------
[2022/12/29 02:58] | VALID(077): [ 50/220] Batch: 0.0331 (0.0584) Data: 0.0097 (0.0377) Loss: 0.5075 (0.5800)
[2022/12/29 02:58] | VALID(077): [100/220] Batch: 0.0320 (0.0458) Data: 0.0104 (0.0241) Loss: 1.2801 (0.6386)
[2022/12/29 02:58] | VALID(077): [150/220] Batch: 0.0365 (0.0418) Data: 0.0128 (0.0196) Loss: 0.5615 (0.6384)
[2022/12/29 02:58] | VALID(077): [200/220] Batch: 0.0407 (0.0407) Data: 0.0122 (0.0174) Loss: 0.2606 (0.6426)
[2022/12/29 02:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:58] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:58] |    VALID(77)      0.6396      0.8120      0.8702      0.8120      0.8120      0.8120      0.9530
[2022/12/29 02:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:58] | ####################################################################################################
[2022/12/29 02:59] | TRAIN(078): [ 50/879] Batch: 0.1194 (0.1601) Data: 0.0104 (0.0460) Loss: 0.4722 (0.4238)
[2022/12/29 02:59] | TRAIN(078): [100/879] Batch: 0.1497 (0.1418) Data: 0.0175 (0.0286) Loss: 0.6630 (0.4323)
[2022/12/29 02:59] | TRAIN(078): [150/879] Batch: 0.1260 (0.1346) Data: 0.0099 (0.0226) Loss: 0.4551 (0.4283)
[2022/12/29 02:59] | TRAIN(078): [200/879] Batch: 0.1345 (0.1314) Data: 0.0096 (0.0195) Loss: 0.5944 (0.4346)
[2022/12/29 02:59] | TRAIN(078): [250/879] Batch: 0.1197 (0.1321) Data: 0.0104 (0.0177) Loss: 0.5097 (0.4332)
[2022/12/29 02:59] | TRAIN(078): [300/879] Batch: 0.1346 (0.1310) Data: 0.0102 (0.0165) Loss: 0.2997 (0.4310)
[2022/12/29 02:59] | TRAIN(078): [350/879] Batch: 0.1197 (0.1305) Data: 0.0101 (0.0157) Loss: 0.2507 (0.4273)
[2022/12/29 02:59] | TRAIN(078): [400/879] Batch: 0.1199 (0.1293) Data: 0.0102 (0.0150) Loss: 0.3367 (0.4299)
[2022/12/29 02:59] | TRAIN(078): [450/879] Batch: 0.1378 (0.1288) Data: 0.0175 (0.0145) Loss: 0.4829 (0.4301)
[2022/12/29 02:59] | TRAIN(078): [500/879] Batch: 0.1173 (0.1288) Data: 0.0072 (0.0141) Loss: 0.3978 (0.4299)
[2022/12/29 03:00] | TRAIN(078): [550/879] Batch: 0.1517 (0.1303) Data: 0.0152 (0.0138) Loss: 0.5387 (0.4299)
[2022/12/29 03:00] | TRAIN(078): [600/879] Batch: 0.1596 (0.1307) Data: 0.0082 (0.0135) Loss: 0.6056 (0.4303)
[2022/12/29 03:00] | TRAIN(078): [650/879] Batch: 0.1222 (0.1318) Data: 0.0107 (0.0132) Loss: 0.3258 (0.4305)
[2022/12/29 03:00] | TRAIN(078): [700/879] Batch: 0.1498 (0.1318) Data: 0.0154 (0.0131) Loss: 0.3233 (0.4305)
[2022/12/29 03:00] | TRAIN(078): [750/879] Batch: 0.1224 (0.1318) Data: 0.0127 (0.0129) Loss: 0.4091 (0.4302)
[2022/12/29 03:00] | TRAIN(078): [800/879] Batch: 0.1183 (0.1322) Data: 0.0101 (0.0128) Loss: 0.3857 (0.4296)
[2022/12/29 03:00] | TRAIN(078): [850/879] Batch: 0.1178 (0.1327) Data: 0.0072 (0.0127) Loss: 0.1659 (0.4279)
[2022/12/29 03:00] | ------------------------------------------------------------
[2022/12/29 03:00] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:00] | ------------------------------------------------------------
[2022/12/29 03:00] |    TRAIN(78)     0:01:56     0:00:11     0:01:45      0.4280
[2022/12/29 03:00] | ------------------------------------------------------------
[2022/12/29 03:00] | VALID(078): [ 50/220] Batch: 0.0319 (0.0676) Data: 0.0071 (0.0374) Loss: 0.4491 (0.5695)
[2022/12/29 03:00] | VALID(078): [100/220] Batch: 0.0428 (0.0511) Data: 0.0148 (0.0236) Loss: 1.2926 (0.6344)
[2022/12/29 03:00] | VALID(078): [150/220] Batch: 0.0397 (0.0464) Data: 0.0109 (0.0191) Loss: 0.5002 (0.6401)
[2022/12/29 03:01] | VALID(078): [200/220] Batch: 0.0429 (0.0449) Data: 0.0125 (0.0170) Loss: 0.2246 (0.6485)
[2022/12/29 03:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:01] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:01] |    VALID(78)      0.6466      0.8127      0.8718      0.8127      0.8127      0.8127      0.9532
[2022/12/29 03:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:01] | ####################################################################################################
[2022/12/29 03:01] | TRAIN(079): [ 50/879] Batch: 0.1215 (0.1722) Data: 0.0077 (0.0436) Loss: 0.4285 (0.3951)
[2022/12/29 03:01] | TRAIN(079): [100/879] Batch: 0.1215 (0.1488) Data: 0.0096 (0.0262) Loss: 0.2835 (0.4162)
[2022/12/29 03:01] | TRAIN(079): [150/879] Batch: 0.1211 (0.1415) Data: 0.0085 (0.0202) Loss: 0.3202 (0.4228)
[2022/12/29 03:01] | TRAIN(079): [200/879] Batch: 0.1182 (0.1376) Data: 0.0072 (0.0173) Loss: 0.3444 (0.4216)
[2022/12/29 03:01] | TRAIN(079): [250/879] Batch: 0.1168 (0.1356) Data: 0.0075 (0.0154) Loss: 0.1489 (0.4267)
[2022/12/29 03:01] | TRAIN(079): [300/879] Batch: 0.1389 (0.1349) Data: 0.0071 (0.0142) Loss: 0.4560 (0.4234)
[2022/12/29 03:01] | TRAIN(079): [350/879] Batch: 0.1323 (0.1347) Data: 0.0072 (0.0133) Loss: 0.4062 (0.4233)
[2022/12/29 03:01] | TRAIN(079): [400/879] Batch: 0.1194 (0.1339) Data: 0.0079 (0.0126) Loss: 0.2078 (0.4212)
[2022/12/29 03:02] | TRAIN(079): [450/879] Batch: 0.1165 (0.1331) Data: 0.0071 (0.0121) Loss: 0.2695 (0.4197)
[2022/12/29 03:02] | TRAIN(079): [500/879] Batch: 0.1231 (0.1326) Data: 0.0095 (0.0118) Loss: 0.2585 (0.4181)
[2022/12/29 03:02] | TRAIN(079): [550/879] Batch: 0.1213 (0.1328) Data: 0.0073 (0.0115) Loss: 0.2368 (0.4198)
[2022/12/29 03:02] | TRAIN(079): [600/879] Batch: 0.1246 (0.1322) Data: 0.0082 (0.0112) Loss: 0.2417 (0.4194)
[2022/12/29 03:02] | TRAIN(079): [650/879] Batch: 0.1203 (0.1316) Data: 0.0079 (0.0109) Loss: 0.4634 (0.4189)
[2022/12/29 03:02] | TRAIN(079): [700/879] Batch: 0.1155 (0.1310) Data: 0.0071 (0.0107) Loss: 0.4596 (0.4218)
[2022/12/29 03:02] | TRAIN(079): [750/879] Batch: 0.1243 (0.1306) Data: 0.0078 (0.0105) Loss: 0.3476 (0.4218)
[2022/12/29 03:02] | TRAIN(079): [800/879] Batch: 0.1238 (0.1306) Data: 0.0075 (0.0103) Loss: 0.3433 (0.4206)
[2022/12/29 03:02] | TRAIN(079): [850/879] Batch: 0.1385 (0.1305) Data: 0.0073 (0.0102) Loss: 0.3870 (0.4197)
[2022/12/29 03:02] | ------------------------------------------------------------
[2022/12/29 03:02] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:02] | ------------------------------------------------------------
[2022/12/29 03:02] |    TRAIN(79)     0:01:54     0:00:08     0:01:45      0.4203
[2022/12/29 03:02] | ------------------------------------------------------------
[2022/12/29 03:02] | VALID(079): [ 50/220] Batch: 0.0409 (0.0666) Data: 0.0110 (0.0374) Loss: 0.5257 (0.5867)
[2022/12/29 03:03] | VALID(079): [100/220] Batch: 0.0396 (0.0534) Data: 0.0092 (0.0234) Loss: 1.3352 (0.6507)
[2022/12/29 03:03] | VALID(079): [150/220] Batch: 0.0312 (0.0486) Data: 0.0145 (0.0187) Loss: 0.4941 (0.6521)
[2022/12/29 03:03] | VALID(079): [200/220] Batch: 0.0403 (0.0459) Data: 0.0089 (0.0163) Loss: 0.2016 (0.6582)
[2022/12/29 03:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:03] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:03] |    VALID(79)      0.6544      0.8147      0.8728      0.8147      0.8147      0.8147      0.9537
[2022/12/29 03:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:03] | ####################################################################################################
[2022/12/29 03:03] | TRAIN(080): [ 50/879] Batch: 0.1203 (0.1628) Data: 0.0068 (0.0445) Loss: 0.3506 (0.4257)
[2022/12/29 03:03] | TRAIN(080): [100/879] Batch: 0.1237 (0.1452) Data: 0.0070 (0.0267) Loss: 0.4067 (0.4119)
[2022/12/29 03:03] | TRAIN(080): [150/879] Batch: 0.1515 (0.1407) Data: 0.0096 (0.0208) Loss: 0.5037 (0.4127)
[2022/12/29 03:03] | TRAIN(080): [200/879] Batch: 0.1328 (0.1377) Data: 0.0072 (0.0176) Loss: 0.8424 (0.4155)
[2022/12/29 03:03] | TRAIN(080): [250/879] Batch: 0.1383 (0.1366) Data: 0.0072 (0.0157) Loss: 0.5729 (0.4180)
[2022/12/29 03:03] | TRAIN(080): [300/879] Batch: 0.1405 (0.1355) Data: 0.0084 (0.0144) Loss: 0.4046 (0.4192)
[2022/12/29 03:03] | TRAIN(080): [350/879] Batch: 0.1413 (0.1348) Data: 0.0076 (0.0136) Loss: 0.6043 (0.4208)
[2022/12/29 03:03] | TRAIN(080): [400/879] Batch: 0.1378 (0.1341) Data: 0.0072 (0.0129) Loss: 0.3554 (0.4203)
[2022/12/29 03:04] | TRAIN(080): [450/879] Batch: 0.1259 (0.1344) Data: 0.0072 (0.0123) Loss: 0.3915 (0.4171)
[2022/12/29 03:04] | TRAIN(080): [500/879] Batch: 0.1218 (0.1338) Data: 0.0082 (0.0119) Loss: 0.4541 (0.4207)
[2022/12/29 03:04] | TRAIN(080): [550/879] Batch: 0.1170 (0.1331) Data: 0.0080 (0.0115) Loss: 0.3351 (0.4186)
[2022/12/29 03:04] | TRAIN(080): [600/879] Batch: 0.1215 (0.1324) Data: 0.0078 (0.0112) Loss: 0.3187 (0.4202)
[2022/12/29 03:04] | TRAIN(080): [650/879] Batch: 0.1183 (0.1318) Data: 0.0095 (0.0110) Loss: 0.8122 (0.4198)
[2022/12/29 03:04] | TRAIN(080): [700/879] Batch: 0.1220 (0.1319) Data: 0.0071 (0.0108) Loss: 0.2524 (0.4193)
[2022/12/29 03:04] | TRAIN(080): [750/879] Batch: 0.1324 (0.1316) Data: 0.0109 (0.0107) Loss: 0.3065 (0.4211)
[2022/12/29 03:04] | TRAIN(080): [800/879] Batch: 0.1232 (0.1314) Data: 0.0078 (0.0105) Loss: 0.2294 (0.4205)
[2022/12/29 03:04] | TRAIN(080): [850/879] Batch: 0.1220 (0.1313) Data: 0.0073 (0.0104) Loss: 0.3255 (0.4205)
[2022/12/29 03:05] | ------------------------------------------------------------
[2022/12/29 03:05] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:05] | ------------------------------------------------------------
[2022/12/29 03:05] |    TRAIN(80)     0:01:55     0:00:09     0:01:46      0.4215
[2022/12/29 03:05] | ------------------------------------------------------------
[2022/12/29 03:05] | VALID(080): [ 50/220] Batch: 0.0361 (0.0686) Data: 0.0079 (0.0378) Loss: 0.4788 (0.5866)
[2022/12/29 03:05] | VALID(080): [100/220] Batch: 0.0408 (0.0543) Data: 0.0093 (0.0237) Loss: 1.3300 (0.6495)
[2022/12/29 03:05] | VALID(080): [150/220] Batch: 0.0404 (0.0498) Data: 0.0092 (0.0190) Loss: 0.5259 (0.6563)
[2022/12/29 03:05] | VALID(080): [200/220] Batch: 0.0374 (0.0476) Data: 0.0145 (0.0166) Loss: 0.2742 (0.6628)
[2022/12/29 03:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:05] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:05] |    VALID(80)      0.6600      0.8121      0.8709      0.8121      0.8121      0.8121      0.9530
[2022/12/29 03:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:05] | ####################################################################################################
[2022/12/29 03:05] | TRAIN(081): [ 50/879] Batch: 0.1314 (0.1691) Data: 0.0080 (0.0437) Loss: 0.3386 (0.4111)
[2022/12/29 03:05] | TRAIN(081): [100/879] Batch: 0.1218 (0.1490) Data: 0.0082 (0.0259) Loss: 0.3972 (0.4292)
[2022/12/29 03:05] | TRAIN(081): [150/879] Batch: 0.1220 (0.1417) Data: 0.0070 (0.0199) Loss: 0.6222 (0.4255)
[2022/12/29 03:05] | TRAIN(081): [200/879] Batch: 0.1329 (0.1396) Data: 0.0072 (0.0168) Loss: 0.3187 (0.4283)
[2022/12/29 03:05] | TRAIN(081): [250/879] Batch: 0.1203 (0.1369) Data: 0.0072 (0.0150) Loss: 0.4631 (0.4292)
[2022/12/29 03:05] | TRAIN(081): [300/879] Batch: 0.1327 (0.1367) Data: 0.0072 (0.0137) Loss: 0.3157 (0.4252)
[2022/12/29 03:05] | TRAIN(081): [350/879] Batch: 0.1203 (0.1350) Data: 0.0072 (0.0129) Loss: 0.6877 (0.4265)
[2022/12/29 03:06] | TRAIN(081): [400/879] Batch: 0.1392 (0.1351) Data: 0.0073 (0.0122) Loss: 0.2153 (0.4228)
[2022/12/29 03:06] | TRAIN(081): [450/879] Batch: 0.1225 (0.1354) Data: 0.0091 (0.0117) Loss: 0.4464 (0.4208)
[2022/12/29 03:06] | TRAIN(081): [500/879] Batch: 0.1251 (0.1348) Data: 0.0074 (0.0113) Loss: 0.3250 (0.4215)
[2022/12/29 03:06] | TRAIN(081): [550/879] Batch: 0.1213 (0.1349) Data: 0.0075 (0.0109) Loss: 0.4408 (0.4249)
[2022/12/29 03:06] | TRAIN(081): [600/879] Batch: 0.1222 (0.1346) Data: 0.0075 (0.0107) Loss: 0.2057 (0.4233)
[2022/12/29 03:06] | TRAIN(081): [650/879] Batch: 0.1354 (0.1345) Data: 0.0076 (0.0105) Loss: 0.2547 (0.4248)
[2022/12/29 03:06] | TRAIN(081): [700/879] Batch: 0.1227 (0.1341) Data: 0.0075 (0.0103) Loss: 0.4133 (0.4234)
[2022/12/29 03:06] | TRAIN(081): [750/879] Batch: 0.1359 (0.1347) Data: 0.0074 (0.0101) Loss: 0.3297 (0.4240)
[2022/12/29 03:06] | TRAIN(081): [800/879] Batch: 0.1395 (0.1350) Data: 0.0082 (0.0099) Loss: 0.1965 (0.4210)
[2022/12/29 03:07] | TRAIN(081): [850/879] Batch: 0.1403 (0.1355) Data: 0.0073 (0.0098) Loss: 0.5059 (0.4205)
[2022/12/29 03:07] | ------------------------------------------------------------
[2022/12/29 03:07] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:07] | ------------------------------------------------------------
[2022/12/29 03:07] |    TRAIN(81)     0:01:59     0:00:08     0:01:50      0.4204
[2022/12/29 03:07] | ------------------------------------------------------------
[2022/12/29 03:07] | VALID(081): [ 50/220] Batch: 0.0404 (0.0697) Data: 0.0093 (0.0382) Loss: 0.5172 (0.5891)
[2022/12/29 03:07] | VALID(081): [100/220] Batch: 0.0321 (0.0527) Data: 0.0085 (0.0235) Loss: 1.3721 (0.6560)
[2022/12/29 03:07] | VALID(081): [150/220] Batch: 0.0279 (0.0464) Data: 0.0090 (0.0185) Loss: 0.4957 (0.6590)
[2022/12/29 03:07] | VALID(081): [200/220] Batch: 0.0318 (0.0434) Data: 0.0072 (0.0163) Loss: 0.2230 (0.6639)
[2022/12/29 03:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:07] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:07] |    VALID(81)      0.6618      0.8126      0.8697      0.8126      0.8126      0.8126      0.9531
[2022/12/29 03:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:07] | ####################################################################################################
[2022/12/29 03:07] | TRAIN(082): [ 50/879] Batch: 0.1364 (0.1776) Data: 0.0072 (0.0463) Loss: 0.4079 (0.4266)
[2022/12/29 03:07] | TRAIN(082): [100/879] Batch: 0.1499 (0.1538) Data: 0.0094 (0.0282) Loss: 0.3688 (0.4094)
[2022/12/29 03:07] | TRAIN(082): [150/879] Batch: 0.1464 (0.1498) Data: 0.0075 (0.0216) Loss: 0.2742 (0.4102)
[2022/12/29 03:07] | TRAIN(082): [200/879] Batch: 0.1211 (0.1442) Data: 0.0074 (0.0182) Loss: 0.4265 (0.4099)
[2022/12/29 03:07] | TRAIN(082): [250/879] Batch: 0.1378 (0.1428) Data: 0.0074 (0.0164) Loss: 0.2805 (0.4088)
[2022/12/29 03:08] | TRAIN(082): [300/879] Batch: 0.1215 (0.1429) Data: 0.0096 (0.0151) Loss: 0.4479 (0.4092)
[2022/12/29 03:08] | TRAIN(082): [350/879] Batch: 0.1592 (0.1406) Data: 0.0098 (0.0141) Loss: 0.3079 (0.4041)
[2022/12/29 03:08] | TRAIN(082): [400/879] Batch: 0.1266 (0.1390) Data: 0.0080 (0.0133) Loss: 0.4616 (0.4059)
[2022/12/29 03:08] | TRAIN(082): [450/879] Batch: 0.1570 (0.1393) Data: 0.0075 (0.0128) Loss: 0.3916 (0.4028)
[2022/12/29 03:08] | TRAIN(082): [500/879] Batch: 0.1385 (0.1394) Data: 0.0070 (0.0123) Loss: 0.3984 (0.4056)
[2022/12/29 03:08] | TRAIN(082): [550/879] Batch: 0.1347 (0.1389) Data: 0.0072 (0.0119) Loss: 0.3125 (0.4059)
[2022/12/29 03:08] | TRAIN(082): [600/879] Batch: 0.1327 (0.1385) Data: 0.0073 (0.0116) Loss: 0.7742 (0.4084)
[2022/12/29 03:08] | TRAIN(082): [650/879] Batch: 0.1231 (0.1375) Data: 0.0093 (0.0113) Loss: 0.4630 (0.4083)
[2022/12/29 03:08] | TRAIN(082): [700/879] Batch: 0.1341 (0.1374) Data: 0.0071 (0.0110) Loss: 0.4262 (0.4107)
[2022/12/29 03:09] | TRAIN(082): [750/879] Batch: 0.1327 (0.1367) Data: 0.0072 (0.0108) Loss: 0.5210 (0.4127)
[2022/12/29 03:09] | TRAIN(082): [800/879] Batch: 0.1167 (0.1358) Data: 0.0072 (0.0106) Loss: 0.4679 (0.4139)
[2022/12/29 03:09] | TRAIN(082): [850/879] Batch: 0.1265 (0.1347) Data: 0.0073 (0.0104) Loss: 0.4899 (0.4145)
[2022/12/29 03:09] | ------------------------------------------------------------
[2022/12/29 03:09] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:09] | ------------------------------------------------------------
[2022/12/29 03:09] |    TRAIN(82)     0:01:58     0:00:09     0:01:49      0.4141
[2022/12/29 03:09] | ------------------------------------------------------------
[2022/12/29 03:09] | VALID(082): [ 50/220] Batch: 0.0406 (0.0682) Data: 0.0091 (0.0360) Loss: 0.4752 (0.5889)
[2022/12/29 03:09] | VALID(082): [100/220] Batch: 0.0409 (0.0548) Data: 0.0092 (0.0228) Loss: 1.3850 (0.6574)
[2022/12/29 03:09] | VALID(082): [150/220] Batch: 0.0407 (0.0502) Data: 0.0094 (0.0184) Loss: 0.5098 (0.6643)
[2022/12/29 03:09] | VALID(082): [200/220] Batch: 0.0404 (0.0479) Data: 0.0091 (0.0161) Loss: 0.2156 (0.6727)
[2022/12/29 03:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:09] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:09] |    VALID(82)      0.6693      0.8076      0.8684      0.8076      0.8076      0.8076      0.9519
[2022/12/29 03:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:09] | ####################################################################################################
[2022/12/29 03:09] | TRAIN(083): [ 50/879] Batch: 0.1496 (0.1779) Data: 0.0089 (0.0455) Loss: 0.3945 (0.4105)
[2022/12/29 03:09] | TRAIN(083): [100/879] Batch: 0.1261 (0.1557) Data: 0.0096 (0.0280) Loss: 0.1708 (0.4024)
[2022/12/29 03:09] | TRAIN(083): [150/879] Batch: 0.1162 (0.1475) Data: 0.0081 (0.0218) Loss: 0.5490 (0.4142)
[2022/12/29 03:09] | TRAIN(083): [200/879] Batch: 0.1275 (0.1436) Data: 0.0068 (0.0188) Loss: 0.3533 (0.4138)
[2022/12/29 03:10] | TRAIN(083): [250/879] Batch: 0.1452 (0.1426) Data: 0.0073 (0.0169) Loss: 0.2972 (0.4122)
[2022/12/29 03:10] | TRAIN(083): [300/879] Batch: 0.1212 (0.1400) Data: 0.0075 (0.0157) Loss: 0.3181 (0.4087)
[2022/12/29 03:10] | TRAIN(083): [350/879] Batch: 0.1221 (0.1397) Data: 0.0080 (0.0147) Loss: 0.3059 (0.4078)
[2022/12/29 03:10] | TRAIN(083): [400/879] Batch: 0.1335 (0.1390) Data: 0.0072 (0.0141) Loss: 0.5167 (0.4042)
[2022/12/29 03:10] | TRAIN(083): [450/879] Batch: 0.1269 (0.1385) Data: 0.0104 (0.0136) Loss: 0.2698 (0.4057)
[2022/12/29 03:10] | TRAIN(083): [500/879] Batch: 0.1341 (0.1379) Data: 0.0069 (0.0131) Loss: 0.3409 (0.4046)
[2022/12/29 03:10] | TRAIN(083): [550/879] Batch: 0.1424 (0.1373) Data: 0.0093 (0.0127) Loss: 0.4973 (0.4061)
[2022/12/29 03:10] | TRAIN(083): [600/879] Batch: 0.1378 (0.1374) Data: 0.0072 (0.0124) Loss: 0.5431 (0.4085)
[2022/12/29 03:10] | TRAIN(083): [650/879] Batch: 0.1201 (0.1374) Data: 0.0072 (0.0120) Loss: 0.4624 (0.4077)
[2022/12/29 03:11] | TRAIN(083): [700/879] Batch: 0.1202 (0.1367) Data: 0.0093 (0.0119) Loss: 0.3496 (0.4085)
[2022/12/29 03:11] | TRAIN(083): [750/879] Batch: 0.1362 (0.1364) Data: 0.0074 (0.0117) Loss: 0.3633 (0.4096)
[2022/12/29 03:11] | TRAIN(083): [800/879] Batch: 0.1257 (0.1362) Data: 0.0083 (0.0115) Loss: 0.3784 (0.4099)
[2022/12/29 03:11] | TRAIN(083): [850/879] Batch: 0.1217 (0.1361) Data: 0.0078 (0.0114) Loss: 0.1831 (0.4101)
[2022/12/29 03:11] | ------------------------------------------------------------
[2022/12/29 03:11] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:11] | ------------------------------------------------------------
[2022/12/29 03:11] |    TRAIN(83)     0:01:59     0:00:09     0:01:49      0.4099
[2022/12/29 03:11] | ------------------------------------------------------------
[2022/12/29 03:11] | VALID(083): [ 50/220] Batch: 0.0407 (0.0683) Data: 0.0091 (0.0362) Loss: 0.4115 (0.5898)
[2022/12/29 03:11] | VALID(083): [100/220] Batch: 0.0407 (0.0545) Data: 0.0092 (0.0229) Loss: 1.3530 (0.6581)
[2022/12/29 03:11] | VALID(083): [150/220] Batch: 0.0402 (0.0500) Data: 0.0092 (0.0184) Loss: 0.5296 (0.6626)
[2022/12/29 03:11] | VALID(083): [200/220] Batch: 0.0435 (0.0477) Data: 0.0100 (0.0162) Loss: 0.2472 (0.6723)
[2022/12/29 03:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:11] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:11] |    VALID(83)      0.6706      0.8108      0.8687      0.8108      0.8108      0.8108      0.9527
[2022/12/29 03:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:11] | ####################################################################################################
[2022/12/29 03:11] | TRAIN(084): [ 50/879] Batch: 0.1533 (0.1755) Data: 0.0175 (0.0476) Loss: 0.3768 (0.3803)
[2022/12/29 03:11] | TRAIN(084): [100/879] Batch: 0.1235 (0.1506) Data: 0.0075 (0.0293) Loss: 0.3582 (0.3951)
[2022/12/29 03:12] | TRAIN(084): [150/879] Batch: 0.1176 (0.1467) Data: 0.0107 (0.0234) Loss: 0.3572 (0.4042)
[2022/12/29 03:12] | TRAIN(084): [200/879] Batch: 0.1357 (0.1446) Data: 0.0075 (0.0203) Loss: 0.3118 (0.4013)
[2022/12/29 03:12] | TRAIN(084): [250/879] Batch: 0.1164 (0.1436) Data: 0.0177 (0.0185) Loss: 0.3421 (0.4046)
[2022/12/29 03:12] | TRAIN(084): [300/879] Batch: 0.1723 (0.1463) Data: 0.0101 (0.0172) Loss: 0.4270 (0.4029)
[2022/12/29 03:12] | TRAIN(084): [350/879] Batch: 0.1685 (0.1473) Data: 0.0154 (0.0163) Loss: 0.2060 (0.4023)
[2022/12/29 03:12] | TRAIN(084): [400/879] Batch: 0.1233 (0.1476) Data: 0.0102 (0.0156) Loss: 0.2912 (0.4057)
[2022/12/29 03:12] | TRAIN(084): [450/879] Batch: 0.1212 (0.1461) Data: 0.0129 (0.0151) Loss: 0.6410 (0.4086)
[2022/12/29 03:12] | TRAIN(084): [500/879] Batch: 0.1184 (0.1448) Data: 0.0079 (0.0147) Loss: 0.1655 (0.4063)
[2022/12/29 03:12] | TRAIN(084): [550/879] Batch: 0.1605 (0.1442) Data: 0.0127 (0.0143) Loss: 0.2886 (0.4073)
[2022/12/29 03:13] | TRAIN(084): [600/879] Batch: 0.1221 (0.1437) Data: 0.0073 (0.0140) Loss: 0.3366 (0.4070)
[2022/12/29 03:13] | TRAIN(084): [650/879] Batch: 0.1164 (0.1430) Data: 0.0133 (0.0138) Loss: 0.5695 (0.4103)
[2022/12/29 03:13] | TRAIN(084): [700/879] Batch: 0.1279 (0.1425) Data: 0.0068 (0.0136) Loss: 0.4853 (0.4118)
[2022/12/29 03:13] | TRAIN(084): [750/879] Batch: 0.1224 (0.1420) Data: 0.0182 (0.0134) Loss: 0.3789 (0.4127)
[2022/12/29 03:13] | TRAIN(084): [800/879] Batch: 0.1343 (0.1417) Data: 0.0099 (0.0133) Loss: 0.4799 (0.4144)
[2022/12/29 03:13] | TRAIN(084): [850/879] Batch: 0.1261 (0.1409) Data: 0.0180 (0.0131) Loss: 0.5042 (0.4119)
[2022/12/29 03:13] | ------------------------------------------------------------
[2022/12/29 03:13] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:13] | ------------------------------------------------------------
[2022/12/29 03:13] |    TRAIN(84)     0:02:03     0:00:11     0:01:51      0.4109
[2022/12/29 03:13] | ------------------------------------------------------------
[2022/12/29 03:13] | VALID(084): [ 50/220] Batch: 0.0361 (0.0631) Data: 0.0084 (0.0361) Loss: 0.3850 (0.5719)
[2022/12/29 03:13] | VALID(084): [100/220] Batch: 0.0323 (0.0497) Data: 0.0175 (0.0235) Loss: 1.3037 (0.6423)
[2022/12/29 03:13] | VALID(084): [150/220] Batch: 0.0261 (0.0452) Data: 0.0080 (0.0191) Loss: 0.5321 (0.6490)
[2022/12/29 03:13] | VALID(084): [200/220] Batch: 0.0408 (0.0441) Data: 0.0092 (0.0169) Loss: 0.2349 (0.6581)
[2022/12/29 03:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:13] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:13] |    VALID(84)      0.6564      0.8160      0.8716      0.8160      0.8160      0.8160      0.9540
[2022/12/29 03:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:13] | ####################################################################################################
[2022/12/29 03:14] | TRAIN(085): [ 50/879] Batch: 0.1300 (0.1705) Data: 0.0073 (0.0460) Loss: 0.3753 (0.3908)
[2022/12/29 03:14] | TRAIN(085): [100/879] Batch: 0.1317 (0.1545) Data: 0.0095 (0.0279) Loss: 0.4623 (0.4038)
[2022/12/29 03:14] | TRAIN(085): [150/879] Batch: 0.1765 (0.1530) Data: 0.0150 (0.0220) Loss: 0.1967 (0.4031)
[2022/12/29 03:14] | TRAIN(085): [200/879] Batch: 0.1335 (0.1464) Data: 0.0068 (0.0185) Loss: 0.3231 (0.4008)
[2022/12/29 03:14] | TRAIN(085): [250/879] Batch: 0.1525 (0.1419) Data: 0.0095 (0.0164) Loss: 0.2543 (0.3969)
[2022/12/29 03:14] | TRAIN(085): [300/879] Batch: 0.1365 (0.1411) Data: 0.0074 (0.0152) Loss: 0.3373 (0.3962)
[2022/12/29 03:14] | TRAIN(085): [350/879] Batch: 0.1378 (0.1418) Data: 0.0071 (0.0142) Loss: 0.6022 (0.4032)
[2022/12/29 03:14] | TRAIN(085): [400/879] Batch: 0.1261 (0.1402) Data: 0.0073 (0.0134) Loss: 0.4359 (0.4021)
[2022/12/29 03:14] | TRAIN(085): [450/879] Batch: 0.1324 (0.1394) Data: 0.0072 (0.0128) Loss: 0.3657 (0.4003)
[2022/12/29 03:15] | TRAIN(085): [500/879] Batch: 0.1229 (0.1381) Data: 0.0080 (0.0123) Loss: 0.6625 (0.3989)
[2022/12/29 03:15] | TRAIN(085): [550/879] Batch: 0.1471 (0.1376) Data: 0.0080 (0.0119) Loss: 0.4646 (0.3994)
[2022/12/29 03:15] | TRAIN(085): [600/879] Batch: 0.1671 (0.1372) Data: 0.0096 (0.0116) Loss: 0.2556 (0.4009)
[2022/12/29 03:15] | TRAIN(085): [650/879] Batch: 0.1201 (0.1369) Data: 0.0078 (0.0113) Loss: 0.4052 (0.4029)
[2022/12/29 03:15] | TRAIN(085): [700/879] Batch: 0.1231 (0.1361) Data: 0.0073 (0.0111) Loss: 0.3406 (0.4032)
[2022/12/29 03:15] | TRAIN(085): [750/879] Batch: 0.1212 (0.1354) Data: 0.0075 (0.0109) Loss: 0.5742 (0.4031)
[2022/12/29 03:15] | TRAIN(085): [800/879] Batch: 0.1237 (0.1348) Data: 0.0081 (0.0107) Loss: 0.5880 (0.4030)
[2022/12/29 03:15] | TRAIN(085): [850/879] Batch: 0.1296 (0.1341) Data: 0.0082 (0.0105) Loss: 0.5012 (0.4030)
[2022/12/29 03:15] | ------------------------------------------------------------
[2022/12/29 03:15] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:15] | ------------------------------------------------------------
[2022/12/29 03:15] |    TRAIN(85)     0:01:57     0:00:09     0:01:48      0.4042
[2022/12/29 03:15] | ------------------------------------------------------------
[2022/12/29 03:15] | VALID(085): [ 50/220] Batch: 0.0273 (0.0646) Data: 0.0080 (0.0364) Loss: 0.4386 (0.5898)
[2022/12/29 03:15] | VALID(085): [100/220] Batch: 0.0403 (0.0499) Data: 0.0109 (0.0225) Loss: 1.3832 (0.6642)
[2022/12/29 03:15] | VALID(085): [150/220] Batch: 0.0426 (0.0467) Data: 0.0097 (0.0181) Loss: 0.5268 (0.6749)
[2022/12/29 03:15] | VALID(085): [200/220] Batch: 0.0357 (0.0448) Data: 0.0073 (0.0158) Loss: 0.2155 (0.6817)
[2022/12/29 03:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:15] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:15] |    VALID(85)      0.6806      0.8150      0.8705      0.8150      0.8150      0.8150      0.9537
[2022/12/29 03:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:15] | ####################################################################################################
[2022/12/29 03:16] | TRAIN(086): [ 50/879] Batch: 0.1280 (0.1698) Data: 0.0171 (0.0450) Loss: 0.5680 (0.4169)
[2022/12/29 03:16] | TRAIN(086): [100/879] Batch: 0.1167 (0.1543) Data: 0.0176 (0.0277) Loss: 0.1997 (0.4165)
[2022/12/29 03:16] | TRAIN(086): [150/879] Batch: 0.1292 (0.1519) Data: 0.0078 (0.0219) Loss: 0.3009 (0.4131)
[2022/12/29 03:16] | TRAIN(086): [200/879] Batch: 0.1178 (0.1474) Data: 0.0081 (0.0190) Loss: 0.3429 (0.4197)
[2022/12/29 03:16] | TRAIN(086): [250/879] Batch: 0.1380 (0.1445) Data: 0.0070 (0.0171) Loss: 0.2716 (0.4133)
[2022/12/29 03:16] | TRAIN(086): [300/879] Batch: 0.1219 (0.1432) Data: 0.0109 (0.0160) Loss: 0.3330 (0.4100)
[2022/12/29 03:16] | TRAIN(086): [350/879] Batch: 0.1541 (0.1424) Data: 0.0093 (0.0150) Loss: 0.2528 (0.4101)
[2022/12/29 03:16] | TRAIN(086): [400/879] Batch: 0.1599 (0.1414) Data: 0.0095 (0.0143) Loss: 0.2339 (0.4075)
[2022/12/29 03:17] | TRAIN(086): [450/879] Batch: 0.1495 (0.1412) Data: 0.0169 (0.0139) Loss: 0.3851 (0.4035)
[2022/12/29 03:17] | TRAIN(086): [500/879] Batch: 0.1459 (0.1403) Data: 0.0096 (0.0134) Loss: 0.2498 (0.4036)
[2022/12/29 03:17] | TRAIN(086): [550/879] Batch: 0.1248 (0.1395) Data: 0.0076 (0.0130) Loss: 0.3628 (0.4011)
[2022/12/29 03:17] | TRAIN(086): [600/879] Batch: 0.1371 (0.1394) Data: 0.0074 (0.0126) Loss: 0.3157 (0.4032)
[2022/12/29 03:17] | TRAIN(086): [650/879] Batch: 0.1227 (0.1390) Data: 0.0108 (0.0123) Loss: 0.1472 (0.4026)
[2022/12/29 03:17] | TRAIN(086): [700/879] Batch: 0.1250 (0.1382) Data: 0.0074 (0.0121) Loss: 0.5154 (0.4046)
[2022/12/29 03:17] | TRAIN(086): [750/879] Batch: 0.1521 (0.1379) Data: 0.0074 (0.0119) Loss: 0.5941 (0.4043)
[2022/12/29 03:17] | TRAIN(086): [800/879] Batch: 0.1220 (0.1381) Data: 0.0073 (0.0117) Loss: 0.4286 (0.4047)
[2022/12/29 03:17] | TRAIN(086): [850/879] Batch: 0.1523 (0.1378) Data: 0.0136 (0.0116) Loss: 0.3239 (0.4046)
[2022/12/29 03:18] | ------------------------------------------------------------
[2022/12/29 03:18] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:18] | ------------------------------------------------------------
[2022/12/29 03:18] |    TRAIN(86)     0:02:00     0:00:10     0:01:50      0.4039
[2022/12/29 03:18] | ------------------------------------------------------------
[2022/12/29 03:18] | VALID(086): [ 50/220] Batch: 0.0405 (0.0686) Data: 0.0104 (0.0379) Loss: 0.4220 (0.5884)
[2022/12/29 03:18] | VALID(086): [100/220] Batch: 0.0410 (0.0538) Data: 0.0100 (0.0236) Loss: 1.4245 (0.6559)
[2022/12/29 03:18] | VALID(086): [150/220] Batch: 0.0406 (0.0494) Data: 0.0092 (0.0188) Loss: 0.5425 (0.6649)
[2022/12/29 03:18] | VALID(086): [200/220] Batch: 0.0476 (0.0474) Data: 0.0120 (0.0165) Loss: 0.2049 (0.6729)
[2022/12/29 03:18] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:18] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:18] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:18] |    VALID(86)      0.6709      0.8117      0.8703      0.8117      0.8117      0.8117      0.9529
[2022/12/29 03:18] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:18] | ####################################################################################################
[2022/12/29 03:18] | TRAIN(087): [ 50/879] Batch: 0.1200 (0.1649) Data: 0.0098 (0.0458) Loss: 0.3857 (0.3965)
[2022/12/29 03:18] | TRAIN(087): [100/879] Batch: 0.1322 (0.1481) Data: 0.0100 (0.0284) Loss: 0.2924 (0.3898)
[2022/12/29 03:18] | TRAIN(087): [150/879] Batch: 0.1468 (0.1486) Data: 0.0175 (0.0223) Loss: 0.6896 (0.3990)
[2022/12/29 03:18] | TRAIN(087): [200/879] Batch: 0.1396 (0.1514) Data: 0.0078 (0.0194) Loss: 0.4834 (0.4038)
[2022/12/29 03:18] | TRAIN(087): [250/879] Batch: 0.1203 (0.1501) Data: 0.0108 (0.0172) Loss: 0.3676 (0.4052)
[2022/12/29 03:18] | TRAIN(087): [300/879] Batch: 0.1560 (0.1470) Data: 0.0082 (0.0160) Loss: 0.2936 (0.4032)
[2022/12/29 03:19] | TRAIN(087): [350/879] Batch: 0.1286 (0.1465) Data: 0.0097 (0.0152) Loss: 0.3562 (0.4053)
[2022/12/29 03:19] | TRAIN(087): [400/879] Batch: 0.1198 (0.1447) Data: 0.0082 (0.0145) Loss: 0.3224 (0.4006)
[2022/12/29 03:19] | TRAIN(087): [450/879] Batch: 0.1581 (0.1450) Data: 0.0077 (0.0137) Loss: 0.3583 (0.3981)
[2022/12/29 03:19] | TRAIN(087): [500/879] Batch: 0.1180 (0.1442) Data: 0.0079 (0.0132) Loss: 0.4205 (0.3992)
[2022/12/29 03:19] | TRAIN(087): [550/879] Batch: 0.1419 (0.1428) Data: 0.0075 (0.0129) Loss: 0.5276 (0.4016)
[2022/12/29 03:19] | TRAIN(087): [600/879] Batch: 0.1403 (0.1432) Data: 0.0104 (0.0126) Loss: 0.3642 (0.4031)
[2022/12/29 03:19] | TRAIN(087): [650/879] Batch: 0.1465 (0.1431) Data: 0.0073 (0.0122) Loss: 0.5038 (0.4055)
[2022/12/29 03:19] | TRAIN(087): [700/879] Batch: 0.1769 (0.1437) Data: 0.0118 (0.0120) Loss: 0.3917 (0.4051)
[2022/12/29 03:19] | TRAIN(087): [750/879] Batch: 0.1323 (0.1429) Data: 0.0075 (0.0117) Loss: 0.3084 (0.4058)
[2022/12/29 03:20] | TRAIN(087): [800/879] Batch: 0.1289 (0.1425) Data: 0.0086 (0.0116) Loss: 0.4178 (0.4039)
[2022/12/29 03:20] | TRAIN(087): [850/879] Batch: 0.1652 (0.1419) Data: 0.0112 (0.0114) Loss: 0.2608 (0.4059)
[2022/12/29 03:20] | ------------------------------------------------------------
[2022/12/29 03:20] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:20] | ------------------------------------------------------------
[2022/12/29 03:20] |    TRAIN(87)     0:02:05     0:00:09     0:01:55      0.4062
[2022/12/29 03:20] | ------------------------------------------------------------
[2022/12/29 03:20] | VALID(087): [ 50/220] Batch: 0.0302 (0.0612) Data: 0.0097 (0.0373) Loss: 0.4053 (0.6001)
[2022/12/29 03:20] | VALID(087): [100/220] Batch: 0.0352 (0.0469) Data: 0.0102 (0.0238) Loss: 1.4478 (0.6733)
[2022/12/29 03:20] | VALID(087): [150/220] Batch: 0.0353 (0.0426) Data: 0.0098 (0.0194) Loss: 0.5769 (0.6820)
[2022/12/29 03:20] | VALID(087): [200/220] Batch: 0.0335 (0.0405) Data: 0.0103 (0.0170) Loss: 0.2220 (0.6894)
[2022/12/29 03:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:20] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:20] |    VALID(87)      0.6868      0.8106      0.8704      0.8106      0.8106      0.8106      0.9526
[2022/12/29 03:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:20] | ####################################################################################################
[2022/12/29 03:20] | TRAIN(088): [ 50/879] Batch: 0.1393 (0.1747) Data: 0.0085 (0.0446) Loss: 0.6020 (0.3989)
[2022/12/29 03:20] | TRAIN(088): [100/879] Batch: 0.1402 (0.1569) Data: 0.0073 (0.0265) Loss: 0.2867 (0.3933)
[2022/12/29 03:20] | TRAIN(088): [150/879] Batch: 0.1169 (0.1466) Data: 0.0073 (0.0203) Loss: 0.5901 (0.4014)
[2022/12/29 03:20] | TRAIN(088): [200/879] Batch: 0.1173 (0.1412) Data: 0.0080 (0.0173) Loss: 0.3655 (0.4032)
[2022/12/29 03:21] | TRAIN(088): [250/879] Batch: 0.1251 (0.1380) Data: 0.0087 (0.0155) Loss: 0.3237 (0.4064)
[2022/12/29 03:21] | TRAIN(088): [300/879] Batch: 0.1286 (0.1365) Data: 0.0077 (0.0142) Loss: 0.2487 (0.4105)
[2022/12/29 03:21] | TRAIN(088): [350/879] Batch: 0.1286 (0.1358) Data: 0.0081 (0.0133) Loss: 0.1933 (0.4084)
[2022/12/29 03:21] | TRAIN(088): [400/879] Batch: 0.1178 (0.1350) Data: 0.0074 (0.0127) Loss: 0.3458 (0.4026)
[2022/12/29 03:21] | TRAIN(088): [450/879] Batch: 0.1178 (0.1338) Data: 0.0084 (0.0122) Loss: 0.6584 (0.4048)
[2022/12/29 03:21] | TRAIN(088): [500/879] Batch: 0.1185 (0.1328) Data: 0.0079 (0.0118) Loss: 0.3134 (0.4028)
[2022/12/29 03:21] | TRAIN(088): [550/879] Batch: 0.1358 (0.1324) Data: 0.0102 (0.0116) Loss: 0.4355 (0.4006)
[2022/12/29 03:21] | TRAIN(088): [600/879] Batch: 0.1376 (0.1324) Data: 0.0072 (0.0113) Loss: 0.2805 (0.4037)
[2022/12/29 03:21] | TRAIN(088): [650/879] Batch: 0.1339 (0.1327) Data: 0.0087 (0.0110) Loss: 0.2846 (0.4047)
[2022/12/29 03:21] | TRAIN(088): [700/879] Batch: 0.1231 (0.1328) Data: 0.0100 (0.0109) Loss: 0.5544 (0.4060)
[2022/12/29 03:22] | TRAIN(088): [750/879] Batch: 0.1305 (0.1324) Data: 0.0075 (0.0107) Loss: 0.3605 (0.4045)
[2022/12/29 03:22] | TRAIN(088): [800/879] Batch: 0.1205 (0.1320) Data: 0.0079 (0.0106) Loss: 0.4323 (0.4053)
[2022/12/29 03:22] | TRAIN(088): [850/879] Batch: 0.1284 (0.1317) Data: 0.0083 (0.0104) Loss: 0.2435 (0.4038)
[2022/12/29 03:22] | ------------------------------------------------------------
[2022/12/29 03:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:22] | ------------------------------------------------------------
[2022/12/29 03:22] |    TRAIN(88)     0:01:55     0:00:09     0:01:46      0.4028
[2022/12/29 03:22] | ------------------------------------------------------------
[2022/12/29 03:22] | VALID(088): [ 50/220] Batch: 0.0554 (0.0659) Data: 0.0080 (0.0371) Loss: 0.4044 (0.6053)
[2022/12/29 03:22] | VALID(088): [100/220] Batch: 0.0273 (0.0491) Data: 0.0073 (0.0225) Loss: 1.4665 (0.6811)
[2022/12/29 03:22] | VALID(088): [150/220] Batch: 0.0319 (0.0439) Data: 0.0070 (0.0181) Loss: 0.5398 (0.6897)
[2022/12/29 03:22] | VALID(088): [200/220] Batch: 0.0286 (0.0417) Data: 0.0079 (0.0159) Loss: 0.2062 (0.6952)
[2022/12/29 03:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:22] |    VALID(88)      0.6933      0.8113      0.8685      0.8113      0.8113      0.8113      0.9528
[2022/12/29 03:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:22] | ####################################################################################################
[2022/12/29 03:22] | TRAIN(089): [ 50/879] Batch: 0.1573 (0.1810) Data: 0.0174 (0.0466) Loss: 0.4136 (0.4261)
[2022/12/29 03:22] | TRAIN(089): [100/879] Batch: 0.1683 (0.1718) Data: 0.0120 (0.0295) Loss: 0.2032 (0.4054)
[2022/12/29 03:22] | TRAIN(089): [150/879] Batch: 0.1312 (0.1668) Data: 0.0100 (0.0234) Loss: 0.6832 (0.4081)
[2022/12/29 03:23] | TRAIN(089): [200/879] Batch: 0.1584 (0.1603) Data: 0.0073 (0.0203) Loss: 0.4171 (0.4018)
[2022/12/29 03:23] | TRAIN(089): [250/879] Batch: 0.1373 (0.1564) Data: 0.0166 (0.0182) Loss: 0.5271 (0.4086)
[2022/12/29 03:23] | TRAIN(089): [300/879] Batch: 0.1564 (0.1531) Data: 0.0083 (0.0170) Loss: 0.2356 (0.4096)
[2022/12/29 03:23] | TRAIN(089): [350/879] Batch: 0.1181 (0.1508) Data: 0.0074 (0.0159) Loss: 0.4355 (0.4084)
[2022/12/29 03:23] | TRAIN(089): [400/879] Batch: 0.1423 (0.1493) Data: 0.0074 (0.0152) Loss: 0.3763 (0.4072)
[2022/12/29 03:23] | TRAIN(089): [450/879] Batch: 0.1551 (0.1479) Data: 0.0141 (0.0146) Loss: 0.1206 (0.4052)
[2022/12/29 03:23] | TRAIN(089): [500/879] Batch: 0.1327 (0.1473) Data: 0.0174 (0.0141) Loss: 0.4464 (0.4075)
[2022/12/29 03:23] | TRAIN(089): [550/879] Batch: 0.1516 (0.1465) Data: 0.0095 (0.0138) Loss: 0.3168 (0.4056)
[2022/12/29 03:23] | TRAIN(089): [600/879] Batch: 0.1381 (0.1452) Data: 0.0171 (0.0134) Loss: 0.4775 (0.4050)
[2022/12/29 03:24] | TRAIN(089): [650/879] Batch: 0.1806 (0.1444) Data: 0.0097 (0.0131) Loss: 0.4239 (0.4049)
[2022/12/29 03:24] | TRAIN(089): [700/879] Batch: 0.1320 (0.1441) Data: 0.0072 (0.0128) Loss: 0.5727 (0.4044)
[2022/12/29 03:24] | TRAIN(089): [750/879] Batch: 0.1590 (0.1439) Data: 0.0097 (0.0125) Loss: 0.3668 (0.4039)
[2022/12/29 03:24] | TRAIN(089): [800/879] Batch: 0.1328 (0.1432) Data: 0.0073 (0.0123) Loss: 0.2805 (0.4026)
[2022/12/29 03:24] | TRAIN(089): [850/879] Batch: 0.1498 (0.1429) Data: 0.0093 (0.0120) Loss: 0.4796 (0.4039)
[2022/12/29 03:24] | ------------------------------------------------------------
[2022/12/29 03:24] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:24] | ------------------------------------------------------------
[2022/12/29 03:24] |    TRAIN(89)     0:02:05     0:00:10     0:01:54      0.4035
[2022/12/29 03:24] | ------------------------------------------------------------
[2022/12/29 03:24] | VALID(089): [ 50/220] Batch: 0.0462 (0.0677) Data: 0.0094 (0.0369) Loss: 0.4336 (0.5940)
[2022/12/29 03:24] | VALID(089): [100/220] Batch: 0.0407 (0.0555) Data: 0.0176 (0.0234) Loss: 1.4751 (0.6708)
[2022/12/29 03:24] | VALID(089): [150/220] Batch: 0.0419 (0.0502) Data: 0.0117 (0.0194) Loss: 0.5848 (0.6800)
[2022/12/29 03:24] | VALID(089): [200/220] Batch: 0.0403 (0.0480) Data: 0.0092 (0.0171) Loss: 0.2252 (0.6855)
[2022/12/29 03:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:24] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:24] |    VALID(89)      0.6842      0.8104      0.8690      0.8104      0.8104      0.8104      0.9526
[2022/12/29 03:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:24] | ####################################################################################################
[2022/12/29 03:24] | TRAIN(090): [ 50/879] Batch: 0.1526 (0.1836) Data: 0.0181 (0.0446) Loss: 0.4055 (0.4316)
[2022/12/29 03:25] | TRAIN(090): [100/879] Batch: 0.1409 (0.1649) Data: 0.0071 (0.0271) Loss: 0.2548 (0.4030)
[2022/12/29 03:25] | TRAIN(090): [150/879] Batch: 0.1184 (0.1578) Data: 0.0070 (0.0212) Loss: 0.4142 (0.4047)
[2022/12/29 03:25] | TRAIN(090): [200/879] Batch: 0.1185 (0.1532) Data: 0.0075 (0.0180) Loss: 0.4596 (0.4110)
[2022/12/29 03:25] | TRAIN(090): [250/879] Batch: 0.1571 (0.1494) Data: 0.0115 (0.0163) Loss: 0.2843 (0.4080)
[2022/12/29 03:25] | TRAIN(090): [300/879] Batch: 0.1302 (0.1473) Data: 0.0082 (0.0150) Loss: 0.1698 (0.4069)
[2022/12/29 03:25] | TRAIN(090): [350/879] Batch: 0.1412 (0.1457) Data: 0.0082 (0.0141) Loss: 0.6136 (0.4063)
[2022/12/29 03:25] | TRAIN(090): [400/879] Batch: 0.1393 (0.1447) Data: 0.0079 (0.0134) Loss: 0.1547 (0.4027)
[2022/12/29 03:25] | TRAIN(090): [450/879] Batch: 0.1252 (0.1445) Data: 0.0071 (0.0128) Loss: 0.4357 (0.4007)
[2022/12/29 03:25] | TRAIN(090): [500/879] Batch: 0.1256 (0.1427) Data: 0.0072 (0.0122) Loss: 0.3750 (0.3982)
[2022/12/29 03:26] | TRAIN(090): [550/879] Batch: 0.1563 (0.1414) Data: 0.0093 (0.0118) Loss: 0.3093 (0.3969)
[2022/12/29 03:26] | TRAIN(090): [600/879] Batch: 0.1161 (0.1405) Data: 0.0072 (0.0114) Loss: 0.4491 (0.3996)
[2022/12/29 03:26] | TRAIN(090): [650/879] Batch: 0.1391 (0.1406) Data: 0.0087 (0.0111) Loss: 0.4580 (0.3992)
[2022/12/29 03:26] | TRAIN(090): [700/879] Batch: 0.1288 (0.1404) Data: 0.0083 (0.0109) Loss: 0.5034 (0.3995)
[2022/12/29 03:26] | TRAIN(090): [750/879] Batch: 0.1400 (0.1398) Data: 0.0074 (0.0107) Loss: 0.4264 (0.4000)
[2022/12/29 03:26] | TRAIN(090): [800/879] Batch: 0.1589 (0.1399) Data: 0.0074 (0.0105) Loss: 0.4025 (0.4003)
[2022/12/29 03:26] | TRAIN(090): [850/879] Batch: 0.1167 (0.1399) Data: 0.0076 (0.0104) Loss: 0.3424 (0.4000)
[2022/12/29 03:26] | ------------------------------------------------------------
[2022/12/29 03:26] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:26] | ------------------------------------------------------------
[2022/12/29 03:26] |    TRAIN(90)     0:02:02     0:00:09     0:01:53      0.4003
[2022/12/29 03:26] | ------------------------------------------------------------
[2022/12/29 03:26] | VALID(090): [ 50/220] Batch: 0.0269 (0.0657) Data: 0.0071 (0.0363) Loss: 0.4867 (0.6128)
[2022/12/29 03:26] | VALID(090): [100/220] Batch: 0.0409 (0.0495) Data: 0.0123 (0.0231) Loss: 1.4776 (0.6822)
[2022/12/29 03:26] | VALID(090): [150/220] Batch: 0.0354 (0.0451) Data: 0.0071 (0.0182) Loss: 0.5836 (0.6904)
[2022/12/29 03:26] | VALID(090): [200/220] Batch: 0.0424 (0.0438) Data: 0.0151 (0.0165) Loss: 0.2186 (0.6987)
[2022/12/29 03:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:26] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:26] |    VALID(90)      0.6962      0.8114      0.8693      0.8114      0.8114      0.8114      0.9529
[2022/12/29 03:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:26] | ####################################################################################################
[2022/12/29 03:27] | TRAIN(091): [ 50/879] Batch: 0.1297 (0.1736) Data: 0.0074 (0.0460) Loss: 0.3015 (0.4062)
[2022/12/29 03:27] | TRAIN(091): [100/879] Batch: 0.1175 (0.1546) Data: 0.0079 (0.0279) Loss: 0.5770 (0.3955)
[2022/12/29 03:27] | TRAIN(091): [150/879] Batch: 0.1309 (0.1469) Data: 0.0082 (0.0219) Loss: 0.4587 (0.4026)
[2022/12/29 03:27] | TRAIN(091): [200/879] Batch: 0.1574 (0.1443) Data: 0.0094 (0.0186) Loss: 0.2291 (0.3927)
[2022/12/29 03:27] | TRAIN(091): [250/879] Batch: 0.1201 (0.1410) Data: 0.0094 (0.0166) Loss: 0.2080 (0.3960)
[2022/12/29 03:27] | TRAIN(091): [300/879] Batch: 0.1210 (0.1400) Data: 0.0108 (0.0154) Loss: 0.3549 (0.4023)
[2022/12/29 03:27] | TRAIN(091): [350/879] Batch: 0.1264 (0.1382) Data: 0.0074 (0.0144) Loss: 0.4473 (0.4066)
[2022/12/29 03:27] | TRAIN(091): [400/879] Batch: 0.1567 (0.1384) Data: 0.0074 (0.0136) Loss: 0.3008 (0.4058)
[2022/12/29 03:28] | TRAIN(091): [450/879] Batch: 0.1629 (0.1400) Data: 0.0076 (0.0132) Loss: 0.2894 (0.4064)
[2022/12/29 03:28] | TRAIN(091): [500/879] Batch: 0.1260 (0.1391) Data: 0.0099 (0.0128) Loss: 0.1525 (0.4011)
[2022/12/29 03:28] | TRAIN(091): [550/879] Batch: 0.1389 (0.1390) Data: 0.0081 (0.0125) Loss: 0.2597 (0.4001)
[2022/12/29 03:28] | TRAIN(091): [600/879] Batch: 0.1242 (0.1385) Data: 0.0085 (0.0122) Loss: 0.1933 (0.4013)
[2022/12/29 03:28] | TRAIN(091): [650/879] Batch: 0.1185 (0.1379) Data: 0.0082 (0.0120) Loss: 0.5259 (0.4030)
[2022/12/29 03:28] | TRAIN(091): [700/879] Batch: 0.1372 (0.1376) Data: 0.0085 (0.0118) Loss: 0.2159 (0.4022)
[2022/12/29 03:28] | TRAIN(091): [750/879] Batch: 0.1376 (0.1373) Data: 0.0072 (0.0116) Loss: 0.5538 (0.4006)
[2022/12/29 03:28] | TRAIN(091): [800/879] Batch: 0.1392 (0.1374) Data: 0.0072 (0.0113) Loss: 0.6365 (0.4025)
[2022/12/29 03:28] | TRAIN(091): [850/879] Batch: 0.1305 (0.1372) Data: 0.0071 (0.0112) Loss: 0.4907 (0.4060)
[2022/12/29 03:28] | ------------------------------------------------------------
[2022/12/29 03:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:28] | ------------------------------------------------------------
[2022/12/29 03:28] |    TRAIN(91)     0:02:00     0:00:09     0:01:50      0.4049
[2022/12/29 03:28] | ------------------------------------------------------------
[2022/12/29 03:29] | VALID(091): [ 50/220] Batch: 0.0322 (0.0604) Data: 0.0232 (0.0349) Loss: 0.4669 (0.5886)
[2022/12/29 03:29] | VALID(091): [100/220] Batch: 0.0319 (0.0466) Data: 0.0071 (0.0212) Loss: 1.4138 (0.6577)
[2022/12/29 03:29] | VALID(091): [150/220] Batch: 0.0321 (0.0417) Data: 0.0071 (0.0166) Loss: 0.5316 (0.6653)
[2022/12/29 03:29] | VALID(091): [200/220] Batch: 0.0318 (0.0393) Data: 0.0074 (0.0143) Loss: 0.2207 (0.6727)
[2022/12/29 03:29] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:29] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:29] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:29] |    VALID(91)      0.6714      0.8158      0.8705      0.8158      0.8158      0.8158      0.9540
[2022/12/29 03:29] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:29] | ####################################################################################################
[2022/12/29 03:29] | TRAIN(092): [ 50/879] Batch: 0.1156 (0.1770) Data: 0.0069 (0.0441) Loss: 0.4207 (0.4216)
[2022/12/29 03:29] | TRAIN(092): [100/879] Batch: 0.1321 (0.1524) Data: 0.0071 (0.0270) Loss: 0.6918 (0.4287)
[2022/12/29 03:29] | TRAIN(092): [150/879] Batch: 0.1342 (0.1487) Data: 0.0167 (0.0209) Loss: 0.5167 (0.4174)
[2022/12/29 03:29] | TRAIN(092): [200/879] Batch: 0.1340 (0.1458) Data: 0.0097 (0.0178) Loss: 0.4235 (0.4115)
[2022/12/29 03:29] | TRAIN(092): [250/879] Batch: 0.1516 (0.1457) Data: 0.0070 (0.0158) Loss: 0.3836 (0.4113)
[2022/12/29 03:29] | TRAIN(092): [300/879] Batch: 0.1387 (0.1438) Data: 0.0074 (0.0147) Loss: 0.4062 (0.4097)
[2022/12/29 03:29] | TRAIN(092): [350/879] Batch: 0.1571 (0.1427) Data: 0.0074 (0.0138) Loss: 0.3407 (0.4072)
[2022/12/29 03:30] | TRAIN(092): [400/879] Batch: 0.1307 (0.1420) Data: 0.0070 (0.0130) Loss: 0.4097 (0.4060)
[2022/12/29 03:30] | TRAIN(092): [450/879] Batch: 0.1233 (0.1399) Data: 0.0104 (0.0124) Loss: 0.3495 (0.4026)
[2022/12/29 03:30] | TRAIN(092): [500/879] Batch: 0.1253 (0.1390) Data: 0.0072 (0.0120) Loss: 0.4382 (0.4018)
[2022/12/29 03:30] | TRAIN(092): [550/879] Batch: 0.1353 (0.1390) Data: 0.0071 (0.0117) Loss: 0.2915 (0.4026)
[2022/12/29 03:30] | TRAIN(092): [600/879] Batch: 0.1276 (0.1382) Data: 0.0073 (0.0114) Loss: 0.3373 (0.4025)
[2022/12/29 03:30] | TRAIN(092): [650/879] Batch: 0.1296 (0.1377) Data: 0.0073 (0.0112) Loss: 0.5779 (0.4008)
[2022/12/29 03:30] | TRAIN(092): [700/879] Batch: 0.1339 (0.1370) Data: 0.0072 (0.0110) Loss: 0.4276 (0.3995)
[2022/12/29 03:30] | TRAIN(092): [750/879] Batch: 0.1275 (0.1365) Data: 0.0073 (0.0107) Loss: 0.2555 (0.3983)
[2022/12/29 03:30] | TRAIN(092): [800/879] Batch: 0.1375 (0.1357) Data: 0.0095 (0.0105) Loss: 0.5372 (0.3996)
[2022/12/29 03:31] | TRAIN(092): [850/879] Batch: 0.1324 (0.1354) Data: 0.0069 (0.0104) Loss: 0.4728 (0.3995)
[2022/12/29 03:31] | ------------------------------------------------------------
[2022/12/29 03:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:31] | ------------------------------------------------------------
[2022/12/29 03:31] |    TRAIN(92)     0:01:58     0:00:09     0:01:49      0.3999
[2022/12/29 03:31] | ------------------------------------------------------------
[2022/12/29 03:31] | VALID(092): [ 50/220] Batch: 0.0406 (0.0682) Data: 0.0092 (0.0357) Loss: 0.4674 (0.5998)
[2022/12/29 03:31] | VALID(092): [100/220] Batch: 0.0350 (0.0544) Data: 0.0073 (0.0226) Loss: 1.4534 (0.6699)
[2022/12/29 03:31] | VALID(092): [150/220] Batch: 0.0303 (0.0485) Data: 0.0070 (0.0179) Loss: 0.5514 (0.6778)
[2022/12/29 03:31] | VALID(092): [200/220] Batch: 0.0409 (0.0456) Data: 0.0093 (0.0157) Loss: 0.2148 (0.6834)
[2022/12/29 03:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:31] |    VALID(92)      0.6818      0.8127      0.8700      0.8127      0.8127      0.8127      0.9532
[2022/12/29 03:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:31] | ####################################################################################################
[2022/12/29 03:31] | TRAIN(093): [ 50/879] Batch: 0.1356 (0.1740) Data: 0.0078 (0.0492) Loss: 0.3352 (0.3957)
[2022/12/29 03:31] | TRAIN(093): [100/879] Batch: 0.1272 (0.1519) Data: 0.0074 (0.0297) Loss: 0.2304 (0.3914)
[2022/12/29 03:31] | TRAIN(093): [150/879] Batch: 0.1200 (0.1444) Data: 0.0151 (0.0232) Loss: 0.5387 (0.3931)
[2022/12/29 03:31] | TRAIN(093): [200/879] Batch: 0.1193 (0.1405) Data: 0.0079 (0.0195) Loss: 0.3395 (0.3914)
[2022/12/29 03:31] | TRAIN(093): [250/879] Batch: 0.1190 (0.1375) Data: 0.0074 (0.0174) Loss: 0.5052 (0.3893)
[2022/12/29 03:31] | TRAIN(093): [300/879] Batch: 0.1245 (0.1361) Data: 0.0074 (0.0160) Loss: 0.3243 (0.3940)
[2022/12/29 03:32] | TRAIN(093): [350/879] Batch: 0.1201 (0.1352) Data: 0.0075 (0.0150) Loss: 0.3831 (0.3891)
[2022/12/29 03:32] | TRAIN(093): [400/879] Batch: 0.1509 (0.1342) Data: 0.0095 (0.0143) Loss: 0.6959 (0.3896)
[2022/12/29 03:32] | TRAIN(093): [450/879] Batch: 0.1210 (0.1334) Data: 0.0073 (0.0137) Loss: 0.3917 (0.3904)
[2022/12/29 03:32] | TRAIN(093): [500/879] Batch: 0.1384 (0.1329) Data: 0.0069 (0.0132) Loss: 0.3713 (0.3907)
[2022/12/29 03:32] | TRAIN(093): [550/879] Batch: 0.1588 (0.1328) Data: 0.0098 (0.0128) Loss: 0.3547 (0.3937)
[2022/12/29 03:32] | TRAIN(093): [600/879] Batch: 0.1201 (0.1324) Data: 0.0158 (0.0125) Loss: 0.5168 (0.3946)
[2022/12/29 03:32] | TRAIN(093): [650/879] Batch: 0.1411 (0.1321) Data: 0.0075 (0.0122) Loss: 0.2973 (0.3952)
[2022/12/29 03:32] | TRAIN(093): [700/879] Batch: 0.1292 (0.1323) Data: 0.0076 (0.0119) Loss: 0.3108 (0.3962)
[2022/12/29 03:32] | TRAIN(093): [750/879] Batch: 0.1198 (0.1317) Data: 0.0074 (0.0117) Loss: 0.1844 (0.3974)
[2022/12/29 03:33] | TRAIN(093): [800/879] Batch: 0.1209 (0.1320) Data: 0.0079 (0.0114) Loss: 0.2461 (0.3970)
[2022/12/29 03:33] | TRAIN(093): [850/879] Batch: 0.1414 (0.1315) Data: 0.0090 (0.0112) Loss: 0.2455 (0.3972)
[2022/12/29 03:33] | ------------------------------------------------------------
[2022/12/29 03:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:33] | ------------------------------------------------------------
[2022/12/29 03:33] |    TRAIN(93)     0:01:55     0:00:09     0:01:45      0.3962
[2022/12/29 03:33] | ------------------------------------------------------------
[2022/12/29 03:33] | VALID(093): [ 50/220] Batch: 0.0380 (0.0686) Data: 0.0091 (0.0370) Loss: 0.4823 (0.5968)
[2022/12/29 03:33] | VALID(093): [100/220] Batch: 0.0406 (0.0549) Data: 0.0104 (0.0236) Loss: 1.5070 (0.6648)
[2022/12/29 03:33] | VALID(093): [150/220] Batch: 0.0429 (0.0501) Data: 0.0078 (0.0189) Loss: 0.5186 (0.6741)
[2022/12/29 03:33] | VALID(093): [200/220] Batch: 0.0399 (0.0477) Data: 0.0091 (0.0166) Loss: 0.2393 (0.6806)
[2022/12/29 03:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:33] |    VALID(93)      0.6790      0.8089      0.8681      0.8089      0.8089      0.8089      0.9522
[2022/12/29 03:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:33] | ####################################################################################################
[2022/12/29 03:33] | TRAIN(094): [ 50/879] Batch: 0.1503 (0.1716) Data: 0.0090 (0.0438) Loss: 0.6438 (0.3931)
[2022/12/29 03:33] | TRAIN(094): [100/879] Batch: 0.1205 (0.1469) Data: 0.0073 (0.0261) Loss: 0.3789 (0.3975)
[2022/12/29 03:33] | TRAIN(094): [150/879] Batch: 0.1201 (0.1408) Data: 0.0079 (0.0202) Loss: 0.4004 (0.3944)
[2022/12/29 03:33] | TRAIN(094): [200/879] Batch: 0.1238 (0.1362) Data: 0.0162 (0.0172) Loss: 0.1910 (0.3863)
[2022/12/29 03:33] | TRAIN(094): [250/879] Batch: 0.1192 (0.1335) Data: 0.0088 (0.0154) Loss: 0.3116 (0.3865)
[2022/12/29 03:34] | TRAIN(094): [300/879] Batch: 0.1196 (0.1315) Data: 0.0058 (0.0142) Loss: 0.3141 (0.3880)
[2022/12/29 03:34] | TRAIN(094): [350/879] Batch: 0.1194 (0.1315) Data: 0.0078 (0.0133) Loss: 0.4712 (0.3898)
[2022/12/29 03:34] | TRAIN(094): [400/879] Batch: 0.1267 (0.1308) Data: 0.0105 (0.0127) Loss: 0.5081 (0.3926)
[2022/12/29 03:34] | TRAIN(094): [450/879] Batch: 0.1224 (0.1312) Data: 0.0090 (0.0122) Loss: 0.1695 (0.3913)
[2022/12/29 03:34] | TRAIN(094): [500/879] Batch: 0.1510 (0.1308) Data: 0.0090 (0.0117) Loss: 0.4399 (0.3905)
[2022/12/29 03:34] | TRAIN(094): [550/879] Batch: 0.1420 (0.1312) Data: 0.0081 (0.0114) Loss: 0.4767 (0.3906)
[2022/12/29 03:34] | TRAIN(094): [600/879] Batch: 0.1238 (0.1309) Data: 0.0166 (0.0112) Loss: 0.2603 (0.3913)
[2022/12/29 03:34] | TRAIN(094): [650/879] Batch: 0.1145 (0.1301) Data: 0.0079 (0.0109) Loss: 0.6279 (0.3925)
[2022/12/29 03:34] | TRAIN(094): [700/879] Batch: 0.1196 (0.1295) Data: 0.0074 (0.0107) Loss: 0.4890 (0.3929)
[2022/12/29 03:34] | TRAIN(094): [750/879] Batch: 0.1150 (0.1296) Data: 0.0072 (0.0106) Loss: 0.2518 (0.3949)
[2022/12/29 03:35] | TRAIN(094): [800/879] Batch: 0.1196 (0.1293) Data: 0.0112 (0.0104) Loss: 0.4246 (0.3963)
[2022/12/29 03:35] | TRAIN(094): [850/879] Batch: 0.1199 (0.1293) Data: 0.0078 (0.0103) Loss: 0.2963 (0.3962)
[2022/12/29 03:35] | ------------------------------------------------------------
[2022/12/29 03:35] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:35] | ------------------------------------------------------------
[2022/12/29 03:35] |    TRAIN(94)     0:01:53     0:00:08     0:01:44      0.3960
[2022/12/29 03:35] | ------------------------------------------------------------
[2022/12/29 03:35] | VALID(094): [ 50/220] Batch: 0.0299 (0.0666) Data: 0.0058 (0.0353) Loss: 0.4428 (0.6046)
[2022/12/29 03:35] | VALID(094): [100/220] Batch: 0.0318 (0.0506) Data: 0.0071 (0.0216) Loss: 1.4044 (0.6734)
[2022/12/29 03:35] | VALID(094): [150/220] Batch: 0.0312 (0.0443) Data: 0.0070 (0.0170) Loss: 0.5395 (0.6801)
[2022/12/29 03:35] | VALID(094): [200/220] Batch: 0.0317 (0.0412) Data: 0.0083 (0.0146) Loss: 0.2271 (0.6878)
[2022/12/29 03:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:35] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:35] |    VALID(94)      0.6858      0.8083      0.8690      0.8083      0.8083      0.8083      0.9521
[2022/12/29 03:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:35] | ####################################################################################################
[2022/12/29 03:35] | TRAIN(095): [ 50/879] Batch: 0.1180 (0.1653) Data: 0.0068 (0.0439) Loss: 0.4662 (0.4009)
[2022/12/29 03:35] | TRAIN(095): [100/879] Batch: 0.1200 (0.1453) Data: 0.0075 (0.0266) Loss: 0.5361 (0.3998)
[2022/12/29 03:35] | TRAIN(095): [150/879] Batch: 0.1203 (0.1392) Data: 0.0075 (0.0206) Loss: 0.4872 (0.4002)
[2022/12/29 03:35] | TRAIN(095): [200/879] Batch: 0.1327 (0.1365) Data: 0.0068 (0.0177) Loss: 0.4367 (0.3978)
[2022/12/29 03:35] | TRAIN(095): [250/879] Batch: 0.1207 (0.1348) Data: 0.0072 (0.0159) Loss: 0.2637 (0.3954)
[2022/12/29 03:36] | TRAIN(095): [300/879] Batch: 0.1184 (0.1334) Data: 0.0075 (0.0148) Loss: 0.3683 (0.3958)
[2022/12/29 03:36] | TRAIN(095): [350/879] Batch: 0.1255 (0.1337) Data: 0.0072 (0.0139) Loss: 0.4255 (0.4002)
[2022/12/29 03:36] | TRAIN(095): [400/879] Batch: 0.1352 (0.1332) Data: 0.0068 (0.0132) Loss: 0.2976 (0.4017)
[2022/12/29 03:36] | TRAIN(095): [450/879] Batch: 0.1197 (0.1325) Data: 0.0076 (0.0126) Loss: 0.3399 (0.3996)
[2022/12/29 03:36] | TRAIN(095): [500/879] Batch: 0.1143 (0.1323) Data: 0.0073 (0.0121) Loss: 0.3353 (0.4010)
[2022/12/29 03:36] | TRAIN(095): [550/879] Batch: 0.1840 (0.1319) Data: 0.0092 (0.0118) Loss: 0.3240 (0.4017)
[2022/12/29 03:36] | TRAIN(095): [600/879] Batch: 0.1204 (0.1320) Data: 0.0108 (0.0115) Loss: 0.7129 (0.4012)
[2022/12/29 03:36] | TRAIN(095): [650/879] Batch: 0.1199 (0.1316) Data: 0.0081 (0.0112) Loss: 0.5501 (0.3989)
[2022/12/29 03:36] | TRAIN(095): [700/879] Batch: 0.1230 (0.1314) Data: 0.0070 (0.0110) Loss: 0.4562 (0.3984)
[2022/12/29 03:37] | TRAIN(095): [750/879] Batch: 0.1234 (0.1315) Data: 0.0149 (0.0108) Loss: 0.5344 (0.3987)
[2022/12/29 03:37] | TRAIN(095): [800/879] Batch: 0.1316 (0.1313) Data: 0.0074 (0.0107) Loss: 0.3042 (0.3993)
[2022/12/29 03:37] | TRAIN(095): [850/879] Batch: 0.1320 (0.1310) Data: 0.0071 (0.0105) Loss: 0.2484 (0.3985)
[2022/12/29 03:37] | ------------------------------------------------------------
[2022/12/29 03:37] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:37] | ------------------------------------------------------------
[2022/12/29 03:37] |    TRAIN(95)     0:01:54     0:00:09     0:01:45      0.3969
[2022/12/29 03:37] | ------------------------------------------------------------
[2022/12/29 03:37] | VALID(095): [ 50/220] Batch: 0.0359 (0.0632) Data: 0.0169 (0.0357) Loss: 0.4701 (0.6024)
[2022/12/29 03:37] | VALID(095): [100/220] Batch: 0.0376 (0.0486) Data: 0.0097 (0.0222) Loss: 1.5436 (0.6743)
[2022/12/29 03:37] | VALID(095): [150/220] Batch: 0.0399 (0.0455) Data: 0.0095 (0.0181) Loss: 0.5291 (0.6844)
[2022/12/29 03:37] | VALID(095): [200/220] Batch: 0.0408 (0.0444) Data: 0.0093 (0.0159) Loss: 0.2337 (0.6925)
[2022/12/29 03:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:37] |    VALID(95)      0.6907      0.8059      0.8677      0.8059      0.8059      0.8059      0.9515
[2022/12/29 03:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:37] | ####################################################################################################
[2022/12/29 03:37] | TRAIN(096): [ 50/879] Batch: 0.1457 (0.1719) Data: 0.0123 (0.0458) Loss: 0.3682 (0.4008)
[2022/12/29 03:37] | TRAIN(096): [100/879] Batch: 0.1611 (0.1571) Data: 0.0094 (0.0283) Loss: 0.4958 (0.4067)
[2022/12/29 03:37] | TRAIN(096): [150/879] Batch: 0.1197 (0.1495) Data: 0.0074 (0.0223) Loss: 0.4165 (0.3955)
[2022/12/29 03:37] | TRAIN(096): [200/879] Batch: 0.1341 (0.1464) Data: 0.0106 (0.0192) Loss: 0.1837 (0.3922)
[2022/12/29 03:38] | TRAIN(096): [250/879] Batch: 0.1451 (0.1444) Data: 0.0097 (0.0174) Loss: 0.2682 (0.3906)
[2022/12/29 03:38] | TRAIN(096): [300/879] Batch: 0.1204 (0.1423) Data: 0.0077 (0.0160) Loss: 0.3408 (0.3945)
[2022/12/29 03:38] | TRAIN(096): [350/879] Batch: 0.1156 (0.1405) Data: 0.0203 (0.0152) Loss: 0.4934 (0.3988)
[2022/12/29 03:38] | TRAIN(096): [400/879] Batch: 0.1274 (0.1392) Data: 0.0111 (0.0146) Loss: 0.4850 (0.3993)
[2022/12/29 03:38] | TRAIN(096): [450/879] Batch: 0.1606 (0.1396) Data: 0.0095 (0.0142) Loss: 0.6958 (0.4001)
[2022/12/29 03:38] | TRAIN(096): [500/879] Batch: 0.1594 (0.1392) Data: 0.0092 (0.0138) Loss: 0.4205 (0.3978)
[2022/12/29 03:38] | TRAIN(096): [550/879] Batch: 0.1215 (0.1388) Data: 0.0075 (0.0134) Loss: 0.5644 (0.4014)
[2022/12/29 03:38] | TRAIN(096): [600/879] Batch: 0.1443 (0.1384) Data: 0.0169 (0.0131) Loss: 0.3470 (0.3990)
[2022/12/29 03:39] | TRAIN(096): [650/879] Batch: 0.1383 (0.1382) Data: 0.0070 (0.0129) Loss: 0.4263 (0.3983)
[2022/12/29 03:39] | TRAIN(096): [700/879] Batch: 0.1327 (0.1382) Data: 0.0080 (0.0127) Loss: 0.3543 (0.3971)
[2022/12/29 03:39] | TRAIN(096): [750/879] Batch: 0.1199 (0.1376) Data: 0.0117 (0.0125) Loss: 0.6115 (0.3967)
[2022/12/29 03:39] | TRAIN(096): [800/879] Batch: 0.1199 (0.1370) Data: 0.0093 (0.0123) Loss: 0.2903 (0.3962)
[2022/12/29 03:39] | TRAIN(096): [850/879] Batch: 0.1537 (0.1366) Data: 0.0121 (0.0120) Loss: 0.7007 (0.3965)
[2022/12/29 03:39] | ------------------------------------------------------------
[2022/12/29 03:39] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:39] | ------------------------------------------------------------
[2022/12/29 03:39] |    TRAIN(96)     0:01:59     0:00:10     0:01:49      0.3954
[2022/12/29 03:39] | ------------------------------------------------------------
[2022/12/29 03:39] | VALID(096): [ 50/220] Batch: 0.0422 (0.0628) Data: 0.0117 (0.0355) Loss: 0.4461 (0.6035)
[2022/12/29 03:39] | VALID(096): [100/220] Batch: 0.0404 (0.0516) Data: 0.0091 (0.0225) Loss: 1.4706 (0.6745)
[2022/12/29 03:39] | VALID(096): [150/220] Batch: 0.0394 (0.0479) Data: 0.0093 (0.0180) Loss: 0.5313 (0.6831)
[2022/12/29 03:39] | VALID(096): [200/220] Batch: 0.0395 (0.0460) Data: 0.0090 (0.0158) Loss: 0.2319 (0.6907)
[2022/12/29 03:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:39] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:39] |    VALID(96)      0.6884      0.8061      0.8685      0.8061      0.8061      0.8061      0.9515
[2022/12/29 03:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:39] | ####################################################################################################
[2022/12/29 03:39] | TRAIN(097): [ 50/879] Batch: 0.1193 (0.1650) Data: 0.0086 (0.0454) Loss: 0.3030 (0.3721)
[2022/12/29 03:39] | TRAIN(097): [100/879] Batch: 0.1180 (0.1454) Data: 0.0079 (0.0271) Loss: 0.5148 (0.3855)
[2022/12/29 03:40] | TRAIN(097): [150/879] Batch: 0.1193 (0.1382) Data: 0.0069 (0.0207) Loss: 0.7500 (0.3963)
[2022/12/29 03:40] | TRAIN(097): [200/879] Batch: 0.1299 (0.1354) Data: 0.0073 (0.0176) Loss: 0.1698 (0.3899)
[2022/12/29 03:40] | TRAIN(097): [250/879] Batch: 0.1258 (0.1334) Data: 0.0077 (0.0157) Loss: 0.8013 (0.3934)
[2022/12/29 03:40] | TRAIN(097): [300/879] Batch: 0.1205 (0.1326) Data: 0.0070 (0.0145) Loss: 0.4664 (0.3938)
[2022/12/29 03:40] | TRAIN(097): [350/879] Batch: 0.1370 (0.1315) Data: 0.0073 (0.0135) Loss: 0.2956 (0.3923)
[2022/12/29 03:40] | TRAIN(097): [400/879] Batch: 0.1202 (0.1309) Data: 0.0073 (0.0129) Loss: 0.5195 (0.3903)
[2022/12/29 03:40] | TRAIN(097): [450/879] Batch: 0.1334 (0.1305) Data: 0.0073 (0.0123) Loss: 0.2575 (0.3929)
[2022/12/29 03:40] | TRAIN(097): [500/879] Batch: 0.1498 (0.1301) Data: 0.0086 (0.0119) Loss: 0.3642 (0.3895)
[2022/12/29 03:40] | TRAIN(097): [550/879] Batch: 0.1197 (0.1301) Data: 0.0167 (0.0116) Loss: 0.2085 (0.3905)
[2022/12/29 03:40] | TRAIN(097): [600/879] Batch: 0.1175 (0.1301) Data: 0.0078 (0.0114) Loss: 0.4174 (0.3904)
[2022/12/29 03:41] | TRAIN(097): [650/879] Batch: 0.1298 (0.1301) Data: 0.0103 (0.0112) Loss: 0.3639 (0.3905)
[2022/12/29 03:41] | TRAIN(097): [700/879] Batch: 0.1413 (0.1304) Data: 0.0102 (0.0112) Loss: 0.3604 (0.3906)
[2022/12/29 03:41] | TRAIN(097): [750/879] Batch: 0.1349 (0.1315) Data: 0.0078 (0.0110) Loss: 0.4057 (0.3912)
[2022/12/29 03:41] | TRAIN(097): [800/879] Batch: 0.1384 (0.1315) Data: 0.0081 (0.0109) Loss: 0.2142 (0.3904)
[2022/12/29 03:41] | TRAIN(097): [850/879] Batch: 0.1201 (0.1317) Data: 0.0075 (0.0109) Loss: 0.4085 (0.3914)
[2022/12/29 03:41] | ------------------------------------------------------------
[2022/12/29 03:41] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:41] | ------------------------------------------------------------
[2022/12/29 03:41] |    TRAIN(97)     0:01:55     0:00:09     0:01:45      0.3907
[2022/12/29 03:41] | ------------------------------------------------------------
[2022/12/29 03:41] | VALID(097): [ 50/220] Batch: 0.0297 (0.0639) Data: 0.0100 (0.0356) Loss: 0.4411 (0.5958)
[2022/12/29 03:41] | VALID(097): [100/220] Batch: 0.0321 (0.0483) Data: 0.0092 (0.0224) Loss: 1.4881 (0.6701)
[2022/12/29 03:41] | VALID(097): [150/220] Batch: 0.0405 (0.0455) Data: 0.0091 (0.0180) Loss: 0.5273 (0.6793)
[2022/12/29 03:41] | VALID(097): [200/220] Batch: 0.0423 (0.0443) Data: 0.0119 (0.0159) Loss: 0.2104 (0.6857)
[2022/12/29 03:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:41] |    VALID(97)      0.6833      0.8100      0.8693      0.8100      0.8100      0.8100      0.9525
[2022/12/29 03:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:41] | ####################################################################################################
[2022/12/29 03:41] | TRAIN(098): [ 50/879] Batch: 0.1660 (0.1728) Data: 0.0099 (0.0467) Loss: 0.3631 (0.3965)
[2022/12/29 03:42] | TRAIN(098): [100/879] Batch: 0.1251 (0.1492) Data: 0.0102 (0.0291) Loss: 0.3468 (0.3928)
[2022/12/29 03:42] | TRAIN(098): [150/879] Batch: 0.1406 (0.1430) Data: 0.0154 (0.0231) Loss: 0.3353 (0.3982)
[2022/12/29 03:42] | TRAIN(098): [200/879] Batch: 0.1528 (0.1415) Data: 0.0174 (0.0200) Loss: 0.4935 (0.4043)
[2022/12/29 03:42] | TRAIN(098): [250/879] Batch: 0.1232 (0.1414) Data: 0.0108 (0.0180) Loss: 0.3550 (0.4032)
[2022/12/29 03:42] | TRAIN(098): [300/879] Batch: 0.1387 (0.1418) Data: 0.0080 (0.0166) Loss: 0.6241 (0.4036)
[2022/12/29 03:42] | TRAIN(098): [350/879] Batch: 0.1387 (0.1408) Data: 0.0072 (0.0155) Loss: 0.4803 (0.4059)
[2022/12/29 03:42] | TRAIN(098): [400/879] Batch: 0.1502 (0.1405) Data: 0.0176 (0.0148) Loss: 0.4018 (0.4042)
[2022/12/29 03:42] | TRAIN(098): [450/879] Batch: 0.1394 (0.1405) Data: 0.0074 (0.0141) Loss: 0.5651 (0.3989)
[2022/12/29 03:42] | TRAIN(098): [500/879] Batch: 0.1412 (0.1407) Data: 0.0073 (0.0136) Loss: 0.3739 (0.4007)
[2022/12/29 03:43] | TRAIN(098): [550/879] Batch: 0.1383 (0.1407) Data: 0.0076 (0.0131) Loss: 0.2829 (0.4006)
[2022/12/29 03:43] | TRAIN(098): [600/879] Batch: 0.1391 (0.1406) Data: 0.0074 (0.0126) Loss: 0.4766 (0.4002)
[2022/12/29 03:43] | TRAIN(098): [650/879] Batch: 0.1400 (0.1406) Data: 0.0073 (0.0122) Loss: 0.4938 (0.3996)
[2022/12/29 03:43] | TRAIN(098): [700/879] Batch: 0.1384 (0.1405) Data: 0.0073 (0.0120) Loss: 0.3216 (0.4012)
[2022/12/29 03:43] | TRAIN(098): [750/879] Batch: 0.1624 (0.1406) Data: 0.0124 (0.0117) Loss: 0.4637 (0.4000)
[2022/12/29 03:43] | TRAIN(098): [800/879] Batch: 0.1221 (0.1403) Data: 0.0111 (0.0115) Loss: 0.2906 (0.3989)
[2022/12/29 03:43] | TRAIN(098): [850/879] Batch: 0.1243 (0.1403) Data: 0.0075 (0.0114) Loss: 0.2503 (0.3978)
[2022/12/29 03:43] | ------------------------------------------------------------
[2022/12/29 03:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:43] | ------------------------------------------------------------
[2022/12/29 03:43] |    TRAIN(98)     0:02:02     0:00:09     0:01:52      0.3976
[2022/12/29 03:43] | ------------------------------------------------------------
[2022/12/29 03:43] | VALID(098): [ 50/220] Batch: 0.0340 (0.0647) Data: 0.0084 (0.0377) Loss: 0.4632 (0.6049)
[2022/12/29 03:43] | VALID(098): [100/220] Batch: 0.0368 (0.0487) Data: 0.0059 (0.0228) Loss: 1.5426 (0.6798)
[2022/12/29 03:43] | VALID(098): [150/220] Batch: 0.0318 (0.0432) Data: 0.0073 (0.0178) Loss: 0.5334 (0.6875)
[2022/12/29 03:43] | VALID(098): [200/220] Batch: 0.0355 (0.0410) Data: 0.0084 (0.0153) Loss: 0.2104 (0.6952)
[2022/12/29 03:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:43] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:43] |    VALID(98)      0.6940      0.8084      0.8679      0.8084      0.8084      0.8084      0.9521
[2022/12/29 03:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:43] | ####################################################################################################
[2022/12/29 03:44] | TRAIN(099): [ 50/879] Batch: 0.1397 (0.1633) Data: 0.0076 (0.0447) Loss: 0.6232 (0.4103)
[2022/12/29 03:44] | TRAIN(099): [100/879] Batch: 0.1301 (0.1492) Data: 0.0120 (0.0281) Loss: 0.3091 (0.3959)
[2022/12/29 03:44] | TRAIN(099): [150/879] Batch: 0.1559 (0.1455) Data: 0.0169 (0.0225) Loss: 0.4860 (0.3983)
[2022/12/29 03:44] | TRAIN(099): [200/879] Batch: 0.1326 (0.1443) Data: 0.0078 (0.0196) Loss: 0.5461 (0.4065)
[2022/12/29 03:44] | TRAIN(099): [250/879] Batch: 0.1175 (0.1419) Data: 0.0175 (0.0177) Loss: 0.4914 (0.3964)
[2022/12/29 03:44] | TRAIN(099): [300/879] Batch: 0.1584 (0.1411) Data: 0.0123 (0.0166) Loss: 0.6627 (0.3951)
[2022/12/29 03:44] | TRAIN(099): [350/879] Batch: 0.1361 (0.1400) Data: 0.0088 (0.0157) Loss: 0.3384 (0.4001)
[2022/12/29 03:44] | TRAIN(099): [400/879] Batch: 0.1296 (0.1391) Data: 0.0096 (0.0150) Loss: 0.2576 (0.3961)
[2022/12/29 03:44] | TRAIN(099): [450/879] Batch: 0.1759 (0.1385) Data: 0.0127 (0.0144) Loss: 0.2137 (0.3977)
[2022/12/29 03:45] | TRAIN(099): [500/879] Batch: 0.1194 (0.1383) Data: 0.0074 (0.0140) Loss: 0.2112 (0.3952)
[2022/12/29 03:45] | TRAIN(099): [550/879] Batch: 0.1385 (0.1381) Data: 0.0069 (0.0136) Loss: 0.5582 (0.3975)
[2022/12/29 03:45] | TRAIN(099): [600/879] Batch: 0.1244 (0.1378) Data: 0.0091 (0.0133) Loss: 0.7204 (0.3960)
[2022/12/29 03:45] | TRAIN(099): [650/879] Batch: 0.1189 (0.1376) Data: 0.0077 (0.0130) Loss: 0.2390 (0.3978)
[2022/12/29 03:45] | TRAIN(099): [700/879] Batch: 0.1204 (0.1372) Data: 0.0073 (0.0127) Loss: 0.4688 (0.3983)
[2022/12/29 03:45] | TRAIN(099): [750/879] Batch: 0.1211 (0.1371) Data: 0.0102 (0.0125) Loss: 0.4902 (0.3984)
[2022/12/29 03:45] | TRAIN(099): [800/879] Batch: 0.1361 (0.1370) Data: 0.0078 (0.0123) Loss: 0.2932 (0.3986)
[2022/12/29 03:45] | TRAIN(099): [850/879] Batch: 0.1242 (0.1367) Data: 0.0068 (0.0122) Loss: 0.6116 (0.3986)
[2022/12/29 03:45] | ------------------------------------------------------------
[2022/12/29 03:45] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:45] | ------------------------------------------------------------
[2022/12/29 03:45] |    TRAIN(99)     0:01:59     0:00:10     0:01:49      0.3995
[2022/12/29 03:45] | ------------------------------------------------------------
[2022/12/29 03:46] | VALID(099): [ 50/220] Batch: 0.0499 (0.0677) Data: 0.0096 (0.0365) Loss: 0.4374 (0.5951)
[2022/12/29 03:46] | VALID(099): [100/220] Batch: 0.0323 (0.0523) Data: 0.0072 (0.0226) Loss: 1.4323 (0.6632)
[2022/12/29 03:46] | VALID(099): [150/220] Batch: 0.0319 (0.0468) Data: 0.0072 (0.0181) Loss: 0.5564 (0.6703)
[2022/12/29 03:46] | VALID(099): [200/220] Batch: 0.0358 (0.0445) Data: 0.0074 (0.0160) Loss: 0.2042 (0.6775)
[2022/12/29 03:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:46] |    VALID(99)      0.6756      0.8128      0.8704      0.8128      0.8128      0.8128      0.9532
[2022/12/29 03:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:46] | ####################################################################################################
