[2022/12/28 21:38] | Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth)
[2022/12/28 21:38] | load model weight from data/pretrained/resnet50.pth
[2022/12/28 21:38] | popping out head
[2022/12/28 21:38] | ---------------------------------------------------------------------------------
[2022/12/28 21:38] |                                    INFORMATION
[2022/12/28 21:38] | ---------------------------------------------------------------------------------
[2022/12/28 21:38] | Project Name              | MECLA
[2022/12/28 21:38] | Project Administrator     | jaejung
[2022/12/28 21:38] | Experiment Name           | eyepacs_v1_resnet50_v2
[2022/12/28 21:38] | Experiment Start Time     | 2022-12-28 21:38:41
[2022/12/28 21:38] | Experiment Model Name     | resnet50
[2022/12/28 21:38] | Experiment Log Directory  | log/eyepacs_v1_resnet50_v2
[2022/12/28 21:38] | ---------------------------------------------------------------------------------
[2022/12/28 21:38] |                                 EXPERIMENT SETUP
[2022/12/28 21:38] | ---------------------------------------------------------------------------------
[2022/12/28 21:38] | train_size                | (224, 224)
[2022/12/28 21:38] | test_size                 | (224, 224)
[2022/12/28 21:38] | center_crop_ptr           | 0.875
[2022/12/28 21:38] | interpolation             | bicubic
[2022/12/28 21:38] | mean                      | (0.485, 0.456, 0.406)
[2022/12/28 21:38] | std                       | (0.229, 0.224, 0.225)
[2022/12/28 21:38] | hflip                     | 0.5
[2022/12/28 21:38] | auto_aug                  | False
[2022/12/28 21:38] | cutmix                    | None
[2022/12/28 21:38] | mixup                     | None
[2022/12/28 21:38] | remode                    | 0.2
[2022/12/28 21:38] | model_name                | resnet50
[2022/12/28 21:38] | lr                        | 0.001
[2022/12/28 21:38] | epoch                     | 2
[2022/12/28 21:38] | criterion                 | ce
[2022/12/28 21:38] | optimizer                 | adamw
[2022/12/28 21:38] | weight_decay              | 0.0001
[2022/12/28 21:38] | scheduler                 | cosine
[2022/12/28 21:38] | warmup_epoch              | 1
[2022/12/28 21:38] | batch_size                | 32
[2022/12/28 21:38] | ---------------------------------------------------------------------------------
[2022/12/28 21:38] |                                   DATA & MODEL
[2022/12/28 21:38] | ---------------------------------------------------------------------------------
[2022/12/28 21:38] | Model Parameters(M)       | 23518277
[2022/12/28 21:38] | Number of Train Examples  | 28100
[2022/12/28 21:38] | Number of Valid Examples  | 7026
[2022/12/28 21:38] | Number of Class           | 5
[2022/12/28 21:38] | Task                      | multiclass
[2022/12/28 21:38] | ---------------------------------------------------------------------------------
[2022/12/28 21:38] | TRAIN(000): [ 50/879] Batch: 0.0804 (0.1193) Data: 0.0101 (0.0473) Loss: 1.3327 (1.4940)
[2022/12/28 21:38] | TRAIN(000): [100/879] Batch: 0.0832 (0.1012) Data: 0.0102 (0.0297) Loss: 0.8280 (1.2880)
[2022/12/28 21:39] | TRAIN(000): [150/879] Batch: 0.0764 (0.0933) Data: 0.0080 (0.0233) Loss: 0.8647 (1.1452)
[2022/12/28 21:39] | TRAIN(000): [200/879] Batch: 0.0874 (0.0906) Data: 0.0148 (0.0203) Loss: 0.7368 (1.0717)
[2022/12/28 21:39] | TRAIN(000): [250/879] Batch: 0.0842 (0.0894) Data: 0.0104 (0.0186) Loss: 0.7378 (1.0258)
[2022/12/28 21:39] | TRAIN(000): [300/879] Batch: 0.0804 (0.0885) Data: 0.0110 (0.0174) Loss: 0.7165 (0.9975)
[2022/12/28 21:39] | TRAIN(000): [350/879] Batch: 0.0855 (0.0881) Data: 0.0114 (0.0167) Loss: 0.8875 (0.9714)
[2022/12/28 21:39] | TRAIN(000): [400/879] Batch: 0.0998 (0.0878) Data: 0.0105 (0.0160) Loss: 0.8551 (0.9495)
[2022/12/28 21:39] | TRAIN(000): [450/879] Batch: 0.0788 (0.0867) Data: 0.0102 (0.0154) Loss: 0.8676 (0.9309)
[2022/12/28 21:39] | TRAIN(000): [500/879] Batch: 0.0832 (0.0863) Data: 0.0109 (0.0150) Loss: 0.9559 (0.9173)
[2022/12/28 21:39] | TRAIN(000): [550/879] Batch: 0.0855 (0.0861) Data: 0.0117 (0.0147) Loss: 0.7055 (0.9053)
[2022/12/28 21:39] | TRAIN(000): [600/879] Batch: 0.0880 (0.0860) Data: 0.0102 (0.0144) Loss: 0.6365 (0.8928)
[2022/12/28 21:39] | TRAIN(000): [650/879] Batch: 0.0755 (0.0858) Data: 0.0084 (0.0142) Loss: 1.0149 (0.8832)
[2022/12/28 21:39] | TRAIN(000): [700/879] Batch: 0.0842 (0.0857) Data: 0.0094 (0.0141) Loss: 0.7922 (0.8763)
[2022/12/28 21:39] | TRAIN(000): [750/879] Batch: 0.0814 (0.0857) Data: 0.0099 (0.0141) Loss: 0.7617 (0.8686)
[2022/12/28 21:39] | TRAIN(000): [800/879] Batch: 0.0983 (0.0855) Data: 0.0102 (0.0141) Loss: 0.5597 (0.8646)
[2022/12/28 21:40] | TRAIN(000): [850/879] Batch: 0.0813 (0.0854) Data: 0.0124 (0.0139) Loss: 1.0373 (0.8575)
[2022/12/28 21:40] | ------------------------------------------------------------
[2022/12/28 21:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 21:40] | ------------------------------------------------------------
[2022/12/28 21:40] |     TRAIN(0)     0:01:15     0:00:12     0:01:03      0.8553
[2022/12/28 21:40] | ------------------------------------------------------------
[2022/12/28 21:40] | VALID(000): [ 50/220] Batch: 0.0236 (0.0590) Data: 0.0105 (0.0467) Loss: 0.6743 (0.7134)
[2022/12/28 21:40] | VALID(000): [100/220] Batch: 0.0232 (0.0437) Data: 0.0099 (0.0308) Loss: 0.8148 (0.7423)
[2022/12/28 21:40] | VALID(000): [150/220] Batch: 0.0243 (0.0378) Data: 0.0137 (0.0253) Loss: 0.5579 (0.7322)
[2022/12/28 21:40] | VALID(000): [200/220] Batch: 0.0252 (0.0346) Data: 0.0199 (0.0223) Loss: 0.3518 (0.7394)
[2022/12/28 21:40] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:40] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 21:40] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:40] |     VALID(0)      0.7377      0.7559      0.7736      0.7559      0.7559      0.7559      0.9390
[2022/12/28 21:40] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:40] | ####################################################################################################
[2022/12/28 21:40] | TRAIN(001): [ 50/879] Batch: 0.0875 (0.1221) Data: 0.0132 (0.0486) Loss: 0.8665 (0.7906)
[2022/12/28 21:40] | TRAIN(001): [100/879] Batch: 0.0747 (0.1005) Data: 0.0085 (0.0296) Loss: 0.7489 (0.7808)
[2022/12/28 21:40] | TRAIN(001): [150/879] Batch: 0.0730 (0.0991) Data: 0.0084 (0.0285) Loss: 0.9703 (0.7553)
[2022/12/28 21:40] | TRAIN(001): [200/879] Batch: 0.0845 (0.0967) Data: 0.0098 (0.0263) Loss: 0.7506 (0.7651)
[2022/12/28 21:40] | TRAIN(001): [250/879] Batch: 0.0955 (0.0937) Data: 0.0099 (0.0235) Loss: 0.6487 (0.7638)
[2022/12/28 21:40] | TRAIN(001): [300/879] Batch: 0.0829 (0.0922) Data: 0.0098 (0.0216) Loss: 0.5645 (0.7593)
[2022/12/28 21:40] | TRAIN(001): [350/879] Batch: 0.0827 (0.0911) Data: 0.0103 (0.0202) Loss: 1.0700 (0.7556)
[2022/12/28 21:40] | TRAIN(001): [400/879] Batch: 0.0844 (0.0904) Data: 0.0098 (0.0193) Loss: 0.8886 (0.7482)
[2022/12/28 21:40] | TRAIN(001): [450/879] Batch: 0.0912 (0.0897) Data: 0.0355 (0.0187) Loss: 0.9711 (0.7466)
[2022/12/28 21:40] | TRAIN(001): [500/879] Batch: 0.0832 (0.0901) Data: 0.0098 (0.0190) Loss: 0.6641 (0.7405)
[2022/12/28 21:41] | TRAIN(001): [550/879] Batch: 0.0729 (0.0894) Data: 0.0086 (0.0183) Loss: 0.8933 (0.7375)
[2022/12/28 21:41] | TRAIN(001): [600/879] Batch: 0.0864 (0.0892) Data: 0.0112 (0.0177) Loss: 0.6369 (0.7301)
[2022/12/28 21:41] | TRAIN(001): [650/879] Batch: 0.0770 (0.0888) Data: 0.0094 (0.0172) Loss: 0.5781 (0.7288)
[2022/12/28 21:41] | TRAIN(001): [700/879] Batch: 0.0958 (0.0882) Data: 0.0094 (0.0167) Loss: 0.5559 (0.7215)
[2022/12/28 21:41] | TRAIN(001): [750/879] Batch: 0.0750 (0.0880) Data: 0.0083 (0.0164) Loss: 0.4184 (0.7223)
[2022/12/28 21:41] | TRAIN(001): [800/879] Batch: 0.0838 (0.0880) Data: 0.0098 (0.0163) Loss: 1.1074 (0.7226)
[2022/12/28 21:41] | TRAIN(001): [850/879] Batch: 0.0843 (0.0878) Data: 0.0135 (0.0161) Loss: 0.5544 (0.7206)
[2022/12/28 21:41] | ------------------------------------------------------------
[2022/12/28 21:41] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 21:41] | ------------------------------------------------------------
[2022/12/28 21:41] |     TRAIN(1)     0:01:17     0:00:13     0:01:03      0.7205
[2022/12/28 21:41] | ------------------------------------------------------------
[2022/12/28 21:41] | VALID(001): [ 50/220] Batch: 0.0154 (0.0558) Data: 0.0107 (0.0439) Loss: 0.5633 (0.6096)
[2022/12/28 21:41] | VALID(001): [100/220] Batch: 0.0239 (0.0415) Data: 0.0104 (0.0295) Loss: 0.7880 (0.6306)
[2022/12/28 21:41] | VALID(001): [150/220] Batch: 0.0234 (0.0364) Data: 0.0117 (0.0237) Loss: 0.5396 (0.6239)
[2022/12/28 21:41] | VALID(001): [200/220] Batch: 0.0237 (0.0338) Data: 0.0105 (0.0208) Loss: 0.3455 (0.6279)
[2022/12/28 21:41] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 21:41] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:41] |     VALID(1)      0.6255      0.7911      0.8382      0.7911      0.7911      0.7911      0.9478
[2022/12/28 21:41] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:41] | ####################################################################################################
