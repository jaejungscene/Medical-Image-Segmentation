[2022/12/28 21:41] | Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/densenet121_ra-50efcf5c.pth)
[2022/12/28 21:41] | ---------------------------------------------------------------------------------
[2022/12/28 21:41] |                                    INFORMATION
[2022/12/28 21:41] | ---------------------------------------------------------------------------------
[2022/12/28 21:41] | Project Name              | MECLA
[2022/12/28 21:41] | Project Administrator     | jaejung
[2022/12/28 21:41] | Experiment Name           | eyepacs_v1_densenet121_v2
[2022/12/28 21:41] | Experiment Start Time     | 2022-12-28 21:41:44
[2022/12/28 21:41] | Experiment Model Name     | densenet121
[2022/12/28 21:41] | Experiment Log Directory  | log/eyepacs_v1_densenet121_v2
[2022/12/28 21:41] | ---------------------------------------------------------------------------------
[2022/12/28 21:41] |                                 EXPERIMENT SETUP
[2022/12/28 21:41] | ---------------------------------------------------------------------------------
[2022/12/28 21:41] | train_size                | (224, 224)
[2022/12/28 21:41] | test_size                 | (224, 224)
[2022/12/28 21:41] | center_crop_ptr           | 0.875
[2022/12/28 21:41] | interpolation             | bicubic
[2022/12/28 21:41] | mean                      | (0.485, 0.456, 0.406)
[2022/12/28 21:41] | std                       | (0.229, 0.224, 0.225)
[2022/12/28 21:41] | hflip                     | 0.5
[2022/12/28 21:41] | auto_aug                  | False
[2022/12/28 21:41] | cutmix                    | None
[2022/12/28 21:41] | mixup                     | None
[2022/12/28 21:41] | remode                    | 0.2
[2022/12/28 21:41] | model_name                | densenet121
[2022/12/28 21:41] | lr                        | 0.001
[2022/12/28 21:41] | epoch                     | 2
[2022/12/28 21:41] | criterion                 | ce
[2022/12/28 21:41] | optimizer                 | adamw
[2022/12/28 21:41] | weight_decay              | 0.0001
[2022/12/28 21:41] | scheduler                 | cosine
[2022/12/28 21:41] | warmup_epoch              | 1
[2022/12/28 21:41] | batch_size                | 32
[2022/12/28 21:41] | ---------------------------------------------------------------------------------
[2022/12/28 21:41] |                                   DATA & MODEL
[2022/12/28 21:41] | ---------------------------------------------------------------------------------
[2022/12/28 21:41] | Model Parameters(M)       | 6958981
[2022/12/28 21:41] | Number of Train Examples  | 28100
[2022/12/28 21:41] | Number of Valid Examples  | 7026
[2022/12/28 21:41] | Number of Class           | 5
[2022/12/28 21:41] | Task                      | multiclass
[2022/12/28 21:41] | ---------------------------------------------------------------------------------
[2022/12/28 21:41] | TRAIN(000): [ 50/879] Batch: 0.1190 (0.1590) Data: 0.0094 (0.0461) Loss: 1.2302 (1.3397)
[2022/12/28 21:42] | TRAIN(000): [100/879] Batch: 0.1229 (0.1401) Data: 0.0099 (0.0282) Loss: 1.1213 (1.1251)
[2022/12/28 21:42] | TRAIN(000): [150/879] Batch: 0.1163 (0.1347) Data: 0.0099 (0.0222) Loss: 0.7090 (1.0381)
[2022/12/28 21:42] | TRAIN(000): [200/879] Batch: 0.1164 (0.1309) Data: 0.0096 (0.0192) Loss: 0.6660 (0.9864)
[2022/12/28 21:42] | TRAIN(000): [250/879] Batch: 0.1160 (0.1286) Data: 0.0090 (0.0173) Loss: 1.0479 (0.9620)
[2022/12/28 21:42] | TRAIN(000): [300/879] Batch: 0.1125 (0.1278) Data: 0.0100 (0.0161) Loss: 1.0135 (0.9316)
[2022/12/28 21:42] | TRAIN(000): [350/879] Batch: 0.1189 (0.1264) Data: 0.0099 (0.0152) Loss: 0.6755 (0.9073)
[2022/12/28 21:42] | TRAIN(000): [400/879] Batch: 0.1166 (0.1256) Data: 0.0091 (0.0145) Loss: 0.6048 (0.8915)
[2022/12/28 21:42] | TRAIN(000): [450/879] Batch: 0.1244 (0.1250) Data: 0.0088 (0.0140) Loss: 0.8817 (0.8823)
[2022/12/28 21:42] | TRAIN(000): [500/879] Batch: 0.1165 (0.1242) Data: 0.0099 (0.0136) Loss: 0.9154 (0.8769)
[2022/12/28 21:42] | TRAIN(000): [550/879] Batch: 0.1284 (0.1237) Data: 0.0105 (0.0132) Loss: 0.8936 (0.8688)
[2022/12/28 21:43] | TRAIN(000): [600/879] Batch: 0.1188 (0.1233) Data: 0.0096 (0.0130) Loss: 0.6784 (0.8633)
[2022/12/28 21:43] | TRAIN(000): [650/879] Batch: 0.1465 (0.1231) Data: 0.0112 (0.0127) Loss: 0.9627 (0.8580)
[2022/12/28 21:43] | TRAIN(000): [700/879] Batch: 0.1201 (0.1228) Data: 0.0099 (0.0125) Loss: 0.7211 (0.8519)
[2022/12/28 21:43] | TRAIN(000): [750/879] Batch: 0.1231 (0.1227) Data: 0.0099 (0.0123) Loss: 0.7528 (0.8508)
[2022/12/28 21:43] | TRAIN(000): [800/879] Batch: 0.1180 (0.1226) Data: 0.0090 (0.0122) Loss: 1.0899 (0.8454)
[2022/12/28 21:43] | TRAIN(000): [850/879] Batch: 0.1190 (0.1224) Data: 0.0093 (0.0120) Loss: 1.1206 (0.8423)
[2022/12/28 21:43] | ------------------------------------------------------------
[2022/12/28 21:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 21:43] | ------------------------------------------------------------
[2022/12/28 21:43] |     TRAIN(0)     0:01:48     0:00:10     0:01:37      0.8408
[2022/12/28 21:43] | ------------------------------------------------------------
[2022/12/28 21:43] | VALID(000): [ 50/220] Batch: 0.0410 (0.0682) Data: 0.0106 (0.0375) Loss: 0.7050 (0.7980)
[2022/12/28 21:43] | VALID(000): [100/220] Batch: 0.0401 (0.0555) Data: 0.0118 (0.0248) Loss: 0.9233 (0.8060)
[2022/12/28 21:43] | VALID(000): [150/220] Batch: 0.0429 (0.0511) Data: 0.0097 (0.0204) Loss: 0.7181 (0.7995)
[2022/12/28 21:43] | VALID(000): [200/220] Batch: 0.0411 (0.0488) Data: 0.0111 (0.0183) Loss: 0.4679 (0.7985)
[2022/12/28 21:43] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:43] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 21:43] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:43] |     VALID(0)      0.7983      0.7469      0.7585      0.7469      0.7469      0.7469      0.9367
[2022/12/28 21:43] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:43] | ####################################################################################################
[2022/12/28 21:43] | TRAIN(001): [ 50/879] Batch: 0.1185 (0.1569) Data: 0.0103 (0.0465) Loss: 0.7322 (0.8139)
[2022/12/28 21:44] | TRAIN(001): [100/879] Batch: 0.1184 (0.1383) Data: 0.0102 (0.0286) Loss: 0.6647 (0.7842)
[2022/12/28 21:44] | TRAIN(001): [150/879] Batch: 0.1210 (0.1322) Data: 0.0099 (0.0224) Loss: 0.6920 (0.7877)
[2022/12/28 21:44] | TRAIN(001): [200/879] Batch: 0.1264 (0.1290) Data: 0.0099 (0.0193) Loss: 0.7749 (0.7877)
[2022/12/28 21:44] | TRAIN(001): [250/879] Batch: 0.1180 (0.1271) Data: 0.0099 (0.0174) Loss: 0.8669 (0.7860)
[2022/12/28 21:44] | TRAIN(001): [300/879] Batch: 0.1264 (0.1268) Data: 0.0105 (0.0162) Loss: 0.6093 (0.7729)
[2022/12/28 21:44] | TRAIN(001): [350/879] Batch: 0.1172 (0.1258) Data: 0.0095 (0.0153) Loss: 0.6664 (0.7720)
[2022/12/28 21:44] | TRAIN(001): [400/879] Batch: 0.1255 (0.1253) Data: 0.0100 (0.0146) Loss: 0.6238 (0.7752)
[2022/12/28 21:44] | TRAIN(001): [450/879] Batch: 0.1179 (0.1245) Data: 0.0100 (0.0141) Loss: 0.5314 (0.7680)
[2022/12/28 21:44] | TRAIN(001): [500/879] Batch: 0.1180 (0.1239) Data: 0.0097 (0.0137) Loss: 0.5938 (0.7645)
[2022/12/28 21:44] | TRAIN(001): [550/879] Batch: 0.1478 (0.1236) Data: 0.0116 (0.0133) Loss: 0.7759 (0.7596)
[2022/12/28 21:45] | TRAIN(001): [600/879] Batch: 0.1184 (0.1234) Data: 0.0100 (0.0131) Loss: 0.5904 (0.7535)
[2022/12/28 21:45] | TRAIN(001): [650/879] Batch: 0.1347 (0.1238) Data: 0.0099 (0.0128) Loss: 0.7297 (0.7501)
[2022/12/28 21:45] | TRAIN(001): [700/879] Batch: 0.1362 (0.1247) Data: 0.0110 (0.0126) Loss: 0.7394 (0.7482)
[2022/12/28 21:45] | TRAIN(001): [750/879] Batch: 0.1133 (0.1244) Data: 0.0092 (0.0124) Loss: 0.5483 (0.7436)
[2022/12/28 21:45] | TRAIN(001): [800/879] Batch: 0.1175 (0.1244) Data: 0.0092 (0.0123) Loss: 0.5802 (0.7410)
[2022/12/28 21:45] | TRAIN(001): [850/879] Batch: 0.1367 (0.1249) Data: 0.0097 (0.0121) Loss: 0.5870 (0.7390)
[2022/12/28 21:45] | ------------------------------------------------------------
[2022/12/28 21:45] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 21:45] | ------------------------------------------------------------
[2022/12/28 21:45] |     TRAIN(1)     0:01:50     0:00:10     0:01:39      0.7373
[2022/12/28 21:45] | ------------------------------------------------------------
[2022/12/28 21:45] | VALID(001): [ 50/220] Batch: 0.0277 (0.0598) Data: 0.0082 (0.0373) Loss: 0.5467 (0.6356)
[2022/12/28 21:45] | VALID(001): [100/220] Batch: 0.0408 (0.0493) Data: 0.0113 (0.0243) Loss: 0.8480 (0.6521)
[2022/12/28 21:45] | VALID(001): [150/220] Batch: 0.0452 (0.0470) Data: 0.0118 (0.0201) Loss: 0.5727 (0.6444)
[2022/12/28 21:45] | VALID(001): [200/220] Batch: 0.0423 (0.0459) Data: 0.0121 (0.0181) Loss: 0.3350 (0.6480)
[2022/12/28 21:45] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:45] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 21:45] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:45] |     VALID(1)      0.6447      0.7832      0.8313      0.7832      0.7832      0.7832      0.9458
[2022/12/28 21:45] | ------------------------------------------------------------------------------------------------
[2022/12/28 21:45] | ####################################################################################################
