[2022/12/28 20:25] | Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth)
[2022/12/28 20:25] | ---------------------------------------------------------------------------------
[2022/12/28 20:25] |                                    INFORMATION
[2022/12/28 20:25] | ---------------------------------------------------------------------------------
[2022/12/28 20:25] | Project Name              | MECLA
[2022/12/28 20:25] | Project Administrator     | jaejung
[2022/12/28 20:25] | Experiment Name           | eyepacs_v1_resnet50_v0
[2022/12/28 20:25] | Experiment Start Time     | 2022-12-28 20:25:00
[2022/12/28 20:25] | Experiment Model Name     | resnet50
[2022/12/28 20:25] | Experiment Log Directory  | log/eyepacs_v1_resnet50_v0
[2022/12/28 20:25] | ---------------------------------------------------------------------------------
[2022/12/28 20:25] |                                 EXPERIMENT SETUP
[2022/12/28 20:25] | ---------------------------------------------------------------------------------
[2022/12/28 20:25] | train_size                | (224, 224)
[2022/12/28 20:25] | test_size                 | (224, 224)
[2022/12/28 20:25] | center_crop_ptr           | 0.875
[2022/12/28 20:25] | interpolation             | bicubic
[2022/12/28 20:25] | mean                      | (0.485, 0.456, 0.406)
[2022/12/28 20:25] | std                       | (0.229, 0.224, 0.225)
[2022/12/28 20:25] | hflip                     | 0.5
[2022/12/28 20:25] | auto_aug                  | False
[2022/12/28 20:25] | cutmix                    | None
[2022/12/28 20:25] | mixup                     | None
[2022/12/28 20:25] | remode                    | 0.2
[2022/12/28 20:25] | model_name                | resnet50
[2022/12/28 20:25] | lr                        | 0.001
[2022/12/28 20:25] | epoch                     | 2
[2022/12/28 20:25] | criterion                 | ce
[2022/12/28 20:25] | optimizer                 | adamw
[2022/12/28 20:25] | weight_decay              | 0.0001
[2022/12/28 20:25] | scheduler                 | cosine
[2022/12/28 20:25] | warmup_epoch              | 1
[2022/12/28 20:25] | batch_size                | 32
[2022/12/28 20:25] | ---------------------------------------------------------------------------------
[2022/12/28 20:25] |                                   DATA & MODEL
[2022/12/28 20:25] | ---------------------------------------------------------------------------------
[2022/12/28 20:25] | Model Parameters(M)       | 23518277
[2022/12/28 20:25] | Number of Train Examples  | 28100
[2022/12/28 20:25] | Number of Valid Examples  | 7026
[2022/12/28 20:25] | Number of Class           | 5
[2022/12/28 20:25] | Task                      | multiclass
[2022/12/28 20:25] | ---------------------------------------------------------------------------------
[2022/12/28 20:25] | TRAIN(000): [ 50/879] Batch: 0.1321 (0.1536) Data: 0.0118 (0.0486) Loss: 1.4250 (1.5737)
[2022/12/28 20:25] | TRAIN(000): [100/879] Batch: 0.1168 (0.1400) Data: 0.0115 (0.0305) Loss: 1.1369 (1.3801)
[2022/12/28 20:25] | TRAIN(000): [150/879] Batch: 0.1246 (0.1339) Data: 0.0116 (0.0242) Loss: 0.7901 (1.2090)
[2022/12/28 20:25] | TRAIN(000): [200/879] Batch: 0.1103 (0.1312) Data: 0.0115 (0.0211) Loss: 1.0855 (1.1340)
[2022/12/28 20:25] | TRAIN(000): [250/879] Batch: 0.1207 (0.1294) Data: 0.0118 (0.0192) Loss: 0.7166 (1.0747)
[2022/12/28 20:25] | TRAIN(000): [300/879] Batch: 0.1051 (0.1251) Data: 0.0093 (0.0176) Loss: 0.8093 (1.0380)
[2022/12/28 20:25] | TRAIN(000): [350/879] Batch: 0.1219 (0.1246) Data: 0.0111 (0.0167) Loss: 0.8710 (1.0093)
[2022/12/28 20:25] | TRAIN(000): [400/879] Batch: 0.1091 (0.1245) Data: 0.0114 (0.0161) Loss: 0.7547 (0.9840)
[2022/12/28 20:25] | TRAIN(000): [450/879] Batch: 0.1189 (0.1247) Data: 0.0119 (0.0155) Loss: 0.6453 (0.9649)
[2022/12/28 20:26] | TRAIN(000): [500/879] Batch: 0.1209 (0.1244) Data: 0.0109 (0.0151) Loss: 0.7626 (0.9481)
[2022/12/28 20:26] | TRAIN(000): [550/879] Batch: 0.1212 (0.1241) Data: 0.0075 (0.0147) Loss: 0.7957 (0.9363)
[2022/12/28 20:26] | TRAIN(000): [600/879] Batch: 0.1194 (0.1243) Data: 0.0112 (0.0144) Loss: 0.8639 (0.9228)
[2022/12/28 20:26] | TRAIN(000): [650/879] Batch: 0.1219 (0.1242) Data: 0.0133 (0.0142) Loss: 0.7926 (0.9137)
[2022/12/28 20:26] | TRAIN(000): [700/879] Batch: 0.1229 (0.1241) Data: 0.0113 (0.0140) Loss: 1.1925 (0.9037)
[2022/12/28 20:26] | TRAIN(000): [750/879] Batch: 0.1333 (0.1236) Data: 0.0095 (0.0138) Loss: 0.6754 (0.8969)
[2022/12/28 20:26] | TRAIN(000): [800/879] Batch: 0.1251 (0.1235) Data: 0.0114 (0.0137) Loss: 0.8876 (0.8909)
[2022/12/28 20:26] | TRAIN(000): [850/879] Batch: 0.1342 (0.1235) Data: 0.0119 (0.0135) Loss: 0.6705 (0.8840)
[2022/12/28 20:26] | ------------------------------------------------------------
[2022/12/28 20:26] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 20:26] | ------------------------------------------------------------
[2022/12/28 20:26] |     TRAIN(0)     0:01:49     0:00:11     0:01:37      0.8799
[2022/12/28 20:26] | ------------------------------------------------------------
[2022/12/28 20:26] | ##################################################
[2022/12/28 20:26] | VALID(000): [ 50/220] Batch: 0.0503 (0.0841) Data: 0.0155 (0.0410) Loss: 0.7123 (0.7285)
[2022/12/28 20:26] | VALID(000): [100/220] Batch: 0.0549 (0.0731) Data: 0.0164 (0.0272) Loss: 0.8527 (0.7678)
[2022/12/28 20:27] | VALID(000): [150/220] Batch: 0.0484 (0.0671) Data: 0.0133 (0.0224) Loss: 0.6481 (0.7579)
[2022/12/28 20:27] | VALID(000): [200/220] Batch: 0.0508 (0.0644) Data: 0.0140 (0.0198) Loss: 0.4170 (0.7566)
[2022/12/28 20:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:27] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 20:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:27] |     VALID(0)      0.7545      0.7588      0.7679      0.7588      0.7588      0.7588      0.9397
[2022/12/28 20:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:27] | ##################################################
[2022/12/28 20:27] | TRAIN(001): [ 50/879] Batch: 0.1189 (0.1535) Data: 0.0119 (0.0501) Loss: 0.8259 (0.7850)
[2022/12/28 20:27] | TRAIN(001): [100/879] Batch: 0.1172 (0.1381) Data: 0.0107 (0.0310) Loss: 1.1749 (0.7841)
[2022/12/28 20:27] | TRAIN(001): [150/879] Batch: 0.1082 (0.1258) Data: 0.0084 (0.0239) Loss: 0.7184 (0.7714)
[2022/12/28 20:27] | TRAIN(001): [200/879] Batch: 0.1285 (0.1233) Data: 0.0098 (0.0206) Loss: 0.7908 (0.7666)
[2022/12/28 20:27] | TRAIN(001): [250/879] Batch: 0.1131 (0.1232) Data: 0.0101 (0.0187) Loss: 0.9491 (0.7614)
[2022/12/28 20:27] | TRAIN(001): [300/879] Batch: 0.1380 (0.1239) Data: 0.0111 (0.0175) Loss: 0.7218 (0.7584)
[2022/12/28 20:27] | TRAIN(001): [350/879] Batch: 0.1404 (0.1259) Data: 0.0117 (0.0166) Loss: 0.6565 (0.7561)
[2022/12/28 20:27] | TRAIN(001): [400/879] Batch: 0.1286 (0.1268) Data: 0.0113 (0.0160) Loss: 0.8411 (0.7584)
[2022/12/28 20:28] | TRAIN(001): [450/879] Batch: 0.1200 (0.1267) Data: 0.0117 (0.0155) Loss: 0.6637 (0.7556)
[2022/12/28 20:28] | TRAIN(001): [500/879] Batch: 0.1303 (0.1266) Data: 0.0110 (0.0151) Loss: 0.7122 (0.7501)
[2022/12/28 20:28] | TRAIN(001): [550/879] Batch: 0.1314 (0.1262) Data: 0.0125 (0.0148) Loss: 0.7976 (0.7479)
[2022/12/28 20:28] | TRAIN(001): [600/879] Batch: 0.1232 (0.1261) Data: 0.0114 (0.0145) Loss: 0.7811 (0.7463)
[2022/12/28 20:28] | TRAIN(001): [650/879] Batch: 0.1204 (0.1261) Data: 0.0113 (0.0143) Loss: 0.3845 (0.7401)
[2022/12/28 20:28] | TRAIN(001): [700/879] Batch: 0.1233 (0.1258) Data: 0.0110 (0.0141) Loss: 0.4257 (0.7369)
[2022/12/28 20:28] | TRAIN(001): [750/879] Batch: 0.1329 (0.1256) Data: 0.0117 (0.0139) Loss: 0.5864 (0.7335)
[2022/12/28 20:28] | TRAIN(001): [800/879] Batch: 0.1299 (0.1254) Data: 0.0099 (0.0137) Loss: 0.7223 (0.7312)
[2022/12/28 20:28] | TRAIN(001): [850/879] Batch: 0.1329 (0.1248) Data: 0.0116 (0.0136) Loss: 0.8579 (0.7297)
[2022/12/28 20:28] | ------------------------------------------------------------
[2022/12/28 20:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 20:28] | ------------------------------------------------------------
[2022/12/28 20:28] |     TRAIN(1)     0:01:49     0:00:11     0:01:37      0.7287
[2022/12/28 20:28] | ------------------------------------------------------------
[2022/12/28 20:28] | ##################################################
[2022/12/28 20:29] | VALID(001): [ 50/220] Batch: 0.0651 (0.0957) Data: 0.0118 (0.0475) Loss: 0.5702 (0.7171)
[2022/12/28 20:29] | VALID(001): [100/220] Batch: 0.0583 (0.0770) Data: 0.0128 (0.0300) Loss: 0.8582 (0.7585)
[2022/12/28 20:29] | VALID(001): [150/220] Batch: 0.0587 (0.0726) Data: 0.0109 (0.0242) Loss: 0.5603 (0.7333)
[2022/12/28 20:29] | VALID(001): [200/220] Batch: 0.0669 (0.0695) Data: 0.0121 (0.0213) Loss: 0.3392 (0.7329)
[2022/12/28 20:29] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:29] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 20:29] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:29] |     VALID(1)      0.7252      0.7827      0.8119      0.7827      0.7827      0.7827      0.9457
[2022/12/28 20:29] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:29] | ##################################################
