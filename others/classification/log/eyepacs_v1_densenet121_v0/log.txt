[2022/12/28 20:29] | Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/densenet121_ra-50efcf5c.pth)
[2022/12/28 20:29] | ---------------------------------------------------------------------------------
[2022/12/28 20:29] |                                    INFORMATION
[2022/12/28 20:29] | ---------------------------------------------------------------------------------
[2022/12/28 20:29] | Project Name              | MECLA
[2022/12/28 20:29] | Project Administrator     | jaejung
[2022/12/28 20:29] | Experiment Name           | eyepacs_v1_densenet121_v0
[2022/12/28 20:29] | Experiment Start Time     | 2022-12-28 20:29:11
[2022/12/28 20:29] | Experiment Model Name     | densenet121
[2022/12/28 20:29] | Experiment Log Directory  | log/eyepacs_v1_densenet121_v0
[2022/12/28 20:29] | ---------------------------------------------------------------------------------
[2022/12/28 20:29] |                                 EXPERIMENT SETUP
[2022/12/28 20:29] | ---------------------------------------------------------------------------------
[2022/12/28 20:29] | train_size                | (224, 224)
[2022/12/28 20:29] | test_size                 | (224, 224)
[2022/12/28 20:29] | center_crop_ptr           | 0.875
[2022/12/28 20:29] | interpolation             | bicubic
[2022/12/28 20:29] | mean                      | (0.485, 0.456, 0.406)
[2022/12/28 20:29] | std                       | (0.229, 0.224, 0.225)
[2022/12/28 20:29] | hflip                     | 0.5
[2022/12/28 20:29] | auto_aug                  | False
[2022/12/28 20:29] | cutmix                    | None
[2022/12/28 20:29] | mixup                     | None
[2022/12/28 20:29] | remode                    | 0.2
[2022/12/28 20:29] | model_name                | densenet121
[2022/12/28 20:29] | lr                        | 0.001
[2022/12/28 20:29] | epoch                     | 2
[2022/12/28 20:29] | criterion                 | ce
[2022/12/28 20:29] | optimizer                 | adamw
[2022/12/28 20:29] | weight_decay              | 0.0001
[2022/12/28 20:29] | scheduler                 | cosine
[2022/12/28 20:29] | warmup_epoch              | 1
[2022/12/28 20:29] | batch_size                | 32
[2022/12/28 20:29] | ---------------------------------------------------------------------------------
[2022/12/28 20:29] |                                   DATA & MODEL
[2022/12/28 20:29] | ---------------------------------------------------------------------------------
[2022/12/28 20:29] | Model Parameters(M)       | 6958981
[2022/12/28 20:29] | Number of Train Examples  | 28100
[2022/12/28 20:29] | Number of Valid Examples  | 7026
[2022/12/28 20:29] | Number of Class           | 5
[2022/12/28 20:29] | Task                      | multiclass
[2022/12/28 20:29] | ---------------------------------------------------------------------------------
[2022/12/28 20:29] | TRAIN(000): [ 50/879] Batch: 0.2495 (0.2818) Data: 0.0114 (0.0446) Loss: 1.1509 (1.4245)
[2022/12/28 20:29] | TRAIN(000): [100/879] Batch: 0.3583 (0.2708) Data: 0.0088 (0.0282) Loss: 0.7343 (1.1803)
[2022/12/28 20:29] | TRAIN(000): [150/879] Batch: 0.2259 (0.2603) Data: 0.0082 (0.0222) Loss: 0.8626 (1.0814)
[2022/12/28 20:30] | TRAIN(000): [200/879] Batch: 0.3149 (0.2570) Data: 0.0113 (0.0194) Loss: 0.9092 (1.0262)
[2022/12/28 20:30] | TRAIN(000): [250/879] Batch: 0.2502 (0.2563) Data: 0.0112 (0.0177) Loss: 0.8951 (0.9869)
[2022/12/28 20:30] | TRAIN(000): [300/879] Batch: 0.2168 (0.2550) Data: 0.0088 (0.0166) Loss: 0.9468 (0.9499)
[2022/12/28 20:30] | TRAIN(000): [350/879] Batch: 0.2357 (0.2528) Data: 0.0093 (0.0157) Loss: 0.8031 (0.9236)
[2022/12/28 20:30] | TRAIN(000): [400/879] Batch: 0.2502 (0.2525) Data: 0.0108 (0.0151) Loss: 1.1093 (0.9094)
[2022/12/28 20:31] | TRAIN(000): [450/879] Batch: 0.2559 (0.2525) Data: 0.0117 (0.0147) Loss: 0.9547 (0.8978)
[2022/12/28 20:31] | TRAIN(000): [500/879] Batch: 0.2424 (0.2522) Data: 0.0108 (0.0143) Loss: 0.8264 (0.8884)
[2022/12/28 20:31] | TRAIN(000): [550/879] Batch: 0.2475 (0.2512) Data: 0.0113 (0.0140) Loss: 1.3021 (0.8788)
[2022/12/28 20:31] | TRAIN(000): [600/879] Batch: 0.2266 (0.2508) Data: 0.0083 (0.0137) Loss: 0.6587 (0.8706)
[2022/12/28 20:31] | TRAIN(000): [650/879] Batch: 0.2034 (0.2510) Data: 0.0093 (0.0135) Loss: 0.6685 (0.8651)
[2022/12/28 20:32] | TRAIN(000): [700/879] Batch: 0.2164 (0.2500) Data: 0.0090 (0.0133) Loss: 0.8103 (0.8626)
[2022/12/28 20:32] | TRAIN(000): [750/879] Batch: 0.2485 (0.2504) Data: 0.0112 (0.0132) Loss: 0.8732 (0.8584)
[2022/12/28 20:32] | TRAIN(000): [800/879] Batch: 0.2039 (0.2496) Data: 0.0085 (0.0130) Loss: 0.9044 (0.8538)
[2022/12/28 20:32] | TRAIN(000): [850/879] Batch: 0.2195 (0.2488) Data: 0.0085 (0.0128) Loss: 1.0112 (0.8497)
[2022/12/28 20:32] | ------------------------------------------------------------
[2022/12/28 20:32] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 20:32] | ------------------------------------------------------------
[2022/12/28 20:32] |     TRAIN(0)     0:03:38     0:00:11     0:03:27      0.8476
[2022/12/28 20:32] | ------------------------------------------------------------
[2022/12/28 20:32] | ##################################################
[2022/12/28 20:32] | VALID(000): [ 50/220] Batch: 0.1256 (0.1338) Data: 0.0116 (0.0442) Loss: 14.1001 (10.9813)
[2022/12/28 20:33] | VALID(000): [100/220] Batch: 0.0884 (0.1217) Data: 0.0072 (0.0270) Loss: 7.2376 (10.1405)
[2022/12/28 20:33] | VALID(000): [150/220] Batch: 0.0894 (0.1130) Data: 0.0068 (0.0208) Loss: 0.6650 (10.0630)
[2022/12/28 20:33] | VALID(000): [200/220] Batch: 0.1083 (0.1114) Data: 0.0092 (0.0179) Loss: 13.8566 (10.1805)
[2022/12/28 20:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 20:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:33] |     VALID(0)     10.5823      0.6258      0.6635      0.6258      0.6258      0.6258      0.9065
[2022/12/28 20:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:33] | ##################################################
[2022/12/28 20:33] | TRAIN(001): [ 50/879] Batch: 0.2554 (0.3049) Data: 0.0105 (0.0471) Loss: 0.7138 (0.8041)
[2022/12/28 20:33] | TRAIN(001): [100/879] Batch: 0.2162 (0.2735) Data: 0.0083 (0.0283) Loss: 0.4940 (0.8140)
[2022/12/28 20:33] | TRAIN(001): [150/879] Batch: 0.2128 (0.2571) Data: 0.0071 (0.0216) Loss: 0.9029 (0.8104)
[2022/12/28 20:34] | TRAIN(001): [200/879] Batch: 0.2683 (0.2567) Data: 0.0107 (0.0186) Loss: 0.7132 (0.8022)
[2022/12/28 20:34] | TRAIN(001): [250/879] Batch: 0.2623 (0.2581) Data: 0.0104 (0.0169) Loss: 0.4568 (0.7915)
[2022/12/28 20:34] | TRAIN(001): [300/879] Batch: 0.2572 (0.2594) Data: 0.0106 (0.0157) Loss: 0.9071 (0.7855)
[2022/12/28 20:34] | TRAIN(001): [350/879] Batch: 0.2471 (0.2594) Data: 0.0093 (0.0148) Loss: 0.7106 (0.7820)
[2022/12/28 20:34] | TRAIN(001): [400/879] Batch: 0.2588 (0.2601) Data: 0.0099 (0.0142) Loss: 0.7843 (0.7805)
[2022/12/28 20:35] | TRAIN(001): [450/879] Batch: 0.2495 (0.2600) Data: 0.0103 (0.0137) Loss: 0.6456 (0.7759)
[2022/12/28 20:35] | TRAIN(001): [500/879] Batch: 0.2038 (0.2582) Data: 0.0083 (0.0132) Loss: 0.6936 (0.7710)
[2022/12/28 20:35] | TRAIN(001): [550/879] Batch: 0.2405 (0.2582) Data: 0.0072 (0.0128) Loss: 0.6227 (0.7674)
[2022/12/28 20:35] | TRAIN(001): [600/879] Batch: 0.2186 (0.2575) Data: 0.0079 (0.0125) Loss: 0.7151 (0.7644)
[2022/12/28 20:36] | TRAIN(001): [650/879] Batch: 0.2245 (0.2542) Data: 0.0082 (0.0121) Loss: 0.7571 (0.7640)
[2022/12/28 20:36] | TRAIN(001): [700/879] Batch: 0.2084 (0.2520) Data: 0.0083 (0.0118) Loss: 0.9027 (0.7611)
[2022/12/28 20:36] | TRAIN(001): [750/879] Batch: 0.2153 (0.2502) Data: 0.0071 (0.0115) Loss: 0.6133 (0.7581)
[2022/12/28 20:36] | TRAIN(001): [800/879] Batch: 0.2158 (0.2480) Data: 0.0084 (0.0113) Loss: 0.9402 (0.7559)
[2022/12/28 20:36] | TRAIN(001): [850/879] Batch: 0.2129 (0.2476) Data: 0.0083 (0.0112) Loss: 0.7470 (0.7514)
[2022/12/28 20:36] | ------------------------------------------------------------
[2022/12/28 20:36] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 20:36] | ------------------------------------------------------------
[2022/12/28 20:36] |     TRAIN(1)     0:03:37     0:00:09     0:03:27      0.7494
[2022/12/28 20:36] | ------------------------------------------------------------
[2022/12/28 20:36] | ##################################################
[2022/12/28 20:37] | VALID(001): [ 50/220] Batch: 0.0930 (0.1360) Data: 0.0074 (0.0441) Loss: 0.5647 (0.6551)
[2022/12/28 20:37] | VALID(001): [100/220] Batch: 0.0886 (0.1141) Data: 0.0113 (0.0261) Loss: 0.8394 (0.6636)
[2022/12/28 20:37] | VALID(001): [150/220] Batch: 0.0845 (0.1075) Data: 0.0065 (0.0201) Loss: 0.5817 (0.6558)
[2022/12/28 20:37] | VALID(001): [200/220] Batch: 0.0942 (0.1039) Data: 0.0081 (0.0171) Loss: 0.3555 (0.6571)
[2022/12/28 20:37] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 20:37] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:37] |     VALID(1)      0.6535      0.7800      0.8216      0.7800      0.7800      0.7800      0.9450
[2022/12/28 20:37] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:37] | ##################################################
