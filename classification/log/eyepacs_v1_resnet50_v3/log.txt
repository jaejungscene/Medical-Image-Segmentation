[2022/12/28 23:08] | Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth)
[2022/12/28 23:08] | load model weight from data/pretrained/resnet50.pth
[2022/12/28 23:08] | popping out head
[2022/12/28 23:08] | ---------------------------------------------------------------------------------
[2022/12/28 23:08] |                                    INFORMATION
[2022/12/28 23:08] | ---------------------------------------------------------------------------------
[2022/12/28 23:08] | Project Name              | MECLA
[2022/12/28 23:08] | Project Administrator     | jaejung
[2022/12/28 23:08] | Experiment Name           | eyepacs_v1_resnet50_v3
[2022/12/28 23:08] | Experiment Start Time     | 2022-12-28 23:08:39
[2022/12/28 23:08] | Experiment Model Name     | resnet50
[2022/12/28 23:08] | Experiment Log Directory  | log/eyepacs_v1_resnet50_v3
[2022/12/28 23:08] | ---------------------------------------------------------------------------------
[2022/12/28 23:08] |                                 EXPERIMENT SETUP
[2022/12/28 23:08] | ---------------------------------------------------------------------------------
[2022/12/28 23:08] | train_size                | (224, 224)
[2022/12/28 23:08] | test_size                 | (224, 224)
[2022/12/28 23:08] | center_crop_ptr           | 0.875
[2022/12/28 23:08] | interpolation             | bicubic
[2022/12/28 23:08] | mean                      | (0.485, 0.456, 0.406)
[2022/12/28 23:08] | std                       | (0.229, 0.224, 0.225)
[2022/12/28 23:08] | hflip                     | 0.5
[2022/12/28 23:08] | auto_aug                  | False
[2022/12/28 23:08] | cutmix                    | None
[2022/12/28 23:08] | mixup                     | None
[2022/12/28 23:08] | remode                    | 0.2
[2022/12/28 23:08] | model_name                | resnet50
[2022/12/28 23:08] | lr                        | 0.001
[2022/12/28 23:08] | epoch                     | 100
[2022/12/28 23:08] | criterion                 | ce
[2022/12/28 23:08] | optimizer                 | adamw
[2022/12/28 23:08] | weight_decay              | 0.0001
[2022/12/28 23:08] | scheduler                 | cosine
[2022/12/28 23:08] | warmup_epoch              | 1
[2022/12/28 23:08] | batch_size                | 32
[2022/12/28 23:08] | ---------------------------------------------------------------------------------
[2022/12/28 23:08] |                                   DATA & MODEL
[2022/12/28 23:08] | ---------------------------------------------------------------------------------
[2022/12/28 23:08] | Model Parameters(M)       | 23518277
[2022/12/28 23:08] | Number of Train Examples  | 28100
[2022/12/28 23:08] | Number of Valid Examples  | 7026
[2022/12/28 23:08] | Number of Class           | 5
[2022/12/28 23:08] | Task                      | multiclass
[2022/12/28 23:08] | ---------------------------------------------------------------------------------
[2022/12/28 23:08] | TRAIN(000): [ 50/879] Batch: 0.1221 (0.1441) Data: 0.0114 (0.0557) Loss: 1.3294 (1.4943)
[2022/12/28 23:09] | TRAIN(000): [100/879] Batch: 0.1350 (0.1358) Data: 0.0105 (0.0338) Loss: 0.8440 (1.2889)
[2022/12/28 23:09] | TRAIN(000): [150/879] Batch: 0.0855 (0.1247) Data: 0.0118 (0.0269) Loss: 0.8305 (1.1445)
[2022/12/28 23:09] | TRAIN(000): [200/879] Batch: 0.1466 (0.1242) Data: 0.0153 (0.0233) Loss: 0.6858 (1.0723)
[2022/12/28 23:09] | TRAIN(000): [250/879] Batch: 0.1181 (0.1213) Data: 0.0107 (0.0209) Loss: 0.7150 (1.0278)
[2022/12/28 23:09] | TRAIN(000): [300/879] Batch: 0.1254 (0.1189) Data: 0.0111 (0.0195) Loss: 0.7528 (0.9984)
[2022/12/28 23:09] | TRAIN(000): [350/879] Batch: 0.0776 (0.1195) Data: 0.0117 (0.0185) Loss: 0.9298 (0.9719)
[2022/12/28 23:09] | TRAIN(000): [400/879] Batch: 0.1439 (0.1161) Data: 0.0129 (0.0176) Loss: 0.7957 (0.9494)
[2022/12/28 23:09] | TRAIN(000): [450/879] Batch: 0.1434 (0.1179) Data: 0.0094 (0.0170) Loss: 0.8302 (0.9301)
[2022/12/28 23:09] | TRAIN(000): [500/879] Batch: 0.0944 (0.1156) Data: 0.0118 (0.0165) Loss: 0.9455 (0.9165)
[2022/12/28 23:09] | TRAIN(000): [550/879] Batch: 0.1400 (0.1161) Data: 0.0098 (0.0161) Loss: 0.6673 (0.9052)
[2022/12/28 23:09] | TRAIN(000): [600/879] Batch: 0.1240 (0.1160) Data: 0.0241 (0.0158) Loss: 0.6316 (0.8923)
[2022/12/28 23:10] | TRAIN(000): [650/879] Batch: 0.1405 (0.1155) Data: 0.0123 (0.0155) Loss: 0.9266 (0.8830)
[2022/12/28 23:10] | TRAIN(000): [700/879] Batch: 0.0749 (0.1160) Data: 0.0130 (0.0152) Loss: 0.7916 (0.8768)
[2022/12/28 23:10] | TRAIN(000): [750/879] Batch: 0.1103 (0.1153) Data: 0.0145 (0.0153) Loss: 0.7318 (0.8682)
[2022/12/28 23:10] | TRAIN(000): [800/879] Batch: 0.1425 (0.1162) Data: 0.0144 (0.0151) Loss: 0.5434 (0.8640)
[2022/12/28 23:10] | TRAIN(000): [850/879] Batch: 0.0926 (0.1150) Data: 0.0122 (0.0150) Loss: 0.9721 (0.8564)
[2022/12/28 23:10] | ------------------------------------------------------------
[2022/12/28 23:10] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:10] | ------------------------------------------------------------
[2022/12/28 23:10] |     TRAIN(0)     0:01:42     0:00:13     0:01:29      0.8543
[2022/12/28 23:10] | ------------------------------------------------------------
[2022/12/28 23:10] | VALID(000): [ 50/220] Batch: 0.0299 (0.0676) Data: 0.0108 (0.0537) Loss: 0.6815 (0.7092)
[2022/12/28 23:10] | VALID(000): [100/220] Batch: 0.0249 (0.0479) Data: 0.0110 (0.0335) Loss: 0.7906 (0.7259)
[2022/12/28 23:10] | VALID(000): [150/220] Batch: 0.0185 (0.0415) Data: 0.0435 (0.0282) Loss: 0.6232 (0.7249)
[2022/12/28 23:10] | VALID(000): [200/220] Batch: 0.0464 (0.0391) Data: 0.0339 (0.0255) Loss: 0.3685 (0.7268)
[2022/12/28 23:10] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:10] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:10] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:10] |     VALID(0)      0.7268      0.7664      0.7806      0.7664      0.7664      0.7664      0.9416
[2022/12/28 23:10] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:10] | ####################################################################################################
[2022/12/28 23:10] | TRAIN(001): [ 50/879] Batch: 0.0757 (0.1546) Data: 0.0100 (0.0495) Loss: 0.9002 (0.7911)
[2022/12/28 23:10] | TRAIN(001): [100/879] Batch: 0.1129 (0.1304) Data: 0.0108 (0.0312) Loss: 0.7529 (0.7824)
[2022/12/28 23:10] | TRAIN(001): [150/879] Batch: 0.0781 (0.1304) Data: 0.0113 (0.0250) Loss: 0.9046 (0.7572)
[2022/12/28 23:11] | TRAIN(001): [200/879] Batch: 0.1450 (0.1206) Data: 0.0113 (0.0218) Loss: 0.7592 (0.7680)
[2022/12/28 23:11] | TRAIN(001): [250/879] Batch: 0.1338 (0.1228) Data: 0.0110 (0.0200) Loss: 0.6533 (0.7710)
[2022/12/28 23:11] | TRAIN(001): [300/879] Batch: 0.0852 (0.1188) Data: 0.0124 (0.0186) Loss: 0.5867 (0.7669)
[2022/12/28 23:11] | TRAIN(001): [350/879] Batch: 0.1448 (0.1190) Data: 0.0156 (0.0178) Loss: 1.1027 (0.7635)
[2022/12/28 23:11] | TRAIN(001): [400/879] Batch: 0.0916 (0.1181) Data: 0.0174 (0.0170) Loss: 0.9166 (0.7579)
[2022/12/28 23:11] | TRAIN(001): [450/879] Batch: 0.1314 (0.1166) Data: 0.0135 (0.0165) Loss: 0.9973 (0.7576)
[2022/12/28 23:11] | TRAIN(001): [500/879] Batch: 0.0926 (0.1181) Data: 0.0185 (0.0161) Loss: 0.6670 (0.7530)
[2022/12/28 23:11] | TRAIN(001): [550/879] Batch: 0.1293 (0.1156) Data: 0.0096 (0.0157) Loss: 0.8571 (0.7513)
[2022/12/28 23:11] | TRAIN(001): [600/879] Batch: 0.1261 (0.1169) Data: 0.0129 (0.0153) Loss: 0.5820 (0.7470)
[2022/12/28 23:11] | TRAIN(001): [650/879] Batch: 0.0776 (0.1158) Data: 0.0116 (0.0150) Loss: 0.6157 (0.7471)
[2022/12/28 23:12] | TRAIN(001): [700/879] Batch: 0.1442 (0.1161) Data: 0.0136 (0.0147) Loss: 0.5817 (0.7414)
[2022/12/28 23:12] | TRAIN(001): [750/879] Batch: 0.1388 (0.1159) Data: 0.0119 (0.0146) Loss: 0.5234 (0.7428)
[2022/12/28 23:12] | TRAIN(001): [800/879] Batch: 0.1359 (0.1153) Data: 0.0129 (0.0144) Loss: 1.1591 (0.7457)
[2022/12/28 23:12] | TRAIN(001): [850/879] Batch: 0.0785 (0.1158) Data: 0.0101 (0.0143) Loss: 0.6111 (0.7454)
[2022/12/28 23:12] | ------------------------------------------------------------
[2022/12/28 23:12] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:12] | ------------------------------------------------------------
[2022/12/28 23:12] |     TRAIN(1)     0:01:40     0:00:12     0:01:28      0.7460
[2022/12/28 23:12] | ------------------------------------------------------------
[2022/12/28 23:12] | VALID(001): [ 50/220] Batch: 0.0395 (0.0663) Data: 0.0210 (0.0521) Loss: 0.5562 (0.6705)
[2022/12/28 23:12] | VALID(001): [100/220] Batch: 0.0397 (0.0547) Data: 0.0366 (0.0416) Loss: 0.9793 (0.6966)
[2022/12/28 23:12] | VALID(001): [150/220] Batch: 0.0494 (0.0507) Data: 0.0244 (0.0383) Loss: 0.5950 (0.6880)
[2022/12/28 23:12] | VALID(001): [200/220] Batch: 0.0253 (0.0470) Data: 0.0109 (0.0340) Loss: 0.3989 (0.6932)
[2022/12/28 23:12] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:12] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:12] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:12] |     VALID(1)      0.6932      0.7716      0.8035      0.7716      0.7716      0.7716      0.9429
[2022/12/28 23:12] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:12] | ####################################################################################################
[2022/12/28 23:12] | TRAIN(002): [ 50/879] Batch: 0.1103 (0.1223) Data: 0.0091 (0.0455) Loss: 0.9232 (0.7348)
[2022/12/28 23:12] | TRAIN(002): [100/879] Batch: 0.0831 (0.1053) Data: 0.0082 (0.0312) Loss: 0.7713 (0.7184)
[2022/12/28 23:12] | TRAIN(002): [150/879] Batch: 0.0975 (0.0986) Data: 0.0081 (0.0258) Loss: 0.7845 (0.7209)
[2022/12/28 23:12] | TRAIN(002): [200/879] Batch: 0.1447 (0.0996) Data: 0.0099 (0.0232) Loss: 0.7470 (0.7131)
[2022/12/28 23:12] | TRAIN(002): [250/879] Batch: 0.1125 (0.1058) Data: 0.0100 (0.0207) Loss: 0.8770 (0.7189)
[2022/12/28 23:13] | TRAIN(002): [300/879] Batch: 0.1196 (0.1098) Data: 0.0088 (0.0191) Loss: 0.9899 (0.7215)
[2022/12/28 23:13] | TRAIN(002): [350/879] Batch: 0.1168 (0.1126) Data: 0.0125 (0.0178) Loss: 0.6748 (0.7140)
[2022/12/28 23:13] | TRAIN(002): [400/879] Batch: 0.0982 (0.1149) Data: 0.0097 (0.0170) Loss: 0.7534 (0.7145)
[2022/12/28 23:13] | TRAIN(002): [450/879] Batch: 0.1278 (0.1165) Data: 0.0113 (0.0163) Loss: 0.8248 (0.7213)
[2022/12/28 23:13] | TRAIN(002): [500/879] Batch: 0.1317 (0.1178) Data: 0.0118 (0.0157) Loss: 0.4917 (0.7221)
[2022/12/28 23:13] | TRAIN(002): [550/879] Batch: 0.1417 (0.1191) Data: 0.0102 (0.0153) Loss: 0.6245 (0.7255)
[2022/12/28 23:13] | TRAIN(002): [600/879] Batch: 0.0949 (0.1199) Data: 0.0154 (0.0149) Loss: 0.7495 (0.7249)
[2022/12/28 23:13] | TRAIN(002): [650/879] Batch: 0.1456 (0.1207) Data: 0.0088 (0.0146) Loss: 0.8763 (0.7236)
[2022/12/28 23:13] | TRAIN(002): [700/879] Batch: 0.1347 (0.1215) Data: 0.0181 (0.0144) Loss: 0.9567 (0.7234)
[2022/12/28 23:14] | TRAIN(002): [750/879] Batch: 0.1344 (0.1220) Data: 0.0105 (0.0141) Loss: 0.7292 (0.7197)
[2022/12/28 23:14] | TRAIN(002): [800/879] Batch: 0.1279 (0.1227) Data: 0.0109 (0.0139) Loss: 0.6310 (0.7194)
[2022/12/28 23:14] | TRAIN(002): [850/879] Batch: 0.1202 (0.1232) Data: 0.0130 (0.0137) Loss: 0.7780 (0.7182)
[2022/12/28 23:14] | ------------------------------------------------------------
[2022/12/28 23:14] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:14] | ------------------------------------------------------------
[2022/12/28 23:14] |     TRAIN(2)     0:01:48     0:00:11     0:01:36      0.7178
[2022/12/28 23:14] | ------------------------------------------------------------
[2022/12/28 23:14] | VALID(002): [ 50/220] Batch: 0.0394 (0.0667) Data: 0.0408 (0.0549) Loss: 0.5637 (0.6538)
[2022/12/28 23:14] | VALID(002): [100/220] Batch: 0.0392 (0.0551) Data: 0.0189 (0.0439) Loss: 0.8185 (0.6803)
[2022/12/28 23:14] | VALID(002): [150/220] Batch: 0.0364 (0.0507) Data: 0.0129 (0.0384) Loss: 0.5725 (0.6732)
[2022/12/28 23:14] | VALID(002): [200/220] Batch: 0.0509 (0.0487) Data: 0.0313 (0.0357) Loss: 0.3492 (0.6804)
[2022/12/28 23:14] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:14] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:14] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:14] |     VALID(2)      0.6785      0.7737      0.8134      0.7737      0.7737      0.7737      0.9434
[2022/12/28 23:14] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:14] | ####################################################################################################
[2022/12/28 23:14] | TRAIN(003): [ 50/879] Batch: 0.1358 (0.1667) Data: 0.0154 (0.0459) Loss: 0.4853 (0.7278)
[2022/12/28 23:14] | TRAIN(003): [100/879] Batch: 0.1441 (0.1491) Data: 0.0092 (0.0286) Loss: 0.8616 (0.7214)
[2022/12/28 23:14] | TRAIN(003): [150/879] Batch: 0.1343 (0.1430) Data: 0.0104 (0.0229) Loss: 0.5225 (0.7121)
[2022/12/28 23:14] | TRAIN(003): [200/879] Batch: 0.1451 (0.1399) Data: 0.0092 (0.0197) Loss: 0.7158 (0.7180)
[2022/12/28 23:15] | TRAIN(003): [250/879] Batch: 0.1442 (0.1382) Data: 0.0157 (0.0180) Loss: 0.5247 (0.7124)
[2022/12/28 23:15] | TRAIN(003): [300/879] Batch: 0.1355 (0.1364) Data: 0.0106 (0.0167) Loss: 0.7716 (0.7060)
[2022/12/28 23:15] | TRAIN(003): [350/879] Batch: 0.1254 (0.1359) Data: 0.0126 (0.0159) Loss: 0.6359 (0.6999)
[2022/12/28 23:15] | TRAIN(003): [400/879] Batch: 0.1165 (0.1358) Data: 0.0091 (0.0154) Loss: 0.6710 (0.7051)
[2022/12/28 23:15] | TRAIN(003): [450/879] Batch: 0.1327 (0.1352) Data: 0.0091 (0.0149) Loss: 0.6942 (0.7070)
[2022/12/28 23:15] | TRAIN(003): [500/879] Batch: 0.1223 (0.1348) Data: 0.0099 (0.0145) Loss: 0.5489 (0.7100)
[2022/12/28 23:15] | TRAIN(003): [550/879] Batch: 0.0784 (0.1319) Data: 0.0088 (0.0143) Loss: 0.5470 (0.7048)
[2022/12/28 23:15] | TRAIN(003): [600/879] Batch: 0.1348 (0.1319) Data: 0.0117 (0.0140) Loss: 0.6918 (0.7018)
[2022/12/28 23:15] | TRAIN(003): [650/879] Batch: 0.0940 (0.1316) Data: 0.0108 (0.0137) Loss: 0.5613 (0.7001)
[2022/12/28 23:16] | TRAIN(003): [700/879] Batch: 0.1302 (0.1295) Data: 0.0085 (0.0135) Loss: 0.6802 (0.7020)
[2022/12/28 23:16] | TRAIN(003): [750/879] Batch: 0.1402 (0.1294) Data: 0.0119 (0.0133) Loss: 0.7932 (0.7040)
[2022/12/28 23:16] | TRAIN(003): [800/879] Batch: 0.1265 (0.1295) Data: 0.0120 (0.0132) Loss: 1.0453 (0.7021)
[2022/12/28 23:16] | TRAIN(003): [850/879] Batch: 0.1079 (0.1295) Data: 0.0101 (0.0130) Loss: 0.4742 (0.7017)
[2022/12/28 23:16] | ------------------------------------------------------------
[2022/12/28 23:16] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:16] | ------------------------------------------------------------
[2022/12/28 23:16] |     TRAIN(3)     0:01:53     0:00:11     0:01:42      0.7015
[2022/12/28 23:16] | ------------------------------------------------------------
[2022/12/28 23:16] | VALID(003): [ 50/220] Batch: 0.0456 (0.0672) Data: 0.0355 (0.0561) Loss: 0.5483 (0.6402)
[2022/12/28 23:16] | VALID(003): [100/220] Batch: 0.0465 (0.0550) Data: 0.0341 (0.0438) Loss: 0.8241 (0.6556)
[2022/12/28 23:16] | VALID(003): [150/220] Batch: 0.0338 (0.0502) Data: 0.0310 (0.0389) Loss: 0.5929 (0.6486)
[2022/12/28 23:16] | VALID(003): [200/220] Batch: 0.0546 (0.0483) Data: 0.0366 (0.0372) Loss: 0.3269 (0.6544)
[2022/12/28 23:16] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:16] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:16] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:16] |     VALID(3)      0.6525      0.7872      0.8279      0.7872      0.7872      0.7872      0.9468
[2022/12/28 23:16] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:16] | ####################################################################################################
[2022/12/28 23:16] | TRAIN(004): [ 50/879] Batch: 0.1468 (0.1665) Data: 0.0112 (0.0444) Loss: 0.9191 (0.7029)
[2022/12/28 23:16] | TRAIN(004): [100/879] Batch: 0.1177 (0.1491) Data: 0.0090 (0.0279) Loss: 0.6197 (0.6859)
[2022/12/28 23:16] | TRAIN(004): [150/879] Batch: 0.1186 (0.1423) Data: 0.0091 (0.0218) Loss: 0.5782 (0.6978)
[2022/12/28 23:17] | TRAIN(004): [200/879] Batch: 0.1418 (0.1390) Data: 0.0083 (0.0187) Loss: 0.4944 (0.6906)
[2022/12/28 23:17] | TRAIN(004): [250/879] Batch: 0.1298 (0.1382) Data: 0.0121 (0.0171) Loss: 0.7811 (0.6873)
[2022/12/28 23:17] | TRAIN(004): [300/879] Batch: 0.1151 (0.1372) Data: 0.0093 (0.0160) Loss: 0.3941 (0.6939)
[2022/12/28 23:17] | TRAIN(004): [350/879] Batch: 0.1248 (0.1361) Data: 0.0146 (0.0152) Loss: 0.5838 (0.6911)
[2022/12/28 23:17] | TRAIN(004): [400/879] Batch: 0.1152 (0.1353) Data: 0.0094 (0.0145) Loss: 0.4616 (0.6891)
[2022/12/28 23:17] | TRAIN(004): [450/879] Batch: 0.1338 (0.1350) Data: 0.0106 (0.0140) Loss: 1.0684 (0.6914)
[2022/12/28 23:17] | TRAIN(004): [500/879] Batch: 0.1181 (0.1351) Data: 0.0110 (0.0137) Loss: 0.5160 (0.6894)
[2022/12/28 23:17] | TRAIN(004): [550/879] Batch: 0.1381 (0.1346) Data: 0.0087 (0.0134) Loss: 0.7938 (0.6862)
[2022/12/28 23:17] | TRAIN(004): [600/879] Batch: 0.1232 (0.1341) Data: 0.0115 (0.0131) Loss: 0.6209 (0.6874)
[2022/12/28 23:18] | TRAIN(004): [650/879] Batch: 0.1231 (0.1337) Data: 0.0113 (0.0128) Loss: 0.5264 (0.6879)
[2022/12/28 23:18] | TRAIN(004): [700/879] Batch: 0.1220 (0.1333) Data: 0.0091 (0.0126) Loss: 0.7726 (0.6886)
[2022/12/28 23:18] | TRAIN(004): [750/879] Batch: 0.1282 (0.1331) Data: 0.0124 (0.0124) Loss: 0.4564 (0.6889)
[2022/12/28 23:18] | TRAIN(004): [800/879] Batch: 0.1083 (0.1330) Data: 0.0095 (0.0123) Loss: 0.7323 (0.6902)
[2022/12/28 23:18] | TRAIN(004): [850/879] Batch: 0.1434 (0.1327) Data: 0.0093 (0.0121) Loss: 0.7879 (0.6894)
[2022/12/28 23:18] | ------------------------------------------------------------
[2022/12/28 23:18] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:18] | ------------------------------------------------------------
[2022/12/28 23:18] |     TRAIN(4)     0:01:56     0:00:10     0:01:45      0.6904
[2022/12/28 23:18] | ------------------------------------------------------------
[2022/12/28 23:18] | VALID(004): [ 50/220] Batch: 0.0423 (0.0650) Data: 0.0143 (0.0502) Loss: 0.5857 (0.6497)
[2022/12/28 23:18] | VALID(004): [100/220] Batch: 0.0481 (0.0541) Data: 0.0374 (0.0407) Loss: 0.9399 (0.6760)
[2022/12/28 23:18] | VALID(004): [150/220] Batch: 0.0301 (0.0503) Data: 0.0449 (0.0381) Loss: 0.5913 (0.6675)
[2022/12/28 23:18] | VALID(004): [200/220] Batch: 0.0462 (0.0486) Data: 0.0316 (0.0368) Loss: 0.3561 (0.6779)
[2022/12/28 23:18] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:18] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:18] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:18] |     VALID(4)      0.6762      0.7743      0.8206      0.7743      0.7743      0.7743      0.9436
[2022/12/28 23:18] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:18] | ####################################################################################################
[2022/12/28 23:18] | TRAIN(005): [ 50/879] Batch: 0.1421 (0.1498) Data: 0.0093 (0.0440) Loss: 0.7144 (0.6640)
[2022/12/28 23:18] | TRAIN(005): [100/879] Batch: 0.1307 (0.1421) Data: 0.0094 (0.0277) Loss: 0.5323 (0.6679)
[2022/12/28 23:19] | TRAIN(005): [150/879] Batch: 0.0781 (0.1327) Data: 0.0088 (0.0224) Loss: 0.7086 (0.6831)
[2022/12/28 23:19] | TRAIN(005): [200/879] Batch: 0.1235 (0.1320) Data: 0.0129 (0.0196) Loss: 0.8923 (0.6895)
[2022/12/28 23:19] | TRAIN(005): [250/879] Batch: 0.1501 (0.1320) Data: 0.0107 (0.0178) Loss: 0.6606 (0.6902)
[2022/12/28 23:19] | TRAIN(005): [300/879] Batch: 0.1430 (0.1322) Data: 0.0088 (0.0167) Loss: 0.6980 (0.6887)
[2022/12/28 23:19] | TRAIN(005): [350/879] Batch: 0.1209 (0.1317) Data: 0.0116 (0.0158) Loss: 0.8078 (0.6942)
[2022/12/28 23:19] | TRAIN(005): [400/879] Batch: 0.1459 (0.1315) Data: 0.0108 (0.0151) Loss: 0.5672 (0.6867)
[2022/12/28 23:19] | TRAIN(005): [450/879] Batch: 0.1434 (0.1321) Data: 0.0104 (0.0147) Loss: 0.5273 (0.6857)
[2022/12/28 23:19] | TRAIN(005): [500/879] Batch: 0.1134 (0.1318) Data: 0.0115 (0.0142) Loss: 0.4707 (0.6836)
[2022/12/28 23:19] | TRAIN(005): [550/879] Batch: 0.1289 (0.1316) Data: 0.0103 (0.0138) Loss: 0.4054 (0.6787)
[2022/12/28 23:20] | TRAIN(005): [600/879] Batch: 0.1427 (0.1313) Data: 0.0095 (0.0135) Loss: 1.0963 (0.6781)
[2022/12/28 23:20] | TRAIN(005): [650/879] Batch: 0.1189 (0.1310) Data: 0.0106 (0.0132) Loss: 0.5204 (0.6752)
[2022/12/28 23:20] | TRAIN(005): [700/879] Batch: 0.1448 (0.1308) Data: 0.0129 (0.0130) Loss: 0.9140 (0.6769)
[2022/12/28 23:20] | TRAIN(005): [750/879] Batch: 0.1295 (0.1309) Data: 0.0132 (0.0129) Loss: 0.6313 (0.6785)
[2022/12/28 23:20] | TRAIN(005): [800/879] Batch: 0.1429 (0.1309) Data: 0.0091 (0.0127) Loss: 0.7454 (0.6800)
[2022/12/28 23:20] | TRAIN(005): [850/879] Batch: 0.0912 (0.1307) Data: 0.0105 (0.0125) Loss: 0.7822 (0.6799)
[2022/12/28 23:20] | ------------------------------------------------------------
[2022/12/28 23:20] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:20] | ------------------------------------------------------------
[2022/12/28 23:20] |     TRAIN(5)     0:01:54     0:00:10     0:01:43      0.6818
[2022/12/28 23:20] | ------------------------------------------------------------
[2022/12/28 23:20] | VALID(005): [ 50/220] Batch: 0.0341 (0.0654) Data: 0.0355 (0.0549) Loss: 0.5311 (0.6221)
[2022/12/28 23:20] | VALID(005): [100/220] Batch: 0.0458 (0.0529) Data: 0.0356 (0.0422) Loss: 0.9231 (0.6514)
[2022/12/28 23:20] | VALID(005): [150/220] Batch: 0.0512 (0.0492) Data: 0.0366 (0.0385) Loss: 0.5629 (0.6474)
[2022/12/28 23:20] | VALID(005): [200/220] Batch: 0.0430 (0.0476) Data: 0.0154 (0.0364) Loss: 0.3751 (0.6535)
[2022/12/28 23:20] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:20] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:20] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:20] |     VALID(5)      0.6503      0.7835      0.8300      0.7835      0.7835      0.7835      0.9459
[2022/12/28 23:20] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:20] | ####################################################################################################
[2022/12/28 23:20] | TRAIN(006): [ 50/879] Batch: 0.1464 (0.1625) Data: 0.0112 (0.0433) Loss: 0.6994 (0.6649)
[2022/12/28 23:21] | TRAIN(006): [100/879] Batch: 0.1382 (0.1459) Data: 0.0164 (0.0282) Loss: 0.7307 (0.6735)
[2022/12/28 23:21] | TRAIN(006): [150/879] Batch: 0.1356 (0.1416) Data: 0.0128 (0.0231) Loss: 0.6636 (0.6911)
[2022/12/28 23:21] | TRAIN(006): [200/879] Batch: 0.1443 (0.1395) Data: 0.0100 (0.0205) Loss: 0.7034 (0.6879)
[2022/12/28 23:21] | TRAIN(006): [250/879] Batch: 0.1400 (0.1383) Data: 0.0108 (0.0187) Loss: 0.6036 (0.6801)
[2022/12/28 23:21] | TRAIN(006): [300/879] Batch: 0.1344 (0.1376) Data: 0.0090 (0.0177) Loss: 0.8460 (0.6781)
[2022/12/28 23:21] | TRAIN(006): [350/879] Batch: 0.1367 (0.1369) Data: 0.0113 (0.0169) Loss: 0.4528 (0.6738)
[2022/12/28 23:21] | TRAIN(006): [400/879] Batch: 0.1539 (0.1363) Data: 0.0145 (0.0163) Loss: 0.6072 (0.6753)
[2022/12/28 23:21] | TRAIN(006): [450/879] Batch: 0.1240 (0.1358) Data: 0.0113 (0.0158) Loss: 0.7019 (0.6767)
[2022/12/28 23:21] | TRAIN(006): [500/879] Batch: 0.1345 (0.1336) Data: 0.0180 (0.0154) Loss: 0.9255 (0.6764)
[2022/12/28 23:22] | TRAIN(006): [550/879] Batch: 0.1316 (0.1335) Data: 0.0088 (0.0151) Loss: 0.8000 (0.6746)
[2022/12/28 23:22] | TRAIN(006): [600/879] Batch: 0.1008 (0.1315) Data: 0.0138 (0.0149) Loss: 0.4247 (0.6759)
[2022/12/28 23:22] | TRAIN(006): [650/879] Batch: 0.1443 (0.1315) Data: 0.0092 (0.0146) Loss: 0.6295 (0.6732)
[2022/12/28 23:22] | TRAIN(006): [700/879] Batch: 0.1448 (0.1316) Data: 0.0108 (0.0143) Loss: 0.7208 (0.6744)
[2022/12/28 23:22] | TRAIN(006): [750/879] Batch: 0.1153 (0.1316) Data: 0.0113 (0.0141) Loss: 0.8313 (0.6753)
[2022/12/28 23:22] | TRAIN(006): [800/879] Batch: 0.1255 (0.1316) Data: 0.0136 (0.0139) Loss: 0.9372 (0.6730)
[2022/12/28 23:22] | TRAIN(006): [850/879] Batch: 0.1257 (0.1318) Data: 0.0125 (0.0137) Loss: 0.6103 (0.6746)
[2022/12/28 23:22] | ------------------------------------------------------------
[2022/12/28 23:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:22] | ------------------------------------------------------------
[2022/12/28 23:22] |     TRAIN(6)     0:01:55     0:00:11     0:01:43      0.6739
[2022/12/28 23:22] | ------------------------------------------------------------
[2022/12/28 23:22] | VALID(006): [ 50/220] Batch: 0.0461 (0.0659) Data: 0.0183 (0.0506) Loss: 0.6240 (0.6172)
[2022/12/28 23:22] | VALID(006): [100/220] Batch: 0.0496 (0.0544) Data: 0.0371 (0.0417) Loss: 0.8897 (0.6391)
[2022/12/28 23:22] | VALID(006): [150/220] Batch: 0.0279 (0.0505) Data: 0.0412 (0.0385) Loss: 0.5665 (0.6349)
[2022/12/28 23:22] | VALID(006): [200/220] Batch: 0.0472 (0.0488) Data: 0.0216 (0.0371) Loss: 0.2967 (0.6378)
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] |     VALID(6)      0.6364      0.7953      0.8351      0.7953      0.7953      0.7953      0.9488
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] | ####################################################################################################
[2022/12/28 23:23] | TRAIN(007): [ 50/879] Batch: 0.1222 (0.1670) Data: 0.0141 (0.0443) Loss: 0.5705 (0.6398)
[2022/12/28 23:23] | TRAIN(007): [100/879] Batch: 0.1433 (0.1485) Data: 0.0090 (0.0273) Loss: 0.3849 (0.6593)
[2022/12/28 23:23] | TRAIN(007): [150/879] Batch: 0.1464 (0.1425) Data: 0.0096 (0.0216) Loss: 0.4513 (0.6620)
[2022/12/28 23:23] | TRAIN(007): [200/879] Batch: 0.1199 (0.1393) Data: 0.0106 (0.0187) Loss: 0.9954 (0.6698)
[2022/12/28 23:23] | TRAIN(007): [250/879] Batch: 0.1218 (0.1384) Data: 0.0092 (0.0171) Loss: 0.5754 (0.6720)
[2022/12/28 23:23] | TRAIN(007): [300/879] Batch: 0.1489 (0.1375) Data: 0.0096 (0.0160) Loss: 0.4986 (0.6679)
[2022/12/28 23:23] | TRAIN(007): [350/879] Batch: 0.1214 (0.1365) Data: 0.0092 (0.0151) Loss: 0.6260 (0.6673)
[2022/12/28 23:23] | TRAIN(007): [400/879] Batch: 0.1394 (0.1357) Data: 0.0107 (0.0144) Loss: 0.8372 (0.6716)
[2022/12/28 23:23] | TRAIN(007): [450/879] Batch: 0.1358 (0.1352) Data: 0.0098 (0.0140) Loss: 0.8244 (0.6763)
[2022/12/28 23:24] | TRAIN(007): [500/879] Batch: 0.1244 (0.1346) Data: 0.0088 (0.0136) Loss: 0.6546 (0.6753)
[2022/12/28 23:24] | TRAIN(007): [550/879] Batch: 0.1286 (0.1343) Data: 0.0089 (0.0133) Loss: 0.6530 (0.6789)
[2022/12/28 23:24] | TRAIN(007): [600/879] Batch: 0.1444 (0.1342) Data: 0.0098 (0.0131) Loss: 0.4828 (0.6765)
[2022/12/28 23:24] | TRAIN(007): [650/879] Batch: 0.1325 (0.1342) Data: 0.0130 (0.0129) Loss: 0.7521 (0.6763)
[2022/12/28 23:24] | TRAIN(007): [700/879] Batch: 0.1266 (0.1340) Data: 0.0113 (0.0127) Loss: 0.9555 (0.6729)
[2022/12/28 23:24] | TRAIN(007): [750/879] Batch: 0.1230 (0.1338) Data: 0.0113 (0.0125) Loss: 0.5879 (0.6700)
[2022/12/28 23:24] | TRAIN(007): [800/879] Batch: 0.1206 (0.1335) Data: 0.0088 (0.0123) Loss: 0.7301 (0.6718)
[2022/12/28 23:24] | TRAIN(007): [850/879] Batch: 0.1466 (0.1333) Data: 0.0099 (0.0122) Loss: 0.9803 (0.6728)
[2022/12/28 23:24] | ------------------------------------------------------------
[2022/12/28 23:24] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:24] | ------------------------------------------------------------
[2022/12/28 23:24] |     TRAIN(7)     0:01:57     0:00:10     0:01:46      0.6711
[2022/12/28 23:24] | ------------------------------------------------------------
[2022/12/28 23:24] | VALID(007): [ 50/220] Batch: 0.0260 (0.0563) Data: 0.1416 (0.0448) Loss: 0.5229 (0.6172)
[2022/12/28 23:24] | VALID(007): [100/220] Batch: 0.0414 (0.0514) Data: 0.0284 (0.0396) Loss: 0.9463 (0.6514)
[2022/12/28 23:24] | VALID(007): [150/220] Batch: 0.0423 (0.0490) Data: 0.0338 (0.0376) Loss: 0.4966 (0.6422)
[2022/12/28 23:25] | VALID(007): [200/220] Batch: 0.0448 (0.0477) Data: 0.0400 (0.0366) Loss: 0.2791 (0.6422)
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] |     VALID(7)      0.6393      0.7939      0.8432      0.7939      0.7939      0.7939      0.9485
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] | ####################################################################################################
[2022/12/28 23:25] | TRAIN(008): [ 50/879] Batch: 0.1208 (0.1449) Data: 0.0108 (0.0489) Loss: 0.6445 (0.6423)
[2022/12/28 23:25] | TRAIN(008): [100/879] Batch: 0.1181 (0.1387) Data: 0.0108 (0.0302) Loss: 0.4076 (0.6395)
[2022/12/28 23:25] | TRAIN(008): [150/879] Batch: 0.1328 (0.1370) Data: 0.0108 (0.0242) Loss: 0.7083 (0.6555)
[2022/12/28 23:25] | TRAIN(008): [200/879] Batch: 0.1314 (0.1362) Data: 0.0107 (0.0210) Loss: 0.5059 (0.6596)
[2022/12/28 23:25] | TRAIN(008): [250/879] Batch: 0.1360 (0.1350) Data: 0.0156 (0.0190) Loss: 0.7034 (0.6608)
[2022/12/28 23:25] | TRAIN(008): [300/879] Batch: 0.1172 (0.1342) Data: 0.0103 (0.0177) Loss: 0.8411 (0.6626)
[2022/12/28 23:25] | TRAIN(008): [350/879] Batch: 0.1355 (0.1336) Data: 0.0105 (0.0168) Loss: 0.5247 (0.6625)
[2022/12/28 23:25] | TRAIN(008): [400/879] Batch: 0.1428 (0.1331) Data: 0.0104 (0.0161) Loss: 0.6478 (0.6635)
[2022/12/28 23:26] | TRAIN(008): [450/879] Batch: 0.1304 (0.1330) Data: 0.0122 (0.0156) Loss: 0.7087 (0.6615)
[2022/12/28 23:26] | TRAIN(008): [500/879] Batch: 0.1578 (0.1331) Data: 0.0109 (0.0153) Loss: 0.3701 (0.6591)
[2022/12/28 23:26] | TRAIN(008): [550/879] Batch: 0.1563 (0.1328) Data: 0.0130 (0.0149) Loss: 0.8922 (0.6581)
[2022/12/28 23:26] | TRAIN(008): [600/879] Batch: 0.1214 (0.1327) Data: 0.0109 (0.0146) Loss: 0.5198 (0.6570)
[2022/12/28 23:26] | TRAIN(008): [650/879] Batch: 0.1148 (0.1324) Data: 0.0179 (0.0144) Loss: 0.7975 (0.6558)
[2022/12/28 23:26] | TRAIN(008): [700/879] Batch: 0.1276 (0.1322) Data: 0.0104 (0.0141) Loss: 0.7523 (0.6526)
[2022/12/28 23:26] | TRAIN(008): [750/879] Batch: 0.1338 (0.1323) Data: 0.0122 (0.0139) Loss: 0.5261 (0.6577)
[2022/12/28 23:26] | TRAIN(008): [800/879] Batch: 0.1158 (0.1322) Data: 0.0178 (0.0138) Loss: 0.9539 (0.6587)
[2022/12/28 23:26] | TRAIN(008): [850/879] Batch: 0.1085 (0.1321) Data: 0.0106 (0.0136) Loss: 0.8682 (0.6597)
[2022/12/28 23:26] | ------------------------------------------------------------
[2022/12/28 23:26] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:26] | ------------------------------------------------------------
[2022/12/28 23:26] |     TRAIN(8)     0:01:55     0:00:11     0:01:44      0.6616
[2022/12/28 23:26] | ------------------------------------------------------------
[2022/12/28 23:27] | VALID(008): [ 50/220] Batch: 0.0316 (0.0661) Data: 0.0128 (0.0521) Loss: 0.5416 (0.5908)
[2022/12/28 23:27] | VALID(008): [100/220] Batch: 0.0367 (0.0539) Data: 0.0182 (0.0413) Loss: 0.9145 (0.6134)
[2022/12/28 23:27] | VALID(008): [150/220] Batch: 0.0353 (0.0495) Data: 0.0388 (0.0376) Loss: 0.5393 (0.6052)
[2022/12/28 23:27] | VALID(008): [200/220] Batch: 0.0475 (0.0479) Data: 0.0350 (0.0362) Loss: 0.3427 (0.6052)
[2022/12/28 23:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:27] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:27] |     VALID(8)      0.6045      0.7997      0.8493      0.7997      0.7997      0.7997      0.9499
[2022/12/28 23:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:27] | ####################################################################################################
[2022/12/28 23:27] | TRAIN(009): [ 50/879] Batch: 0.1400 (0.1671) Data: 0.0082 (0.0452) Loss: 0.6027 (0.6499)
[2022/12/28 23:27] | TRAIN(009): [100/879] Batch: 0.1259 (0.1498) Data: 0.0099 (0.0284) Loss: 0.7290 (0.6406)
[2022/12/28 23:27] | TRAIN(009): [150/879] Batch: 0.1240 (0.1434) Data: 0.0092 (0.0228) Loss: 0.3691 (0.6434)
[2022/12/28 23:27] | TRAIN(009): [200/879] Batch: 0.1285 (0.1397) Data: 0.0101 (0.0198) Loss: 0.6008 (0.6472)
[2022/12/28 23:27] | TRAIN(009): [250/879] Batch: 0.1179 (0.1384) Data: 0.0113 (0.0181) Loss: 0.5925 (0.6428)
[2022/12/28 23:27] | TRAIN(009): [300/879] Batch: 0.1506 (0.1376) Data: 0.0116 (0.0170) Loss: 0.5548 (0.6461)
[2022/12/28 23:27] | TRAIN(009): [350/879] Batch: 0.1489 (0.1379) Data: 0.0107 (0.0163) Loss: 0.9045 (0.6557)
[2022/12/28 23:28] | TRAIN(009): [400/879] Batch: 0.1360 (0.1350) Data: 0.0096 (0.0156) Loss: 0.5281 (0.6594)
[2022/12/28 23:28] | TRAIN(009): [450/879] Batch: 0.1266 (0.1346) Data: 0.0120 (0.0151) Loss: 0.9180 (0.6603)
[2022/12/28 23:28] | TRAIN(009): [500/879] Batch: 0.0884 (0.1316) Data: 0.0099 (0.0146) Loss: 0.8240 (0.6611)
[2022/12/28 23:28] | TRAIN(009): [550/879] Batch: 0.1286 (0.1314) Data: 0.0109 (0.0142) Loss: 0.6844 (0.6601)
[2022/12/28 23:28] | TRAIN(009): [600/879] Batch: 0.1269 (0.1315) Data: 0.0123 (0.0138) Loss: 0.6358 (0.6572)
[2022/12/28 23:28] | TRAIN(009): [650/879] Batch: 0.1216 (0.1319) Data: 0.0097 (0.0136) Loss: 0.4383 (0.6577)
[2022/12/28 23:28] | TRAIN(009): [700/879] Batch: 0.1196 (0.1317) Data: 0.0108 (0.0134) Loss: 0.5610 (0.6562)
[2022/12/28 23:28] | TRAIN(009): [750/879] Batch: 0.1274 (0.1315) Data: 0.0095 (0.0131) Loss: 0.6739 (0.6578)
[2022/12/28 23:28] | TRAIN(009): [800/879] Batch: 0.1173 (0.1315) Data: 0.0089 (0.0129) Loss: 0.4922 (0.6584)
[2022/12/28 23:29] | TRAIN(009): [850/879] Batch: 0.1247 (0.1314) Data: 0.0177 (0.0128) Loss: 0.5170 (0.6555)
[2022/12/28 23:29] | ------------------------------------------------------------
[2022/12/28 23:29] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:29] | ------------------------------------------------------------
[2022/12/28 23:29] |     TRAIN(9)     0:01:55     0:00:11     0:01:44      0.6561
[2022/12/28 23:29] | ------------------------------------------------------------
[2022/12/28 23:29] | VALID(009): [ 50/220] Batch: 0.0336 (0.0669) Data: 0.0289 (0.0557) Loss: 0.5425 (0.6520)
[2022/12/28 23:29] | VALID(009): [100/220] Batch: 0.0462 (0.0551) Data: 0.0360 (0.0443) Loss: 1.0155 (0.6842)
[2022/12/28 23:29] | VALID(009): [150/220] Batch: 0.0453 (0.0511) Data: 0.0372 (0.0406) Loss: 0.5616 (0.6760)
[2022/12/28 23:29] | VALID(009): [200/220] Batch: 0.0500 (0.0489) Data: 0.0366 (0.0384) Loss: 0.3348 (0.6825)
[2022/12/28 23:29] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:29] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:29] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:29] |     VALID(9)      0.6793      0.7781      0.8306      0.7781      0.7781      0.7781      0.9445
[2022/12/28 23:29] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:29] | ####################################################################################################
[2022/12/28 23:29] | TRAIN(010): [ 50/879] Batch: 0.1220 (0.1689) Data: 0.0112 (0.0445) Loss: 0.6927 (0.7016)
[2022/12/28 23:29] | TRAIN(010): [100/879] Batch: 0.1358 (0.1495) Data: 0.0100 (0.0275) Loss: 0.8298 (0.6744)
[2022/12/28 23:29] | TRAIN(010): [150/879] Batch: 0.1254 (0.1429) Data: 0.0092 (0.0217) Loss: 0.6167 (0.6570)
[2022/12/28 23:29] | TRAIN(010): [200/879] Batch: 0.1177 (0.1402) Data: 0.0093 (0.0189) Loss: 0.8411 (0.6589)
[2022/12/28 23:29] | TRAIN(010): [250/879] Batch: 0.1272 (0.1379) Data: 0.0122 (0.0171) Loss: 0.5675 (0.6552)
[2022/12/28 23:29] | TRAIN(010): [300/879] Batch: 0.1203 (0.1369) Data: 0.0105 (0.0160) Loss: 0.6536 (0.6538)
[2022/12/28 23:30] | TRAIN(010): [350/879] Batch: 0.1432 (0.1362) Data: 0.0093 (0.0151) Loss: 0.5934 (0.6536)
[2022/12/28 23:30] | TRAIN(010): [400/879] Batch: 0.1262 (0.1356) Data: 0.0106 (0.0146) Loss: 0.3681 (0.6521)
[2022/12/28 23:30] | TRAIN(010): [450/879] Batch: 0.1227 (0.1348) Data: 0.0135 (0.0140) Loss: 1.0506 (0.6500)
[2022/12/28 23:30] | TRAIN(010): [500/879] Batch: 0.1248 (0.1345) Data: 0.0115 (0.0137) Loss: 0.7549 (0.6506)
[2022/12/28 23:30] | TRAIN(010): [550/879] Batch: 0.1199 (0.1339) Data: 0.0106 (0.0134) Loss: 0.8874 (0.6535)
[2022/12/28 23:30] | TRAIN(010): [600/879] Batch: 0.1370 (0.1336) Data: 0.0112 (0.0131) Loss: 0.5789 (0.6530)
[2022/12/28 23:30] | TRAIN(010): [650/879] Batch: 0.1227 (0.1333) Data: 0.0107 (0.0129) Loss: 0.4495 (0.6537)
[2022/12/28 23:30] | TRAIN(010): [700/879] Batch: 0.1299 (0.1331) Data: 0.0085 (0.0127) Loss: 0.5409 (0.6523)
[2022/12/28 23:30] | TRAIN(010): [750/879] Batch: 0.1468 (0.1329) Data: 0.0109 (0.0125) Loss: 0.8890 (0.6538)
[2022/12/28 23:31] | TRAIN(010): [800/879] Batch: 0.0927 (0.1327) Data: 0.0126 (0.0124) Loss: 0.6627 (0.6537)
[2022/12/28 23:31] | TRAIN(010): [850/879] Batch: 0.1384 (0.1315) Data: 0.0107 (0.0123) Loss: 0.5977 (0.6524)
[2022/12/28 23:31] | ------------------------------------------------------------
[2022/12/28 23:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:31] | ------------------------------------------------------------
[2022/12/28 23:31] |    TRAIN(10)     0:01:55     0:00:10     0:01:44      0.6520
[2022/12/28 23:31] | ------------------------------------------------------------
[2022/12/28 23:31] | VALID(010): [ 50/220] Batch: 0.0428 (0.0691) Data: 0.0375 (0.0559) Loss: 0.6288 (0.6390)
[2022/12/28 23:31] | VALID(010): [100/220] Batch: 0.0248 (0.0528) Data: 0.0124 (0.0407) Loss: 0.9281 (0.6445)
[2022/12/28 23:31] | VALID(010): [150/220] Batch: 0.0233 (0.0448) Data: 0.0086 (0.0320) Loss: 0.5142 (0.6444)
[2022/12/28 23:31] | VALID(010): [200/220] Batch: 0.0418 (0.0437) Data: 0.0151 (0.0316) Loss: 0.3610 (0.6413)
[2022/12/28 23:31] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:31] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:31] |    VALID(10)      0.6419      0.7842      0.8452      0.7842      0.7842      0.7842      0.9461
[2022/12/28 23:31] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:31] | ####################################################################################################
[2022/12/28 23:31] | TRAIN(011): [ 50/879] Batch: 0.1318 (0.1610) Data: 0.0111 (0.0441) Loss: 0.8934 (0.6414)
[2022/12/28 23:31] | TRAIN(011): [100/879] Batch: 0.1549 (0.1472) Data: 0.0106 (0.0278) Loss: 0.7449 (0.6419)
[2022/12/28 23:31] | TRAIN(011): [150/879] Batch: 0.1442 (0.1418) Data: 0.0099 (0.0220) Loss: 0.7037 (0.6333)
[2022/12/28 23:31] | TRAIN(011): [200/879] Batch: 0.1207 (0.1384) Data: 0.0087 (0.0192) Loss: 0.5105 (0.6407)
[2022/12/28 23:31] | TRAIN(011): [250/879] Batch: 0.1300 (0.1371) Data: 0.0126 (0.0176) Loss: 0.5248 (0.6426)
[2022/12/28 23:32] | TRAIN(011): [300/879] Batch: 0.1456 (0.1357) Data: 0.0095 (0.0164) Loss: 1.0327 (0.6435)
[2022/12/28 23:32] | TRAIN(011): [350/879] Batch: 0.1464 (0.1351) Data: 0.0117 (0.0157) Loss: 0.9019 (0.6437)
[2022/12/28 23:32] | TRAIN(011): [400/879] Batch: 0.1197 (0.1343) Data: 0.0087 (0.0150) Loss: 0.7237 (0.6465)
[2022/12/28 23:32] | TRAIN(011): [450/879] Batch: 0.1317 (0.1338) Data: 0.0103 (0.0146) Loss: 0.5589 (0.6488)
[2022/12/28 23:32] | TRAIN(011): [500/879] Batch: 0.1169 (0.1333) Data: 0.0090 (0.0142) Loss: 0.7605 (0.6476)
[2022/12/28 23:32] | TRAIN(011): [550/879] Batch: 0.1420 (0.1329) Data: 0.0096 (0.0138) Loss: 0.7294 (0.6436)
[2022/12/28 23:32] | TRAIN(011): [600/879] Batch: 0.1248 (0.1324) Data: 0.0110 (0.0135) Loss: 0.5810 (0.6437)
[2022/12/28 23:32] | TRAIN(011): [650/879] Batch: 0.1470 (0.1322) Data: 0.0112 (0.0133) Loss: 0.4887 (0.6476)
[2022/12/28 23:32] | TRAIN(011): [700/879] Batch: 0.1381 (0.1320) Data: 0.0089 (0.0131) Loss: 0.6760 (0.6481)
[2022/12/28 23:32] | TRAIN(011): [750/879] Batch: 0.1309 (0.1321) Data: 0.0123 (0.0130) Loss: 0.4659 (0.6487)
[2022/12/28 23:33] | TRAIN(011): [800/879] Batch: 0.1141 (0.1321) Data: 0.0093 (0.0129) Loss: 0.8791 (0.6494)
[2022/12/28 23:33] | TRAIN(011): [850/879] Batch: 0.1379 (0.1321) Data: 0.0108 (0.0127) Loss: 0.6841 (0.6479)
[2022/12/28 23:33] | ------------------------------------------------------------
[2022/12/28 23:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:33] | ------------------------------------------------------------
[2022/12/28 23:33] |    TRAIN(11)     0:01:56     0:00:11     0:01:44      0.6480
[2022/12/28 23:33] | ------------------------------------------------------------
[2022/12/28 23:33] | VALID(011): [ 50/220] Batch: 0.0460 (0.0673) Data: 0.0356 (0.0557) Loss: 0.4130 (0.5831)
[2022/12/28 23:33] | VALID(011): [100/220] Batch: 0.0358 (0.0540) Data: 0.0235 (0.0429) Loss: 0.9270 (0.6205)
[2022/12/28 23:33] | VALID(011): [150/220] Batch: 0.0392 (0.0501) Data: 0.0406 (0.0393) Loss: 0.5300 (0.6167)
[2022/12/28 23:33] | VALID(011): [200/220] Batch: 0.0496 (0.0482) Data: 0.0347 (0.0374) Loss: 0.3846 (0.6229)
[2022/12/28 23:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:33] |    VALID(11)      0.6204      0.7980      0.8516      0.7980      0.7980      0.7980      0.9495
[2022/12/28 23:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:33] | ####################################################################################################
[2022/12/28 23:33] | TRAIN(012): [ 50/879] Batch: 0.1306 (0.1690) Data: 0.0086 (0.0459) Loss: 0.5621 (0.6493)
[2022/12/28 23:33] | TRAIN(012): [100/879] Batch: 0.1429 (0.1500) Data: 0.0170 (0.0288) Loss: 0.8066 (0.6590)
[2022/12/28 23:33] | TRAIN(012): [150/879] Batch: 0.1291 (0.1429) Data: 0.0110 (0.0230) Loss: 0.4665 (0.6711)
[2022/12/28 23:33] | TRAIN(012): [200/879] Batch: 0.1211 (0.1401) Data: 0.0169 (0.0202) Loss: 0.5868 (0.6597)
[2022/12/28 23:34] | TRAIN(012): [250/879] Batch: 0.1221 (0.1378) Data: 0.0098 (0.0182) Loss: 0.4525 (0.6443)
[2022/12/28 23:34] | TRAIN(012): [300/879] Batch: 0.0731 (0.1358) Data: 0.0134 (0.0170) Loss: 0.4854 (0.6505)
[2022/12/28 23:34] | TRAIN(012): [350/879] Batch: 0.1369 (0.1332) Data: 0.0103 (0.0162) Loss: 0.3750 (0.6492)
[2022/12/28 23:34] | TRAIN(012): [400/879] Batch: 0.1333 (0.1330) Data: 0.0105 (0.0156) Loss: 0.9937 (0.6449)
[2022/12/28 23:34] | TRAIN(012): [450/879] Batch: 0.1366 (0.1299) Data: 0.0098 (0.0150) Loss: 0.7042 (0.6386)
[2022/12/28 23:34] | TRAIN(012): [500/879] Batch: 0.1388 (0.1297) Data: 0.0094 (0.0145) Loss: 0.5866 (0.6401)
[2022/12/28 23:34] | TRAIN(012): [550/879] Batch: 0.1180 (0.1296) Data: 0.0091 (0.0141) Loss: 1.0294 (0.6412)
[2022/12/28 23:34] | TRAIN(012): [600/879] Batch: 0.1288 (0.1300) Data: 0.0109 (0.0139) Loss: 0.6230 (0.6405)
[2022/12/28 23:34] | TRAIN(012): [650/879] Batch: 0.1433 (0.1298) Data: 0.0094 (0.0135) Loss: 0.7419 (0.6384)
[2022/12/28 23:34] | TRAIN(012): [700/879] Batch: 0.1204 (0.1296) Data: 0.0107 (0.0133) Loss: 0.9949 (0.6425)
[2022/12/28 23:35] | TRAIN(012): [750/879] Batch: 0.1286 (0.1293) Data: 0.0104 (0.0131) Loss: 0.8656 (0.6439)
[2022/12/28 23:35] | TRAIN(012): [800/879] Batch: 0.1437 (0.1293) Data: 0.0096 (0.0129) Loss: 0.5482 (0.6440)
[2022/12/28 23:35] | TRAIN(012): [850/879] Batch: 0.1210 (0.1294) Data: 0.0104 (0.0127) Loss: 0.6209 (0.6434)
[2022/12/28 23:35] | ------------------------------------------------------------
[2022/12/28 23:35] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:35] | ------------------------------------------------------------
[2022/12/28 23:35] |    TRAIN(12)     0:01:53     0:00:11     0:01:42      0.6435
[2022/12/28 23:35] | ------------------------------------------------------------
[2022/12/28 23:35] | VALID(012): [ 50/220] Batch: 0.0334 (0.0662) Data: 0.0174 (0.0551) Loss: 0.5859 (0.6373)
[2022/12/28 23:35] | VALID(012): [100/220] Batch: 0.0483 (0.0539) Data: 0.0206 (0.0432) Loss: 0.9552 (0.6645)
[2022/12/28 23:35] | VALID(012): [150/220] Batch: 0.0497 (0.0493) Data: 0.0238 (0.0390) Loss: 0.6214 (0.6570)
[2022/12/28 23:35] | VALID(012): [200/220] Batch: 0.0280 (0.0476) Data: 0.0412 (0.0373) Loss: 0.2114 (0.6667)
[2022/12/28 23:35] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:35] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:35] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:35] |    VALID(12)      0.6649      0.7874      0.8420      0.7874      0.7874      0.7874      0.9468
[2022/12/28 23:35] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:35] | ####################################################################################################
[2022/12/28 23:35] | TRAIN(013): [ 50/879] Batch: 0.1443 (0.1680) Data: 0.0072 (0.0465) Loss: 0.4901 (0.6452)
[2022/12/28 23:35] | TRAIN(013): [100/879] Batch: 0.1154 (0.1476) Data: 0.0136 (0.0285) Loss: 0.6841 (0.6445)
[2022/12/28 23:35] | TRAIN(013): [150/879] Batch: 0.1441 (0.1414) Data: 0.0088 (0.0228) Loss: 0.5687 (0.6352)
[2022/12/28 23:35] | TRAIN(013): [200/879] Batch: 0.1442 (0.1382) Data: 0.0093 (0.0196) Loss: 0.6706 (0.6431)
[2022/12/28 23:36] | TRAIN(013): [250/879] Batch: 0.1425 (0.1364) Data: 0.0088 (0.0178) Loss: 0.4601 (0.6427)
[2022/12/28 23:36] | TRAIN(013): [300/879] Batch: 0.1261 (0.1352) Data: 0.0112 (0.0165) Loss: 0.8339 (0.6398)
[2022/12/28 23:36] | TRAIN(013): [350/879] Batch: 0.1312 (0.1343) Data: 0.0123 (0.0156) Loss: 0.5733 (0.6385)
[2022/12/28 23:36] | TRAIN(013): [400/879] Batch: 0.1241 (0.1337) Data: 0.0096 (0.0150) Loss: 0.4678 (0.6411)
[2022/12/28 23:36] | TRAIN(013): [450/879] Batch: 0.1462 (0.1337) Data: 0.0126 (0.0146) Loss: 0.7314 (0.6398)
[2022/12/28 23:36] | TRAIN(013): [500/879] Batch: 0.1183 (0.1337) Data: 0.0132 (0.0142) Loss: 0.7023 (0.6377)
[2022/12/28 23:36] | TRAIN(013): [550/879] Batch: 0.1254 (0.1337) Data: 0.0102 (0.0139) Loss: 0.5936 (0.6380)
[2022/12/28 23:36] | TRAIN(013): [600/879] Batch: 0.1184 (0.1333) Data: 0.0104 (0.0136) Loss: 0.4333 (0.6367)
[2022/12/28 23:36] | TRAIN(013): [650/879] Batch: 0.1463 (0.1328) Data: 0.0083 (0.0133) Loss: 0.5019 (0.6365)
[2022/12/28 23:37] | TRAIN(013): [700/879] Batch: 0.1226 (0.1325) Data: 0.0108 (0.0131) Loss: 0.6826 (0.6355)
[2022/12/28 23:37] | TRAIN(013): [750/879] Batch: 0.1220 (0.1326) Data: 0.0113 (0.0129) Loss: 0.5928 (0.6373)
[2022/12/28 23:37] | TRAIN(013): [800/879] Batch: 0.1446 (0.1312) Data: 0.0124 (0.0128) Loss: 0.6394 (0.6369)
[2022/12/28 23:37] | TRAIN(013): [850/879] Batch: 0.1341 (0.1316) Data: 0.0110 (0.0127) Loss: 0.6915 (0.6381)
[2022/12/28 23:37] | ------------------------------------------------------------
[2022/12/28 23:37] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:37] | ------------------------------------------------------------
[2022/12/28 23:37] |    TRAIN(13)     0:01:55     0:00:11     0:01:44      0.6386
[2022/12/28 23:37] | ------------------------------------------------------------
[2022/12/28 23:37] | VALID(013): [ 50/220] Batch: 0.0492 (0.0587) Data: 0.0179 (0.0450) Loss: 0.4733 (0.5939)
[2022/12/28 23:37] | VALID(013): [100/220] Batch: 0.0588 (0.0502) Data: 0.0267 (0.0372) Loss: 0.8611 (0.6192)
[2022/12/28 23:37] | VALID(013): [150/220] Batch: 0.0476 (0.0473) Data: 0.0343 (0.0351) Loss: 0.4955 (0.6080)
[2022/12/28 23:37] | VALID(013): [200/220] Batch: 0.0397 (0.0458) Data: 0.0186 (0.0337) Loss: 0.3311 (0.6088)
[2022/12/28 23:37] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:37] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:37] |    VALID(13)      0.6107      0.8006      0.8561      0.8006      0.8006      0.8006      0.9501
[2022/12/28 23:37] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:37] | ####################################################################################################
[2022/12/28 23:37] | TRAIN(014): [ 50/879] Batch: 0.1214 (0.1639) Data: 0.0112 (0.0453) Loss: 0.9318 (0.6529)
[2022/12/28 23:37] | TRAIN(014): [100/879] Batch: 0.1200 (0.1477) Data: 0.0130 (0.0284) Loss: 0.3770 (0.6328)
[2022/12/28 23:37] | TRAIN(014): [150/879] Batch: 0.1285 (0.1427) Data: 0.0131 (0.0227) Loss: 0.9008 (0.6452)
[2022/12/28 23:38] | TRAIN(014): [200/879] Batch: 0.1174 (0.1410) Data: 0.0113 (0.0201) Loss: 0.4683 (0.6447)
[2022/12/28 23:38] | TRAIN(014): [250/879] Batch: 0.1370 (0.1391) Data: 0.0137 (0.0183) Loss: 0.5952 (0.6360)
[2022/12/28 23:38] | TRAIN(014): [300/879] Batch: 0.1273 (0.1378) Data: 0.0132 (0.0171) Loss: 0.5302 (0.6333)
[2022/12/28 23:38] | TRAIN(014): [350/879] Batch: 0.1466 (0.1371) Data: 0.0106 (0.0163) Loss: 0.8364 (0.6303)
[2022/12/28 23:38] | TRAIN(014): [400/879] Batch: 0.1428 (0.1364) Data: 0.0094 (0.0156) Loss: 0.7085 (0.6301)
[2022/12/28 23:38] | TRAIN(014): [450/879] Batch: 0.1257 (0.1365) Data: 0.0122 (0.0153) Loss: 0.4901 (0.6281)
[2022/12/28 23:38] | TRAIN(014): [500/879] Batch: 0.1266 (0.1361) Data: 0.0109 (0.0149) Loss: 0.7582 (0.6297)
[2022/12/28 23:38] | TRAIN(014): [550/879] Batch: 0.1256 (0.1362) Data: 0.0124 (0.0147) Loss: 0.5848 (0.6314)
[2022/12/28 23:38] | TRAIN(014): [600/879] Batch: 0.1452 (0.1362) Data: 0.0111 (0.0145) Loss: 0.8088 (0.6299)
[2022/12/28 23:39] | TRAIN(014): [650/879] Batch: 0.1247 (0.1361) Data: 0.0109 (0.0143) Loss: 0.4240 (0.6309)
[2022/12/28 23:39] | TRAIN(014): [700/879] Batch: 0.1448 (0.1357) Data: 0.0101 (0.0140) Loss: 0.4314 (0.6323)
[2022/12/28 23:39] | TRAIN(014): [750/879] Batch: 0.1413 (0.1353) Data: 0.0102 (0.0138) Loss: 0.4965 (0.6333)
[2022/12/28 23:39] | TRAIN(014): [800/879] Batch: 0.1433 (0.1350) Data: 0.0097 (0.0136) Loss: 0.7492 (0.6316)
[2022/12/28 23:39] | TRAIN(014): [850/879] Batch: 0.1291 (0.1347) Data: 0.0118 (0.0135) Loss: 0.4002 (0.6326)
[2022/12/28 23:39] | ------------------------------------------------------------
[2022/12/28 23:39] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:39] | ------------------------------------------------------------
[2022/12/28 23:39] |    TRAIN(14)     0:01:58     0:00:11     0:01:46      0.6335
[2022/12/28 23:39] | ------------------------------------------------------------
[2022/12/28 23:39] | VALID(014): [ 50/220] Batch: 0.0456 (0.0666) Data: 0.0395 (0.0565) Loss: 0.5113 (0.5765)
[2022/12/28 23:39] | VALID(014): [100/220] Batch: 0.0464 (0.0553) Data: 0.0393 (0.0454) Loss: 0.8876 (0.5887)
[2022/12/28 23:39] | VALID(014): [150/220] Batch: 0.0357 (0.0512) Data: 0.0267 (0.0409) Loss: 0.5082 (0.5863)
[2022/12/28 23:39] | VALID(014): [200/220] Batch: 0.0336 (0.0493) Data: 0.0377 (0.0390) Loss: 0.2996 (0.5892)
[2022/12/28 23:39] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:39] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:39] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:39] |    VALID(14)      0.5887      0.8042      0.8580      0.8042      0.8042      0.8042      0.9510
[2022/12/28 23:39] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:39] | ####################################################################################################
[2022/12/28 23:39] | TRAIN(015): [ 50/879] Batch: 0.1278 (0.1684) Data: 0.0127 (0.0456) Loss: 0.5515 (0.5971)
[2022/12/28 23:40] | TRAIN(015): [100/879] Batch: 0.1409 (0.1526) Data: 0.0123 (0.0290) Loss: 0.7233 (0.6277)
[2022/12/28 23:40] | TRAIN(015): [150/879] Batch: 0.1397 (0.1479) Data: 0.0122 (0.0236) Loss: 0.3821 (0.6323)
[2022/12/28 23:40] | TRAIN(015): [200/879] Batch: 0.0853 (0.1433) Data: 0.0119 (0.0207) Loss: 0.5054 (0.6257)
[2022/12/28 23:40] | TRAIN(015): [250/879] Batch: 0.1333 (0.1387) Data: 0.0113 (0.0189) Loss: 0.9311 (0.6286)
[2022/12/28 23:40] | TRAIN(015): [300/879] Batch: 0.1292 (0.1377) Data: 0.0111 (0.0176) Loss: 0.7794 (0.6233)
[2022/12/28 23:40] | TRAIN(015): [350/879] Batch: 0.1278 (0.1323) Data: 0.0118 (0.0167) Loss: 0.4010 (0.6218)
[2022/12/28 23:40] | TRAIN(015): [400/879] Batch: 0.1417 (0.1326) Data: 0.0108 (0.0161) Loss: 0.5963 (0.6232)
[2022/12/28 23:40] | TRAIN(015): [450/879] Batch: 0.1251 (0.1325) Data: 0.0110 (0.0157) Loss: 0.9917 (0.6263)
[2022/12/28 23:40] | TRAIN(015): [500/879] Batch: 0.1255 (0.1325) Data: 0.0112 (0.0152) Loss: 0.5502 (0.6287)
[2022/12/28 23:40] | TRAIN(015): [550/879] Batch: 0.1190 (0.1327) Data: 0.0113 (0.0149) Loss: 0.5477 (0.6296)
[2022/12/28 23:41] | TRAIN(015): [600/879] Batch: 0.1205 (0.1329) Data: 0.0139 (0.0147) Loss: 0.6645 (0.6298)
[2022/12/28 23:41] | TRAIN(015): [650/879] Batch: 0.1252 (0.1330) Data: 0.0114 (0.0144) Loss: 0.6823 (0.6292)
[2022/12/28 23:41] | TRAIN(015): [700/879] Batch: 0.1367 (0.1329) Data: 0.0119 (0.0143) Loss: 0.7389 (0.6289)
[2022/12/28 23:41] | TRAIN(015): [750/879] Batch: 0.1416 (0.1330) Data: 0.0102 (0.0141) Loss: 1.2125 (0.6283)
[2022/12/28 23:41] | TRAIN(015): [800/879] Batch: 0.1470 (0.1330) Data: 0.0113 (0.0140) Loss: 0.5410 (0.6275)
[2022/12/28 23:41] | TRAIN(015): [850/879] Batch: 0.1489 (0.1330) Data: 0.0123 (0.0138) Loss: 0.7172 (0.6284)
[2022/12/28 23:41] | ------------------------------------------------------------
[2022/12/28 23:41] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:41] | ------------------------------------------------------------
[2022/12/28 23:41] |    TRAIN(15)     0:01:56     0:00:12     0:01:44      0.6273
[2022/12/28 23:41] | ------------------------------------------------------------
[2022/12/28 23:41] | VALID(015): [ 50/220] Batch: 0.0458 (0.0666) Data: 0.0339 (0.0539) Loss: 0.5025 (0.5898)
[2022/12/28 23:41] | VALID(015): [100/220] Batch: 0.0436 (0.0551) Data: 0.0366 (0.0438) Loss: 0.9131 (0.6162)
[2022/12/28 23:41] | VALID(015): [150/220] Batch: 0.0356 (0.0511) Data: 0.0407 (0.0401) Loss: 0.4411 (0.6127)
[2022/12/28 23:41] | VALID(015): [200/220] Batch: 0.0327 (0.0492) Data: 0.0356 (0.0383) Loss: 0.3589 (0.6164)
[2022/12/28 23:41] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:41] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:41] |    VALID(15)      0.6147      0.8026      0.8497      0.8026      0.8026      0.8026      0.9506
[2022/12/28 23:41] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:41] | ####################################################################################################
[2022/12/28 23:42] | TRAIN(016): [ 50/879] Batch: 0.1241 (0.1739) Data: 0.0125 (0.0505) Loss: 0.8331 (0.6838)
[2022/12/28 23:42] | TRAIN(016): [100/879] Batch: 0.1395 (0.1526) Data: 0.0125 (0.0310) Loss: 0.7112 (0.6598)
[2022/12/28 23:42] | TRAIN(016): [150/879] Batch: 0.1542 (0.1480) Data: 0.0127 (0.0250) Loss: 0.7559 (0.6505)
[2022/12/28 23:42] | TRAIN(016): [200/879] Batch: 0.1366 (0.1453) Data: 0.0099 (0.0219) Loss: 0.7866 (0.6493)
[2022/12/28 23:42] | TRAIN(016): [250/879] Batch: 0.1449 (0.1429) Data: 0.0104 (0.0198) Loss: 0.4434 (0.6404)
[2022/12/28 23:42] | TRAIN(016): [300/879] Batch: 0.1490 (0.1416) Data: 0.0132 (0.0184) Loss: 0.5581 (0.6359)
[2022/12/28 23:42] | TRAIN(016): [350/879] Batch: 0.1331 (0.1415) Data: 0.0123 (0.0176) Loss: 0.6218 (0.6332)
[2022/12/28 23:42] | TRAIN(016): [400/879] Batch: 0.1368 (0.1407) Data: 0.0130 (0.0169) Loss: 0.3386 (0.6319)
[2022/12/28 23:42] | TRAIN(016): [450/879] Batch: 0.1288 (0.1404) Data: 0.0123 (0.0164) Loss: 0.4828 (0.6285)
[2022/12/28 23:43] | TRAIN(016): [500/879] Batch: 0.1269 (0.1396) Data: 0.0112 (0.0158) Loss: 0.5394 (0.6279)
[2022/12/28 23:43] | TRAIN(016): [550/879] Batch: 0.1442 (0.1389) Data: 0.0107 (0.0154) Loss: 0.8354 (0.6261)
[2022/12/28 23:43] | TRAIN(016): [600/879] Batch: 0.1176 (0.1371) Data: 0.0139 (0.0151) Loss: 0.4793 (0.6282)
[2022/12/28 23:43] | TRAIN(016): [650/879] Batch: 0.1322 (0.1369) Data: 0.0113 (0.0149) Loss: 0.8306 (0.6272)
[2022/12/28 23:43] | TRAIN(016): [700/879] Batch: 0.0879 (0.1361) Data: 0.0301 (0.0147) Loss: 0.5760 (0.6264)
[2022/12/28 23:43] | TRAIN(016): [750/879] Batch: 0.1452 (0.1351) Data: 0.0144 (0.0145) Loss: 0.6601 (0.6277)
[2022/12/28 23:43] | TRAIN(016): [800/879] Batch: 0.1298 (0.1350) Data: 0.0124 (0.0143) Loss: 0.3560 (0.6257)
[2022/12/28 23:43] | TRAIN(016): [850/879] Batch: 0.1396 (0.1349) Data: 0.0104 (0.0142) Loss: 0.5901 (0.6267)
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] |    TRAIN(16)     0:01:58     0:00:12     0:01:46      0.6262
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] | VALID(016): [ 50/220] Batch: 0.0460 (0.0675) Data: 0.0324 (0.0567) Loss: 0.4576 (0.5709)
[2022/12/28 23:43] | VALID(016): [100/220] Batch: 0.0499 (0.0556) Data: 0.0289 (0.0455) Loss: 0.9216 (0.5893)
[2022/12/28 23:44] | VALID(016): [150/220] Batch: 0.0483 (0.0505) Data: 0.0352 (0.0399) Loss: 0.6838 (0.5892)
[2022/12/28 23:44] | VALID(016): [200/220] Batch: 0.0322 (0.0487) Data: 0.0409 (0.0378) Loss: 0.3603 (0.5875)
[2022/12/28 23:44] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:44] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:44] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:44] |    VALID(16)      0.5847      0.8064      0.8619      0.8064      0.8064      0.8064      0.9516
[2022/12/28 23:44] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:44] | ####################################################################################################
[2022/12/28 23:44] | TRAIN(017): [ 50/879] Batch: 0.1362 (0.1737) Data: 0.0158 (0.0491) Loss: 0.6391 (0.6387)
[2022/12/28 23:44] | TRAIN(017): [100/879] Batch: 0.1310 (0.1535) Data: 0.0124 (0.0306) Loss: 0.3788 (0.6262)
[2022/12/28 23:44] | TRAIN(017): [150/879] Batch: 0.1403 (0.1458) Data: 0.0113 (0.0242) Loss: 0.7869 (0.6271)
[2022/12/28 23:44] | TRAIN(017): [200/879] Batch: 0.1220 (0.1423) Data: 0.0110 (0.0209) Loss: 0.4973 (0.6177)
[2022/12/28 23:44] | TRAIN(017): [250/879] Batch: 0.1402 (0.1406) Data: 0.0100 (0.0192) Loss: 0.4682 (0.6163)
[2022/12/28 23:44] | TRAIN(017): [300/879] Batch: 0.1261 (0.1393) Data: 0.0129 (0.0180) Loss: 0.6039 (0.6108)
[2022/12/28 23:44] | TRAIN(017): [350/879] Batch: 0.1240 (0.1382) Data: 0.0136 (0.0170) Loss: 0.5896 (0.6114)
[2022/12/28 23:44] | TRAIN(017): [400/879] Batch: 0.1393 (0.1375) Data: 0.0107 (0.0163) Loss: 0.5150 (0.6170)
[2022/12/28 23:45] | TRAIN(017): [450/879] Batch: 0.1271 (0.1369) Data: 0.0118 (0.0157) Loss: 0.2066 (0.6205)
[2022/12/28 23:45] | TRAIN(017): [500/879] Batch: 0.1244 (0.1365) Data: 0.0110 (0.0153) Loss: 0.4928 (0.6228)
[2022/12/28 23:45] | TRAIN(017): [550/879] Batch: 0.1411 (0.1366) Data: 0.0105 (0.0150) Loss: 0.5875 (0.6195)
[2022/12/28 23:45] | TRAIN(017): [600/879] Batch: 0.1278 (0.1365) Data: 0.0116 (0.0148) Loss: 0.5807 (0.6183)
[2022/12/28 23:45] | TRAIN(017): [650/879] Batch: 0.1445 (0.1365) Data: 0.0123 (0.0146) Loss: 0.4179 (0.6212)
[2022/12/28 23:45] | TRAIN(017): [700/879] Batch: 0.1217 (0.1366) Data: 0.0108 (0.0145) Loss: 0.4264 (0.6216)
[2022/12/28 23:45] | TRAIN(017): [750/879] Batch: 0.1240 (0.1363) Data: 0.0115 (0.0142) Loss: 0.8088 (0.6236)
[2022/12/28 23:45] | TRAIN(017): [800/879] Batch: 0.1253 (0.1361) Data: 0.0144 (0.0141) Loss: 0.4989 (0.6218)
[2022/12/28 23:45] | TRAIN(017): [850/879] Batch: 0.1183 (0.1361) Data: 0.0111 (0.0140) Loss: 1.0288 (0.6218)
[2022/12/28 23:46] | ------------------------------------------------------------
[2022/12/28 23:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:46] | ------------------------------------------------------------
[2022/12/28 23:46] |    TRAIN(17)     0:01:59     0:00:12     0:01:47      0.6216
[2022/12/28 23:46] | ------------------------------------------------------------
[2022/12/28 23:46] | VALID(017): [ 50/220] Batch: 0.0357 (0.0676) Data: 0.0331 (0.0556) Loss: 0.5282 (0.5657)
[2022/12/28 23:46] | VALID(017): [100/220] Batch: 0.0335 (0.0556) Data: 0.0362 (0.0437) Loss: 0.8239 (0.5860)
[2022/12/28 23:46] | VALID(017): [150/220] Batch: 0.0344 (0.0517) Data: 0.0303 (0.0402) Loss: 0.5303 (0.5844)
[2022/12/28 23:46] | VALID(017): [200/220] Batch: 0.0355 (0.0496) Data: 0.0333 (0.0384) Loss: 0.3223 (0.5869)
[2022/12/28 23:46] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:46] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:46] |    VALID(17)      0.5850      0.8073      0.8599      0.8073      0.8073      0.8073      0.9518
[2022/12/28 23:46] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:46] | ####################################################################################################
[2022/12/28 23:46] | TRAIN(018): [ 50/879] Batch: 0.1383 (0.1733) Data: 0.0105 (0.0498) Loss: 0.4593 (0.6213)
[2022/12/28 23:46] | TRAIN(018): [100/879] Batch: 0.0905 (0.1492) Data: 0.0116 (0.0310) Loss: 0.8169 (0.6189)
[2022/12/28 23:46] | TRAIN(018): [150/879] Batch: 0.1246 (0.1388) Data: 0.0125 (0.0246) Loss: 0.4817 (0.6291)
[2022/12/28 23:46] | TRAIN(018): [200/879] Batch: 0.1262 (0.1383) Data: 0.0127 (0.0215) Loss: 0.5191 (0.6328)
[2022/12/28 23:46] | TRAIN(018): [250/879] Batch: 0.1328 (0.1373) Data: 0.0097 (0.0196) Loss: 0.7156 (0.6192)
[2022/12/28 23:46] | TRAIN(018): [300/879] Batch: 0.1346 (0.1367) Data: 0.0104 (0.0183) Loss: 0.5927 (0.6146)
[2022/12/28 23:47] | TRAIN(018): [350/879] Batch: 0.1413 (0.1362) Data: 0.0099 (0.0173) Loss: 0.6845 (0.6156)
[2022/12/28 23:47] | TRAIN(018): [400/879] Batch: 0.1290 (0.1359) Data: 0.0154 (0.0167) Loss: 0.6727 (0.6165)
[2022/12/28 23:47] | TRAIN(018): [450/879] Batch: 0.1235 (0.1356) Data: 0.0154 (0.0161) Loss: 0.6337 (0.6155)
[2022/12/28 23:47] | TRAIN(018): [500/879] Batch: 0.1335 (0.1356) Data: 0.0098 (0.0157) Loss: 0.6047 (0.6154)
[2022/12/28 23:47] | TRAIN(018): [550/879] Batch: 0.1203 (0.1353) Data: 0.0087 (0.0153) Loss: 0.6398 (0.6130)
[2022/12/28 23:47] | TRAIN(018): [600/879] Batch: 0.1292 (0.1354) Data: 0.0083 (0.0149) Loss: 0.5309 (0.6143)
[2022/12/28 23:47] | TRAIN(018): [650/879] Batch: 0.1325 (0.1353) Data: 0.0113 (0.0146) Loss: 0.5012 (0.6173)
[2022/12/28 23:47] | TRAIN(018): [700/879] Batch: 0.1272 (0.1353) Data: 0.0155 (0.0144) Loss: 0.4120 (0.6171)
[2022/12/28 23:47] | TRAIN(018): [750/879] Batch: 0.1267 (0.1352) Data: 0.0099 (0.0142) Loss: 0.3771 (0.6166)
[2022/12/28 23:48] | TRAIN(018): [800/879] Batch: 0.1310 (0.1351) Data: 0.0086 (0.0140) Loss: 0.8557 (0.6173)
[2022/12/28 23:48] | TRAIN(018): [850/879] Batch: 0.1310 (0.1352) Data: 0.0106 (0.0139) Loss: 0.7241 (0.6197)
[2022/12/28 23:48] | ------------------------------------------------------------
[2022/12/28 23:48] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:48] | ------------------------------------------------------------
[2022/12/28 23:48] |    TRAIN(18)     0:01:58     0:00:12     0:01:46      0.6192
[2022/12/28 23:48] | ------------------------------------------------------------
[2022/12/28 23:48] | VALID(018): [ 50/220] Batch: 0.0549 (0.0725) Data: 0.0361 (0.0615) Loss: 0.5266 (0.5748)
[2022/12/28 23:48] | VALID(018): [100/220] Batch: 0.0482 (0.0581) Data: 0.0365 (0.0475) Loss: 0.9452 (0.6130)
[2022/12/28 23:48] | VALID(018): [150/220] Batch: 0.0601 (0.0534) Data: 0.0316 (0.0427) Loss: 0.5323 (0.6076)
[2022/12/28 23:48] | VALID(018): [200/220] Batch: 0.0540 (0.0509) Data: 0.0322 (0.0398) Loss: 0.2721 (0.6117)
[2022/12/28 23:48] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:48] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:48] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:48] |    VALID(18)      0.6085      0.8070      0.8579      0.8070      0.8070      0.8070      0.9518
[2022/12/28 23:48] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:48] | ####################################################################################################
[2022/12/28 23:48] | TRAIN(019): [ 50/879] Batch: 0.1297 (0.1726) Data: 0.0124 (0.0487) Loss: 1.2047 (0.6000)
[2022/12/28 23:48] | TRAIN(019): [100/879] Batch: 0.1508 (0.1567) Data: 0.0120 (0.0310) Loss: 0.6273 (0.6098)
[2022/12/28 23:48] | TRAIN(019): [150/879] Batch: 0.1379 (0.1500) Data: 0.0110 (0.0248) Loss: 0.9719 (0.6160)
[2022/12/28 23:48] | TRAIN(019): [200/879] Batch: 0.1473 (0.1456) Data: 0.0121 (0.0214) Loss: 0.4378 (0.6079)
[2022/12/28 23:49] | TRAIN(019): [250/879] Batch: 0.1413 (0.1444) Data: 0.0101 (0.0196) Loss: 0.9676 (0.6106)
[2022/12/28 23:49] | TRAIN(019): [300/879] Batch: 0.1318 (0.1436) Data: 0.0145 (0.0184) Loss: 0.6401 (0.6097)
[2022/12/28 23:49] | TRAIN(019): [350/879] Batch: 0.0749 (0.1418) Data: 0.0099 (0.0174) Loss: 0.5878 (0.6089)
[2022/12/28 23:49] | TRAIN(019): [400/879] Batch: 0.1300 (0.1384) Data: 0.0107 (0.0167) Loss: 1.0329 (0.6120)
[2022/12/28 23:49] | TRAIN(019): [450/879] Batch: 0.1337 (0.1378) Data: 0.0117 (0.0161) Loss: 0.6590 (0.6106)
[2022/12/28 23:49] | TRAIN(019): [500/879] Batch: 0.1314 (0.1349) Data: 0.0145 (0.0157) Loss: 0.8281 (0.6128)
[2022/12/28 23:49] | TRAIN(019): [550/879] Batch: 0.1470 (0.1349) Data: 0.0103 (0.0154) Loss: 0.5714 (0.6138)
[2022/12/28 23:49] | TRAIN(019): [600/879] Batch: 0.1362 (0.1344) Data: 0.0107 (0.0152) Loss: 0.5606 (0.6135)
[2022/12/28 23:49] | TRAIN(019): [650/879] Batch: 0.1254 (0.1344) Data: 0.0111 (0.0149) Loss: 0.4469 (0.6142)
[2022/12/28 23:49] | TRAIN(019): [700/879] Batch: 0.1290 (0.1345) Data: 0.0126 (0.0147) Loss: 0.3950 (0.6160)
[2022/12/28 23:50] | TRAIN(019): [750/879] Batch: 0.1254 (0.1344) Data: 0.0124 (0.0145) Loss: 0.6823 (0.6163)
[2022/12/28 23:50] | TRAIN(019): [800/879] Batch: 0.1439 (0.1343) Data: 0.0122 (0.0144) Loss: 0.6255 (0.6177)
[2022/12/28 23:50] | TRAIN(019): [850/879] Batch: 0.1258 (0.1342) Data: 0.0131 (0.0142) Loss: 0.4660 (0.6165)
[2022/12/28 23:50] | ------------------------------------------------------------
[2022/12/28 23:50] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:50] | ------------------------------------------------------------
[2022/12/28 23:50] |    TRAIN(19)     0:01:57     0:00:12     0:01:45      0.6171
[2022/12/28 23:50] | ------------------------------------------------------------
[2022/12/28 23:50] | VALID(019): [ 50/220] Batch: 0.0440 (0.0682) Data: 0.0205 (0.0577) Loss: 0.5654 (0.5787)
[2022/12/28 23:50] | VALID(019): [100/220] Batch: 0.0352 (0.0560) Data: 0.0319 (0.0458) Loss: 0.9892 (0.5926)
[2022/12/28 23:50] | VALID(019): [150/220] Batch: 0.0366 (0.0519) Data: 0.0360 (0.0417) Loss: 0.5351 (0.5915)
[2022/12/28 23:50] | VALID(019): [200/220] Batch: 0.0466 (0.0500) Data: 0.0342 (0.0398) Loss: 0.3097 (0.5901)
[2022/12/28 23:50] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:50] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:50] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:50] |    VALID(19)      0.5884      0.8046      0.8614      0.8046      0.8046      0.8046      0.9511
[2022/12/28 23:50] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:50] | ####################################################################################################
[2022/12/28 23:50] | TRAIN(020): [ 50/879] Batch: 0.1319 (0.1689) Data: 0.0126 (0.0462) Loss: 0.4314 (0.6157)
[2022/12/28 23:50] | TRAIN(020): [100/879] Batch: 0.1371 (0.1545) Data: 0.0158 (0.0297) Loss: 0.8751 (0.6207)
[2022/12/28 23:50] | TRAIN(020): [150/879] Batch: 0.1236 (0.1471) Data: 0.0120 (0.0236) Loss: 0.6004 (0.6153)
[2022/12/28 23:51] | TRAIN(020): [200/879] Batch: 0.1146 (0.1436) Data: 0.0132 (0.0207) Loss: 0.5399 (0.6137)
[2022/12/28 23:51] | TRAIN(020): [250/879] Batch: 0.1235 (0.1416) Data: 0.0098 (0.0189) Loss: 0.5243 (0.6152)
[2022/12/28 23:51] | TRAIN(020): [300/879] Batch: 0.1404 (0.1401) Data: 0.0106 (0.0176) Loss: 0.6327 (0.6110)
[2022/12/28 23:51] | TRAIN(020): [350/879] Batch: 0.1254 (0.1391) Data: 0.0115 (0.0168) Loss: 0.3476 (0.6108)
[2022/12/28 23:51] | TRAIN(020): [400/879] Batch: 0.1301 (0.1383) Data: 0.0117 (0.0162) Loss: 0.5506 (0.6112)
[2022/12/28 23:51] | TRAIN(020): [450/879] Batch: 0.1342 (0.1377) Data: 0.0140 (0.0157) Loss: 0.4435 (0.6050)
[2022/12/28 23:51] | TRAIN(020): [500/879] Batch: 0.1310 (0.1370) Data: 0.0116 (0.0152) Loss: 0.6072 (0.6064)
[2022/12/28 23:51] | TRAIN(020): [550/879] Batch: 0.1416 (0.1367) Data: 0.0126 (0.0149) Loss: 0.7296 (0.6072)
[2022/12/28 23:51] | TRAIN(020): [600/879] Batch: 0.1315 (0.1363) Data: 0.0115 (0.0146) Loss: 0.3679 (0.6115)
[2022/12/28 23:52] | TRAIN(020): [650/879] Batch: 0.1456 (0.1360) Data: 0.0128 (0.0144) Loss: 0.7797 (0.6110)
[2022/12/28 23:52] | TRAIN(020): [700/879] Batch: 0.1487 (0.1363) Data: 0.0119 (0.0143) Loss: 0.7026 (0.6080)
[2022/12/28 23:52] | TRAIN(020): [750/879] Batch: 0.0766 (0.1352) Data: 0.0112 (0.0142) Loss: 0.6220 (0.6103)
[2022/12/28 23:52] | TRAIN(020): [800/879] Batch: 0.1429 (0.1354) Data: 0.0123 (0.0141) Loss: 0.6160 (0.6095)
[2022/12/28 23:52] | TRAIN(020): [850/879] Batch: 0.0908 (0.1346) Data: 0.0108 (0.0140) Loss: 0.5440 (0.6111)
[2022/12/28 23:52] | ------------------------------------------------------------
[2022/12/28 23:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:52] | ------------------------------------------------------------
[2022/12/28 23:52] |    TRAIN(20)     0:01:57     0:00:12     0:01:45      0.6116
[2022/12/28 23:52] | ------------------------------------------------------------
[2022/12/28 23:52] | VALID(020): [ 50/220] Batch: 0.0452 (0.0644) Data: 0.0339 (0.0538) Loss: 0.5190 (0.5610)
[2022/12/28 23:52] | VALID(020): [100/220] Batch: 0.0315 (0.0537) Data: 0.0408 (0.0433) Loss: 0.9045 (0.5777)
[2022/12/28 23:52] | VALID(020): [150/220] Batch: 0.0291 (0.0503) Data: 0.0451 (0.0399) Loss: 0.4602 (0.5712)
[2022/12/28 23:52] | VALID(020): [200/220] Batch: 0.0452 (0.0487) Data: 0.0349 (0.0380) Loss: 0.2736 (0.5745)
[2022/12/28 23:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:52] |    VALID(20)      0.5727      0.8104      0.8666      0.8104      0.8104      0.8104      0.9526
[2022/12/28 23:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:52] | ####################################################################################################
[2022/12/28 23:52] | TRAIN(021): [ 50/879] Batch: 0.1442 (0.1720) Data: 0.0111 (0.0474) Loss: 0.5202 (0.6233)
[2022/12/28 23:52] | TRAIN(021): [100/879] Batch: 0.1361 (0.1539) Data: 0.0116 (0.0302) Loss: 0.7625 (0.6119)
[2022/12/28 23:53] | TRAIN(021): [150/879] Batch: 0.1286 (0.1465) Data: 0.0132 (0.0240) Loss: 0.8113 (0.6078)
[2022/12/28 23:53] | TRAIN(021): [200/879] Batch: 0.1234 (0.1429) Data: 0.0114 (0.0208) Loss: 0.6116 (0.6107)
[2022/12/28 23:53] | TRAIN(021): [250/879] Batch: 0.1304 (0.1415) Data: 0.0125 (0.0191) Loss: 0.6836 (0.6138)
[2022/12/28 23:53] | TRAIN(021): [300/879] Batch: 0.1459 (0.1404) Data: 0.0117 (0.0178) Loss: 0.4467 (0.6113)
[2022/12/28 23:53] | TRAIN(021): [350/879] Batch: 0.1295 (0.1390) Data: 0.0126 (0.0170) Loss: 0.6444 (0.6094)
[2022/12/28 23:53] | TRAIN(021): [400/879] Batch: 0.1450 (0.1384) Data: 0.0105 (0.0164) Loss: 0.8227 (0.6103)
[2022/12/28 23:53] | TRAIN(021): [450/879] Batch: 0.1343 (0.1376) Data: 0.0121 (0.0158) Loss: 0.5614 (0.6133)
[2022/12/28 23:53] | TRAIN(021): [500/879] Batch: 0.1294 (0.1370) Data: 0.0115 (0.0154) Loss: 0.5021 (0.6100)
[2022/12/28 23:53] | TRAIN(021): [550/879] Batch: 0.1346 (0.1368) Data: 0.0128 (0.0151) Loss: 0.6533 (0.6093)
[2022/12/28 23:54] | TRAIN(021): [600/879] Batch: 0.1235 (0.1366) Data: 0.0110 (0.0149) Loss: 0.5850 (0.6090)
[2022/12/28 23:54] | TRAIN(021): [650/879] Batch: 0.1211 (0.1363) Data: 0.0109 (0.0146) Loss: 0.4506 (0.6083)
[2022/12/28 23:54] | TRAIN(021): [700/879] Batch: 0.1469 (0.1361) Data: 0.0123 (0.0144) Loss: 0.4376 (0.6080)
[2022/12/28 23:54] | TRAIN(021): [750/879] Batch: 0.1411 (0.1361) Data: 0.0098 (0.0142) Loss: 0.6810 (0.6092)
[2022/12/28 23:54] | TRAIN(021): [800/879] Batch: 0.1271 (0.1359) Data: 0.0124 (0.0140) Loss: 0.8472 (0.6092)
[2022/12/28 23:54] | TRAIN(021): [850/879] Batch: 0.1239 (0.1359) Data: 0.0119 (0.0139) Loss: 0.5716 (0.6088)
[2022/12/28 23:54] | ------------------------------------------------------------
[2022/12/28 23:54] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:54] | ------------------------------------------------------------
[2022/12/28 23:54] |    TRAIN(21)     0:01:59     0:00:12     0:01:47      0.6089
[2022/12/28 23:54] | ------------------------------------------------------------
[2022/12/28 23:54] | VALID(021): [ 50/220] Batch: 0.0461 (0.0695) Data: 0.0409 (0.0572) Loss: 0.5069 (0.5707)
[2022/12/28 23:54] | VALID(021): [100/220] Batch: 0.0467 (0.0568) Data: 0.0393 (0.0444) Loss: 0.9345 (0.5910)
[2022/12/28 23:54] | VALID(021): [150/220] Batch: 0.0488 (0.0526) Data: 0.0262 (0.0408) Loss: 0.4788 (0.5841)
[2022/12/28 23:54] | VALID(021): [200/220] Batch: 0.0341 (0.0500) Data: 0.0326 (0.0383) Loss: 0.2739 (0.5853)
[2022/12/28 23:54] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:54] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:54] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:54] |    VALID(21)      0.5828      0.8059      0.8652      0.8059      0.8059      0.8059      0.9515
[2022/12/28 23:54] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:54] | ####################################################################################################
[2022/12/28 23:55] | TRAIN(022): [ 50/879] Batch: 0.1225 (0.1733) Data: 0.0112 (0.0489) Loss: 0.8981 (0.6169)
[2022/12/28 23:55] | TRAIN(022): [100/879] Batch: 0.1237 (0.1546) Data: 0.0105 (0.0307) Loss: 0.6093 (0.5959)
[2022/12/28 23:55] | TRAIN(022): [150/879] Batch: 0.1216 (0.1417) Data: 0.0146 (0.0246) Loss: 0.7534 (0.6032)
[2022/12/28 23:55] | TRAIN(022): [200/879] Batch: 0.1271 (0.1398) Data: 0.0114 (0.0214) Loss: 0.7069 (0.6066)
[2022/12/28 23:55] | TRAIN(022): [250/879] Batch: 0.0866 (0.1357) Data: 0.0120 (0.0194) Loss: 0.5132 (0.6075)
[2022/12/28 23:55] | TRAIN(022): [300/879] Batch: 0.1190 (0.1335) Data: 0.0099 (0.0181) Loss: 0.6223 (0.6013)
[2022/12/28 23:55] | TRAIN(022): [350/879] Batch: 0.1266 (0.1332) Data: 0.0111 (0.0171) Loss: 0.3908 (0.5997)
[2022/12/28 23:55] | TRAIN(022): [400/879] Batch: 0.1210 (0.1337) Data: 0.0132 (0.0165) Loss: 0.7480 (0.6025)
[2022/12/28 23:55] | TRAIN(022): [450/879] Batch: 0.1412 (0.1339) Data: 0.0099 (0.0160) Loss: 0.6671 (0.6008)
[2022/12/28 23:55] | TRAIN(022): [500/879] Batch: 0.1414 (0.1340) Data: 0.0148 (0.0156) Loss: 0.7830 (0.6031)
[2022/12/28 23:56] | TRAIN(022): [550/879] Batch: 0.1283 (0.1338) Data: 0.0123 (0.0152) Loss: 0.7677 (0.6011)
[2022/12/28 23:56] | TRAIN(022): [600/879] Batch: 0.1381 (0.1336) Data: 0.0108 (0.0149) Loss: 0.7016 (0.6000)
[2022/12/28 23:56] | TRAIN(022): [650/879] Batch: 0.1284 (0.1335) Data: 0.0117 (0.0146) Loss: 0.6486 (0.5997)
[2022/12/28 23:56] | TRAIN(022): [700/879] Batch: 0.1181 (0.1334) Data: 0.0114 (0.0144) Loss: 0.8678 (0.6002)
[2022/12/28 23:56] | TRAIN(022): [750/879] Batch: 0.1451 (0.1333) Data: 0.0105 (0.0142) Loss: 0.3437 (0.6003)
[2022/12/28 23:56] | TRAIN(022): [800/879] Batch: 0.1247 (0.1332) Data: 0.0117 (0.0140) Loss: 1.1872 (0.6014)
[2022/12/28 23:56] | TRAIN(022): [850/879] Batch: 0.1332 (0.1335) Data: 0.0143 (0.0140) Loss: 0.5054 (0.6016)
[2022/12/28 23:56] | ------------------------------------------------------------
[2022/12/28 23:56] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:56] | ------------------------------------------------------------
[2022/12/28 23:56] |    TRAIN(22)     0:01:57     0:00:12     0:01:45      0.6007
[2022/12/28 23:56] | ------------------------------------------------------------
[2022/12/28 23:56] | VALID(022): [ 50/220] Batch: 0.0488 (0.0702) Data: 0.0264 (0.0584) Loss: 0.4786 (0.5546)
[2022/12/28 23:56] | VALID(022): [100/220] Batch: 0.0463 (0.0569) Data: 0.0350 (0.0459) Loss: 0.8151 (0.5777)
[2022/12/28 23:56] | VALID(022): [150/220] Batch: 0.0507 (0.0526) Data: 0.0239 (0.0419) Loss: 0.5876 (0.5751)
[2022/12/28 23:57] | VALID(022): [200/220] Batch: 0.0463 (0.0503) Data: 0.0278 (0.0396) Loss: 0.2737 (0.5738)
[2022/12/28 23:57] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:57] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:57] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:57] |    VALID(22)      0.5722      0.8108      0.8675      0.8108      0.8108      0.8108      0.9527
[2022/12/28 23:57] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:57] | ####################################################################################################
[2022/12/28 23:57] | TRAIN(023): [ 50/879] Batch: 0.1423 (0.1684) Data: 0.0106 (0.0447) Loss: 0.8007 (0.6343)
[2022/12/28 23:57] | TRAIN(023): [100/879] Batch: 0.1280 (0.1502) Data: 0.0115 (0.0283) Loss: 0.4913 (0.6292)
[2022/12/28 23:57] | TRAIN(023): [150/879] Batch: 0.1441 (0.1442) Data: 0.0101 (0.0225) Loss: 0.6762 (0.6045)
[2022/12/28 23:57] | TRAIN(023): [200/879] Batch: 0.1329 (0.1418) Data: 0.0129 (0.0199) Loss: 0.3967 (0.6034)
[2022/12/28 23:57] | TRAIN(023): [250/879] Batch: 0.1268 (0.1401) Data: 0.0122 (0.0183) Loss: 0.7307 (0.6030)
[2022/12/28 23:57] | TRAIN(023): [300/879] Batch: 0.1213 (0.1388) Data: 0.0126 (0.0172) Loss: 0.6733 (0.6026)
[2022/12/28 23:57] | TRAIN(023): [350/879] Batch: 0.1440 (0.1376) Data: 0.0098 (0.0164) Loss: 0.5265 (0.5982)
[2022/12/28 23:57] | TRAIN(023): [400/879] Batch: 0.1378 (0.1367) Data: 0.0104 (0.0158) Loss: 0.7945 (0.5996)
[2022/12/28 23:58] | TRAIN(023): [450/879] Batch: 0.1377 (0.1363) Data: 0.0107 (0.0152) Loss: 0.4559 (0.6022)
[2022/12/28 23:58] | TRAIN(023): [500/879] Batch: 0.1192 (0.1358) Data: 0.0128 (0.0148) Loss: 0.6525 (0.5999)
[2022/12/28 23:58] | TRAIN(023): [550/879] Batch: 0.1312 (0.1336) Data: 0.0114 (0.0145) Loss: 0.5712 (0.5985)
[2022/12/28 23:58] | TRAIN(023): [600/879] Batch: 0.1292 (0.1336) Data: 0.0109 (0.0143) Loss: 0.5320 (0.6000)
[2022/12/28 23:58] | TRAIN(023): [650/879] Batch: 0.0962 (0.1324) Data: 0.0124 (0.0141) Loss: 0.5375 (0.5995)
[2022/12/28 23:58] | TRAIN(023): [700/879] Batch: 0.1419 (0.1322) Data: 0.0099 (0.0140) Loss: 0.2879 (0.5998)
[2022/12/28 23:58] | TRAIN(023): [750/879] Batch: 0.1422 (0.1322) Data: 0.0125 (0.0138) Loss: 0.2872 (0.5990)
[2022/12/28 23:58] | TRAIN(023): [800/879] Batch: 0.1460 (0.1324) Data: 0.0102 (0.0137) Loss: 0.5377 (0.5997)
[2022/12/28 23:58] | TRAIN(023): [850/879] Batch: 0.1317 (0.1323) Data: 0.0120 (0.0136) Loss: 0.8007 (0.6007)
[2022/12/28 23:58] | ------------------------------------------------------------
[2022/12/28 23:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:58] | ------------------------------------------------------------
[2022/12/28 23:58] |    TRAIN(23)     0:01:56     0:00:11     0:01:44      0.6007
[2022/12/28 23:58] | ------------------------------------------------------------
[2022/12/28 23:59] | VALID(023): [ 50/220] Batch: 0.0464 (0.0680) Data: 0.0341 (0.0576) Loss: 0.5568 (0.5682)
[2022/12/28 23:59] | VALID(023): [100/220] Batch: 0.0459 (0.0559) Data: 0.0390 (0.0458) Loss: 0.8528 (0.5831)
[2022/12/28 23:59] | VALID(023): [150/220] Batch: 0.0434 (0.0519) Data: 0.0340 (0.0417) Loss: 0.5420 (0.5867)
[2022/12/28 23:59] | VALID(023): [200/220] Batch: 0.0512 (0.0497) Data: 0.0363 (0.0395) Loss: 0.3400 (0.5822)
[2022/12/28 23:59] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:59] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:59] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:59] |    VALID(23)      0.5799      0.8079      0.8651      0.8079      0.8079      0.8079      0.9520
[2022/12/28 23:59] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:59] | ####################################################################################################
[2022/12/28 23:59] | TRAIN(024): [ 50/879] Batch: 0.1221 (0.1699) Data: 0.0110 (0.0477) Loss: 0.6149 (0.5603)
[2022/12/28 23:59] | TRAIN(024): [100/879] Batch: 0.1270 (0.1511) Data: 0.0109 (0.0298) Loss: 0.8433 (0.5726)
[2022/12/28 23:59] | TRAIN(024): [150/879] Batch: 0.1251 (0.1462) Data: 0.0133 (0.0240) Loss: 0.4348 (0.5713)
[2022/12/28 23:59] | TRAIN(024): [200/879] Batch: 0.1414 (0.1427) Data: 0.0098 (0.0209) Loss: 0.4836 (0.5773)
[2022/12/28 23:59] | TRAIN(024): [250/879] Batch: 0.1489 (0.1420) Data: 0.0134 (0.0193) Loss: 0.6182 (0.5806)
[2022/12/28 23:59] | TRAIN(024): [300/879] Batch: 0.1321 (0.1410) Data: 0.0101 (0.0181) Loss: 0.5566 (0.5929)
[2022/12/28 23:59] | TRAIN(024): [350/879] Batch: 0.1412 (0.1400) Data: 0.0106 (0.0171) Loss: 0.6285 (0.5909)
[2022/12/29 00:00] | TRAIN(024): [400/879] Batch: 0.1473 (0.1396) Data: 0.0122 (0.0165) Loss: 0.6225 (0.5919)
[2022/12/29 00:00] | TRAIN(024): [450/879] Batch: 0.1327 (0.1393) Data: 0.0119 (0.0160) Loss: 0.5036 (0.5935)
[2022/12/29 00:00] | TRAIN(024): [500/879] Batch: 0.1347 (0.1390) Data: 0.0147 (0.0157) Loss: 0.6031 (0.5945)
[2022/12/29 00:00] | TRAIN(024): [550/879] Batch: 0.1195 (0.1389) Data: 0.0106 (0.0154) Loss: 0.5422 (0.5957)
[2022/12/29 00:00] | TRAIN(024): [600/879] Batch: 0.1255 (0.1383) Data: 0.0111 (0.0151) Loss: 0.7309 (0.5977)
[2022/12/29 00:00] | TRAIN(024): [650/879] Batch: 0.1283 (0.1376) Data: 0.0121 (0.0148) Loss: 0.6479 (0.5976)
[2022/12/29 00:00] | TRAIN(024): [700/879] Batch: 0.1320 (0.1374) Data: 0.0117 (0.0146) Loss: 0.3886 (0.5943)
[2022/12/29 00:00] | TRAIN(024): [750/879] Batch: 0.1439 (0.1371) Data: 0.0097 (0.0144) Loss: 0.5633 (0.5942)
[2022/12/29 00:00] | TRAIN(024): [800/879] Batch: 0.1263 (0.1367) Data: 0.0110 (0.0142) Loss: 0.4960 (0.5930)
[2022/12/29 00:01] | TRAIN(024): [850/879] Batch: 0.1355 (0.1365) Data: 0.0102 (0.0140) Loss: 0.6530 (0.5925)
[2022/12/29 00:01] | ------------------------------------------------------------
[2022/12/29 00:01] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:01] | ------------------------------------------------------------
[2022/12/29 00:01] |    TRAIN(24)     0:01:59     0:00:12     0:01:47      0.5928
[2022/12/29 00:01] | ------------------------------------------------------------
[2022/12/29 00:01] | VALID(024): [ 50/220] Batch: 0.0258 (0.0612) Data: 0.0136 (0.0496) Loss: 0.5298 (0.6196)
[2022/12/29 00:01] | VALID(024): [100/220] Batch: 0.0416 (0.0448) Data: 0.0385 (0.0336) Loss: 0.8581 (0.6452)
[2022/12/29 00:01] | VALID(024): [150/220] Batch: 0.0478 (0.0450) Data: 0.0325 (0.0339) Loss: 0.4972 (0.6351)
[2022/12/29 00:01] | VALID(024): [200/220] Batch: 0.0462 (0.0449) Data: 0.0375 (0.0339) Loss: 0.1965 (0.6344)
[2022/12/29 00:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:01] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:01] |    VALID(24)      0.6407      0.8023      0.8620      0.8023      0.8023      0.8023      0.9506
[2022/12/29 00:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:01] | ####################################################################################################
[2022/12/29 00:01] | TRAIN(025): [ 50/879] Batch: 0.1281 (0.1467) Data: 0.0121 (0.0465) Loss: 0.5631 (0.5887)
[2022/12/29 00:01] | TRAIN(025): [100/879] Batch: 0.1458 (0.1407) Data: 0.0148 (0.0294) Loss: 0.8317 (0.5760)
[2022/12/29 00:01] | TRAIN(025): [150/879] Batch: 0.1346 (0.1381) Data: 0.0107 (0.0236) Loss: 0.6095 (0.5869)
[2022/12/29 00:01] | TRAIN(025): [200/879] Batch: 0.1407 (0.1376) Data: 0.0120 (0.0208) Loss: 0.7771 (0.5892)
[2022/12/29 00:01] | TRAIN(025): [250/879] Batch: 0.1230 (0.1372) Data: 0.0145 (0.0191) Loss: 0.6565 (0.5949)
[2022/12/29 00:01] | TRAIN(025): [300/879] Batch: 0.1446 (0.1362) Data: 0.0101 (0.0178) Loss: 0.5370 (0.5952)
[2022/12/29 00:02] | TRAIN(025): [350/879] Batch: 0.1213 (0.1357) Data: 0.0107 (0.0169) Loss: 0.4583 (0.5942)
[2022/12/29 00:02] | TRAIN(025): [400/879] Batch: 0.1449 (0.1356) Data: 0.0126 (0.0162) Loss: 0.6253 (0.5940)
[2022/12/29 00:02] | TRAIN(025): [450/879] Batch: 0.1402 (0.1356) Data: 0.0098 (0.0157) Loss: 0.5846 (0.5964)
[2022/12/29 00:02] | TRAIN(025): [500/879] Batch: 0.1269 (0.1352) Data: 0.0114 (0.0153) Loss: 0.9087 (0.5959)
[2022/12/29 00:02] | TRAIN(025): [550/879] Batch: 0.1374 (0.1351) Data: 0.0102 (0.0149) Loss: 0.4061 (0.5961)
[2022/12/29 00:02] | TRAIN(025): [600/879] Batch: 0.1416 (0.1347) Data: 0.0100 (0.0146) Loss: 0.5573 (0.5946)
[2022/12/29 00:02] | TRAIN(025): [650/879] Batch: 0.1259 (0.1345) Data: 0.0108 (0.0144) Loss: 0.5724 (0.5968)
[2022/12/29 00:02] | TRAIN(025): [700/879] Batch: 0.1368 (0.1344) Data: 0.0102 (0.0141) Loss: 0.5276 (0.5969)
[2022/12/29 00:02] | TRAIN(025): [750/879] Batch: 0.1334 (0.1342) Data: 0.0110 (0.0140) Loss: 0.4598 (0.5984)
[2022/12/29 00:03] | TRAIN(025): [800/879] Batch: 0.1508 (0.1342) Data: 0.0109 (0.0138) Loss: 0.3345 (0.5961)
[2022/12/29 00:03] | TRAIN(025): [850/879] Batch: 0.1307 (0.1341) Data: 0.0114 (0.0137) Loss: 0.7203 (0.5972)
[2022/12/29 00:03] | ------------------------------------------------------------
[2022/12/29 00:03] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:03] | ------------------------------------------------------------
[2022/12/29 00:03] |    TRAIN(25)     0:01:57     0:00:11     0:01:45      0.5970
[2022/12/29 00:03] | ------------------------------------------------------------
[2022/12/29 00:03] | VALID(025): [ 50/220] Batch: 0.0426 (0.0699) Data: 0.0373 (0.0597) Loss: 0.6204 (0.5695)
[2022/12/29 00:03] | VALID(025): [100/220] Batch: 0.0350 (0.0567) Data: 0.0364 (0.0465) Loss: 1.0025 (0.5871)
[2022/12/29 00:03] | VALID(025): [150/220] Batch: 0.0411 (0.0524) Data: 0.0405 (0.0423) Loss: 0.5681 (0.5897)
[2022/12/29 00:03] | VALID(025): [200/220] Batch: 0.0500 (0.0503) Data: 0.0369 (0.0402) Loss: 0.3239 (0.5846)
[2022/12/29 00:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:03] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:03] |    VALID(25)      0.5804      0.8059      0.8667      0.8059      0.8059      0.8059      0.9515
[2022/12/29 00:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:03] | ####################################################################################################
[2022/12/29 00:03] | TRAIN(026): [ 50/879] Batch: 0.1324 (0.1688) Data: 0.0115 (0.0471) Loss: 0.7200 (0.5886)
[2022/12/29 00:03] | TRAIN(026): [100/879] Batch: 0.1199 (0.1506) Data: 0.0116 (0.0294) Loss: 0.4862 (0.5936)
[2022/12/29 00:03] | TRAIN(026): [150/879] Batch: 0.1423 (0.1444) Data: 0.0100 (0.0234) Loss: 0.8656 (0.5931)
[2022/12/29 00:03] | TRAIN(026): [200/879] Batch: 0.1511 (0.1427) Data: 0.0121 (0.0206) Loss: 0.3693 (0.5936)
[2022/12/29 00:04] | TRAIN(026): [250/879] Batch: 0.1315 (0.1413) Data: 0.0132 (0.0190) Loss: 0.6615 (0.5921)
[2022/12/29 00:04] | TRAIN(026): [300/879] Batch: 0.0779 (0.1402) Data: 0.0112 (0.0179) Loss: 0.5968 (0.5947)
[2022/12/29 00:04] | TRAIN(026): [350/879] Batch: 0.1281 (0.1372) Data: 0.0120 (0.0171) Loss: 0.4700 (0.5895)
[2022/12/29 00:04] | TRAIN(026): [400/879] Batch: 0.1330 (0.1368) Data: 0.0131 (0.0164) Loss: 0.5288 (0.5943)
[2022/12/29 00:04] | TRAIN(026): [450/879] Batch: 0.1406 (0.1336) Data: 0.0089 (0.0159) Loss: 0.4739 (0.5938)
[2022/12/29 00:04] | TRAIN(026): [500/879] Batch: 0.1214 (0.1334) Data: 0.0107 (0.0154) Loss: 0.5434 (0.5938)
[2022/12/29 00:04] | TRAIN(026): [550/879] Batch: 0.1198 (0.1335) Data: 0.0136 (0.0151) Loss: 0.5221 (0.5964)
[2022/12/29 00:04] | TRAIN(026): [600/879] Batch: 0.1411 (0.1333) Data: 0.0134 (0.0148) Loss: 0.4803 (0.5950)
[2022/12/29 00:04] | TRAIN(026): [650/879] Batch: 0.1404 (0.1332) Data: 0.0098 (0.0145) Loss: 0.5923 (0.5923)
[2022/12/29 00:05] | TRAIN(026): [700/879] Batch: 0.1250 (0.1330) Data: 0.0117 (0.0143) Loss: 0.6389 (0.5907)
[2022/12/29 00:05] | TRAIN(026): [750/879] Batch: 0.1316 (0.1330) Data: 0.0102 (0.0141) Loss: 0.7172 (0.5899)
[2022/12/29 00:05] | TRAIN(026): [800/879] Batch: 0.1444 (0.1329) Data: 0.0107 (0.0139) Loss: 1.1315 (0.5887)
[2022/12/29 00:05] | TRAIN(026): [850/879] Batch: 0.1239 (0.1328) Data: 0.0110 (0.0137) Loss: 0.7169 (0.5915)
[2022/12/29 00:05] | ------------------------------------------------------------
[2022/12/29 00:05] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:05] | ------------------------------------------------------------
[2022/12/29 00:05] |    TRAIN(26)     0:01:56     0:00:11     0:01:44      0.5911
[2022/12/29 00:05] | ------------------------------------------------------------
[2022/12/29 00:05] | VALID(026): [ 50/220] Batch: 0.0500 (0.0683) Data: 0.0316 (0.0571) Loss: 0.5905 (0.5547)
[2022/12/29 00:05] | VALID(026): [100/220] Batch: 0.0491 (0.0559) Data: 0.0273 (0.0454) Loss: 0.9208 (0.5844)
[2022/12/29 00:05] | VALID(026): [150/220] Batch: 0.0485 (0.0518) Data: 0.0241 (0.0414) Loss: 0.4625 (0.5783)
[2022/12/29 00:05] | VALID(026): [200/220] Batch: 0.0480 (0.0497) Data: 0.0243 (0.0393) Loss: 0.2130 (0.5801)
[2022/12/29 00:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:05] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:05] |    VALID(26)      0.5757      0.8145      0.8685      0.8145      0.8145      0.8145      0.9536
[2022/12/29 00:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:05] | ####################################################################################################
[2022/12/29 00:05] | TRAIN(027): [ 50/879] Batch: 0.1245 (0.1687) Data: 0.0174 (0.0469) Loss: 0.6067 (0.5889)
[2022/12/29 00:05] | TRAIN(027): [100/879] Batch: 0.1521 (0.1515) Data: 0.0121 (0.0295) Loss: 0.3932 (0.5905)
[2022/12/29 00:05] | TRAIN(027): [150/879] Batch: 0.1587 (0.1470) Data: 0.0123 (0.0238) Loss: 0.4196 (0.5791)
[2022/12/29 00:06] | TRAIN(027): [200/879] Batch: 0.1373 (0.1442) Data: 0.0170 (0.0210) Loss: 0.4042 (0.5742)
[2022/12/29 00:06] | TRAIN(027): [250/879] Batch: 0.1230 (0.1418) Data: 0.0119 (0.0190) Loss: 0.3842 (0.5833)
[2022/12/29 00:06] | TRAIN(027): [300/879] Batch: 0.1227 (0.1400) Data: 0.0108 (0.0178) Loss: 0.4588 (0.5832)
[2022/12/29 00:06] | TRAIN(027): [350/879] Batch: 0.1339 (0.1388) Data: 0.0137 (0.0168) Loss: 0.5903 (0.5816)
[2022/12/29 00:06] | TRAIN(027): [400/879] Batch: 0.1449 (0.1384) Data: 0.0090 (0.0161) Loss: 0.5794 (0.5840)
[2022/12/29 00:06] | TRAIN(027): [450/879] Batch: 0.1276 (0.1376) Data: 0.0116 (0.0155) Loss: 0.3313 (0.5822)
[2022/12/29 00:06] | TRAIN(027): [500/879] Batch: 0.1417 (0.1371) Data: 0.0102 (0.0151) Loss: 0.4784 (0.5834)
[2022/12/29 00:06] | TRAIN(027): [550/879] Batch: 0.1212 (0.1366) Data: 0.0114 (0.0147) Loss: 0.3769 (0.5848)
[2022/12/29 00:06] | TRAIN(027): [600/879] Batch: 0.1241 (0.1362) Data: 0.0134 (0.0144) Loss: 0.8430 (0.5864)
[2022/12/29 00:07] | TRAIN(027): [650/879] Batch: 0.1375 (0.1359) Data: 0.0127 (0.0142) Loss: 0.4386 (0.5861)
[2022/12/29 00:07] | TRAIN(027): [700/879] Batch: 0.0823 (0.1355) Data: 0.0084 (0.0140) Loss: 0.7391 (0.5869)
[2022/12/29 00:07] | TRAIN(027): [750/879] Batch: 0.1378 (0.1346) Data: 0.0120 (0.0139) Loss: 0.6121 (0.5873)
[2022/12/29 00:07] | TRAIN(027): [800/879] Batch: 0.1429 (0.1347) Data: 0.0115 (0.0137) Loss: 0.5864 (0.5885)
[2022/12/29 00:07] | TRAIN(027): [850/879] Batch: 0.1311 (0.1333) Data: 0.0235 (0.0136) Loss: 0.3970 (0.5871)
[2022/12/29 00:07] | ------------------------------------------------------------
[2022/12/29 00:07] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:07] | ------------------------------------------------------------
[2022/12/29 00:07] |    TRAIN(27)     0:01:57     0:00:11     0:01:45      0.5878
[2022/12/29 00:07] | ------------------------------------------------------------
[2022/12/29 00:07] | VALID(027): [ 50/220] Batch: 0.0490 (0.0729) Data: 0.0368 (0.0610) Loss: 0.4868 (0.5392)
[2022/12/29 00:07] | VALID(027): [100/220] Batch: 0.0434 (0.0585) Data: 0.0338 (0.0473) Loss: 0.8155 (0.5724)
[2022/12/29 00:07] | VALID(027): [150/220] Batch: 0.0304 (0.0533) Data: 0.0412 (0.0419) Loss: 0.4801 (0.5790)
[2022/12/29 00:07] | VALID(027): [200/220] Batch: 0.0498 (0.0508) Data: 0.0216 (0.0394) Loss: 0.2481 (0.5805)
[2022/12/29 00:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:07] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:07] |    VALID(27)      0.5765      0.8064      0.8673      0.8064      0.8064      0.8064      0.9516
[2022/12/29 00:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:07] | ####################################################################################################
[2022/12/29 00:07] | TRAIN(028): [ 50/879] Batch: 0.1395 (0.1693) Data: 0.0081 (0.0477) Loss: 0.6306 (0.6000)
[2022/12/29 00:07] | TRAIN(028): [100/879] Batch: 0.1242 (0.1509) Data: 0.0115 (0.0302) Loss: 0.4950 (0.5858)
[2022/12/29 00:08] | TRAIN(028): [150/879] Batch: 0.1373 (0.1455) Data: 0.0087 (0.0239) Loss: 0.6287 (0.5741)
[2022/12/29 00:08] | TRAIN(028): [200/879] Batch: 0.1223 (0.1426) Data: 0.0103 (0.0208) Loss: 0.6274 (0.5809)
[2022/12/29 00:08] | TRAIN(028): [250/879] Batch: 0.1318 (0.1413) Data: 0.0095 (0.0190) Loss: 0.4264 (0.5804)
[2022/12/29 00:08] | TRAIN(028): [300/879] Batch: 0.1244 (0.1402) Data: 0.0122 (0.0180) Loss: 0.4975 (0.5799)
[2022/12/29 00:08] | TRAIN(028): [350/879] Batch: 0.1450 (0.1396) Data: 0.0123 (0.0171) Loss: 0.3791 (0.5814)
[2022/12/29 00:08] | TRAIN(028): [400/879] Batch: 0.1370 (0.1386) Data: 0.0112 (0.0165) Loss: 0.6097 (0.5877)
[2022/12/29 00:08] | TRAIN(028): [450/879] Batch: 0.1255 (0.1375) Data: 0.0107 (0.0159) Loss: 0.5830 (0.5841)
[2022/12/29 00:08] | TRAIN(028): [500/879] Batch: 0.1195 (0.1370) Data: 0.0109 (0.0154) Loss: 0.5395 (0.5872)
[2022/12/29 00:08] | TRAIN(028): [550/879] Batch: 0.1362 (0.1367) Data: 0.0127 (0.0151) Loss: 0.4743 (0.5880)
[2022/12/29 00:09] | TRAIN(028): [600/879] Batch: 0.1334 (0.1362) Data: 0.0127 (0.0148) Loss: 0.7082 (0.5904)
[2022/12/29 00:09] | TRAIN(028): [650/879] Batch: 0.1340 (0.1361) Data: 0.0126 (0.0146) Loss: 0.7451 (0.5908)
[2022/12/29 00:09] | TRAIN(028): [700/879] Batch: 0.1378 (0.1364) Data: 0.0124 (0.0144) Loss: 0.6444 (0.5883)
[2022/12/29 00:09] | TRAIN(028): [750/879] Batch: 0.1369 (0.1366) Data: 0.0125 (0.0143) Loss: 0.4884 (0.5888)
[2022/12/29 00:09] | TRAIN(028): [800/879] Batch: 0.1320 (0.1366) Data: 0.0111 (0.0142) Loss: 0.4991 (0.5871)
[2022/12/29 00:09] | TRAIN(028): [850/879] Batch: 0.1486 (0.1364) Data: 0.0127 (0.0140) Loss: 0.6397 (0.5857)
[2022/12/29 00:09] | ------------------------------------------------------------
[2022/12/29 00:09] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:09] | ------------------------------------------------------------
[2022/12/29 00:09] |    TRAIN(28)     0:01:59     0:00:12     0:01:47      0.5829
[2022/12/29 00:09] | ------------------------------------------------------------
[2022/12/29 00:09] | VALID(028): [ 50/220] Batch: 0.0473 (0.0728) Data: 0.0215 (0.0619) Loss: 0.5520 (0.5617)
[2022/12/29 00:09] | VALID(028): [100/220] Batch: 0.0350 (0.0584) Data: 0.0355 (0.0479) Loss: 0.9445 (0.5914)
[2022/12/29 00:09] | VALID(028): [150/220] Batch: 0.0207 (0.0535) Data: 0.0437 (0.0432) Loss: 0.5734 (0.5874)
[2022/12/29 00:09] | VALID(028): [200/220] Batch: 0.0350 (0.0511) Data: 0.0336 (0.0407) Loss: 0.2489 (0.5871)
[2022/12/29 00:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:09] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:09] |    VALID(28)      0.5839      0.8138      0.8674      0.8138      0.8138      0.8138      0.9535
[2022/12/29 00:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:09] | ####################################################################################################
[2022/12/29 00:10] | TRAIN(029): [ 50/879] Batch: 0.1346 (0.1706) Data: 0.0123 (0.0484) Loss: 0.4073 (0.5651)
[2022/12/29 00:10] | TRAIN(029): [100/879] Batch: 0.0938 (0.1415) Data: 0.0139 (0.0302) Loss: 0.5765 (0.5812)
[2022/12/29 00:10] | TRAIN(029): [150/879] Batch: 0.1277 (0.1394) Data: 0.0130 (0.0242) Loss: 0.7534 (0.5819)
[2022/12/29 00:10] | TRAIN(029): [200/879] Batch: 0.0849 (0.1353) Data: 0.0119 (0.0211) Loss: 0.4665 (0.5860)
[2022/12/29 00:10] | TRAIN(029): [250/879] Batch: 0.1270 (0.1324) Data: 0.0113 (0.0192) Loss: 0.6712 (0.5754)
[2022/12/29 00:10] | TRAIN(029): [300/879] Batch: 0.1465 (0.1328) Data: 0.0124 (0.0180) Loss: 0.5969 (0.5813)
[2022/12/29 00:10] | TRAIN(029): [350/879] Batch: 0.1487 (0.1336) Data: 0.0129 (0.0173) Loss: 0.8387 (0.5824)
[2022/12/29 00:10] | TRAIN(029): [400/879] Batch: 0.1232 (0.1336) Data: 0.0113 (0.0166) Loss: 0.2867 (0.5840)
[2022/12/29 00:10] | TRAIN(029): [450/879] Batch: 0.1327 (0.1334) Data: 0.0122 (0.0160) Loss: 0.3041 (0.5772)
[2022/12/29 00:11] | TRAIN(029): [500/879] Batch: 0.1288 (0.1333) Data: 0.0124 (0.0156) Loss: 0.2789 (0.5803)
[2022/12/29 00:11] | TRAIN(029): [550/879] Batch: 0.1453 (0.1334) Data: 0.0125 (0.0153) Loss: 0.5629 (0.5761)
[2022/12/29 00:11] | TRAIN(029): [600/879] Batch: 0.1228 (0.1332) Data: 0.0135 (0.0150) Loss: 0.8406 (0.5757)
[2022/12/29 00:11] | TRAIN(029): [650/879] Batch: 0.1532 (0.1335) Data: 0.0118 (0.0148) Loss: 0.6845 (0.5769)
[2022/12/29 00:11] | TRAIN(029): [700/879] Batch: 0.1390 (0.1338) Data: 0.0113 (0.0146) Loss: 0.8593 (0.5777)
[2022/12/29 00:11] | TRAIN(029): [750/879] Batch: 0.1293 (0.1337) Data: 0.0090 (0.0144) Loss: 0.7775 (0.5779)
[2022/12/29 00:11] | TRAIN(029): [800/879] Batch: 0.1397 (0.1337) Data: 0.0114 (0.0143) Loss: 0.8106 (0.5778)
[2022/12/29 00:11] | TRAIN(029): [850/879] Batch: 0.1343 (0.1337) Data: 0.0123 (0.0141) Loss: 0.7035 (0.5778)
[2022/12/29 00:11] | ------------------------------------------------------------
[2022/12/29 00:11] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:11] | ------------------------------------------------------------
[2022/12/29 00:11] |    TRAIN(29)     0:01:57     0:00:12     0:01:45      0.5780
[2022/12/29 00:11] | ------------------------------------------------------------
[2022/12/29 00:11] | VALID(029): [ 50/220] Batch: 0.0301 (0.0687) Data: 0.0390 (0.0561) Loss: 0.5794 (0.5514)
[2022/12/29 00:11] | VALID(029): [100/220] Batch: 0.0316 (0.0564) Data: 0.0360 (0.0442) Loss: 0.8748 (0.5805)
[2022/12/29 00:12] | VALID(029): [150/220] Batch: 0.0508 (0.0524) Data: 0.0370 (0.0397) Loss: 0.5684 (0.5756)
[2022/12/29 00:12] | VALID(029): [200/220] Batch: 0.0424 (0.0503) Data: 0.0333 (0.0380) Loss: 0.2408 (0.5793)
[2022/12/29 00:12] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:12] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:12] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:12] |    VALID(29)      0.5763      0.8150      0.8693      0.8150      0.8150      0.8150      0.9537
[2022/12/29 00:12] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:12] | ####################################################################################################
[2022/12/29 00:12] | TRAIN(030): [ 50/879] Batch: 0.1487 (0.1778) Data: 0.0125 (0.0528) Loss: 0.4258 (0.5630)
[2022/12/29 00:12] | TRAIN(030): [100/879] Batch: 0.1244 (0.1558) Data: 0.0109 (0.0325) Loss: 0.7145 (0.5684)
[2022/12/29 00:12] | TRAIN(030): [150/879] Batch: 0.1330 (0.1485) Data: 0.0128 (0.0257) Loss: 0.6495 (0.5680)
[2022/12/29 00:12] | TRAIN(030): [200/879] Batch: 0.1287 (0.1447) Data: 0.0112 (0.0222) Loss: 0.3261 (0.5696)
[2022/12/29 00:12] | TRAIN(030): [250/879] Batch: 0.1470 (0.1419) Data: 0.0114 (0.0200) Loss: 0.7048 (0.5741)
[2022/12/29 00:12] | TRAIN(030): [300/879] Batch: 0.1229 (0.1403) Data: 0.0108 (0.0186) Loss: 0.8464 (0.5747)
[2022/12/29 00:12] | TRAIN(030): [350/879] Batch: 0.1383 (0.1393) Data: 0.0100 (0.0175) Loss: 0.5340 (0.5751)
[2022/12/29 00:13] | TRAIN(030): [400/879] Batch: 0.1454 (0.1385) Data: 0.0098 (0.0167) Loss: 0.3414 (0.5713)
[2022/12/29 00:13] | TRAIN(030): [450/879] Batch: 0.1299 (0.1378) Data: 0.0113 (0.0161) Loss: 0.7432 (0.5732)
[2022/12/29 00:13] | TRAIN(030): [500/879] Batch: 0.1413 (0.1353) Data: 0.0130 (0.0157) Loss: 0.8211 (0.5762)
[2022/12/29 00:13] | TRAIN(030): [550/879] Batch: 0.1319 (0.1353) Data: 0.0118 (0.0154) Loss: 0.2979 (0.5762)
[2022/12/29 00:13] | TRAIN(030): [600/879] Batch: 0.1028 (0.1331) Data: 0.0093 (0.0151) Loss: 0.4526 (0.5742)
[2022/12/29 00:13] | TRAIN(030): [650/879] Batch: 0.1419 (0.1331) Data: 0.0104 (0.0148) Loss: 0.9794 (0.5726)
[2022/12/29 00:13] | TRAIN(030): [700/879] Batch: 0.1449 (0.1330) Data: 0.0104 (0.0146) Loss: 0.5903 (0.5710)
[2022/12/29 00:13] | TRAIN(030): [750/879] Batch: 0.1243 (0.1329) Data: 0.0131 (0.0144) Loss: 0.7233 (0.5705)
[2022/12/29 00:13] | TRAIN(030): [800/879] Batch: 0.1394 (0.1327) Data: 0.0123 (0.0142) Loss: 0.4125 (0.5727)
[2022/12/29 00:13] | TRAIN(030): [850/879] Batch: 0.1306 (0.1329) Data: 0.0116 (0.0141) Loss: 0.5211 (0.5750)
[2022/12/29 00:14] | ------------------------------------------------------------
[2022/12/29 00:14] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:14] | ------------------------------------------------------------
[2022/12/29 00:14] |    TRAIN(30)     0:01:56     0:00:12     0:01:44      0.5751
[2022/12/29 00:14] | ------------------------------------------------------------
[2022/12/29 00:14] | VALID(030): [ 50/220] Batch: 0.0312 (0.0680) Data: 0.0150 (0.0570) Loss: 0.4782 (0.5371)
[2022/12/29 00:14] | VALID(030): [100/220] Batch: 0.0318 (0.0543) Data: 0.0357 (0.0430) Loss: 0.9145 (0.5730)
[2022/12/29 00:14] | VALID(030): [150/220] Batch: 0.0470 (0.0506) Data: 0.0219 (0.0394) Loss: 0.4971 (0.5726)
[2022/12/29 00:14] | VALID(030): [200/220] Batch: 0.0457 (0.0488) Data: 0.0397 (0.0376) Loss: 0.2737 (0.5755)
[2022/12/29 00:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:14] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:14] |    VALID(30)      0.5727      0.8134      0.8733      0.8134      0.8134      0.8134      0.9534
[2022/12/29 00:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:14] | ####################################################################################################
[2022/12/29 00:14] | TRAIN(031): [ 50/879] Batch: 0.1256 (0.1680) Data: 0.0118 (0.0467) Loss: 0.8470 (0.5825)
[2022/12/29 00:14] | TRAIN(031): [100/879] Batch: 0.1246 (0.1506) Data: 0.0132 (0.0292) Loss: 0.6395 (0.5709)
[2022/12/29 00:14] | TRAIN(031): [150/879] Batch: 0.1371 (0.1456) Data: 0.0131 (0.0235) Loss: 0.3471 (0.5661)
[2022/12/29 00:14] | TRAIN(031): [200/879] Batch: 0.1425 (0.1427) Data: 0.0094 (0.0205) Loss: 0.6174 (0.5720)
[2022/12/29 00:14] | TRAIN(031): [250/879] Batch: 0.1278 (0.1406) Data: 0.0120 (0.0186) Loss: 0.7024 (0.5701)
[2022/12/29 00:14] | TRAIN(031): [300/879] Batch: 0.1463 (0.1392) Data: 0.0105 (0.0174) Loss: 0.4991 (0.5679)
[2022/12/29 00:15] | TRAIN(031): [350/879] Batch: 0.1427 (0.1385) Data: 0.0124 (0.0166) Loss: 0.3458 (0.5672)
[2022/12/29 00:15] | TRAIN(031): [400/879] Batch: 0.1447 (0.1385) Data: 0.0115 (0.0160) Loss: 0.7180 (0.5692)
[2022/12/29 00:15] | TRAIN(031): [450/879] Batch: 0.1396 (0.1379) Data: 0.0124 (0.0156) Loss: 0.6726 (0.5679)
[2022/12/29 00:15] | TRAIN(031): [500/879] Batch: 0.1415 (0.1375) Data: 0.0095 (0.0152) Loss: 0.7071 (0.5657)
[2022/12/29 00:15] | TRAIN(031): [550/879] Batch: 0.0987 (0.1366) Data: 0.0108 (0.0148) Loss: 0.6830 (0.5673)
[2022/12/29 00:15] | TRAIN(031): [600/879] Batch: 0.1191 (0.1360) Data: 0.0108 (0.0146) Loss: 0.5532 (0.5685)
[2022/12/29 00:15] | TRAIN(031): [650/879] Batch: 0.1460 (0.1358) Data: 0.0105 (0.0143) Loss: 0.6289 (0.5689)
[2022/12/29 00:15] | TRAIN(031): [700/879] Batch: 0.1343 (0.1356) Data: 0.0132 (0.0141) Loss: 0.3401 (0.5682)
[2022/12/29 00:15] | TRAIN(031): [750/879] Batch: 0.1400 (0.1354) Data: 0.0129 (0.0139) Loss: 0.7544 (0.5686)
[2022/12/29 00:16] | TRAIN(031): [800/879] Batch: 0.1382 (0.1353) Data: 0.0095 (0.0137) Loss: 0.6427 (0.5692)
[2022/12/29 00:16] | TRAIN(031): [850/879] Batch: 0.1038 (0.1351) Data: 0.0135 (0.0136) Loss: 0.6669 (0.5712)
[2022/12/29 00:16] | ------------------------------------------------------------
[2022/12/29 00:16] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:16] | ------------------------------------------------------------
[2022/12/29 00:16] |    TRAIN(31)     0:01:58     0:00:11     0:01:46      0.5717
[2022/12/29 00:16] | ------------------------------------------------------------
[2022/12/29 00:16] | VALID(031): [ 50/220] Batch: 0.0479 (0.0732) Data: 0.0410 (0.0627) Loss: 0.5439 (0.5498)
[2022/12/29 00:16] | VALID(031): [100/220] Batch: 0.0439 (0.0591) Data: 0.0390 (0.0482) Loss: 0.8094 (0.5649)
[2022/12/29 00:16] | VALID(031): [150/220] Batch: 0.0456 (0.0544) Data: 0.0369 (0.0434) Loss: 0.5466 (0.5679)
[2022/12/29 00:16] | VALID(031): [200/220] Batch: 0.0417 (0.0520) Data: 0.0279 (0.0409) Loss: 0.2858 (0.5686)
[2022/12/29 00:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:16] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:16] |    VALID(31)      0.5642      0.8143      0.8738      0.8143      0.8143      0.8143      0.9536
[2022/12/29 00:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:16] | ####################################################################################################
[2022/12/29 00:16] | TRAIN(032): [ 50/879] Batch: 0.1470 (0.1718) Data: 0.0116 (0.0572) Loss: 0.4714 (0.5586)
[2022/12/29 00:16] | TRAIN(032): [100/879] Batch: 0.1248 (0.1514) Data: 0.0116 (0.0347) Loss: 0.5034 (0.5636)
[2022/12/29 00:16] | TRAIN(032): [150/879] Batch: 0.1373 (0.1452) Data: 0.0108 (0.0273) Loss: 0.7559 (0.5700)
[2022/12/29 00:16] | TRAIN(032): [200/879] Batch: 0.1321 (0.1418) Data: 0.0117 (0.0235) Loss: 0.7226 (0.5578)
[2022/12/29 00:16] | TRAIN(032): [250/879] Batch: 0.1366 (0.1398) Data: 0.0097 (0.0212) Loss: 0.6814 (0.5541)
[2022/12/29 00:17] | TRAIN(032): [300/879] Batch: 0.1339 (0.1386) Data: 0.0117 (0.0197) Loss: 0.5963 (0.5594)
[2022/12/29 00:17] | TRAIN(032): [350/879] Batch: 0.1275 (0.1376) Data: 0.0143 (0.0186) Loss: 0.4731 (0.5587)
[2022/12/29 00:17] | TRAIN(032): [400/879] Batch: 0.1227 (0.1371) Data: 0.0158 (0.0178) Loss: 0.3708 (0.5625)
[2022/12/29 00:17] | TRAIN(032): [450/879] Batch: 0.1227 (0.1365) Data: 0.0104 (0.0171) Loss: 0.6014 (0.5606)
[2022/12/29 00:17] | TRAIN(032): [500/879] Batch: 0.1402 (0.1360) Data: 0.0109 (0.0165) Loss: 0.7223 (0.5610)
[2022/12/29 00:17] | TRAIN(032): [550/879] Batch: 0.1371 (0.1355) Data: 0.0129 (0.0160) Loss: 0.6351 (0.5617)
[2022/12/29 00:17] | TRAIN(032): [600/879] Batch: 0.1183 (0.1352) Data: 0.0112 (0.0157) Loss: 0.6925 (0.5640)
[2022/12/29 00:17] | TRAIN(032): [650/879] Batch: 0.1447 (0.1351) Data: 0.0120 (0.0154) Loss: 0.5846 (0.5651)
[2022/12/29 00:17] | TRAIN(032): [700/879] Batch: 0.1533 (0.1348) Data: 0.0122 (0.0151) Loss: 0.4467 (0.5664)
[2022/12/29 00:18] | TRAIN(032): [750/879] Batch: 0.1344 (0.1348) Data: 0.0102 (0.0149) Loss: 0.6083 (0.5673)
[2022/12/29 00:18] | TRAIN(032): [800/879] Batch: 0.1246 (0.1346) Data: 0.0121 (0.0147) Loss: 0.4871 (0.5688)
[2022/12/29 00:18] | TRAIN(032): [850/879] Batch: 0.1286 (0.1345) Data: 0.0104 (0.0145) Loss: 0.5951 (0.5699)
[2022/12/29 00:18] | ------------------------------------------------------------
[2022/12/29 00:18] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:18] | ------------------------------------------------------------
[2022/12/29 00:18] |    TRAIN(32)     0:01:58     0:00:12     0:01:45      0.5688
[2022/12/29 00:18] | ------------------------------------------------------------
[2022/12/29 00:18] | VALID(032): [ 50/220] Batch: 0.0299 (0.0690) Data: 0.0412 (0.0574) Loss: 0.5105 (0.5341)
[2022/12/29 00:18] | VALID(032): [100/220] Batch: 0.0498 (0.0559) Data: 0.0363 (0.0444) Loss: 0.9654 (0.5559)
[2022/12/29 00:18] | VALID(032): [150/220] Batch: 0.0474 (0.0517) Data: 0.0361 (0.0406) Loss: 0.3904 (0.5525)
[2022/12/29 00:18] | VALID(032): [200/220] Batch: 0.0540 (0.0495) Data: 0.0253 (0.0381) Loss: 0.2896 (0.5587)
[2022/12/29 00:18] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:18] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:18] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:18] |    VALID(32)      0.5556      0.8180      0.8776      0.8180      0.8180      0.8180      0.9545
[2022/12/29 00:18] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:18] | ####################################################################################################
[2022/12/29 00:18] | TRAIN(033): [ 50/879] Batch: 0.1443 (0.1713) Data: 0.0113 (0.0475) Loss: 0.6125 (0.5510)
[2022/12/29 00:18] | TRAIN(033): [100/879] Batch: 0.1429 (0.1529) Data: 0.0078 (0.0297) Loss: 0.5979 (0.5616)
[2022/12/29 00:18] | TRAIN(033): [150/879] Batch: 0.1375 (0.1461) Data: 0.0101 (0.0236) Loss: 0.6507 (0.5638)
[2022/12/29 00:19] | TRAIN(033): [200/879] Batch: 0.1451 (0.1426) Data: 0.0190 (0.0208) Loss: 0.6503 (0.5643)
[2022/12/29 00:19] | TRAIN(033): [250/879] Batch: 0.1341 (0.1413) Data: 0.0109 (0.0191) Loss: 0.9117 (0.5701)
[2022/12/29 00:19] | TRAIN(033): [300/879] Batch: 0.0899 (0.1367) Data: 0.0114 (0.0179) Loss: 0.4601 (0.5711)
[2022/12/29 00:19] | TRAIN(033): [350/879] Batch: 0.1447 (0.1367) Data: 0.0132 (0.0171) Loss: 0.8658 (0.5703)
[2022/12/29 00:19] | TRAIN(033): [400/879] Batch: 0.0768 (0.1347) Data: 0.0115 (0.0165) Loss: 0.5592 (0.5695)
[2022/12/29 00:19] | TRAIN(033): [450/879] Batch: 0.1227 (0.1327) Data: 0.0123 (0.0160) Loss: 0.6367 (0.5618)
[2022/12/29 00:19] | TRAIN(033): [500/879] Batch: 0.1237 (0.1328) Data: 0.0131 (0.0156) Loss: 0.5578 (0.5703)
[2022/12/29 00:19] | TRAIN(033): [550/879] Batch: 0.1429 (0.1329) Data: 0.0102 (0.0153) Loss: 0.4911 (0.5685)
[2022/12/29 00:19] | TRAIN(033): [600/879] Batch: 0.1287 (0.1327) Data: 0.0156 (0.0150) Loss: 0.2180 (0.5682)
[2022/12/29 00:19] | TRAIN(033): [650/879] Batch: 0.1402 (0.1327) Data: 0.0121 (0.0148) Loss: 0.2795 (0.5676)
[2022/12/29 00:20] | TRAIN(033): [700/879] Batch: 0.1229 (0.1327) Data: 0.0115 (0.0145) Loss: 0.5227 (0.5663)
[2022/12/29 00:20] | TRAIN(033): [750/879] Batch: 0.1322 (0.1327) Data: 0.0121 (0.0143) Loss: 0.6855 (0.5660)
[2022/12/29 00:20] | TRAIN(033): [800/879] Batch: 0.1286 (0.1325) Data: 0.0110 (0.0142) Loss: 0.5618 (0.5659)
[2022/12/29 00:20] | TRAIN(033): [850/879] Batch: 0.1260 (0.1324) Data: 0.0138 (0.0140) Loss: 0.5569 (0.5670)
[2022/12/29 00:20] | ------------------------------------------------------------
[2022/12/29 00:20] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:20] | ------------------------------------------------------------
[2022/12/29 00:20] |    TRAIN(33)     0:01:56     0:00:12     0:01:44      0.5685
[2022/12/29 00:20] | ------------------------------------------------------------
[2022/12/29 00:20] | VALID(033): [ 50/220] Batch: 0.0319 (0.0686) Data: 0.0357 (0.0579) Loss: 0.5920 (0.5584)
[2022/12/29 00:20] | VALID(033): [100/220] Batch: 0.0320 (0.0552) Data: 0.0368 (0.0443) Loss: 0.9225 (0.5776)
[2022/12/29 00:20] | VALID(033): [150/220] Batch: 0.0462 (0.0505) Data: 0.0385 (0.0397) Loss: 0.5632 (0.5805)
[2022/12/29 00:20] | VALID(033): [200/220] Batch: 0.0501 (0.0487) Data: 0.0359 (0.0380) Loss: 0.3079 (0.5802)
[2022/12/29 00:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:20] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:20] |    VALID(33)      0.5756      0.8117      0.8727      0.8117      0.8117      0.8117      0.9529
[2022/12/29 00:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:20] | ####################################################################################################
[2022/12/29 00:20] | TRAIN(034): [ 50/879] Batch: 0.1453 (0.1719) Data: 0.0102 (0.0483) Loss: 0.6462 (0.5821)
[2022/12/29 00:20] | TRAIN(034): [100/879] Batch: 0.1014 (0.1520) Data: 0.0119 (0.0303) Loss: 0.5194 (0.5570)
[2022/12/29 00:21] | TRAIN(034): [150/879] Batch: 0.1428 (0.1455) Data: 0.0100 (0.0241) Loss: 0.4921 (0.5457)
[2022/12/29 00:21] | TRAIN(034): [200/879] Batch: 0.1313 (0.1417) Data: 0.0118 (0.0209) Loss: 0.8718 (0.5472)
[2022/12/29 00:21] | TRAIN(034): [250/879] Batch: 0.1341 (0.1398) Data: 0.0100 (0.0190) Loss: 0.3204 (0.5508)
[2022/12/29 00:21] | TRAIN(034): [300/879] Batch: 0.1199 (0.1382) Data: 0.0113 (0.0178) Loss: 0.4145 (0.5507)
[2022/12/29 00:21] | TRAIN(034): [350/879] Batch: 0.1289 (0.1373) Data: 0.0113 (0.0168) Loss: 0.3443 (0.5481)
[2022/12/29 00:21] | TRAIN(034): [400/879] Batch: 0.1261 (0.1367) Data: 0.0108 (0.0161) Loss: 0.5589 (0.5483)
[2022/12/29 00:21] | TRAIN(034): [450/879] Batch: 0.1225 (0.1363) Data: 0.0136 (0.0156) Loss: 0.6729 (0.5515)
[2022/12/29 00:21] | TRAIN(034): [500/879] Batch: 0.1275 (0.1359) Data: 0.0105 (0.0152) Loss: 0.5901 (0.5532)
[2022/12/29 00:21] | TRAIN(034): [550/879] Batch: 0.1120 (0.1355) Data: 0.0112 (0.0148) Loss: 0.4408 (0.5552)
[2022/12/29 00:22] | TRAIN(034): [600/879] Batch: 0.1404 (0.1350) Data: 0.0138 (0.0144) Loss: 0.6850 (0.5575)
[2022/12/29 00:22] | TRAIN(034): [650/879] Batch: 0.1257 (0.1348) Data: 0.0125 (0.0143) Loss: 0.4395 (0.5582)
[2022/12/29 00:22] | TRAIN(034): [700/879] Batch: 0.0859 (0.1342) Data: 0.0090 (0.0141) Loss: 0.3526 (0.5569)
[2022/12/29 00:22] | TRAIN(034): [750/879] Batch: 0.1273 (0.1334) Data: 0.0120 (0.0139) Loss: 0.6615 (0.5574)
[2022/12/29 00:22] | TRAIN(034): [800/879] Batch: 0.1291 (0.1335) Data: 0.0117 (0.0138) Loss: 0.6824 (0.5589)
[2022/12/29 00:22] | TRAIN(034): [850/879] Batch: 0.1404 (0.1320) Data: 0.0099 (0.0137) Loss: 0.4000 (0.5597)
[2022/12/29 00:22] | ------------------------------------------------------------
[2022/12/29 00:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:22] | ------------------------------------------------------------
[2022/12/29 00:22] |    TRAIN(34)     0:01:55     0:00:11     0:01:43      0.5593
[2022/12/29 00:22] | ------------------------------------------------------------
[2022/12/29 00:22] | VALID(034): [ 50/220] Batch: 0.0517 (0.0691) Data: 0.0170 (0.0566) Loss: 0.4687 (0.5538)
[2022/12/29 00:22] | VALID(034): [100/220] Batch: 0.0511 (0.0553) Data: 0.0340 (0.0436) Loss: 0.9344 (0.5933)
[2022/12/29 00:22] | VALID(034): [150/220] Batch: 0.0355 (0.0512) Data: 0.0262 (0.0393) Loss: 0.5489 (0.5932)
[2022/12/29 00:22] | VALID(034): [200/220] Batch: 0.0530 (0.0485) Data: 0.0353 (0.0364) Loss: 0.2014 (0.5933)
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] |    VALID(34)      0.5882      0.8061      0.8674      0.8061      0.8061      0.8061      0.9515
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] | ####################################################################################################
[2022/12/29 00:22] | TRAIN(035): [ 50/879] Batch: 0.1414 (0.1701) Data: 0.0081 (0.0492) Loss: 0.9931 (0.5897)
[2022/12/29 00:23] | TRAIN(035): [100/879] Batch: 0.1282 (0.1521) Data: 0.0144 (0.0302) Loss: 0.4616 (0.5676)
[2022/12/29 00:23] | TRAIN(035): [150/879] Batch: 0.1172 (0.1456) Data: 0.0083 (0.0240) Loss: 0.4890 (0.5767)
[2022/12/29 00:23] | TRAIN(035): [200/879] Batch: 0.1309 (0.1422) Data: 0.0179 (0.0207) Loss: 0.6098 (0.5631)
[2022/12/29 00:23] | TRAIN(035): [250/879] Batch: 0.1222 (0.1398) Data: 0.0107 (0.0184) Loss: 0.4684 (0.5568)
[2022/12/29 00:23] | TRAIN(035): [300/879] Batch: 0.1166 (0.1381) Data: 0.0082 (0.0168) Loss: 0.6606 (0.5587)
[2022/12/29 00:23] | TRAIN(035): [350/879] Batch: 0.1442 (0.1368) Data: 0.0078 (0.0156) Loss: 0.7124 (0.5588)
[2022/12/29 00:23] | TRAIN(035): [400/879] Batch: 0.1366 (0.1359) Data: 0.0100 (0.0148) Loss: 0.5670 (0.5594)
[2022/12/29 00:23] | TRAIN(035): [450/879] Batch: 0.1408 (0.1355) Data: 0.0074 (0.0141) Loss: 0.3072 (0.5590)
[2022/12/29 00:23] | TRAIN(035): [500/879] Batch: 0.1205 (0.1349) Data: 0.0082 (0.0136) Loss: 0.7731 (0.5594)
[2022/12/29 00:24] | TRAIN(035): [550/879] Batch: 0.0983 (0.1344) Data: 0.0092 (0.0131) Loss: 0.4704 (0.5615)
[2022/12/29 00:24] | TRAIN(035): [600/879] Batch: 0.1297 (0.1339) Data: 0.0070 (0.0128) Loss: 0.6447 (0.5592)
[2022/12/29 00:24] | TRAIN(035): [650/879] Batch: 0.1344 (0.1336) Data: 0.0107 (0.0124) Loss: 0.5896 (0.5604)
[2022/12/29 00:24] | TRAIN(035): [700/879] Batch: 0.1416 (0.1335) Data: 0.0070 (0.0122) Loss: 0.6186 (0.5598)
[2022/12/29 00:24] | TRAIN(035): [750/879] Batch: 0.1273 (0.1333) Data: 0.0085 (0.0120) Loss: 0.6701 (0.5596)
[2022/12/29 00:24] | TRAIN(035): [800/879] Batch: 0.1224 (0.1330) Data: 0.0075 (0.0118) Loss: 0.6678 (0.5586)
[2022/12/29 00:24] | TRAIN(035): [850/879] Batch: 0.1234 (0.1326) Data: 0.0079 (0.0116) Loss: 0.5982 (0.5577)
[2022/12/29 00:24] | ------------------------------------------------------------
[2022/12/29 00:24] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:24] | ------------------------------------------------------------
[2022/12/29 00:24] |    TRAIN(35)     0:01:56     0:00:10     0:01:46      0.5569
[2022/12/29 00:24] | ------------------------------------------------------------
[2022/12/29 00:24] | VALID(035): [ 50/220] Batch: 0.0318 (0.0710) Data: 0.0181 (0.0599) Loss: 0.5223 (0.5917)
[2022/12/29 00:24] | VALID(035): [100/220] Batch: 0.0478 (0.0560) Data: 0.0271 (0.0430) Loss: 0.9883 (0.6058)
[2022/12/29 00:24] | VALID(035): [150/220] Batch: 0.0361 (0.0510) Data: 0.0122 (0.0365) Loss: 0.4280 (0.6042)
[2022/12/29 00:24] | VALID(035): [200/220] Batch: 0.0484 (0.0486) Data: 0.0177 (0.0334) Loss: 0.3367 (0.6042)
[2022/12/29 00:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:24] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:24] |    VALID(35)      0.6033      0.8059      0.8732      0.8059      0.8059      0.8059      0.9515
[2022/12/29 00:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:24] | ####################################################################################################
[2022/12/29 00:25] | TRAIN(036): [ 50/879] Batch: 0.1217 (0.1681) Data: 0.0103 (0.0484) Loss: 0.9061 (0.5561)
[2022/12/29 00:25] | TRAIN(036): [100/879] Batch: 0.1219 (0.1487) Data: 0.0082 (0.0285) Loss: 0.3313 (0.5588)
[2022/12/29 00:25] | TRAIN(036): [150/879] Batch: 0.1327 (0.1423) Data: 0.0087 (0.0219) Loss: 0.8143 (0.5733)
[2022/12/29 00:25] | TRAIN(036): [200/879] Batch: 0.1452 (0.1354) Data: 0.0099 (0.0188) Loss: 0.5219 (0.5725)
[2022/12/29 00:25] | TRAIN(036): [250/879] Batch: 0.1410 (0.1363) Data: 0.0084 (0.0171) Loss: 0.5489 (0.5709)
[2022/12/29 00:25] | TRAIN(036): [300/879] Batch: 0.1391 (0.1322) Data: 0.0078 (0.0158) Loss: 0.7823 (0.5663)
[2022/12/29 00:25] | TRAIN(036): [350/879] Batch: 0.1397 (0.1322) Data: 0.0083 (0.0148) Loss: 0.6568 (0.5626)
[2022/12/29 00:25] | TRAIN(036): [400/879] Batch: 0.1496 (0.1325) Data: 0.0106 (0.0142) Loss: 0.3622 (0.5613)
[2022/12/29 00:25] | TRAIN(036): [450/879] Batch: 0.1185 (0.1322) Data: 0.0082 (0.0135) Loss: 0.5183 (0.5575)
[2022/12/29 00:25] | TRAIN(036): [500/879] Batch: 0.1448 (0.1320) Data: 0.0098 (0.0130) Loss: 0.8847 (0.5543)
[2022/12/29 00:26] | TRAIN(036): [550/879] Batch: 0.1345 (0.1320) Data: 0.0117 (0.0126) Loss: 0.6463 (0.5560)
[2022/12/29 00:26] | TRAIN(036): [600/879] Batch: 0.0970 (0.1323) Data: 0.0108 (0.0125) Loss: 0.7875 (0.5570)
[2022/12/29 00:26] | TRAIN(036): [650/879] Batch: 0.1258 (0.1325) Data: 0.0104 (0.0123) Loss: 0.5444 (0.5570)
[2022/12/29 00:26] | TRAIN(036): [700/879] Batch: 0.1395 (0.1326) Data: 0.0097 (0.0121) Loss: 0.7711 (0.5562)
[2022/12/29 00:26] | TRAIN(036): [750/879] Batch: 0.1380 (0.1330) Data: 0.0091 (0.0120) Loss: 0.4713 (0.5554)
[2022/12/29 00:26] | TRAIN(036): [800/879] Batch: 0.1387 (0.1332) Data: 0.0101 (0.0118) Loss: 0.6473 (0.5549)
[2022/12/29 00:26] | TRAIN(036): [850/879] Batch: 0.1368 (0.1332) Data: 0.0080 (0.0117) Loss: 0.6071 (0.5542)
[2022/12/29 00:26] | ------------------------------------------------------------
[2022/12/29 00:26] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:26] | ------------------------------------------------------------
[2022/12/29 00:26] |    TRAIN(36)     0:01:56     0:00:10     0:01:46      0.5539
[2022/12/29 00:26] | ------------------------------------------------------------
[2022/12/29 00:26] | VALID(036): [ 50/220] Batch: 0.0478 (0.0740) Data: 0.0345 (0.0633) Loss: 0.5638 (0.5397)
[2022/12/29 00:26] | VALID(036): [100/220] Batch: 0.0444 (0.0577) Data: 0.0385 (0.0474) Loss: 0.8540 (0.5721)
[2022/12/29 00:26] | VALID(036): [150/220] Batch: 0.0483 (0.0524) Data: 0.0337 (0.0420) Loss: 0.4302 (0.5742)
[2022/12/29 00:27] | VALID(036): [200/220] Batch: 0.0436 (0.0497) Data: 0.0392 (0.0393) Loss: 0.2880 (0.5767)
[2022/12/29 00:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:27] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:27] |    VALID(36)      0.5759      0.8175      0.8753      0.8175      0.8175      0.8175      0.9544
[2022/12/29 00:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:27] | ####################################################################################################
[2022/12/29 00:27] | TRAIN(037): [ 50/879] Batch: 0.1269 (0.1755) Data: 0.0096 (0.0473) Loss: 0.7226 (0.5349)
[2022/12/29 00:27] | TRAIN(037): [100/879] Batch: 0.1388 (0.1530) Data: 0.0073 (0.0280) Loss: 0.3621 (0.5388)
[2022/12/29 00:27] | TRAIN(037): [150/879] Batch: 0.1207 (0.1451) Data: 0.0082 (0.0215) Loss: 0.3208 (0.5542)
[2022/12/29 00:27] | TRAIN(037): [200/879] Batch: 0.1377 (0.1416) Data: 0.0075 (0.0183) Loss: 0.4888 (0.5601)
[2022/12/29 00:27] | TRAIN(037): [250/879] Batch: 0.1399 (0.1392) Data: 0.0097 (0.0164) Loss: 0.5545 (0.5560)
[2022/12/29 00:27] | TRAIN(037): [300/879] Batch: 0.1428 (0.1376) Data: 0.0076 (0.0150) Loss: 0.5924 (0.5576)
[2022/12/29 00:27] | TRAIN(037): [350/879] Batch: 0.1218 (0.1364) Data: 0.0098 (0.0141) Loss: 1.0673 (0.5645)
[2022/12/29 00:27] | TRAIN(037): [400/879] Batch: 0.1229 (0.1355) Data: 0.0080 (0.0134) Loss: 0.5230 (0.5601)
[2022/12/29 00:28] | TRAIN(037): [450/879] Batch: 0.1226 (0.1351) Data: 0.0084 (0.0129) Loss: 0.6916 (0.5563)
[2022/12/29 00:28] | TRAIN(037): [500/879] Batch: 0.1388 (0.1350) Data: 0.0081 (0.0126) Loss: 0.4146 (0.5541)
[2022/12/29 00:28] | TRAIN(037): [550/879] Batch: 0.1312 (0.1349) Data: 0.0073 (0.0123) Loss: 0.4722 (0.5519)
[2022/12/29 00:28] | TRAIN(037): [600/879] Batch: 0.1196 (0.1333) Data: 0.0105 (0.0120) Loss: 0.5641 (0.5525)
[2022/12/29 00:28] | TRAIN(037): [650/879] Batch: 0.1425 (0.1332) Data: 0.0090 (0.0117) Loss: 0.4948 (0.5515)
[2022/12/29 00:28] | TRAIN(037): [700/879] Batch: 0.1285 (0.1313) Data: 0.0104 (0.0115) Loss: 0.5495 (0.5501)
[2022/12/29 00:28] | TRAIN(037): [750/879] Batch: 0.1437 (0.1314) Data: 0.0083 (0.0114) Loss: 0.4680 (0.5495)
[2022/12/29 00:28] | TRAIN(037): [800/879] Batch: 0.1407 (0.1315) Data: 0.0092 (0.0112) Loss: 0.5746 (0.5500)
[2022/12/29 00:28] | TRAIN(037): [850/879] Batch: 0.1227 (0.1314) Data: 0.0086 (0.0111) Loss: 0.5100 (0.5492)
[2022/12/29 00:28] | ------------------------------------------------------------
[2022/12/29 00:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:28] | ------------------------------------------------------------
[2022/12/29 00:28] |    TRAIN(37)     0:01:55     0:00:09     0:01:45      0.5486
[2022/12/29 00:28] | ------------------------------------------------------------
[2022/12/29 00:29] | VALID(037): [ 50/220] Batch: 0.0352 (0.0708) Data: 0.0189 (0.0584) Loss: 0.4945 (0.5302)
[2022/12/29 00:29] | VALID(037): [100/220] Batch: 0.0375 (0.0564) Data: 0.0187 (0.0448) Loss: 0.9973 (0.5684)
[2022/12/29 00:29] | VALID(037): [150/220] Batch: 0.0427 (0.0515) Data: 0.0190 (0.0402) Loss: 0.4321 (0.5672)
[2022/12/29 00:29] | VALID(037): [200/220] Batch: 0.0481 (0.0491) Data: 0.0282 (0.0379) Loss: 0.2604 (0.5708)
[2022/12/29 00:29] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:29] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:29] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:29] |    VALID(37)      0.5680      0.8164      0.8782      0.8164      0.8164      0.8164      0.9541
[2022/12/29 00:29] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:29] | ####################################################################################################
[2022/12/29 00:29] | TRAIN(038): [ 50/879] Batch: 0.1256 (0.1718) Data: 0.0083 (0.0471) Loss: 0.4520 (0.5488)
[2022/12/29 00:29] | TRAIN(038): [100/879] Batch: 0.1206 (0.1506) Data: 0.0081 (0.0279) Loss: 0.4082 (0.5556)
[2022/12/29 00:29] | TRAIN(038): [150/879] Batch: 0.1206 (0.1437) Data: 0.0088 (0.0215) Loss: 0.7921 (0.5537)
[2022/12/29 00:29] | TRAIN(038): [200/879] Batch: 0.1215 (0.1400) Data: 0.0079 (0.0182) Loss: 0.6953 (0.5485)
[2022/12/29 00:29] | TRAIN(038): [250/879] Batch: 0.1426 (0.1379) Data: 0.0080 (0.0162) Loss: 0.6288 (0.5488)
[2022/12/29 00:29] | TRAIN(038): [300/879] Batch: 0.1364 (0.1380) Data: 0.0106 (0.0152) Loss: 0.3702 (0.5457)
[2022/12/29 00:29] | TRAIN(038): [350/879] Batch: 0.1164 (0.1373) Data: 0.0085 (0.0144) Loss: 0.7589 (0.5433)
[2022/12/29 00:30] | TRAIN(038): [400/879] Batch: 0.1214 (0.1362) Data: 0.0082 (0.0136) Loss: 0.5019 (0.5427)
[2022/12/29 00:30] | TRAIN(038): [450/879] Batch: 0.1216 (0.1352) Data: 0.0081 (0.0130) Loss: 0.4765 (0.5452)
[2022/12/29 00:30] | TRAIN(038): [500/879] Batch: 0.1297 (0.1347) Data: 0.0083 (0.0125) Loss: 0.4762 (0.5463)
[2022/12/29 00:30] | TRAIN(038): [550/879] Batch: 0.1432 (0.1342) Data: 0.0076 (0.0122) Loss: 1.0789 (0.5485)
[2022/12/29 00:30] | TRAIN(038): [600/879] Batch: 0.1409 (0.1341) Data: 0.0075 (0.0119) Loss: 0.3442 (0.5484)
[2022/12/29 00:30] | TRAIN(038): [650/879] Batch: 0.1233 (0.1339) Data: 0.0105 (0.0117) Loss: 0.7697 (0.5461)
[2022/12/29 00:30] | TRAIN(038): [700/879] Batch: 0.1211 (0.1335) Data: 0.0082 (0.0115) Loss: 0.4717 (0.5417)
[2022/12/29 00:30] | TRAIN(038): [750/879] Batch: 0.1443 (0.1334) Data: 0.0070 (0.0113) Loss: 0.4374 (0.5427)
[2022/12/29 00:30] | TRAIN(038): [800/879] Batch: 0.1221 (0.1331) Data: 0.0085 (0.0111) Loss: 0.3191 (0.5416)
[2022/12/29 00:31] | TRAIN(038): [850/879] Batch: 0.1229 (0.1329) Data: 0.0090 (0.0109) Loss: 0.6644 (0.5427)
[2022/12/29 00:31] | ------------------------------------------------------------
[2022/12/29 00:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:31] | ------------------------------------------------------------
[2022/12/29 00:31] |    TRAIN(38)     0:01:56     0:00:09     0:01:47      0.5429
[2022/12/29 00:31] | ------------------------------------------------------------
[2022/12/29 00:31] | VALID(038): [ 50/220] Batch: 0.0310 (0.0712) Data: 0.0390 (0.0582) Loss: 0.5786 (0.5498)
[2022/12/29 00:31] | VALID(038): [100/220] Batch: 0.0441 (0.0559) Data: 0.0391 (0.0442) Loss: 0.9190 (0.5746)
[2022/12/29 00:31] | VALID(038): [150/220] Batch: 0.0438 (0.0512) Data: 0.0329 (0.0400) Loss: 0.4350 (0.5783)
[2022/12/29 00:31] | VALID(038): [200/220] Batch: 0.0440 (0.0488) Data: 0.0319 (0.0378) Loss: 0.1868 (0.5779)
[2022/12/29 00:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:31] |    VALID(38)      0.5756      0.8170      0.8724      0.8170      0.8170      0.8170      0.9542
[2022/12/29 00:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:31] | ####################################################################################################
[2022/12/29 00:31] | TRAIN(039): [ 50/879] Batch: 0.1291 (0.1536) Data: 0.0106 (0.0486) Loss: 0.5908 (0.5801)
[2022/12/29 00:31] | TRAIN(039): [100/879] Batch: 0.1321 (0.1439) Data: 0.0093 (0.0297) Loss: 0.7898 (0.5555)
[2022/12/29 00:31] | TRAIN(039): [150/879] Batch: 0.1248 (0.1319) Data: 0.0106 (0.0235) Loss: 0.1893 (0.5528)
[2022/12/29 00:31] | TRAIN(039): [200/879] Batch: 0.1411 (0.1320) Data: 0.0075 (0.0200) Loss: 0.4114 (0.5440)
[2022/12/29 00:31] | TRAIN(039): [250/879] Batch: 0.1453 (0.1321) Data: 0.0108 (0.0181) Loss: 0.4706 (0.5417)
[2022/12/29 00:31] | TRAIN(039): [300/879] Batch: 0.1181 (0.1324) Data: 0.0082 (0.0167) Loss: 0.4848 (0.5436)
[2022/12/29 00:32] | TRAIN(039): [350/879] Batch: 0.1193 (0.1319) Data: 0.0081 (0.0156) Loss: 0.4972 (0.5415)
[2022/12/29 00:32] | TRAIN(039): [400/879] Batch: 0.1447 (0.1322) Data: 0.0187 (0.0148) Loss: 0.4913 (0.5403)
[2022/12/29 00:32] | TRAIN(039): [450/879] Batch: 0.1207 (0.1321) Data: 0.0105 (0.0142) Loss: 0.3720 (0.5377)
[2022/12/29 00:32] | TRAIN(039): [500/879] Batch: 0.1254 (0.1320) Data: 0.0084 (0.0137) Loss: 0.5210 (0.5389)
[2022/12/29 00:32] | TRAIN(039): [550/879] Batch: 0.1285 (0.1320) Data: 0.0085 (0.0133) Loss: 0.5663 (0.5412)
[2022/12/29 00:32] | TRAIN(039): [600/879] Batch: 0.1433 (0.1319) Data: 0.0078 (0.0129) Loss: 0.5575 (0.5403)
[2022/12/29 00:32] | TRAIN(039): [650/879] Batch: 0.1427 (0.1317) Data: 0.0096 (0.0126) Loss: 0.5488 (0.5395)
[2022/12/29 00:32] | TRAIN(039): [700/879] Batch: 0.1243 (0.1316) Data: 0.0086 (0.0123) Loss: 0.5535 (0.5385)
[2022/12/29 00:32] | TRAIN(039): [750/879] Batch: 0.1469 (0.1315) Data: 0.0078 (0.0121) Loss: 0.8452 (0.5411)
[2022/12/29 00:33] | TRAIN(039): [800/879] Batch: 0.1290 (0.1315) Data: 0.0089 (0.0119) Loss: 0.4738 (0.5415)
[2022/12/29 00:33] | TRAIN(039): [850/879] Batch: 0.1229 (0.1314) Data: 0.0100 (0.0117) Loss: 0.5509 (0.5408)
[2022/12/29 00:33] | ------------------------------------------------------------
[2022/12/29 00:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:33] | ------------------------------------------------------------
[2022/12/29 00:33] |    TRAIN(39)     0:01:55     0:00:10     0:01:45      0.5417
[2022/12/29 00:33] | ------------------------------------------------------------
[2022/12/29 00:33] | VALID(039): [ 50/220] Batch: 0.0453 (0.0707) Data: 0.0288 (0.0593) Loss: 0.4721 (0.5344)
[2022/12/29 00:33] | VALID(039): [100/220] Batch: 0.0407 (0.0563) Data: 0.0200 (0.0454) Loss: 0.7136 (0.5566)
[2022/12/29 00:33] | VALID(039): [150/220] Batch: 0.0442 (0.0506) Data: 0.0330 (0.0385) Loss: 0.4641 (0.5605)
[2022/12/29 00:33] | VALID(039): [200/220] Batch: 0.0312 (0.0481) Data: 0.0385 (0.0358) Loss: 0.3377 (0.5632)
[2022/12/29 00:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:33] |    VALID(39)      0.5612      0.8131      0.8745      0.8131      0.8131      0.8131      0.9533
[2022/12/29 00:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:33] | ####################################################################################################
[2022/12/29 00:33] | TRAIN(040): [ 50/879] Batch: 0.1336 (0.1709) Data: 0.0130 (0.0494) Loss: 0.4751 (0.5715)
[2022/12/29 00:33] | TRAIN(040): [100/879] Batch: 0.1238 (0.1525) Data: 0.0080 (0.0309) Loss: 0.5768 (0.5348)
[2022/12/29 00:33] | TRAIN(040): [150/879] Batch: 0.1413 (0.1448) Data: 0.0101 (0.0239) Loss: 0.5216 (0.5336)
[2022/12/29 00:33] | TRAIN(040): [200/879] Batch: 0.1222 (0.1418) Data: 0.0110 (0.0203) Loss: 0.4233 (0.5282)
[2022/12/29 00:33] | TRAIN(040): [250/879] Batch: 0.1513 (0.1402) Data: 0.0182 (0.0183) Loss: 0.7796 (0.5340)
[2022/12/29 00:34] | TRAIN(040): [300/879] Batch: 0.1366 (0.1390) Data: 0.0091 (0.0169) Loss: 0.3499 (0.5389)
[2022/12/29 00:34] | TRAIN(040): [350/879] Batch: 0.1331 (0.1381) Data: 0.0088 (0.0158) Loss: 0.3346 (0.5405)
[2022/12/29 00:34] | TRAIN(040): [400/879] Batch: 0.1437 (0.1377) Data: 0.0191 (0.0150) Loss: 0.4102 (0.5428)
[2022/12/29 00:34] | TRAIN(040): [450/879] Batch: 0.1059 (0.1347) Data: 0.0081 (0.0144) Loss: 0.5656 (0.5392)
[2022/12/29 00:34] | TRAIN(040): [500/879] Batch: 0.1315 (0.1347) Data: 0.0083 (0.0139) Loss: 0.6366 (0.5394)
[2022/12/29 00:34] | TRAIN(040): [550/879] Batch: 0.0926 (0.1336) Data: 0.0093 (0.0135) Loss: 0.7238 (0.5388)
[2022/12/29 00:34] | TRAIN(040): [600/879] Batch: 0.1249 (0.1323) Data: 0.0080 (0.0131) Loss: 0.3893 (0.5372)
[2022/12/29 00:34] | TRAIN(040): [650/879] Batch: 0.1272 (0.1321) Data: 0.0096 (0.0127) Loss: 0.5773 (0.5379)
[2022/12/29 00:34] | TRAIN(040): [700/879] Batch: 0.1440 (0.1319) Data: 0.0071 (0.0124) Loss: 0.5765 (0.5374)
[2022/12/29 00:35] | TRAIN(040): [750/879] Batch: 0.1466 (0.1322) Data: 0.0097 (0.0122) Loss: 0.4729 (0.5396)
[2022/12/29 00:35] | TRAIN(040): [800/879] Batch: 0.1468 (0.1322) Data: 0.0113 (0.0121) Loss: 0.6999 (0.5418)
[2022/12/29 00:35] | TRAIN(040): [850/879] Batch: 0.1419 (0.1323) Data: 0.0070 (0.0119) Loss: 0.4206 (0.5423)
[2022/12/29 00:35] | ------------------------------------------------------------
[2022/12/29 00:35] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:35] | ------------------------------------------------------------
[2022/12/29 00:35] |    TRAIN(40)     0:01:56     0:00:10     0:01:45      0.5419
[2022/12/29 00:35] | ------------------------------------------------------------
[2022/12/29 00:35] | VALID(040): [ 50/220] Batch: 0.0352 (0.0713) Data: 0.0211 (0.0586) Loss: 0.6047 (0.5454)
[2022/12/29 00:35] | VALID(040): [100/220] Batch: 0.0333 (0.0566) Data: 0.0164 (0.0447) Loss: 0.9802 (0.5726)
[2022/12/29 00:35] | VALID(040): [150/220] Batch: 0.0314 (0.0516) Data: 0.0325 (0.0401) Loss: 0.5289 (0.5699)
[2022/12/29 00:35] | VALID(040): [200/220] Batch: 0.0390 (0.0492) Data: 0.0340 (0.0378) Loss: 0.2829 (0.5690)
[2022/12/29 00:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:35] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:35] |    VALID(40)      0.5639      0.8147      0.8745      0.8147      0.8147      0.8147      0.9537
[2022/12/29 00:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:35] | ####################################################################################################
[2022/12/29 00:35] | TRAIN(041): [ 50/879] Batch: 0.1196 (0.1772) Data: 0.0080 (0.0504) Loss: 0.7551 (0.5478)
[2022/12/29 00:35] | TRAIN(041): [100/879] Batch: 0.1413 (0.1542) Data: 0.0074 (0.0298) Loss: 0.4867 (0.5519)
[2022/12/29 00:35] | TRAIN(041): [150/879] Batch: 0.1499 (0.1478) Data: 0.0096 (0.0232) Loss: 0.5696 (0.5544)
[2022/12/29 00:35] | TRAIN(041): [200/879] Batch: 0.1366 (0.1453) Data: 0.0097 (0.0200) Loss: 0.3257 (0.5500)
[2022/12/29 00:36] | TRAIN(041): [250/879] Batch: 0.1375 (0.1427) Data: 0.0074 (0.0178) Loss: 0.4894 (0.5449)
[2022/12/29 00:36] | TRAIN(041): [300/879] Batch: 0.1229 (0.1404) Data: 0.0077 (0.0162) Loss: 0.4649 (0.5410)
[2022/12/29 00:36] | TRAIN(041): [350/879] Batch: 0.1188 (0.1390) Data: 0.0082 (0.0151) Loss: 0.3154 (0.5412)
[2022/12/29 00:36] | TRAIN(041): [400/879] Batch: 0.1248 (0.1378) Data: 0.0085 (0.0142) Loss: 0.6407 (0.5389)
[2022/12/29 00:36] | TRAIN(041): [450/879] Batch: 0.1263 (0.1370) Data: 0.0090 (0.0136) Loss: 0.3865 (0.5393)
[2022/12/29 00:36] | TRAIN(041): [500/879] Batch: 0.1416 (0.1363) Data: 0.0081 (0.0131) Loss: 0.5594 (0.5399)
[2022/12/29 00:36] | TRAIN(041): [550/879] Batch: 0.1399 (0.1356) Data: 0.0070 (0.0126) Loss: 0.6430 (0.5401)
[2022/12/29 00:36] | TRAIN(041): [600/879] Batch: 0.1227 (0.1350) Data: 0.0090 (0.0123) Loss: 0.3813 (0.5423)
[2022/12/29 00:36] | TRAIN(041): [650/879] Batch: 0.1435 (0.1348) Data: 0.0106 (0.0120) Loss: 0.6193 (0.5396)
[2022/12/29 00:37] | TRAIN(041): [700/879] Batch: 0.1423 (0.1352) Data: 0.0097 (0.0119) Loss: 0.4362 (0.5395)
[2022/12/29 00:37] | TRAIN(041): [750/879] Batch: 0.1298 (0.1353) Data: 0.0114 (0.0118) Loss: 0.3721 (0.5377)
[2022/12/29 00:37] | TRAIN(041): [800/879] Batch: 0.1216 (0.1349) Data: 0.0101 (0.0116) Loss: 0.6707 (0.5383)
[2022/12/29 00:37] | TRAIN(041): [850/879] Batch: 0.0794 (0.1342) Data: 0.0084 (0.0114) Loss: 0.4825 (0.5364)
[2022/12/29 00:37] | ------------------------------------------------------------
[2022/12/29 00:37] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:37] | ------------------------------------------------------------
[2022/12/29 00:37] |    TRAIN(41)     0:01:57     0:00:09     0:01:47      0.5370
[2022/12/29 00:37] | ------------------------------------------------------------
[2022/12/29 00:37] | VALID(041): [ 50/220] Batch: 0.0465 (0.0730) Data: 0.0239 (0.0602) Loss: 0.6226 (0.5373)
[2022/12/29 00:37] | VALID(041): [100/220] Batch: 0.0386 (0.0578) Data: 0.0317 (0.0462) Loss: 0.8960 (0.5635)
[2022/12/29 00:37] | VALID(041): [150/220] Batch: 0.0412 (0.0530) Data: 0.0298 (0.0417) Loss: 0.5068 (0.5644)
[2022/12/29 00:37] | VALID(041): [200/220] Batch: 0.0218 (0.0488) Data: 0.0151 (0.0376) Loss: 0.1941 (0.5652)
[2022/12/29 00:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:37] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:37] |    VALID(41)      0.5610      0.8190      0.8759      0.8190      0.8190      0.8190      0.9547
[2022/12/29 00:37] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:37] | ####################################################################################################
[2022/12/29 00:37] | TRAIN(042): [ 50/879] Batch: 0.1365 (0.1725) Data: 0.0087 (0.0487) Loss: 0.4162 (0.5230)
[2022/12/29 00:37] | TRAIN(042): [100/879] Batch: 0.1214 (0.1531) Data: 0.0081 (0.0291) Loss: 0.4832 (0.5292)
[2022/12/29 00:37] | TRAIN(042): [150/879] Batch: 0.1236 (0.1461) Data: 0.0113 (0.0225) Loss: 0.5741 (0.5309)
[2022/12/29 00:38] | TRAIN(042): [200/879] Batch: 0.1450 (0.1421) Data: 0.0072 (0.0190) Loss: 0.5366 (0.5407)
[2022/12/29 00:38] | TRAIN(042): [250/879] Batch: 0.1412 (0.1393) Data: 0.0070 (0.0168) Loss: 0.4634 (0.5346)
[2022/12/29 00:38] | TRAIN(042): [300/879] Batch: 0.1248 (0.1377) Data: 0.0076 (0.0154) Loss: 0.2558 (0.5379)
[2022/12/29 00:38] | TRAIN(042): [350/879] Batch: 0.1227 (0.1367) Data: 0.0082 (0.0145) Loss: 0.2405 (0.5397)
[2022/12/29 00:38] | TRAIN(042): [400/879] Batch: 0.1110 (0.1359) Data: 0.0110 (0.0137) Loss: 0.5983 (0.5323)
[2022/12/29 00:38] | TRAIN(042): [450/879] Batch: 0.1419 (0.1356) Data: 0.0077 (0.0133) Loss: 0.8016 (0.5307)
[2022/12/29 00:38] | TRAIN(042): [500/879] Batch: 0.1258 (0.1352) Data: 0.0079 (0.0128) Loss: 0.7069 (0.5316)
[2022/12/29 00:38] | TRAIN(042): [550/879] Batch: 0.1256 (0.1347) Data: 0.0087 (0.0124) Loss: 0.3058 (0.5312)
[2022/12/29 00:38] | TRAIN(042): [600/879] Batch: 0.1177 (0.1342) Data: 0.0084 (0.0121) Loss: 0.7453 (0.5337)
[2022/12/29 00:39] | TRAIN(042): [650/879] Batch: 0.1391 (0.1338) Data: 0.0139 (0.0119) Loss: 0.6198 (0.5335)
[2022/12/29 00:39] | TRAIN(042): [700/879] Batch: 0.1210 (0.1335) Data: 0.0081 (0.0116) Loss: 0.5453 (0.5342)
[2022/12/29 00:39] | TRAIN(042): [750/879] Batch: 0.1437 (0.1335) Data: 0.0072 (0.0114) Loss: 0.6542 (0.5324)
[2022/12/29 00:39] | TRAIN(042): [800/879] Batch: 0.1213 (0.1331) Data: 0.0083 (0.0112) Loss: 0.6881 (0.5342)
[2022/12/29 00:39] | TRAIN(042): [850/879] Batch: 0.1219 (0.1329) Data: 0.0081 (0.0111) Loss: 0.6195 (0.5339)
[2022/12/29 00:39] | ------------------------------------------------------------
[2022/12/29 00:39] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:39] | ------------------------------------------------------------
[2022/12/29 00:39] |    TRAIN(42)     0:01:56     0:00:09     0:01:47      0.5347
[2022/12/29 00:39] | ------------------------------------------------------------
[2022/12/29 00:39] | VALID(042): [ 50/220] Batch: 0.0485 (0.0715) Data: 0.0372 (0.0563) Loss: 0.6307 (0.5490)
[2022/12/29 00:39] | VALID(042): [100/220] Batch: 0.0446 (0.0570) Data: 0.0293 (0.0419) Loss: 0.9767 (0.5738)
[2022/12/29 00:39] | VALID(042): [150/220] Batch: 0.0442 (0.0520) Data: 0.0137 (0.0370) Loss: 0.6084 (0.5763)
[2022/12/29 00:39] | VALID(042): [200/220] Batch: 0.0345 (0.0495) Data: 0.0239 (0.0349) Loss: 0.2922 (0.5730)
[2022/12/29 00:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:39] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:39] |    VALID(42)      0.5684      0.8195      0.8703      0.8195      0.8195      0.8195      0.9549
[2022/12/29 00:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:39] | ####################################################################################################
[2022/12/29 00:39] | TRAIN(043): [ 50/879] Batch: 0.0997 (0.1728) Data: 0.0083 (0.0491) Loss: 0.6290 (0.4738)
[2022/12/29 00:39] | TRAIN(043): [100/879] Batch: 0.1282 (0.1525) Data: 0.0094 (0.0294) Loss: 0.6664 (0.4938)
[2022/12/29 00:40] | TRAIN(043): [150/879] Batch: 0.1280 (0.1451) Data: 0.0106 (0.0226) Loss: 0.5833 (0.5023)
[2022/12/29 00:40] | TRAIN(043): [200/879] Batch: 0.1307 (0.1416) Data: 0.0102 (0.0194) Loss: 0.6545 (0.5069)
[2022/12/29 00:40] | TRAIN(043): [250/879] Batch: 0.1171 (0.1396) Data: 0.0085 (0.0174) Loss: 0.4618 (0.5076)
[2022/12/29 00:40] | TRAIN(043): [300/879] Batch: 0.0835 (0.1364) Data: 0.0076 (0.0161) Loss: 0.5492 (0.5071)
[2022/12/29 00:40] | TRAIN(043): [350/879] Batch: 0.1416 (0.1348) Data: 0.0164 (0.0151) Loss: 0.5086 (0.5111)
[2022/12/29 00:40] | TRAIN(043): [400/879] Batch: 0.0984 (0.1342) Data: 0.0081 (0.0144) Loss: 0.4596 (0.5139)
[2022/12/29 00:40] | TRAIN(043): [450/879] Batch: 0.1278 (0.1319) Data: 0.0080 (0.0139) Loss: 0.3686 (0.5173)
[2022/12/29 00:40] | TRAIN(043): [500/879] Batch: 0.1361 (0.1316) Data: 0.0075 (0.0134) Loss: 0.7523 (0.5196)
[2022/12/29 00:40] | TRAIN(043): [550/879] Batch: 0.1273 (0.1316) Data: 0.0113 (0.0130) Loss: 0.6749 (0.5228)
[2022/12/29 00:41] | TRAIN(043): [600/879] Batch: 0.1391 (0.1316) Data: 0.0106 (0.0127) Loss: 0.5721 (0.5244)
[2022/12/29 00:41] | TRAIN(043): [650/879] Batch: 0.1437 (0.1317) Data: 0.0074 (0.0125) Loss: 0.6182 (0.5241)
[2022/12/29 00:41] | TRAIN(043): [700/879] Batch: 0.1264 (0.1320) Data: 0.0116 (0.0123) Loss: 0.7968 (0.5257)
[2022/12/29 00:41] | TRAIN(043): [750/879] Batch: 0.1440 (0.1320) Data: 0.0074 (0.0121) Loss: 0.6553 (0.5269)
[2022/12/29 00:41] | TRAIN(043): [800/879] Batch: 0.1451 (0.1320) Data: 0.0093 (0.0120) Loss: 0.3794 (0.5264)
[2022/12/29 00:41] | TRAIN(043): [850/879] Batch: 0.1455 (0.1322) Data: 0.0097 (0.0118) Loss: 0.6139 (0.5257)
[2022/12/29 00:41] | ------------------------------------------------------------
[2022/12/29 00:41] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:41] | ------------------------------------------------------------
[2022/12/29 00:41] |    TRAIN(43)     0:01:56     0:00:10     0:01:45      0.5253
[2022/12/29 00:41] | ------------------------------------------------------------
[2022/12/29 00:41] | VALID(043): [ 50/220] Batch: 0.0462 (0.0704) Data: 0.0391 (0.0581) Loss: 0.4797 (0.5442)
[2022/12/29 00:41] | VALID(043): [100/220] Batch: 0.0436 (0.0561) Data: 0.0327 (0.0447) Loss: 0.8332 (0.5729)
[2022/12/29 00:41] | VALID(043): [150/220] Batch: 0.0438 (0.0513) Data: 0.0349 (0.0402) Loss: 0.4577 (0.5718)
[2022/12/29 00:41] | VALID(043): [200/220] Batch: 0.0482 (0.0490) Data: 0.0288 (0.0381) Loss: 0.2380 (0.5747)
[2022/12/29 00:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:41] |    VALID(43)      0.5690      0.8151      0.8764      0.8151      0.8151      0.8151      0.9538
[2022/12/29 00:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:41] | ####################################################################################################
[2022/12/29 00:41] | TRAIN(044): [ 50/879] Batch: 0.1336 (0.1688) Data: 0.0090 (0.0465) Loss: 0.3771 (0.5239)
[2022/12/29 00:42] | TRAIN(044): [100/879] Batch: 0.1387 (0.1507) Data: 0.0075 (0.0284) Loss: 0.6147 (0.5262)
[2022/12/29 00:42] | TRAIN(044): [150/879] Batch: 0.1310 (0.1442) Data: 0.0094 (0.0221) Loss: 0.3388 (0.5233)
[2022/12/29 00:42] | TRAIN(044): [200/879] Batch: 0.1415 (0.1415) Data: 0.0091 (0.0190) Loss: 0.5209 (0.5236)
[2022/12/29 00:42] | TRAIN(044): [250/879] Batch: 0.1214 (0.1391) Data: 0.0092 (0.0171) Loss: 0.4684 (0.5214)
[2022/12/29 00:42] | TRAIN(044): [300/879] Batch: 0.1438 (0.1380) Data: 0.0076 (0.0158) Loss: 0.6777 (0.5188)
[2022/12/29 00:42] | TRAIN(044): [350/879] Batch: 0.1411 (0.1376) Data: 0.0125 (0.0150) Loss: 0.3917 (0.5176)
[2022/12/29 00:42] | TRAIN(044): [400/879] Batch: 0.1453 (0.1365) Data: 0.0099 (0.0143) Loss: 0.8900 (0.5211)
[2022/12/29 00:42] | TRAIN(044): [450/879] Batch: 0.1194 (0.1360) Data: 0.0085 (0.0137) Loss: 0.5359 (0.5219)
[2022/12/29 00:42] | TRAIN(044): [500/879] Batch: 0.1165 (0.1356) Data: 0.0084 (0.0133) Loss: 0.2419 (0.5235)
[2022/12/29 00:43] | TRAIN(044): [550/879] Batch: 0.1336 (0.1355) Data: 0.0082 (0.0129) Loss: 0.4389 (0.5220)
[2022/12/29 00:43] | TRAIN(044): [600/879] Batch: 0.1193 (0.1350) Data: 0.0082 (0.0126) Loss: 0.4424 (0.5218)
[2022/12/29 00:43] | TRAIN(044): [650/879] Batch: 0.1215 (0.1347) Data: 0.0084 (0.0122) Loss: 0.3907 (0.5202)
[2022/12/29 00:43] | TRAIN(044): [700/879] Batch: 0.1203 (0.1344) Data: 0.0081 (0.0120) Loss: 0.4453 (0.5207)
[2022/12/29 00:43] | TRAIN(044): [750/879] Batch: 0.1264 (0.1330) Data: 0.0087 (0.0118) Loss: 0.3031 (0.5234)
[2022/12/29 00:43] | TRAIN(044): [800/879] Batch: 0.1275 (0.1329) Data: 0.0092 (0.0115) Loss: 0.4322 (0.5240)
[2022/12/29 00:43] | TRAIN(044): [850/879] Batch: 0.1201 (0.1313) Data: 0.0098 (0.0114) Loss: 0.7857 (0.5229)
[2022/12/29 00:43] | ------------------------------------------------------------
[2022/12/29 00:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:43] | ------------------------------------------------------------
[2022/12/29 00:43] |    TRAIN(44)     0:01:55     0:00:09     0:01:45      0.5234
[2022/12/29 00:43] | ------------------------------------------------------------
[2022/12/29 00:43] | VALID(044): [ 50/220] Batch: 0.0448 (0.0702) Data: 0.0381 (0.0591) Loss: 0.4715 (0.5446)
[2022/12/29 00:43] | VALID(044): [100/220] Batch: 0.0344 (0.0556) Data: 0.0391 (0.0449) Loss: 0.9557 (0.5688)
[2022/12/29 00:43] | VALID(044): [150/220] Batch: 0.0312 (0.0509) Data: 0.0393 (0.0402) Loss: 0.5324 (0.5706)
[2022/12/29 00:43] | VALID(044): [200/220] Batch: 0.0444 (0.0484) Data: 0.0294 (0.0376) Loss: 0.2724 (0.5693)
[2022/12/29 00:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:43] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:43] |    VALID(44)      0.5652      0.8164      0.8786      0.8164      0.8164      0.8164      0.9541
[2022/12/29 00:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:43] | ####################################################################################################
[2022/12/29 00:44] | TRAIN(045): [ 50/879] Batch: 0.1252 (0.1747) Data: 0.0180 (0.0494) Loss: 0.2115 (0.4945)
[2022/12/29 00:44] | TRAIN(045): [100/879] Batch: 0.1427 (0.1558) Data: 0.0094 (0.0299) Loss: 0.5058 (0.4943)
[2022/12/29 00:44] | TRAIN(045): [150/879] Batch: 0.1427 (0.1472) Data: 0.0074 (0.0230) Loss: 0.6019 (0.5127)
[2022/12/29 00:44] | TRAIN(045): [200/879] Batch: 0.1319 (0.1433) Data: 0.0088 (0.0196) Loss: 0.4051 (0.5209)
[2022/12/29 00:44] | TRAIN(045): [250/879] Batch: 0.1231 (0.1403) Data: 0.0084 (0.0176) Loss: 0.4468 (0.5221)
[2022/12/29 00:44] | TRAIN(045): [300/879] Batch: 0.1300 (0.1388) Data: 0.0079 (0.0161) Loss: 0.5318 (0.5197)
[2022/12/29 00:44] | TRAIN(045): [350/879] Batch: 0.1394 (0.1381) Data: 0.0079 (0.0150) Loss: 0.4506 (0.5197)
[2022/12/29 00:44] | TRAIN(045): [400/879] Batch: 0.1467 (0.1371) Data: 0.0093 (0.0142) Loss: 0.4307 (0.5198)
[2022/12/29 00:44] | TRAIN(045): [450/879] Batch: 0.1356 (0.1371) Data: 0.0093 (0.0137) Loss: 0.2054 (0.5169)
[2022/12/29 00:45] | TRAIN(045): [500/879] Batch: 0.1236 (0.1365) Data: 0.0081 (0.0132) Loss: 0.7668 (0.5173)
[2022/12/29 00:45] | TRAIN(045): [550/879] Batch: 0.1383 (0.1360) Data: 0.0102 (0.0128) Loss: 0.5567 (0.5198)
[2022/12/29 00:45] | TRAIN(045): [600/879] Batch: 0.1236 (0.1355) Data: 0.0082 (0.0124) Loss: 0.5389 (0.5215)
[2022/12/29 00:45] | TRAIN(045): [650/879] Batch: 0.1249 (0.1351) Data: 0.0085 (0.0121) Loss: 0.5750 (0.5201)
[2022/12/29 00:45] | TRAIN(045): [700/879] Batch: 0.1188 (0.1346) Data: 0.0081 (0.0118) Loss: 0.3880 (0.5190)
[2022/12/29 00:45] | TRAIN(045): [750/879] Batch: 0.1422 (0.1342) Data: 0.0070 (0.0116) Loss: 0.7046 (0.5219)
[2022/12/29 00:45] | TRAIN(045): [800/879] Batch: 0.1411 (0.1339) Data: 0.0094 (0.0114) Loss: 0.4216 (0.5207)
[2022/12/29 00:45] | TRAIN(045): [850/879] Batch: 0.1417 (0.1337) Data: 0.0078 (0.0112) Loss: 0.5639 (0.5201)
[2022/12/29 00:45] | ------------------------------------------------------------
[2022/12/29 00:45] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:45] | ------------------------------------------------------------
[2022/12/29 00:45] |    TRAIN(45)     0:01:57     0:00:09     0:01:47      0.5199
[2022/12/29 00:45] | ------------------------------------------------------------
[2022/12/29 00:45] | VALID(045): [ 50/220] Batch: 0.0345 (0.0727) Data: 0.0235 (0.0605) Loss: 0.5656 (0.5770)
[2022/12/29 00:46] | VALID(045): [100/220] Batch: 0.0443 (0.0562) Data: 0.0186 (0.0448) Loss: 1.1095 (0.6154)
[2022/12/29 00:46] | VALID(045): [150/220] Batch: 0.0274 (0.0512) Data: 0.0391 (0.0399) Loss: 0.6047 (0.6173)
[2022/12/29 00:46] | VALID(045): [200/220] Batch: 0.0322 (0.0487) Data: 0.0270 (0.0376) Loss: 0.3035 (0.6225)
[2022/12/29 00:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:46] |    VALID(45)      0.6188      0.8154      0.8704      0.8154      0.8154      0.8154      0.9538
[2022/12/29 00:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:46] | ####################################################################################################
[2022/12/29 00:46] | TRAIN(046): [ 50/879] Batch: 0.1160 (0.1751) Data: 0.0090 (0.0522) Loss: 0.8152 (0.4957)
[2022/12/29 00:46] | TRAIN(046): [100/879] Batch: 0.1411 (0.1547) Data: 0.0075 (0.0327) Loss: 1.0338 (0.5360)
[2022/12/29 00:46] | TRAIN(046): [150/879] Batch: 0.0750 (0.1468) Data: 0.0102 (0.0261) Loss: 0.5555 (0.5343)
[2022/12/29 00:46] | TRAIN(046): [200/879] Batch: 0.1349 (0.1408) Data: 0.0077 (0.0223) Loss: 0.4322 (0.5228)
[2022/12/29 00:46] | TRAIN(046): [250/879] Batch: 0.1355 (0.1392) Data: 0.0084 (0.0201) Loss: 0.4856 (0.5238)
[2022/12/29 00:46] | TRAIN(046): [300/879] Batch: 0.1307 (0.1344) Data: 0.0092 (0.0186) Loss: 0.7582 (0.5238)
[2022/12/29 00:46] | TRAIN(046): [350/879] Batch: 0.1158 (0.1340) Data: 0.0163 (0.0173) Loss: 0.4346 (0.5257)
[2022/12/29 00:46] | TRAIN(046): [400/879] Batch: 0.1326 (0.1337) Data: 0.0078 (0.0164) Loss: 0.5134 (0.5230)
[2022/12/29 00:47] | TRAIN(046): [450/879] Batch: 0.1343 (0.1336) Data: 0.0098 (0.0157) Loss: 0.3498 (0.5246)
[2022/12/29 00:47] | TRAIN(046): [500/879] Batch: 0.1205 (0.1333) Data: 0.0081 (0.0151) Loss: 0.5005 (0.5205)
[2022/12/29 00:47] | TRAIN(046): [550/879] Batch: 0.1458 (0.1333) Data: 0.0107 (0.0146) Loss: 0.7962 (0.5199)
[2022/12/29 00:47] | TRAIN(046): [600/879] Batch: 0.1244 (0.1331) Data: 0.0079 (0.0141) Loss: 0.2895 (0.5211)
[2022/12/29 00:47] | TRAIN(046): [650/879] Batch: 0.1446 (0.1332) Data: 0.0107 (0.0138) Loss: 0.5121 (0.5186)
[2022/12/29 00:47] | TRAIN(046): [700/879] Batch: 0.1426 (0.1331) Data: 0.0155 (0.0135) Loss: 0.4315 (0.5189)
[2022/12/29 00:47] | TRAIN(046): [750/879] Batch: 0.1514 (0.1330) Data: 0.0093 (0.0133) Loss: 0.5110 (0.5189)
[2022/12/29 00:47] | TRAIN(046): [800/879] Batch: 0.1243 (0.1329) Data: 0.0087 (0.0130) Loss: 0.5571 (0.5180)
[2022/12/29 00:47] | TRAIN(046): [850/879] Batch: 0.1364 (0.1330) Data: 0.0093 (0.0129) Loss: 0.3065 (0.5164)
[2022/12/29 00:48] | ------------------------------------------------------------
[2022/12/29 00:48] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:48] | ------------------------------------------------------------
[2022/12/29 00:48] |    TRAIN(46)     0:01:56     0:00:11     0:01:45      0.5175
[2022/12/29 00:48] | ------------------------------------------------------------
[2022/12/29 00:48] | VALID(046): [ 50/220] Batch: 0.0486 (0.0720) Data: 0.0246 (0.0602) Loss: 0.6517 (0.5737)
[2022/12/29 00:48] | VALID(046): [100/220] Batch: 0.0494 (0.0572) Data: 0.0362 (0.0456) Loss: 1.0695 (0.5913)
[2022/12/29 00:48] | VALID(046): [150/220] Batch: 0.0457 (0.0520) Data: 0.0388 (0.0402) Loss: 0.4845 (0.5977)
[2022/12/29 00:48] | VALID(046): [200/220] Batch: 0.0347 (0.0496) Data: 0.0214 (0.0377) Loss: 0.3500 (0.5948)
[2022/12/29 00:48] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:48] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:48] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:48] |    VALID(46)      0.5894      0.8076      0.8727      0.8076      0.8076      0.8076      0.9519
[2022/12/29 00:48] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:48] | ####################################################################################################
[2022/12/29 00:48] | TRAIN(047): [ 50/879] Batch: 0.1234 (0.1714) Data: 0.0081 (0.0508) Loss: 0.4291 (0.4974)
[2022/12/29 00:48] | TRAIN(047): [100/879] Batch: 0.1197 (0.1509) Data: 0.0079 (0.0301) Loss: 0.5178 (0.4978)
[2022/12/29 00:48] | TRAIN(047): [150/879] Batch: 0.1377 (0.1452) Data: 0.0094 (0.0232) Loss: 0.5922 (0.4980)
[2022/12/29 00:48] | TRAIN(047): [200/879] Batch: 0.1266 (0.1419) Data: 0.0086 (0.0197) Loss: 0.5974 (0.4992)
[2022/12/29 00:48] | TRAIN(047): [250/879] Batch: 0.1458 (0.1404) Data: 0.0092 (0.0176) Loss: 0.3395 (0.5029)
[2022/12/29 00:48] | TRAIN(047): [300/879] Batch: 0.1355 (0.1399) Data: 0.0095 (0.0163) Loss: 0.3191 (0.5025)
[2022/12/29 00:49] | TRAIN(047): [350/879] Batch: 0.1266 (0.1386) Data: 0.0087 (0.0152) Loss: 0.3006 (0.5039)
[2022/12/29 00:49] | TRAIN(047): [400/879] Batch: 0.1297 (0.1375) Data: 0.0088 (0.0144) Loss: 0.6001 (0.5025)
[2022/12/29 00:49] | TRAIN(047): [450/879] Batch: 0.1409 (0.1366) Data: 0.0072 (0.0137) Loss: 0.4024 (0.5062)
[2022/12/29 00:49] | TRAIN(047): [500/879] Batch: 0.1437 (0.1359) Data: 0.0074 (0.0132) Loss: 0.3356 (0.5081)
[2022/12/29 00:49] | TRAIN(047): [550/879] Batch: 0.0989 (0.1353) Data: 0.0075 (0.0128) Loss: 0.5220 (0.5076)
[2022/12/29 00:49] | TRAIN(047): [600/879] Batch: 0.1265 (0.1330) Data: 0.0089 (0.0124) Loss: 0.5899 (0.5076)
[2022/12/29 00:49] | TRAIN(047): [650/879] Batch: 0.1406 (0.1333) Data: 0.0098 (0.0122) Loss: 0.4301 (0.5076)
[2022/12/29 00:49] | TRAIN(047): [700/879] Batch: 0.1289 (0.1316) Data: 0.0080 (0.0120) Loss: 0.6174 (0.5082)
[2022/12/29 00:49] | TRAIN(047): [750/879] Batch: 0.1207 (0.1316) Data: 0.0080 (0.0118) Loss: 0.4225 (0.5076)
[2022/12/29 00:49] | TRAIN(047): [800/879] Batch: 0.1386 (0.1314) Data: 0.0091 (0.0116) Loss: 0.5792 (0.5084)
[2022/12/29 00:50] | TRAIN(047): [850/879] Batch: 0.1476 (0.1313) Data: 0.0093 (0.0114) Loss: 0.6306 (0.5100)
[2022/12/29 00:50] | ------------------------------------------------------------
[2022/12/29 00:50] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:50] | ------------------------------------------------------------
[2022/12/29 00:50] |    TRAIN(47)     0:01:55     0:00:09     0:01:45      0.5104
[2022/12/29 00:50] | ------------------------------------------------------------
[2022/12/29 00:50] | VALID(047): [ 50/220] Batch: 0.0433 (0.0698) Data: 0.0358 (0.0570) Loss: 0.5963 (0.5342)
[2022/12/29 00:50] | VALID(047): [100/220] Batch: 0.0443 (0.0557) Data: 0.0350 (0.0442) Loss: 1.0131 (0.5657)
[2022/12/29 00:50] | VALID(047): [150/220] Batch: 0.0434 (0.0510) Data: 0.0348 (0.0398) Loss: 0.5049 (0.5685)
[2022/12/29 00:50] | VALID(047): [200/220] Batch: 0.0442 (0.0486) Data: 0.0383 (0.0376) Loss: 0.2798 (0.5735)
[2022/12/29 00:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:50] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:50] |    VALID(47)      0.5684      0.8168      0.8778      0.8168      0.8168      0.8168      0.9542
[2022/12/29 00:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:50] | ####################################################################################################
[2022/12/29 00:50] | TRAIN(048): [ 50/879] Batch: 0.1233 (0.1747) Data: 0.0083 (0.0491) Loss: 0.4414 (0.5029)
[2022/12/29 00:50] | TRAIN(048): [100/879] Batch: 0.1203 (0.1524) Data: 0.0085 (0.0293) Loss: 0.6447 (0.5071)
[2022/12/29 00:50] | TRAIN(048): [150/879] Batch: 0.1401 (0.1464) Data: 0.0074 (0.0228) Loss: 0.4757 (0.5062)
[2022/12/29 00:50] | TRAIN(048): [200/879] Batch: 0.1392 (0.1430) Data: 0.0124 (0.0195) Loss: 0.5790 (0.5120)
[2022/12/29 00:50] | TRAIN(048): [250/879] Batch: 0.1338 (0.1407) Data: 0.0079 (0.0175) Loss: 0.5901 (0.5052)
[2022/12/29 00:51] | TRAIN(048): [300/879] Batch: 0.1360 (0.1394) Data: 0.0070 (0.0162) Loss: 0.3412 (0.5065)
[2022/12/29 00:51] | TRAIN(048): [350/879] Batch: 0.1168 (0.1380) Data: 0.0099 (0.0152) Loss: 0.3995 (0.5112)
[2022/12/29 00:51] | TRAIN(048): [400/879] Batch: 0.1455 (0.1375) Data: 0.0100 (0.0145) Loss: 0.5445 (0.5108)
[2022/12/29 00:51] | TRAIN(048): [450/879] Batch: 0.1342 (0.1369) Data: 0.0095 (0.0139) Loss: 0.6330 (0.5121)
[2022/12/29 00:51] | TRAIN(048): [500/879] Batch: 0.1184 (0.1366) Data: 0.0084 (0.0134) Loss: 0.4723 (0.5150)
[2022/12/29 00:51] | TRAIN(048): [550/879] Batch: 0.1457 (0.1362) Data: 0.0094 (0.0130) Loss: 0.4243 (0.5144)
[2022/12/29 00:51] | TRAIN(048): [600/879] Batch: 0.1322 (0.1360) Data: 0.0083 (0.0127) Loss: 0.4396 (0.5159)
[2022/12/29 00:51] | TRAIN(048): [650/879] Batch: 0.1403 (0.1356) Data: 0.0077 (0.0124) Loss: 0.3414 (0.5139)
[2022/12/29 00:51] | TRAIN(048): [700/879] Batch: 0.1402 (0.1357) Data: 0.0107 (0.0122) Loss: 0.4871 (0.5137)
[2022/12/29 00:52] | TRAIN(048): [750/879] Batch: 0.1194 (0.1355) Data: 0.0081 (0.0120) Loss: 0.4300 (0.5132)
[2022/12/29 00:52] | TRAIN(048): [800/879] Batch: 0.1394 (0.1351) Data: 0.0072 (0.0118) Loss: 0.6221 (0.5107)
[2022/12/29 00:52] | TRAIN(048): [850/879] Batch: 0.1205 (0.1347) Data: 0.0101 (0.0116) Loss: 0.6785 (0.5101)
[2022/12/29 00:52] | ------------------------------------------------------------
[2022/12/29 00:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:52] | ------------------------------------------------------------
[2022/12/29 00:52] |    TRAIN(48)     0:01:58     0:00:10     0:01:48      0.5085
[2022/12/29 00:52] | ------------------------------------------------------------
[2022/12/29 00:52] | VALID(048): [ 50/220] Batch: 0.0511 (0.0738) Data: 0.0341 (0.0627) Loss: 0.4854 (0.5562)
[2022/12/29 00:52] | VALID(048): [100/220] Batch: 0.0472 (0.0578) Data: 0.0292 (0.0465) Loss: 1.1137 (0.5884)
[2022/12/29 00:52] | VALID(048): [150/220] Batch: 0.0483 (0.0525) Data: 0.0330 (0.0400) Loss: 0.5646 (0.5955)
[2022/12/29 00:52] | VALID(048): [200/220] Batch: 0.0247 (0.0494) Data: 0.0095 (0.0361) Loss: 0.2189 (0.5966)
[2022/12/29 00:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:52] |    VALID(48)      0.5922      0.8167      0.8762      0.8167      0.8167      0.8167      0.9542
[2022/12/29 00:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:52] | ####################################################################################################
[2022/12/29 00:52] | TRAIN(049): [ 50/879] Batch: 0.1344 (0.1744) Data: 0.0143 (0.0501) Loss: 0.5621 (0.5002)
[2022/12/29 00:52] | TRAIN(049): [100/879] Batch: 0.0862 (0.1443) Data: 0.0100 (0.0311) Loss: 0.4814 (0.5143)
[2022/12/29 00:52] | TRAIN(049): [150/879] Batch: 0.1262 (0.1408) Data: 0.0138 (0.0244) Loss: 0.4695 (0.5032)
[2022/12/29 00:52] | TRAIN(049): [200/879] Batch: 0.1199 (0.1385) Data: 0.0107 (0.0210) Loss: 0.3843 (0.4991)
[2022/12/29 00:53] | TRAIN(049): [250/879] Batch: 0.1396 (0.1378) Data: 0.0151 (0.0190) Loss: 0.6594 (0.4969)
[2022/12/29 00:53] | TRAIN(049): [300/879] Batch: 0.1247 (0.1368) Data: 0.0095 (0.0175) Loss: 0.6680 (0.4999)
[2022/12/29 00:53] | TRAIN(049): [350/879] Batch: 0.1387 (0.1361) Data: 0.0168 (0.0164) Loss: 0.4607 (0.5030)
[2022/12/29 00:53] | TRAIN(049): [400/879] Batch: 0.1361 (0.1360) Data: 0.0180 (0.0156) Loss: 0.4191 (0.5007)
[2022/12/29 00:53] | TRAIN(049): [450/879] Batch: 0.1486 (0.1359) Data: 0.0090 (0.0151) Loss: 0.4512 (0.4959)
[2022/12/29 00:53] | TRAIN(049): [500/879] Batch: 0.1313 (0.1357) Data: 0.0101 (0.0146) Loss: 0.7149 (0.4926)
[2022/12/29 00:53] | TRAIN(049): [550/879] Batch: 0.1222 (0.1356) Data: 0.0100 (0.0142) Loss: 0.5527 (0.4924)
[2022/12/29 00:53] | TRAIN(049): [600/879] Batch: 0.1328 (0.1355) Data: 0.0188 (0.0139) Loss: 0.6348 (0.4962)
[2022/12/29 00:53] | TRAIN(049): [650/879] Batch: 0.1391 (0.1353) Data: 0.0169 (0.0135) Loss: 0.6810 (0.4970)
[2022/12/29 00:54] | TRAIN(049): [700/879] Batch: 0.1321 (0.1352) Data: 0.0097 (0.0133) Loss: 0.2756 (0.4975)
[2022/12/29 00:54] | TRAIN(049): [750/879] Batch: 0.1413 (0.1349) Data: 0.0094 (0.0130) Loss: 0.5706 (0.4975)
[2022/12/29 00:54] | TRAIN(049): [800/879] Batch: 0.1433 (0.1345) Data: 0.0082 (0.0127) Loss: 0.5210 (0.4969)
[2022/12/29 00:54] | TRAIN(049): [850/879] Batch: 0.1188 (0.1342) Data: 0.0081 (0.0125) Loss: 0.8306 (0.4981)
[2022/12/29 00:54] | ------------------------------------------------------------
[2022/12/29 00:54] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:54] | ------------------------------------------------------------
[2022/12/29 00:54] |    TRAIN(49)     0:01:57     0:00:10     0:01:46      0.4980
[2022/12/29 00:54] | ------------------------------------------------------------
[2022/12/29 00:54] | VALID(049): [ 50/220] Batch: 0.0473 (0.0707) Data: 0.0244 (0.0598) Loss: 0.4034 (0.5775)
[2022/12/29 00:54] | VALID(049): [100/220] Batch: 0.0449 (0.0561) Data: 0.0209 (0.0455) Loss: 0.9604 (0.5949)
[2022/12/29 00:54] | VALID(049): [150/220] Batch: 0.0295 (0.0512) Data: 0.0362 (0.0406) Loss: 0.4474 (0.5929)
[2022/12/29 00:54] | VALID(049): [200/220] Batch: 0.0484 (0.0489) Data: 0.0294 (0.0381) Loss: 0.2093 (0.5886)
[2022/12/29 00:54] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:54] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:54] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:54] |    VALID(49)      0.5848      0.8170      0.8793      0.8170      0.8170      0.8170      0.9542
[2022/12/29 00:54] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:54] | ####################################################################################################
[2022/12/29 00:54] | TRAIN(050): [ 50/879] Batch: 0.1227 (0.1711) Data: 0.0081 (0.0496) Loss: 0.6133 (0.5184)
[2022/12/29 00:54] | TRAIN(050): [100/879] Batch: 0.1286 (0.1535) Data: 0.0101 (0.0299) Loss: 0.6362 (0.5132)
[2022/12/29 00:54] | TRAIN(050): [150/879] Batch: 0.1462 (0.1476) Data: 0.0094 (0.0232) Loss: 0.6543 (0.5120)
[2022/12/29 00:55] | TRAIN(050): [200/879] Batch: 0.1422 (0.1437) Data: 0.0094 (0.0197) Loss: 0.3091 (0.5019)
[2022/12/29 00:55] | TRAIN(050): [250/879] Batch: 0.1484 (0.1410) Data: 0.0098 (0.0175) Loss: 0.5345 (0.4996)
[2022/12/29 00:55] | TRAIN(050): [300/879] Batch: 0.1192 (0.1395) Data: 0.0084 (0.0161) Loss: 0.4659 (0.4965)
[2022/12/29 00:55] | TRAIN(050): [350/879] Batch: 0.1243 (0.1383) Data: 0.0098 (0.0151) Loss: 0.5676 (0.4958)
[2022/12/29 00:55] | TRAIN(050): [400/879] Batch: 0.1206 (0.1352) Data: 0.0072 (0.0144) Loss: 0.5091 (0.4971)
[2022/12/29 00:55] | TRAIN(050): [450/879] Batch: 0.1326 (0.1348) Data: 0.0093 (0.0138) Loss: 0.3271 (0.4942)
[2022/12/29 00:55] | TRAIN(050): [500/879] Batch: 0.0748 (0.1326) Data: 0.0077 (0.0134) Loss: 0.7574 (0.4934)
[2022/12/29 00:55] | TRAIN(050): [550/879] Batch: 0.1215 (0.1320) Data: 0.0083 (0.0129) Loss: 0.6693 (0.4943)
[2022/12/29 00:55] | TRAIN(050): [600/879] Batch: 0.1433 (0.1320) Data: 0.0069 (0.0125) Loss: 0.9000 (0.4954)
[2022/12/29 00:56] | TRAIN(050): [650/879] Batch: 0.1191 (0.1318) Data: 0.0081 (0.0122) Loss: 0.5596 (0.4992)
[2022/12/29 00:56] | TRAIN(050): [700/879] Batch: 0.1347 (0.1316) Data: 0.0073 (0.0119) Loss: 0.3061 (0.4988)
[2022/12/29 00:56] | TRAIN(050): [750/879] Batch: 0.1228 (0.1314) Data: 0.0083 (0.0117) Loss: 0.5563 (0.4984)
[2022/12/29 00:56] | TRAIN(050): [800/879] Batch: 0.1442 (0.1313) Data: 0.0070 (0.0115) Loss: 0.4627 (0.4992)
[2022/12/29 00:56] | TRAIN(050): [850/879] Batch: 0.1218 (0.1311) Data: 0.0082 (0.0113) Loss: 0.3866 (0.5000)
[2022/12/29 00:56] | ------------------------------------------------------------
[2022/12/29 00:56] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:56] | ------------------------------------------------------------
[2022/12/29 00:56] |    TRAIN(50)     0:01:55     0:00:09     0:01:45      0.4991
[2022/12/29 00:56] | ------------------------------------------------------------
[2022/12/29 00:56] | VALID(050): [ 50/220] Batch: 0.0449 (0.0721) Data: 0.0216 (0.0580) Loss: 0.5357 (0.6051)
[2022/12/29 00:56] | VALID(050): [100/220] Batch: 0.0342 (0.0569) Data: 0.0235 (0.0446) Loss: 0.9889 (0.6254)
[2022/12/29 00:56] | VALID(050): [150/220] Batch: 0.0281 (0.0517) Data: 0.0394 (0.0401) Loss: 0.4774 (0.6217)
[2022/12/29 00:56] | VALID(050): [200/220] Batch: 0.0314 (0.0492) Data: 0.0392 (0.0379) Loss: 0.3167 (0.6121)
[2022/12/29 00:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:56] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:56] |    VALID(50)      0.6072      0.8207      0.8795      0.8207      0.8207      0.8207      0.9552
[2022/12/29 00:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:56] | ####################################################################################################
[2022/12/29 00:56] | TRAIN(051): [ 50/879] Batch: 0.1399 (0.1676) Data: 0.0077 (0.0453) Loss: 0.3503 (0.5051)
[2022/12/29 00:56] | TRAIN(051): [100/879] Batch: 0.1422 (0.1489) Data: 0.0076 (0.0271) Loss: 0.4758 (0.4881)
[2022/12/29 00:57] | TRAIN(051): [150/879] Batch: 0.1207 (0.1439) Data: 0.0084 (0.0212) Loss: 0.3584 (0.4876)
[2022/12/29 00:57] | TRAIN(051): [200/879] Batch: 0.0946 (0.1403) Data: 0.0116 (0.0181) Loss: 0.5295 (0.4891)
[2022/12/29 00:57] | TRAIN(051): [250/879] Batch: 0.1383 (0.1381) Data: 0.0082 (0.0162) Loss: 0.4089 (0.4941)
[2022/12/29 00:57] | TRAIN(051): [300/879] Batch: 0.1315 (0.1366) Data: 0.0069 (0.0149) Loss: 0.5656 (0.4901)
[2022/12/29 00:57] | TRAIN(051): [350/879] Batch: 0.1245 (0.1355) Data: 0.0080 (0.0139) Loss: 0.3849 (0.4910)
[2022/12/29 00:57] | TRAIN(051): [400/879] Batch: 0.1252 (0.1348) Data: 0.0083 (0.0132) Loss: 0.2909 (0.4903)
[2022/12/29 00:57] | TRAIN(051): [450/879] Batch: 0.1180 (0.1341) Data: 0.0083 (0.0127) Loss: 0.4905 (0.4941)
[2022/12/29 00:57] | TRAIN(051): [500/879] Batch: 0.1390 (0.1336) Data: 0.0071 (0.0122) Loss: 0.5297 (0.4956)
[2022/12/29 00:57] | TRAIN(051): [550/879] Batch: 0.1419 (0.1335) Data: 0.0075 (0.0119) Loss: 0.5246 (0.4933)
[2022/12/29 00:58] | TRAIN(051): [600/879] Batch: 0.1186 (0.1333) Data: 0.0087 (0.0117) Loss: 0.2370 (0.4941)
[2022/12/29 00:58] | TRAIN(051): [650/879] Batch: 0.1441 (0.1330) Data: 0.0073 (0.0114) Loss: 0.5456 (0.4945)
[2022/12/29 00:58] | TRAIN(051): [700/879] Batch: 0.1215 (0.1327) Data: 0.0110 (0.0112) Loss: 0.3458 (0.4924)
[2022/12/29 00:58] | TRAIN(051): [750/879] Batch: 0.1477 (0.1327) Data: 0.0100 (0.0111) Loss: 0.5256 (0.4927)
[2022/12/29 00:58] | TRAIN(051): [800/879] Batch: 0.1421 (0.1328) Data: 0.0069 (0.0110) Loss: 0.4735 (0.4929)
[2022/12/29 00:58] | TRAIN(051): [850/879] Batch: 0.1419 (0.1317) Data: 0.0125 (0.0109) Loss: 0.5662 (0.4940)
[2022/12/29 00:58] | ------------------------------------------------------------
[2022/12/29 00:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:58] | ------------------------------------------------------------
[2022/12/29 00:58] |    TRAIN(51)     0:01:55     0:00:09     0:01:46      0.4935
[2022/12/29 00:58] | ------------------------------------------------------------
[2022/12/29 00:58] | VALID(051): [ 50/220] Batch: 0.0456 (0.0739) Data: 0.0376 (0.0619) Loss: 0.5861 (0.5984)
[2022/12/29 00:58] | VALID(051): [100/220] Batch: 0.0284 (0.0526) Data: 0.0062 (0.0404) Loss: 0.9981 (0.6240)
[2022/12/29 00:58] | VALID(051): [150/220] Batch: 0.0434 (0.0455) Data: 0.0322 (0.0332) Loss: 0.5159 (0.6202)
[2022/12/29 00:58] | VALID(051): [200/220] Batch: 0.0443 (0.0445) Data: 0.0348 (0.0325) Loss: 0.3132 (0.6117)
[2022/12/29 00:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:58] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:58] |    VALID(51)      0.6052      0.8161      0.8780      0.8161      0.8161      0.8161      0.9540
[2022/12/29 00:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:58] | ####################################################################################################
[2022/12/29 00:58] | TRAIN(052): [ 50/879] Batch: 0.1290 (0.1704) Data: 0.0086 (0.0477) Loss: 0.6108 (0.5114)
[2022/12/29 00:59] | TRAIN(052): [100/879] Batch: 0.1200 (0.1505) Data: 0.0078 (0.0286) Loss: 0.6020 (0.4932)
[2022/12/29 00:59] | TRAIN(052): [150/879] Batch: 0.1365 (0.1450) Data: 0.0091 (0.0224) Loss: 0.6673 (0.4862)
[2022/12/29 00:59] | TRAIN(052): [200/879] Batch: 0.1225 (0.1412) Data: 0.0078 (0.0189) Loss: 0.4103 (0.4966)
[2022/12/29 00:59] | TRAIN(052): [250/879] Batch: 0.1428 (0.1387) Data: 0.0071 (0.0168) Loss: 0.4253 (0.4926)
[2022/12/29 00:59] | TRAIN(052): [300/879] Batch: 0.1292 (0.1376) Data: 0.0087 (0.0155) Loss: 0.3842 (0.4953)
[2022/12/29 00:59] | TRAIN(052): [350/879] Batch: 0.1469 (0.1374) Data: 0.0094 (0.0146) Loss: 0.3819 (0.4897)
[2022/12/29 00:59] | TRAIN(052): [400/879] Batch: 0.1324 (0.1370) Data: 0.0097 (0.0139) Loss: 0.9040 (0.4892)
[2022/12/29 00:59] | TRAIN(052): [450/879] Batch: 0.1427 (0.1363) Data: 0.0082 (0.0133) Loss: 0.5594 (0.4888)
[2022/12/29 00:59] | TRAIN(052): [500/879] Batch: 0.1356 (0.1357) Data: 0.0071 (0.0129) Loss: 0.5411 (0.4874)
[2022/12/29 01:00] | TRAIN(052): [550/879] Batch: 0.1196 (0.1351) Data: 0.0080 (0.0125) Loss: 0.4497 (0.4857)
[2022/12/29 01:00] | TRAIN(052): [600/879] Batch: 0.1406 (0.1346) Data: 0.0075 (0.0121) Loss: 0.3415 (0.4861)
[2022/12/29 01:00] | TRAIN(052): [650/879] Batch: 0.1224 (0.1342) Data: 0.0081 (0.0118) Loss: 0.6105 (0.4847)
[2022/12/29 01:00] | TRAIN(052): [700/879] Batch: 0.1431 (0.1339) Data: 0.0094 (0.0116) Loss: 0.4728 (0.4844)
[2022/12/29 01:00] | TRAIN(052): [750/879] Batch: 0.1191 (0.1339) Data: 0.0080 (0.0114) Loss: 0.4539 (0.4841)
[2022/12/29 01:00] | TRAIN(052): [800/879] Batch: 0.1362 (0.1335) Data: 0.0075 (0.0113) Loss: 0.3840 (0.4855)
[2022/12/29 01:00] | TRAIN(052): [850/879] Batch: 0.1366 (0.1335) Data: 0.0096 (0.0111) Loss: 0.4192 (0.4858)
[2022/12/29 01:00] | ------------------------------------------------------------
[2022/12/29 01:00] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:00] | ------------------------------------------------------------
[2022/12/29 01:00] |    TRAIN(52)     0:01:57     0:00:09     0:01:47      0.4870
[2022/12/29 01:00] | ------------------------------------------------------------
[2022/12/29 01:00] | VALID(052): [ 50/220] Batch: 0.0438 (0.0708) Data: 0.0363 (0.0558) Loss: 0.5942 (0.6025)
[2022/12/29 01:00] | VALID(052): [100/220] Batch: 0.0482 (0.0562) Data: 0.0200 (0.0425) Loss: 0.8632 (0.6157)
[2022/12/29 01:00] | VALID(052): [150/220] Batch: 0.0347 (0.0513) Data: 0.0207 (0.0383) Loss: 0.5152 (0.6150)
[2022/12/29 01:00] | VALID(052): [200/220] Batch: 0.0308 (0.0488) Data: 0.0366 (0.0364) Loss: 0.3002 (0.6099)
[2022/12/29 01:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:00] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:00] |    VALID(52)      0.6058      0.7995      0.8742      0.7995      0.7995      0.7995      0.9499
[2022/12/29 01:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:00] | ####################################################################################################
[2022/12/29 01:01] | TRAIN(053): [ 50/879] Batch: 0.1397 (0.1696) Data: 0.0072 (0.0468) Loss: 0.5992 (0.4532)
[2022/12/29 01:01] | TRAIN(053): [100/879] Batch: 0.1241 (0.1494) Data: 0.0082 (0.0277) Loss: 0.4567 (0.4605)
[2022/12/29 01:01] | TRAIN(053): [150/879] Batch: 0.1378 (0.1424) Data: 0.0073 (0.0213) Loss: 0.4665 (0.4651)
[2022/12/29 01:01] | TRAIN(053): [200/879] Batch: 0.1249 (0.1389) Data: 0.0080 (0.0181) Loss: 0.2588 (0.4648)
[2022/12/29 01:01] | TRAIN(053): [250/879] Batch: 0.1281 (0.1375) Data: 0.0088 (0.0162) Loss: 0.7336 (0.4713)
[2022/12/29 01:01] | TRAIN(053): [300/879] Batch: 0.1421 (0.1341) Data: 0.0104 (0.0150) Loss: 0.5865 (0.4732)
[2022/12/29 01:01] | TRAIN(053): [350/879] Batch: 0.1410 (0.1341) Data: 0.0108 (0.0143) Loss: 0.4609 (0.4745)
[2022/12/29 01:01] | TRAIN(053): [400/879] Batch: 0.1230 (0.1304) Data: 0.0065 (0.0136) Loss: 0.6822 (0.4753)
[2022/12/29 01:01] | TRAIN(053): [450/879] Batch: 0.1336 (0.1302) Data: 0.0082 (0.0130) Loss: 0.6884 (0.4786)
[2022/12/29 01:02] | TRAIN(053): [500/879] Batch: 0.1456 (0.1300) Data: 0.0102 (0.0126) Loss: 0.4595 (0.4744)
[2022/12/29 01:02] | TRAIN(053): [550/879] Batch: 0.1403 (0.1304) Data: 0.0082 (0.0123) Loss: 0.3447 (0.4772)
[2022/12/29 01:02] | TRAIN(053): [600/879] Batch: 0.1443 (0.1308) Data: 0.0095 (0.0121) Loss: 0.4257 (0.4781)
[2022/12/29 01:02] | TRAIN(053): [650/879] Batch: 0.1315 (0.1311) Data: 0.0099 (0.0119) Loss: 0.3501 (0.4779)
[2022/12/29 01:02] | TRAIN(053): [700/879] Batch: 0.1368 (0.1314) Data: 0.0095 (0.0118) Loss: 0.5882 (0.4745)
[2022/12/29 01:02] | TRAIN(053): [750/879] Batch: 0.1331 (0.1316) Data: 0.0100 (0.0116) Loss: 0.6757 (0.4743)
[2022/12/29 01:02] | TRAIN(053): [800/879] Batch: 0.1223 (0.1315) Data: 0.0080 (0.0114) Loss: 0.6516 (0.4789)
[2022/12/29 01:02] | TRAIN(053): [850/879] Batch: 0.1455 (0.1317) Data: 0.0096 (0.0113) Loss: 0.4619 (0.4788)
[2022/12/29 01:02] | ------------------------------------------------------------
[2022/12/29 01:02] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:02] | ------------------------------------------------------------
[2022/12/29 01:02] |    TRAIN(53)     0:01:55     0:00:09     0:01:45      0.4793
[2022/12/29 01:02] | ------------------------------------------------------------
[2022/12/29 01:02] | VALID(053): [ 50/220] Batch: 0.0311 (0.0724) Data: 0.0348 (0.0618) Loss: 0.5778 (0.5363)
[2022/12/29 01:02] | VALID(053): [100/220] Batch: 0.0367 (0.0568) Data: 0.0344 (0.0464) Loss: 0.9432 (0.5724)
[2022/12/29 01:03] | VALID(053): [150/220] Batch: 0.0437 (0.0514) Data: 0.0173 (0.0410) Loss: 0.5158 (0.5745)
[2022/12/29 01:03] | VALID(053): [200/220] Batch: 0.0405 (0.0490) Data: 0.0201 (0.0385) Loss: 0.1619 (0.5778)
[2022/12/29 01:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:03] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:03] |    VALID(53)      0.5743      0.8232      0.8793      0.8232      0.8232      0.8232      0.9558
[2022/12/29 01:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:03] | ####################################################################################################
[2022/12/29 01:03] | TRAIN(054): [ 50/879] Batch: 0.1408 (0.1731) Data: 0.0079 (0.0500) Loss: 0.4430 (0.4965)
[2022/12/29 01:03] | TRAIN(054): [100/879] Batch: 0.1252 (0.1509) Data: 0.0077 (0.0295) Loss: 0.4039 (0.4895)
[2022/12/29 01:03] | TRAIN(054): [150/879] Batch: 0.1202 (0.1438) Data: 0.0096 (0.0227) Loss: 0.3773 (0.4840)
[2022/12/29 01:03] | TRAIN(054): [200/879] Batch: 0.1404 (0.1407) Data: 0.0078 (0.0193) Loss: 0.3642 (0.4879)
[2022/12/29 01:03] | TRAIN(054): [250/879] Batch: 0.1466 (0.1391) Data: 0.0098 (0.0173) Loss: 0.7755 (0.4877)
[2022/12/29 01:03] | TRAIN(054): [300/879] Batch: 0.1386 (0.1375) Data: 0.0073 (0.0158) Loss: 0.7294 (0.4865)
[2022/12/29 01:03] | TRAIN(054): [350/879] Batch: 0.1385 (0.1361) Data: 0.0073 (0.0147) Loss: 0.5136 (0.4853)
[2022/12/29 01:03] | TRAIN(054): [400/879] Batch: 0.1221 (0.1356) Data: 0.0096 (0.0140) Loss: 0.5222 (0.4861)
[2022/12/29 01:04] | TRAIN(054): [450/879] Batch: 0.1176 (0.1349) Data: 0.0103 (0.0134) Loss: 0.5656 (0.4851)
[2022/12/29 01:04] | TRAIN(054): [500/879] Batch: 0.1245 (0.1344) Data: 0.0096 (0.0129) Loss: 0.6569 (0.4880)
[2022/12/29 01:04] | TRAIN(054): [550/879] Batch: 0.1170 (0.1341) Data: 0.0081 (0.0125) Loss: 0.4382 (0.4860)
[2022/12/29 01:04] | TRAIN(054): [600/879] Batch: 0.1400 (0.1337) Data: 0.0107 (0.0121) Loss: 0.4471 (0.4862)
[2022/12/29 01:04] | TRAIN(054): [650/879] Batch: 0.1391 (0.1333) Data: 0.0072 (0.0118) Loss: 0.4757 (0.4854)
[2022/12/29 01:04] | TRAIN(054): [700/879] Batch: 0.1302 (0.1323) Data: 0.0084 (0.0117) Loss: 0.4399 (0.4872)
[2022/12/29 01:04] | TRAIN(054): [750/879] Batch: 0.1382 (0.1321) Data: 0.0079 (0.0115) Loss: 0.4743 (0.4864)
[2022/12/29 01:04] | TRAIN(054): [800/879] Batch: 0.1375 (0.1304) Data: 0.0099 (0.0113) Loss: 0.4301 (0.4869)
[2022/12/29 01:04] | TRAIN(054): [850/879] Batch: 0.1358 (0.1303) Data: 0.0072 (0.0111) Loss: 0.2899 (0.4845)
[2022/12/29 01:04] | ------------------------------------------------------------
[2022/12/29 01:04] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:04] | ------------------------------------------------------------
[2022/12/29 01:04] |    TRAIN(54)     0:01:54     0:00:09     0:01:44      0.4847
[2022/12/29 01:04] | ------------------------------------------------------------
[2022/12/29 01:05] | VALID(054): [ 50/220] Batch: 0.0376 (0.0722) Data: 0.0209 (0.0597) Loss: 0.5568 (0.5751)
[2022/12/29 01:05] | VALID(054): [100/220] Batch: 0.0281 (0.0568) Data: 0.0392 (0.0455) Loss: 1.0336 (0.6123)
[2022/12/29 01:05] | VALID(054): [150/220] Batch: 0.0338 (0.0518) Data: 0.0382 (0.0398) Loss: 0.5127 (0.6125)
[2022/12/29 01:05] | VALID(054): [200/220] Batch: 0.0432 (0.0492) Data: 0.0344 (0.0376) Loss: 0.2136 (0.6148)
[2022/12/29 01:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:05] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:05] |    VALID(54)      0.6102      0.8195      0.8765      0.8195      0.8195      0.8195      0.9549
[2022/12/29 01:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:05] | ####################################################################################################
[2022/12/29 01:05] | TRAIN(055): [ 50/879] Batch: 0.1167 (0.1676) Data: 0.0080 (0.0453) Loss: 0.3604 (0.4683)
[2022/12/29 01:05] | TRAIN(055): [100/879] Batch: 0.1272 (0.1486) Data: 0.0074 (0.0270) Loss: 0.4342 (0.4601)
[2022/12/29 01:05] | TRAIN(055): [150/879] Batch: 0.1245 (0.1423) Data: 0.0110 (0.0209) Loss: 0.7221 (0.4624)
[2022/12/29 01:05] | TRAIN(055): [200/879] Batch: 0.1196 (0.1400) Data: 0.0103 (0.0178) Loss: 0.6796 (0.4704)
[2022/12/29 01:05] | TRAIN(055): [250/879] Batch: 0.1220 (0.1380) Data: 0.0085 (0.0160) Loss: 0.4241 (0.4753)
[2022/12/29 01:05] | TRAIN(055): [300/879] Batch: 0.1239 (0.1368) Data: 0.0100 (0.0148) Loss: 0.3913 (0.4800)
[2022/12/29 01:05] | TRAIN(055): [350/879] Batch: 0.1211 (0.1360) Data: 0.0081 (0.0139) Loss: 0.3316 (0.4787)
[2022/12/29 01:06] | TRAIN(055): [400/879] Batch: 0.1006 (0.1350) Data: 0.0102 (0.0132) Loss: 0.4018 (0.4809)
[2022/12/29 01:06] | TRAIN(055): [450/879] Batch: 0.1201 (0.1342) Data: 0.0081 (0.0127) Loss: 0.7034 (0.4778)
[2022/12/29 01:06] | TRAIN(055): [500/879] Batch: 0.1395 (0.1337) Data: 0.0075 (0.0123) Loss: 0.1498 (0.4743)
[2022/12/29 01:06] | TRAIN(055): [550/879] Batch: 0.1296 (0.1335) Data: 0.0097 (0.0120) Loss: 0.3657 (0.4769)
[2022/12/29 01:06] | TRAIN(055): [600/879] Batch: 0.1237 (0.1331) Data: 0.0081 (0.0117) Loss: 0.5131 (0.4756)
[2022/12/29 01:06] | TRAIN(055): [650/879] Batch: 0.1198 (0.1328) Data: 0.0081 (0.0115) Loss: 0.6152 (0.4741)
[2022/12/29 01:06] | TRAIN(055): [700/879] Batch: 0.1391 (0.1327) Data: 0.0079 (0.0113) Loss: 0.3583 (0.4752)
[2022/12/29 01:06] | TRAIN(055): [750/879] Batch: 0.1162 (0.1328) Data: 0.0082 (0.0111) Loss: 0.4791 (0.4757)
[2022/12/29 01:06] | TRAIN(055): [800/879] Batch: 0.1407 (0.1325) Data: 0.0076 (0.0110) Loss: 0.5823 (0.4766)
[2022/12/29 01:07] | TRAIN(055): [850/879] Batch: 0.1213 (0.1323) Data: 0.0082 (0.0108) Loss: 0.2321 (0.4773)
[2022/12/29 01:07] | ------------------------------------------------------------
[2022/12/29 01:07] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:07] | ------------------------------------------------------------
[2022/12/29 01:07] |    TRAIN(55)     0:01:56     0:00:09     0:01:46      0.4762
[2022/12/29 01:07] | ------------------------------------------------------------
[2022/12/29 01:07] | VALID(055): [ 50/220] Batch: 0.0438 (0.0710) Data: 0.0215 (0.0580) Loss: 0.6178 (0.5545)
[2022/12/29 01:07] | VALID(055): [100/220] Batch: 0.0349 (0.0565) Data: 0.0190 (0.0443) Loss: 1.0784 (0.5840)
[2022/12/29 01:07] | VALID(055): [150/220] Batch: 0.0307 (0.0515) Data: 0.0160 (0.0393) Loss: 0.6259 (0.5909)
[2022/12/29 01:07] | VALID(055): [200/220] Batch: 0.0328 (0.0490) Data: 0.0260 (0.0367) Loss: 0.3911 (0.5892)
[2022/12/29 01:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:07] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:07] |    VALID(55)      0.5877      0.8110      0.8757      0.8110      0.8110      0.8110      0.9527
[2022/12/29 01:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:07] | ####################################################################################################
[2022/12/29 01:07] | TRAIN(056): [ 50/879] Batch: 0.1207 (0.1688) Data: 0.0082 (0.0463) Loss: 0.5311 (0.4731)
[2022/12/29 01:07] | TRAIN(056): [100/879] Batch: 0.1406 (0.1507) Data: 0.0100 (0.0278) Loss: 0.6429 (0.4689)
[2022/12/29 01:07] | TRAIN(056): [150/879] Batch: 0.1349 (0.1388) Data: 0.0094 (0.0217) Loss: 0.4646 (0.4504)
[2022/12/29 01:07] | TRAIN(056): [200/879] Batch: 0.1354 (0.1385) Data: 0.0111 (0.0188) Loss: 0.5132 (0.4540)
[2022/12/29 01:07] | TRAIN(056): [250/879] Batch: 0.1238 (0.1321) Data: 0.0081 (0.0169) Loss: 0.5710 (0.4616)
[2022/12/29 01:07] | TRAIN(056): [300/879] Batch: 0.1408 (0.1315) Data: 0.0073 (0.0154) Loss: 0.4395 (0.4637)
[2022/12/29 01:08] | TRAIN(056): [350/879] Batch: 0.1349 (0.1314) Data: 0.0103 (0.0145) Loss: 0.3378 (0.4659)
[2022/12/29 01:08] | TRAIN(056): [400/879] Batch: 0.1340 (0.1312) Data: 0.0088 (0.0137) Loss: 0.5596 (0.4604)
[2022/12/29 01:08] | TRAIN(056): [450/879] Batch: 0.1422 (0.1312) Data: 0.0073 (0.0132) Loss: 0.2834 (0.4606)
[2022/12/29 01:08] | TRAIN(056): [500/879] Batch: 0.1232 (0.1309) Data: 0.0076 (0.0127) Loss: 0.4131 (0.4630)
[2022/12/29 01:08] | TRAIN(056): [550/879] Batch: 0.1517 (0.1313) Data: 0.0097 (0.0124) Loss: 0.5891 (0.4640)
[2022/12/29 01:08] | TRAIN(056): [600/879] Batch: 0.1360 (0.1315) Data: 0.0082 (0.0122) Loss: 0.3808 (0.4648)
[2022/12/29 01:08] | TRAIN(056): [650/879] Batch: 0.1428 (0.1315) Data: 0.0096 (0.0119) Loss: 0.2674 (0.4654)
[2022/12/29 01:08] | TRAIN(056): [700/879] Batch: 0.1457 (0.1316) Data: 0.0107 (0.0117) Loss: 0.2782 (0.4644)
[2022/12/29 01:08] | TRAIN(056): [750/879] Batch: 0.1318 (0.1319) Data: 0.0098 (0.0116) Loss: 0.6273 (0.4657)
[2022/12/29 01:09] | TRAIN(056): [800/879] Batch: 0.1146 (0.1322) Data: 0.0082 (0.0115) Loss: 0.5558 (0.4658)
[2022/12/29 01:09] | TRAIN(056): [850/879] Batch: 0.1434 (0.1320) Data: 0.0073 (0.0113) Loss: 0.1285 (0.4678)
[2022/12/29 01:09] | ------------------------------------------------------------
[2022/12/29 01:09] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:09] | ------------------------------------------------------------
[2022/12/29 01:09] |    TRAIN(56)     0:01:55     0:00:09     0:01:45      0.4678
[2022/12/29 01:09] | ------------------------------------------------------------
[2022/12/29 01:09] | VALID(056): [ 50/220] Batch: 0.0476 (0.0700) Data: 0.0336 (0.0569) Loss: 0.6298 (0.5543)
[2022/12/29 01:09] | VALID(056): [100/220] Batch: 0.0510 (0.0557) Data: 0.0341 (0.0439) Loss: 1.1159 (0.5898)
[2022/12/29 01:09] | VALID(056): [150/220] Batch: 0.0528 (0.0510) Data: 0.0293 (0.0381) Loss: 0.6203 (0.5996)
[2022/12/29 01:09] | VALID(056): [200/220] Batch: 0.0482 (0.0487) Data: 0.0286 (0.0351) Loss: 0.2393 (0.6044)
[2022/12/29 01:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:09] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:09] |    VALID(56)      0.6029      0.8170      0.8753      0.8170      0.8170      0.8170      0.9542
[2022/12/29 01:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:09] | ####################################################################################################
[2022/12/29 01:09] | TRAIN(057): [ 50/879] Batch: 0.1178 (0.1706) Data: 0.0082 (0.0485) Loss: 0.7524 (0.4571)
[2022/12/29 01:09] | TRAIN(057): [100/879] Batch: 0.1411 (0.1522) Data: 0.0071 (0.0289) Loss: 0.3252 (0.4732)
[2022/12/29 01:09] | TRAIN(057): [150/879] Batch: 0.1222 (0.1442) Data: 0.0090 (0.0220) Loss: 0.4299 (0.4733)
[2022/12/29 01:09] | TRAIN(057): [200/879] Batch: 0.1417 (0.1407) Data: 0.0076 (0.0187) Loss: 0.4345 (0.4716)
[2022/12/29 01:09] | TRAIN(057): [250/879] Batch: 0.1333 (0.1388) Data: 0.0098 (0.0167) Loss: 0.3801 (0.4660)
[2022/12/29 01:10] | TRAIN(057): [300/879] Batch: 0.1361 (0.1383) Data: 0.0091 (0.0156) Loss: 0.6899 (0.4622)
[2022/12/29 01:10] | TRAIN(057): [350/879] Batch: 0.1385 (0.1381) Data: 0.0095 (0.0148) Loss: 0.5097 (0.4621)
[2022/12/29 01:10] | TRAIN(057): [400/879] Batch: 0.1239 (0.1374) Data: 0.0081 (0.0140) Loss: 0.3598 (0.4619)
[2022/12/29 01:10] | TRAIN(057): [450/879] Batch: 0.1197 (0.1364) Data: 0.0081 (0.0134) Loss: 0.4205 (0.4660)
[2022/12/29 01:10] | TRAIN(057): [500/879] Batch: 0.1483 (0.1361) Data: 0.0117 (0.0129) Loss: 0.5601 (0.4661)
[2022/12/29 01:10] | TRAIN(057): [550/879] Batch: 0.1332 (0.1343) Data: 0.0084 (0.0126) Loss: 0.3561 (0.4655)
[2022/12/29 01:10] | TRAIN(057): [600/879] Batch: 0.1176 (0.1340) Data: 0.0107 (0.0123) Loss: 0.3266 (0.4666)
[2022/12/29 01:10] | TRAIN(057): [650/879] Batch: 0.0781 (0.1321) Data: 0.0076 (0.0120) Loss: 1.0689 (0.4678)
[2022/12/29 01:10] | TRAIN(057): [700/879] Batch: 0.1432 (0.1315) Data: 0.0069 (0.0117) Loss: 0.2520 (0.4691)
[2022/12/29 01:11] | TRAIN(057): [750/879] Batch: 0.1329 (0.1315) Data: 0.0096 (0.0115) Loss: 0.4820 (0.4666)
[2022/12/29 01:11] | TRAIN(057): [800/879] Batch: 0.1412 (0.1313) Data: 0.0072 (0.0114) Loss: 0.6449 (0.4667)
[2022/12/29 01:11] | TRAIN(057): [850/879] Batch: 0.1313 (0.1315) Data: 0.0098 (0.0113) Loss: 0.2856 (0.4665)
[2022/12/29 01:11] | ------------------------------------------------------------
[2022/12/29 01:11] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:11] | ------------------------------------------------------------
[2022/12/29 01:11] |    TRAIN(57)     0:01:55     0:00:09     0:01:45      0.4666
[2022/12/29 01:11] | ------------------------------------------------------------
[2022/12/29 01:11] | VALID(057): [ 50/220] Batch: 0.0476 (0.0710) Data: 0.0313 (0.0582) Loss: 0.7543 (0.5682)
[2022/12/29 01:11] | VALID(057): [100/220] Batch: 0.0488 (0.0563) Data: 0.0345 (0.0448) Loss: 1.0131 (0.5932)
[2022/12/29 01:11] | VALID(057): [150/220] Batch: 0.0483 (0.0514) Data: 0.0352 (0.0403) Loss: 0.5567 (0.5969)
[2022/12/29 01:11] | VALID(057): [200/220] Batch: 0.0324 (0.0489) Data: 0.0337 (0.0375) Loss: 0.3113 (0.5967)
[2022/12/29 01:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:11] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:11] |    VALID(57)      0.5927      0.8168      0.8773      0.8168      0.8168      0.8168      0.9542
[2022/12/29 01:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:11] | ####################################################################################################
[2022/12/29 01:11] | TRAIN(058): [ 50/879] Batch: 0.1385 (0.1776) Data: 0.0103 (0.0481) Loss: 0.5237 (0.4701)
[2022/12/29 01:11] | TRAIN(058): [100/879] Batch: 0.1392 (0.1547) Data: 0.0102 (0.0293) Loss: 0.4258 (0.4587)
[2022/12/29 01:11] | TRAIN(058): [150/879] Batch: 0.1254 (0.1477) Data: 0.0099 (0.0227) Loss: 0.4091 (0.4666)
[2022/12/29 01:11] | TRAIN(058): [200/879] Batch: 0.1244 (0.1436) Data: 0.0080 (0.0192) Loss: 0.4412 (0.4639)
[2022/12/29 01:12] | TRAIN(058): [250/879] Batch: 0.1221 (0.1413) Data: 0.0079 (0.0171) Loss: 0.1567 (0.4605)
[2022/12/29 01:12] | TRAIN(058): [300/879] Batch: 0.1308 (0.1396) Data: 0.0085 (0.0157) Loss: 0.4911 (0.4623)
[2022/12/29 01:12] | TRAIN(058): [350/879] Batch: 0.1173 (0.1390) Data: 0.0082 (0.0148) Loss: 0.4627 (0.4632)
[2022/12/29 01:12] | TRAIN(058): [400/879] Batch: 0.1395 (0.1378) Data: 0.0073 (0.0140) Loss: 0.3784 (0.4618)
[2022/12/29 01:12] | TRAIN(058): [450/879] Batch: 0.1212 (0.1370) Data: 0.0080 (0.0134) Loss: 0.3367 (0.4555)
[2022/12/29 01:12] | TRAIN(058): [500/879] Batch: 0.1173 (0.1362) Data: 0.0080 (0.0129) Loss: 0.4435 (0.4567)
[2022/12/29 01:12] | TRAIN(058): [550/879] Batch: 0.1206 (0.1356) Data: 0.0076 (0.0125) Loss: 0.2571 (0.4571)
[2022/12/29 01:12] | TRAIN(058): [600/879] Batch: 0.1194 (0.1350) Data: 0.0078 (0.0121) Loss: 0.3582 (0.4596)
[2022/12/29 01:12] | TRAIN(058): [650/879] Batch: 0.1448 (0.1345) Data: 0.0080 (0.0118) Loss: 0.3997 (0.4589)
[2022/12/29 01:13] | TRAIN(058): [700/879] Batch: 0.1247 (0.1341) Data: 0.0093 (0.0116) Loss: 0.6224 (0.4582)
[2022/12/29 01:13] | TRAIN(058): [750/879] Batch: 0.1444 (0.1338) Data: 0.0072 (0.0114) Loss: 0.3813 (0.4565)
[2022/12/29 01:13] | TRAIN(058): [800/879] Batch: 0.1241 (0.1335) Data: 0.0082 (0.0112) Loss: 0.3908 (0.4571)
[2022/12/29 01:13] | TRAIN(058): [850/879] Batch: 0.1421 (0.1334) Data: 0.0071 (0.0110) Loss: 0.4527 (0.4582)
[2022/12/29 01:13] | ------------------------------------------------------------
[2022/12/29 01:13] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:13] | ------------------------------------------------------------
[2022/12/29 01:13] |    TRAIN(58)     0:01:57     0:00:09     0:01:47      0.4588
[2022/12/29 01:13] | ------------------------------------------------------------
[2022/12/29 01:13] | VALID(058): [ 50/220] Batch: 0.0475 (0.0732) Data: 0.0343 (0.0613) Loss: 0.6004 (0.5742)
[2022/12/29 01:13] | VALID(058): [100/220] Batch: 0.0476 (0.0576) Data: 0.0288 (0.0461) Loss: 1.0609 (0.6032)
[2022/12/29 01:13] | VALID(058): [150/220] Batch: 0.0440 (0.0524) Data: 0.0316 (0.0411) Loss: 0.7606 (0.6119)
[2022/12/29 01:13] | VALID(058): [200/220] Batch: 0.0230 (0.0461) Data: 0.0080 (0.0344) Loss: 0.5147 (0.6115)
[2022/12/29 01:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:13] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:13] |    VALID(58)      0.6080      0.8138      0.8727      0.8138      0.8138      0.8138      0.9535
[2022/12/29 01:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:13] | ####################################################################################################
[2022/12/29 01:13] | TRAIN(059): [ 50/879] Batch: 0.1269 (0.1734) Data: 0.0072 (0.0501) Loss: 0.4272 (0.4491)
[2022/12/29 01:13] | TRAIN(059): [100/879] Batch: 0.1232 (0.1398) Data: 0.0090 (0.0297) Loss: 0.2623 (0.4532)
[2022/12/29 01:13] | TRAIN(059): [150/879] Batch: 0.1254 (0.1373) Data: 0.0082 (0.0229) Loss: 0.2924 (0.4523)
[2022/12/29 01:14] | TRAIN(059): [200/879] Batch: 0.1235 (0.1359) Data: 0.0111 (0.0197) Loss: 0.5489 (0.4573)
[2022/12/29 01:14] | TRAIN(059): [250/879] Batch: 0.1392 (0.1352) Data: 0.0138 (0.0177) Loss: 0.6622 (0.4596)
[2022/12/29 01:14] | TRAIN(059): [300/879] Batch: 0.1234 (0.1342) Data: 0.0081 (0.0162) Loss: 0.3336 (0.4593)
[2022/12/29 01:14] | TRAIN(059): [350/879] Batch: 0.1172 (0.1338) Data: 0.0080 (0.0153) Loss: 0.5089 (0.4588)
[2022/12/29 01:14] | TRAIN(059): [400/879] Batch: 0.1217 (0.1330) Data: 0.0094 (0.0144) Loss: 0.4089 (0.4612)
[2022/12/29 01:14] | TRAIN(059): [450/879] Batch: 0.1195 (0.1327) Data: 0.0126 (0.0139) Loss: 0.2953 (0.4603)
[2022/12/29 01:14] | TRAIN(059): [500/879] Batch: 0.1287 (0.1325) Data: 0.0104 (0.0134) Loss: 0.6401 (0.4606)
[2022/12/29 01:14] | TRAIN(059): [550/879] Batch: 0.1187 (0.1326) Data: 0.0078 (0.0130) Loss: 0.4015 (0.4607)
[2022/12/29 01:14] | TRAIN(059): [600/879] Batch: 0.1261 (0.1323) Data: 0.0087 (0.0127) Loss: 0.3696 (0.4596)
[2022/12/29 01:15] | TRAIN(059): [650/879] Batch: 0.1359 (0.1321) Data: 0.0071 (0.0123) Loss: 0.2426 (0.4588)
[2022/12/29 01:15] | TRAIN(059): [700/879] Batch: 0.1358 (0.1319) Data: 0.0070 (0.0120) Loss: 0.5618 (0.4581)
[2022/12/29 01:15] | TRAIN(059): [750/879] Batch: 0.1383 (0.1318) Data: 0.0073 (0.0118) Loss: 0.4549 (0.4575)
[2022/12/29 01:15] | TRAIN(059): [800/879] Batch: 0.1292 (0.1317) Data: 0.0084 (0.0116) Loss: 0.5040 (0.4585)
[2022/12/29 01:15] | TRAIN(059): [850/879] Batch: 0.1383 (0.1316) Data: 0.0073 (0.0114) Loss: 0.6676 (0.4587)
[2022/12/29 01:15] | ------------------------------------------------------------
[2022/12/29 01:15] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:15] | ------------------------------------------------------------
[2022/12/29 01:15] |    TRAIN(59)     0:01:55     0:00:09     0:01:45      0.4583
[2022/12/29 01:15] | ------------------------------------------------------------
[2022/12/29 01:15] | VALID(059): [ 50/220] Batch: 0.0422 (0.0713) Data: 0.0337 (0.0579) Loss: 0.5738 (0.5496)
[2022/12/29 01:15] | VALID(059): [100/220] Batch: 0.0439 (0.0565) Data: 0.0371 (0.0446) Loss: 1.1196 (0.5891)
[2022/12/29 01:15] | VALID(059): [150/220] Batch: 0.0475 (0.0516) Data: 0.0253 (0.0401) Loss: 0.6315 (0.5959)
[2022/12/29 01:15] | VALID(059): [200/220] Batch: 0.0439 (0.0490) Data: 0.0195 (0.0377) Loss: 0.3418 (0.5959)
[2022/12/29 01:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:15] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:15] |    VALID(59)      0.5950      0.8190      0.8817      0.8190      0.8190      0.8190      0.9547
[2022/12/29 01:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:15] | ####################################################################################################
[2022/12/29 01:15] | TRAIN(060): [ 50/879] Batch: 0.1207 (0.1740) Data: 0.0081 (0.0512) Loss: 0.5348 (0.4436)
[2022/12/29 01:15] | TRAIN(060): [100/879] Batch: 0.1220 (0.1525) Data: 0.0084 (0.0307) Loss: 0.3903 (0.4309)
[2022/12/29 01:16] | TRAIN(060): [150/879] Batch: 0.1259 (0.1453) Data: 0.0092 (0.0238) Loss: 0.5220 (0.4422)
[2022/12/29 01:16] | TRAIN(060): [200/879] Batch: 0.1253 (0.1429) Data: 0.0084 (0.0204) Loss: 0.7091 (0.4502)
[2022/12/29 01:16] | TRAIN(060): [250/879] Batch: 0.1203 (0.1411) Data: 0.0081 (0.0184) Loss: 0.6004 (0.4437)
[2022/12/29 01:16] | TRAIN(060): [300/879] Batch: 0.1292 (0.1396) Data: 0.0106 (0.0169) Loss: 0.7404 (0.4427)
[2022/12/29 01:16] | TRAIN(060): [350/879] Batch: 0.1324 (0.1387) Data: 0.0081 (0.0159) Loss: 0.4160 (0.4429)
[2022/12/29 01:16] | TRAIN(060): [400/879] Batch: 0.1312 (0.1353) Data: 0.0121 (0.0151) Loss: 0.3605 (0.4412)
[2022/12/29 01:16] | TRAIN(060): [450/879] Batch: 0.1261 (0.1350) Data: 0.0093 (0.0144) Loss: 0.6681 (0.4438)
[2022/12/29 01:16] | TRAIN(060): [500/879] Batch: 0.0845 (0.1336) Data: 0.0072 (0.0140) Loss: 0.2734 (0.4438)
[2022/12/29 01:16] | TRAIN(060): [550/879] Batch: 0.1432 (0.1324) Data: 0.0074 (0.0135) Loss: 0.6496 (0.4449)
[2022/12/29 01:17] | TRAIN(060): [600/879] Batch: 0.1438 (0.1320) Data: 0.0112 (0.0132) Loss: 0.2141 (0.4464)
[2022/12/29 01:17] | TRAIN(060): [650/879] Batch: 0.1217 (0.1318) Data: 0.0081 (0.0128) Loss: 0.4356 (0.4470)
[2022/12/29 01:17] | TRAIN(060): [700/879] Batch: 0.1435 (0.1316) Data: 0.0086 (0.0126) Loss: 0.3584 (0.4480)
[2022/12/29 01:17] | TRAIN(060): [750/879] Batch: 0.1200 (0.1314) Data: 0.0087 (0.0123) Loss: 0.6782 (0.4506)
[2022/12/29 01:17] | TRAIN(060): [800/879] Batch: 0.1194 (0.1314) Data: 0.0082 (0.0121) Loss: 0.3147 (0.4477)
[2022/12/29 01:17] | TRAIN(060): [850/879] Batch: 0.1378 (0.1314) Data: 0.0089 (0.0119) Loss: 0.3858 (0.4475)
[2022/12/29 01:17] | ------------------------------------------------------------
[2022/12/29 01:17] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:17] | ------------------------------------------------------------
[2022/12/29 01:17] |    TRAIN(60)     0:01:55     0:00:10     0:01:44      0.4483
[2022/12/29 01:17] | ------------------------------------------------------------
[2022/12/29 01:17] | VALID(060): [ 50/220] Batch: 0.0417 (0.0701) Data: 0.0220 (0.0580) Loss: 0.5518 (0.6102)
[2022/12/29 01:17] | VALID(060): [100/220] Batch: 0.0419 (0.0557) Data: 0.0145 (0.0442) Loss: 1.1186 (0.6402)
[2022/12/29 01:17] | VALID(060): [150/220] Batch: 0.0486 (0.0514) Data: 0.0289 (0.0388) Loss: 0.6363 (0.6461)
[2022/12/29 01:17] | VALID(060): [200/220] Batch: 0.0433 (0.0489) Data: 0.0382 (0.0366) Loss: 0.4211 (0.6495)
[2022/12/29 01:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:17] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:17] |    VALID(60)      0.6447      0.7995      0.8703      0.7995      0.7995      0.7995      0.9499
[2022/12/29 01:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:17] | ####################################################################################################
[2022/12/29 01:17] | TRAIN(061): [ 50/879] Batch: 0.1255 (0.1698) Data: 0.0090 (0.0481) Loss: 0.5426 (0.4440)
[2022/12/29 01:18] | TRAIN(061): [100/879] Batch: 0.1391 (0.1513) Data: 0.0100 (0.0288) Loss: 0.5909 (0.4373)
[2022/12/29 01:18] | TRAIN(061): [150/879] Batch: 0.1129 (0.1439) Data: 0.0103 (0.0221) Loss: 0.4312 (0.4396)
[2022/12/29 01:18] | TRAIN(061): [200/879] Batch: 0.1403 (0.1423) Data: 0.0093 (0.0191) Loss: 0.5164 (0.4379)
[2022/12/29 01:18] | TRAIN(061): [250/879] Batch: 0.1288 (0.1409) Data: 0.0090 (0.0173) Loss: 0.1268 (0.4332)
[2022/12/29 01:18] | TRAIN(061): [300/879] Batch: 0.1292 (0.1388) Data: 0.0108 (0.0158) Loss: 0.4244 (0.4267)
[2022/12/29 01:18] | TRAIN(061): [350/879] Batch: 0.1495 (0.1388) Data: 0.0100 (0.0150) Loss: 0.2679 (0.4305)
[2022/12/29 01:18] | TRAIN(061): [400/879] Batch: 0.1267 (0.1386) Data: 0.0114 (0.0144) Loss: 0.9023 (0.4307)
[2022/12/29 01:18] | TRAIN(061): [450/879] Batch: 0.1224 (0.1376) Data: 0.0101 (0.0137) Loss: 0.6540 (0.4305)
[2022/12/29 01:18] | TRAIN(061): [500/879] Batch: 0.1451 (0.1366) Data: 0.0071 (0.0132) Loss: 0.5830 (0.4299)
[2022/12/29 01:19] | TRAIN(061): [550/879] Batch: 0.1185 (0.1361) Data: 0.0063 (0.0128) Loss: 0.4236 (0.4325)
[2022/12/29 01:19] | TRAIN(061): [600/879] Batch: 0.1222 (0.1356) Data: 0.0102 (0.0125) Loss: 0.3386 (0.4344)
[2022/12/29 01:19] | TRAIN(061): [650/879] Batch: 0.1224 (0.1352) Data: 0.0089 (0.0122) Loss: 0.4576 (0.4352)
[2022/12/29 01:19] | TRAIN(061): [700/879] Batch: 0.1168 (0.1352) Data: 0.0078 (0.0120) Loss: 0.3004 (0.4371)
[2022/12/29 01:19] | TRAIN(061): [750/879] Batch: 0.1181 (0.1351) Data: 0.0085 (0.0118) Loss: 0.3318 (0.4385)
[2022/12/29 01:19] | TRAIN(061): [800/879] Batch: 0.0786 (0.1343) Data: 0.0096 (0.0116) Loss: 0.3666 (0.4388)
[2022/12/29 01:19] | TRAIN(061): [850/879] Batch: 0.1329 (0.1334) Data: 0.0096 (0.0114) Loss: 0.3811 (0.4397)
[2022/12/29 01:19] | ------------------------------------------------------------
[2022/12/29 01:19] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:19] | ------------------------------------------------------------
[2022/12/29 01:19] |    TRAIN(61)     0:01:57     0:00:09     0:01:47      0.4388
[2022/12/29 01:19] | ------------------------------------------------------------
[2022/12/29 01:19] | VALID(061): [ 50/220] Batch: 0.0223 (0.0686) Data: 0.0081 (0.0556) Loss: 0.6641 (0.5843)
[2022/12/29 01:19] | VALID(061): [100/220] Batch: 0.0226 (0.0497) Data: 0.0079 (0.0366) Loss: 1.1489 (0.6217)
[2022/12/29 01:19] | VALID(061): [150/220] Batch: 0.0314 (0.0464) Data: 0.0310 (0.0339) Loss: 0.6351 (0.6314)
[2022/12/29 01:19] | VALID(061): [200/220] Batch: 0.0488 (0.0453) Data: 0.0299 (0.0330) Loss: 0.2631 (0.6269)
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] |    VALID(61)      0.6230      0.8080      0.8702      0.8080      0.8080      0.8080      0.9520
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] | ####################################################################################################
[2022/12/29 01:20] | TRAIN(062): [ 50/879] Batch: 0.1013 (0.1697) Data: 0.0072 (0.0471) Loss: 0.2284 (0.4196)
[2022/12/29 01:20] | TRAIN(062): [100/879] Batch: 0.1084 (0.1510) Data: 0.0091 (0.0283) Loss: 0.4976 (0.4283)
[2022/12/29 01:20] | TRAIN(062): [150/879] Batch: 0.1290 (0.1435) Data: 0.0082 (0.0216) Loss: 0.2399 (0.4125)
[2022/12/29 01:20] | TRAIN(062): [200/879] Batch: 0.1435 (0.1403) Data: 0.0071 (0.0184) Loss: 0.8083 (0.4107)
[2022/12/29 01:20] | TRAIN(062): [250/879] Batch: 0.1199 (0.1392) Data: 0.0084 (0.0166) Loss: 0.5908 (0.4199)
[2022/12/29 01:20] | TRAIN(062): [300/879] Batch: 0.1204 (0.1374) Data: 0.0082 (0.0152) Loss: 0.2962 (0.4205)
[2022/12/29 01:20] | TRAIN(062): [350/879] Batch: 0.1263 (0.1361) Data: 0.0078 (0.0143) Loss: 0.3313 (0.4287)
[2022/12/29 01:20] | TRAIN(062): [400/879] Batch: 0.1389 (0.1352) Data: 0.0071 (0.0135) Loss: 0.2378 (0.4261)
[2022/12/29 01:20] | TRAIN(062): [450/879] Batch: 0.1415 (0.1352) Data: 0.0070 (0.0131) Loss: 0.5328 (0.4268)
[2022/12/29 01:21] | TRAIN(062): [500/879] Batch: 0.1242 (0.1345) Data: 0.0099 (0.0126) Loss: 0.4963 (0.4291)
[2022/12/29 01:21] | TRAIN(062): [550/879] Batch: 0.1222 (0.1340) Data: 0.0081 (0.0122) Loss: 0.5880 (0.4312)
[2022/12/29 01:21] | TRAIN(062): [600/879] Batch: 0.1223 (0.1337) Data: 0.0087 (0.0119) Loss: 0.2820 (0.4313)
[2022/12/29 01:21] | TRAIN(062): [650/879] Batch: 0.1230 (0.1334) Data: 0.0105 (0.0116) Loss: 0.4674 (0.4336)
[2022/12/29 01:21] | TRAIN(062): [700/879] Batch: 0.1427 (0.1332) Data: 0.0077 (0.0114) Loss: 0.5854 (0.4323)
[2022/12/29 01:21] | TRAIN(062): [750/879] Batch: 0.1212 (0.1329) Data: 0.0080 (0.0112) Loss: 0.5712 (0.4319)
[2022/12/29 01:21] | TRAIN(062): [800/879] Batch: 0.1229 (0.1327) Data: 0.0091 (0.0110) Loss: 0.8128 (0.4329)
[2022/12/29 01:21] | TRAIN(062): [850/879] Batch: 0.1246 (0.1325) Data: 0.0081 (0.0109) Loss: 0.3817 (0.4322)
[2022/12/29 01:21] | ------------------------------------------------------------
[2022/12/29 01:21] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:21] | ------------------------------------------------------------
[2022/12/29 01:21] |    TRAIN(62)     0:01:56     0:00:09     0:01:46      0.4331
[2022/12/29 01:21] | ------------------------------------------------------------
[2022/12/29 01:21] | VALID(062): [ 50/220] Batch: 0.0440 (0.0713) Data: 0.0384 (0.0601) Loss: 0.5701 (0.5981)
[2022/12/29 01:21] | VALID(062): [100/220] Batch: 0.0477 (0.0566) Data: 0.0261 (0.0458) Loss: 1.0940 (0.6237)
[2022/12/29 01:22] | VALID(062): [150/220] Batch: 0.0475 (0.0516) Data: 0.0244 (0.0409) Loss: 0.6811 (0.6291)
[2022/12/29 01:22] | VALID(062): [200/220] Batch: 0.0444 (0.0492) Data: 0.0226 (0.0385) Loss: 0.4551 (0.6304)
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] |    VALID(62)      0.6278      0.8080      0.8714      0.8080      0.8080      0.8080      0.9520
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] | ####################################################################################################
[2022/12/29 01:22] | TRAIN(063): [ 50/879] Batch: 0.1293 (0.1702) Data: 0.0118 (0.0488) Loss: 0.3874 (0.4536)
[2022/12/29 01:22] | TRAIN(063): [100/879] Batch: 0.1377 (0.1525) Data: 0.0082 (0.0293) Loss: 0.3596 (0.4483)
[2022/12/29 01:22] | TRAIN(063): [150/879] Batch: 0.1240 (0.1450) Data: 0.0085 (0.0224) Loss: 0.5594 (0.4248)
[2022/12/29 01:22] | TRAIN(063): [200/879] Batch: 0.1434 (0.1413) Data: 0.0076 (0.0191) Loss: 0.4767 (0.4280)
[2022/12/29 01:22] | TRAIN(063): [250/879] Batch: 0.0843 (0.1371) Data: 0.0085 (0.0170) Loss: 0.4103 (0.4228)
[2022/12/29 01:22] | TRAIN(063): [300/879] Batch: 0.1328 (0.1356) Data: 0.0105 (0.0159) Loss: 0.3077 (0.4211)
[2022/12/29 01:22] | TRAIN(063): [350/879] Batch: 0.0861 (0.1349) Data: 0.0092 (0.0149) Loss: 0.9545 (0.4190)
[2022/12/29 01:22] | TRAIN(063): [400/879] Batch: 0.1221 (0.1316) Data: 0.0081 (0.0141) Loss: 0.8393 (0.4199)
[2022/12/29 01:23] | TRAIN(063): [450/879] Batch: 0.1202 (0.1317) Data: 0.0080 (0.0136) Loss: 0.2932 (0.4212)
[2022/12/29 01:23] | TRAIN(063): [500/879] Batch: 0.1250 (0.1315) Data: 0.0096 (0.0131) Loss: 0.6348 (0.4219)
[2022/12/29 01:23] | TRAIN(063): [550/879] Batch: 0.1408 (0.1313) Data: 0.0073 (0.0126) Loss: 0.3167 (0.4237)
[2022/12/29 01:23] | TRAIN(063): [600/879] Batch: 0.1220 (0.1311) Data: 0.0081 (0.0123) Loss: 0.3275 (0.4247)
[2022/12/29 01:23] | TRAIN(063): [650/879] Batch: 0.1285 (0.1314) Data: 0.0101 (0.0120) Loss: 0.6143 (0.4258)
[2022/12/29 01:23] | TRAIN(063): [700/879] Batch: 0.1273 (0.1316) Data: 0.0105 (0.0118) Loss: 0.1930 (0.4274)
[2022/12/29 01:23] | TRAIN(063): [750/879] Batch: 0.1327 (0.1321) Data: 0.0096 (0.0117) Loss: 0.4690 (0.4294)
[2022/12/29 01:23] | TRAIN(063): [800/879] Batch: 0.1349 (0.1321) Data: 0.0094 (0.0115) Loss: 0.5023 (0.4309)
[2022/12/29 01:23] | TRAIN(063): [850/879] Batch: 0.1468 (0.1325) Data: 0.0113 (0.0115) Loss: 0.5188 (0.4332)
[2022/12/29 01:24] | ------------------------------------------------------------
[2022/12/29 01:24] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:24] | ------------------------------------------------------------
[2022/12/29 01:24] |    TRAIN(63)     0:01:56     0:00:10     0:01:46      0.4335
[2022/12/29 01:24] | ------------------------------------------------------------
[2022/12/29 01:24] | VALID(063): [ 50/220] Batch: 0.0311 (0.0723) Data: 0.0393 (0.0618) Loss: 0.5665 (0.5726)
[2022/12/29 01:24] | VALID(063): [100/220] Batch: 0.0496 (0.0571) Data: 0.0340 (0.0464) Loss: 1.1076 (0.6218)
[2022/12/29 01:24] | VALID(063): [150/220] Batch: 0.0285 (0.0518) Data: 0.0397 (0.0409) Loss: 0.5425 (0.6308)
[2022/12/29 01:24] | VALID(063): [200/220] Batch: 0.0479 (0.0494) Data: 0.0345 (0.0383) Loss: 0.5008 (0.6308)
[2022/12/29 01:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:24] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:24] |    VALID(63)      0.6289      0.8134      0.8722      0.8134      0.8134      0.8134      0.9534
[2022/12/29 01:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:24] | ####################################################################################################
[2022/12/29 01:24] | TRAIN(064): [ 50/879] Batch: 0.1298 (0.1712) Data: 0.0111 (0.0488) Loss: 0.3473 (0.3906)
[2022/12/29 01:24] | TRAIN(064): [100/879] Batch: 0.1188 (0.1528) Data: 0.0082 (0.0299) Loss: 0.4369 (0.4088)
[2022/12/29 01:24] | TRAIN(064): [150/879] Batch: 0.1301 (0.1465) Data: 0.0110 (0.0235) Loss: 0.2681 (0.4122)
[2022/12/29 01:24] | TRAIN(064): [200/879] Batch: 0.1185 (0.1428) Data: 0.0083 (0.0200) Loss: 0.2725 (0.4151)
[2022/12/29 01:24] | TRAIN(064): [250/879] Batch: 0.1266 (0.1408) Data: 0.0075 (0.0179) Loss: 0.4676 (0.4193)
[2022/12/29 01:24] | TRAIN(064): [300/879] Batch: 0.1226 (0.1395) Data: 0.0087 (0.0164) Loss: 0.3194 (0.4184)
[2022/12/29 01:25] | TRAIN(064): [350/879] Batch: 0.1236 (0.1385) Data: 0.0092 (0.0154) Loss: 0.2881 (0.4252)
[2022/12/29 01:25] | TRAIN(064): [400/879] Batch: 0.1443 (0.1378) Data: 0.0095 (0.0146) Loss: 0.4911 (0.4270)
[2022/12/29 01:25] | TRAIN(064): [450/879] Batch: 0.1190 (0.1373) Data: 0.0083 (0.0140) Loss: 0.3312 (0.4283)
[2022/12/29 01:25] | TRAIN(064): [500/879] Batch: 0.1218 (0.1365) Data: 0.0100 (0.0135) Loss: 0.2436 (0.4237)
[2022/12/29 01:25] | TRAIN(064): [550/879] Batch: 0.1417 (0.1360) Data: 0.0075 (0.0131) Loss: 0.3117 (0.4214)
[2022/12/29 01:25] | TRAIN(064): [600/879] Batch: 0.1439 (0.1357) Data: 0.0088 (0.0127) Loss: 0.3758 (0.4210)
[2022/12/29 01:25] | TRAIN(064): [650/879] Batch: 0.0802 (0.1344) Data: 0.0101 (0.0124) Loss: 0.4535 (0.4212)
[2022/12/29 01:25] | TRAIN(064): [700/879] Batch: 0.1414 (0.1338) Data: 0.0082 (0.0122) Loss: 0.4551 (0.4192)
[2022/12/29 01:25] | TRAIN(064): [750/879] Batch: 0.1268 (0.1336) Data: 0.0084 (0.0120) Loss: 0.4804 (0.4202)
[2022/12/29 01:25] | TRAIN(064): [800/879] Batch: 0.1419 (0.1317) Data: 0.0070 (0.0117) Loss: 0.2726 (0.4210)
[2022/12/29 01:26] | TRAIN(064): [850/879] Batch: 0.1227 (0.1317) Data: 0.0082 (0.0116) Loss: 0.5711 (0.4209)
[2022/12/29 01:26] | ------------------------------------------------------------
[2022/12/29 01:26] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:26] | ------------------------------------------------------------
[2022/12/29 01:26] |    TRAIN(64)     0:01:55     0:00:10     0:01:45      0.4215
[2022/12/29 01:26] | ------------------------------------------------------------
[2022/12/29 01:26] | VALID(064): [ 50/220] Batch: 0.0465 (0.0730) Data: 0.0343 (0.0593) Loss: 0.5553 (0.5813)
[2022/12/29 01:26] | VALID(064): [100/220] Batch: 0.0444 (0.0573) Data: 0.0318 (0.0453) Loss: 1.3207 (0.6404)
[2022/12/29 01:26] | VALID(064): [150/220] Batch: 0.0477 (0.0523) Data: 0.0244 (0.0406) Loss: 0.6776 (0.6492)
[2022/12/29 01:26] | VALID(064): [200/220] Batch: 0.0482 (0.0496) Data: 0.0238 (0.0383) Loss: 0.4770 (0.6501)
[2022/12/29 01:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:26] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:26] |    VALID(64)      0.6472      0.8222      0.8763      0.8222      0.8222      0.8222      0.9556
[2022/12/29 01:26] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:26] | ####################################################################################################
[2022/12/29 01:26] | TRAIN(065): [ 50/879] Batch: 0.1491 (0.1777) Data: 0.0182 (0.0526) Loss: 0.3964 (0.4493)
[2022/12/29 01:26] | TRAIN(065): [100/879] Batch: 0.1210 (0.1556) Data: 0.0185 (0.0320) Loss: 0.4068 (0.4384)
[2022/12/29 01:26] | TRAIN(065): [150/879] Batch: 0.1227 (0.1483) Data: 0.0099 (0.0250) Loss: 0.4629 (0.4228)
[2022/12/29 01:26] | TRAIN(065): [200/879] Batch: 0.1413 (0.1451) Data: 0.0119 (0.0217) Loss: 0.4299 (0.4266)
[2022/12/29 01:26] | TRAIN(065): [250/879] Batch: 0.1433 (0.1425) Data: 0.0071 (0.0193) Loss: 0.3194 (0.4269)
[2022/12/29 01:27] | TRAIN(065): [300/879] Batch: 0.1389 (0.1409) Data: 0.0108 (0.0179) Loss: 0.3839 (0.4290)
[2022/12/29 01:27] | TRAIN(065): [350/879] Batch: 0.1184 (0.1395) Data: 0.0081 (0.0166) Loss: 0.4720 (0.4326)
[2022/12/29 01:27] | TRAIN(065): [400/879] Batch: 0.1450 (0.1385) Data: 0.0074 (0.0158) Loss: 0.5542 (0.4349)
[2022/12/29 01:27] | TRAIN(065): [450/879] Batch: 0.1369 (0.1380) Data: 0.0217 (0.0151) Loss: 0.3960 (0.4318)
[2022/12/29 01:27] | TRAIN(065): [500/879] Batch: 0.1452 (0.1376) Data: 0.0097 (0.0146) Loss: 0.7299 (0.4304)
[2022/12/29 01:27] | TRAIN(065): [550/879] Batch: 0.1355 (0.1371) Data: 0.0088 (0.0141) Loss: 0.6337 (0.4316)
[2022/12/29 01:27] | TRAIN(065): [600/879] Batch: 0.1362 (0.1368) Data: 0.0077 (0.0137) Loss: 0.5804 (0.4288)
[2022/12/29 01:27] | TRAIN(065): [650/879] Batch: 0.1214 (0.1364) Data: 0.0082 (0.0133) Loss: 0.6085 (0.4285)
[2022/12/29 01:27] | TRAIN(065): [700/879] Batch: 0.1223 (0.1360) Data: 0.0113 (0.0130) Loss: 0.1881 (0.4288)
[2022/12/29 01:28] | TRAIN(065): [750/879] Batch: 0.1480 (0.1357) Data: 0.0096 (0.0128) Loss: 0.3754 (0.4266)
[2022/12/29 01:28] | TRAIN(065): [800/879] Batch: 0.1078 (0.1356) Data: 0.0072 (0.0126) Loss: 0.5054 (0.4260)
[2022/12/29 01:28] | TRAIN(065): [850/879] Batch: 0.1159 (0.1354) Data: 0.0084 (0.0124) Loss: 0.4450 (0.4236)
[2022/12/29 01:28] | ------------------------------------------------------------
[2022/12/29 01:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:28] | ------------------------------------------------------------
[2022/12/29 01:28] |    TRAIN(65)     0:01:58     0:00:10     0:01:47      0.4232
[2022/12/29 01:28] | ------------------------------------------------------------
[2022/12/29 01:28] | VALID(065): [ 50/220] Batch: 0.0322 (0.0710) Data: 0.0378 (0.0591) Loss: 0.5875 (0.6154)
[2022/12/29 01:28] | VALID(065): [100/220] Batch: 0.0267 (0.0563) Data: 0.0390 (0.0450) Loss: 1.3263 (0.6645)
[2022/12/29 01:28] | VALID(065): [150/220] Batch: 0.0475 (0.0512) Data: 0.0332 (0.0400) Loss: 0.6143 (0.6706)
[2022/12/29 01:28] | VALID(065): [200/220] Batch: 0.0441 (0.0488) Data: 0.0335 (0.0377) Loss: 0.4247 (0.6689)
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] |    VALID(65)      0.6666      0.8161      0.8750      0.8161      0.8161      0.8161      0.9540
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] | ####################################################################################################
[2022/12/29 01:28] | TRAIN(066): [ 50/879] Batch: 0.1438 (0.1732) Data: 0.0072 (0.0493) Loss: 0.3912 (0.3948)
[2022/12/29 01:28] | TRAIN(066): [100/879] Batch: 0.1435 (0.1436) Data: 0.0102 (0.0300) Loss: 0.5301 (0.3936)
[2022/12/29 01:28] | TRAIN(066): [150/879] Batch: 0.1401 (0.1418) Data: 0.0105 (0.0234) Loss: 0.5160 (0.4008)
[2022/12/29 01:28] | TRAIN(066): [200/879] Batch: 0.0928 (0.1319) Data: 0.0080 (0.0197) Loss: 0.6661 (0.4022)
[2022/12/29 01:29] | TRAIN(066): [250/879] Batch: 0.1178 (0.1317) Data: 0.0101 (0.0176) Loss: 0.4014 (0.3964)
[2022/12/29 01:29] | TRAIN(066): [300/879] Batch: 0.1233 (0.1319) Data: 0.0096 (0.0162) Loss: 0.4431 (0.4041)
[2022/12/29 01:29] | TRAIN(066): [350/879] Batch: 0.1238 (0.1316) Data: 0.0113 (0.0151) Loss: 0.2494 (0.4074)
[2022/12/29 01:29] | TRAIN(066): [400/879] Batch: 0.1222 (0.1312) Data: 0.0082 (0.0143) Loss: 0.5193 (0.4063)
[2022/12/29 01:29] | TRAIN(066): [450/879] Batch: 0.1373 (0.1311) Data: 0.0079 (0.0136) Loss: 0.2994 (0.4070)
[2022/12/29 01:29] | TRAIN(066): [500/879] Batch: 0.1330 (0.1317) Data: 0.0100 (0.0133) Loss: 0.3742 (0.4087)
[2022/12/29 01:29] | TRAIN(066): [550/879] Batch: 0.1322 (0.1318) Data: 0.0116 (0.0129) Loss: 0.3215 (0.4136)
[2022/12/29 01:29] | TRAIN(066): [600/879] Batch: 0.1438 (0.1320) Data: 0.0071 (0.0126) Loss: 0.3850 (0.4158)
[2022/12/29 01:29] | TRAIN(066): [650/879] Batch: 0.1389 (0.1323) Data: 0.0098 (0.0124) Loss: 0.2800 (0.4147)
[2022/12/29 01:30] | TRAIN(066): [700/879] Batch: 0.1494 (0.1322) Data: 0.0093 (0.0122) Loss: 0.3954 (0.4155)
[2022/12/29 01:30] | TRAIN(066): [750/879] Batch: 0.1442 (0.1323) Data: 0.0073 (0.0120) Loss: 0.3555 (0.4159)
[2022/12/29 01:30] | TRAIN(066): [800/879] Batch: 0.1216 (0.1320) Data: 0.0084 (0.0117) Loss: 0.9140 (0.4148)
[2022/12/29 01:30] | TRAIN(066): [850/879] Batch: 0.1212 (0.1318) Data: 0.0083 (0.0115) Loss: 0.6085 (0.4136)
[2022/12/29 01:30] | ------------------------------------------------------------
[2022/12/29 01:30] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:30] | ------------------------------------------------------------
[2022/12/29 01:30] |    TRAIN(66)     0:01:55     0:00:10     0:01:45      0.4141
[2022/12/29 01:30] | ------------------------------------------------------------
[2022/12/29 01:30] | VALID(066): [ 50/220] Batch: 0.0428 (0.0720) Data: 0.0190 (0.0594) Loss: 0.6151 (0.6163)
[2022/12/29 01:30] | VALID(066): [100/220] Batch: 0.0443 (0.0565) Data: 0.0364 (0.0450) Loss: 1.2661 (0.6604)
[2022/12/29 01:30] | VALID(066): [150/220] Batch: 0.0480 (0.0515) Data: 0.0258 (0.0404) Loss: 0.6787 (0.6693)
[2022/12/29 01:30] | VALID(066): [200/220] Batch: 0.0437 (0.0490) Data: 0.0292 (0.0369) Loss: 0.3639 (0.6680)
[2022/12/29 01:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:30] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:30] |    VALID(66)      0.6668      0.8116      0.8715      0.8116      0.8116      0.8116      0.9529
[2022/12/29 01:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:30] | ####################################################################################################
[2022/12/29 01:30] | TRAIN(067): [ 50/879] Batch: 0.1379 (0.1713) Data: 0.0073 (0.0498) Loss: 0.6162 (0.3880)
[2022/12/29 01:30] | TRAIN(067): [100/879] Batch: 0.1259 (0.1505) Data: 0.0115 (0.0298) Loss: 0.4210 (0.3979)
[2022/12/29 01:30] | TRAIN(067): [150/879] Batch: 0.1433 (0.1439) Data: 0.0078 (0.0229) Loss: 0.4659 (0.4135)
[2022/12/29 01:31] | TRAIN(067): [200/879] Batch: 0.1394 (0.1406) Data: 0.0074 (0.0194) Loss: 0.6336 (0.4046)
[2022/12/29 01:31] | TRAIN(067): [250/879] Batch: 0.1221 (0.1382) Data: 0.0085 (0.0172) Loss: 0.2287 (0.4047)
[2022/12/29 01:31] | TRAIN(067): [300/879] Batch: 0.1316 (0.1372) Data: 0.0082 (0.0159) Loss: 0.4305 (0.4057)
[2022/12/29 01:31] | TRAIN(067): [350/879] Batch: 0.1245 (0.1363) Data: 0.0080 (0.0149) Loss: 0.2916 (0.4125)
[2022/12/29 01:31] | TRAIN(067): [400/879] Batch: 0.1174 (0.1355) Data: 0.0076 (0.0140) Loss: 0.3018 (0.4122)
[2022/12/29 01:31] | TRAIN(067): [450/879] Batch: 0.1415 (0.1348) Data: 0.0095 (0.0135) Loss: 0.4266 (0.4100)
[2022/12/29 01:31] | TRAIN(067): [500/879] Batch: 0.1366 (0.1327) Data: 0.0100 (0.0130) Loss: 0.4854 (0.4075)
[2022/12/29 01:31] | TRAIN(067): [550/879] Batch: 0.1391 (0.1325) Data: 0.0096 (0.0127) Loss: 0.4522 (0.4082)
[2022/12/29 01:31] | TRAIN(067): [600/879] Batch: 0.0740 (0.1315) Data: 0.0075 (0.0123) Loss: 0.3185 (0.4062)
[2022/12/29 01:32] | TRAIN(067): [650/879] Batch: 0.1426 (0.1303) Data: 0.0075 (0.0121) Loss: 0.5269 (0.4059)
[2022/12/29 01:32] | TRAIN(067): [700/879] Batch: 0.1356 (0.1303) Data: 0.0089 (0.0118) Loss: 0.4156 (0.4070)
[2022/12/29 01:32] | TRAIN(067): [750/879] Batch: 0.1171 (0.1301) Data: 0.0080 (0.0116) Loss: 0.3129 (0.4076)
[2022/12/29 01:32] | TRAIN(067): [800/879] Batch: 0.1222 (0.1300) Data: 0.0082 (0.0114) Loss: 0.4408 (0.4072)
[2022/12/29 01:32] | TRAIN(067): [850/879] Batch: 0.1335 (0.1300) Data: 0.0087 (0.0112) Loss: 0.6994 (0.4084)
[2022/12/29 01:32] | ------------------------------------------------------------
[2022/12/29 01:32] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:32] | ------------------------------------------------------------
[2022/12/29 01:32] |    TRAIN(67)     0:01:54     0:00:09     0:01:44      0.4094
[2022/12/29 01:32] | ------------------------------------------------------------
[2022/12/29 01:32] | VALID(067): [ 50/220] Batch: 0.0277 (0.0701) Data: 0.0366 (0.0559) Loss: 0.5491 (0.6052)
[2022/12/29 01:32] | VALID(067): [100/220] Batch: 0.0494 (0.0560) Data: 0.0337 (0.0427) Loss: 1.4460 (0.6522)
[2022/12/29 01:32] | VALID(067): [150/220] Batch: 0.0444 (0.0512) Data: 0.0342 (0.0387) Loss: 0.6549 (0.6597)
[2022/12/29 01:32] | VALID(067): [200/220] Batch: 0.0478 (0.0489) Data: 0.0347 (0.0368) Loss: 0.3601 (0.6609)
[2022/12/29 01:32] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:32] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:32] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:32] |    VALID(67)      0.6564      0.8128      0.8725      0.8128      0.8128      0.8128      0.9532
[2022/12/29 01:32] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:32] | ####################################################################################################
[2022/12/29 01:32] | TRAIN(068): [ 50/879] Batch: 0.1252 (0.1674) Data: 0.0090 (0.0454) Loss: 0.2344 (0.4056)
[2022/12/29 01:32] | TRAIN(068): [100/879] Batch: 0.1508 (0.1498) Data: 0.0098 (0.0274) Loss: 0.5259 (0.4035)
[2022/12/29 01:33] | TRAIN(068): [150/879] Batch: 0.1214 (0.1441) Data: 0.0081 (0.0212) Loss: 0.4998 (0.3986)
[2022/12/29 01:33] | TRAIN(068): [200/879] Batch: 0.1341 (0.1403) Data: 0.0084 (0.0180) Loss: 0.3390 (0.4001)
[2022/12/29 01:33] | TRAIN(068): [250/879] Batch: 0.1198 (0.1381) Data: 0.0079 (0.0160) Loss: 0.3784 (0.3987)
[2022/12/29 01:33] | TRAIN(068): [300/879] Batch: 0.1213 (0.1365) Data: 0.0092 (0.0147) Loss: 0.3879 (0.3993)
[2022/12/29 01:33] | TRAIN(068): [350/879] Batch: 0.1327 (0.1360) Data: 0.0076 (0.0139) Loss: 0.3716 (0.3959)
[2022/12/29 01:33] | TRAIN(068): [400/879] Batch: 0.1327 (0.1351) Data: 0.0080 (0.0133) Loss: 0.4335 (0.4001)
[2022/12/29 01:33] | TRAIN(068): [450/879] Batch: 0.1396 (0.1346) Data: 0.0076 (0.0128) Loss: 0.4574 (0.4019)
[2022/12/29 01:33] | TRAIN(068): [500/879] Batch: 0.1312 (0.1343) Data: 0.0091 (0.0125) Loss: 0.5761 (0.4000)
[2022/12/29 01:33] | TRAIN(068): [550/879] Batch: 0.1414 (0.1338) Data: 0.0075 (0.0120) Loss: 0.2890 (0.3992)
[2022/12/29 01:34] | TRAIN(068): [600/879] Batch: 0.1426 (0.1334) Data: 0.0092 (0.0117) Loss: 0.4490 (0.3968)
[2022/12/29 01:34] | TRAIN(068): [650/879] Batch: 0.1176 (0.1332) Data: 0.0082 (0.0115) Loss: 0.3404 (0.3971)
[2022/12/29 01:34] | TRAIN(068): [700/879] Batch: 0.1161 (0.1328) Data: 0.0080 (0.0113) Loss: 0.4909 (0.3978)
[2022/12/29 01:34] | TRAIN(068): [750/879] Batch: 0.1198 (0.1325) Data: 0.0081 (0.0111) Loss: 0.3146 (0.4005)
[2022/12/29 01:34] | TRAIN(068): [800/879] Batch: 0.1417 (0.1323) Data: 0.0072 (0.0109) Loss: 0.3201 (0.4026)
[2022/12/29 01:34] | TRAIN(068): [850/879] Batch: 0.1480 (0.1323) Data: 0.0100 (0.0108) Loss: 0.4029 (0.4026)
[2022/12/29 01:34] | ------------------------------------------------------------
[2022/12/29 01:34] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:34] | ------------------------------------------------------------
[2022/12/29 01:34] |    TRAIN(68)     0:01:56     0:00:09     0:01:46      0.4026
[2022/12/29 01:34] | ------------------------------------------------------------
[2022/12/29 01:34] | VALID(068): [ 50/220] Batch: 0.0473 (0.0730) Data: 0.0258 (0.0598) Loss: 0.5920 (0.6286)
[2022/12/29 01:34] | VALID(068): [100/220] Batch: 0.0365 (0.0517) Data: 0.0075 (0.0385) Loss: 1.2228 (0.6716)
[2022/12/29 01:34] | VALID(068): [150/220] Batch: 0.0544 (0.0453) Data: 0.0311 (0.0323) Loss: 0.6723 (0.6712)
[2022/12/29 01:34] | VALID(068): [200/220] Batch: 0.0416 (0.0447) Data: 0.0452 (0.0323) Loss: 0.3563 (0.6719)
[2022/12/29 01:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:34] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:34] |    VALID(68)      0.6711      0.8104      0.8718      0.8104      0.8104      0.8104      0.9526
[2022/12/29 01:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:34] | ####################################################################################################
[2022/12/29 01:34] | TRAIN(069): [ 50/879] Batch: 0.0833 (0.1626) Data: 0.0120 (0.0491) Loss: 0.4077 (0.3967)
[2022/12/29 01:35] | TRAIN(069): [100/879] Batch: 0.1428 (0.1410) Data: 0.0085 (0.0300) Loss: 0.2315 (0.3990)
[2022/12/29 01:35] | TRAIN(069): [150/879] Batch: 0.1182 (0.1380) Data: 0.0080 (0.0235) Loss: 0.4007 (0.4032)
[2022/12/29 01:35] | TRAIN(069): [200/879] Batch: 0.1386 (0.1372) Data: 0.0096 (0.0204) Loss: 0.4970 (0.3950)
[2022/12/29 01:35] | TRAIN(069): [250/879] Batch: 0.1549 (0.1362) Data: 0.0080 (0.0184) Loss: 0.3874 (0.3979)
[2022/12/29 01:35] | TRAIN(069): [300/879] Batch: 0.1221 (0.1348) Data: 0.0082 (0.0168) Loss: 0.5323 (0.3985)
[2022/12/29 01:35] | TRAIN(069): [350/879] Batch: 0.1154 (0.1343) Data: 0.0122 (0.0157) Loss: 0.5531 (0.4013)
[2022/12/29 01:35] | TRAIN(069): [400/879] Batch: 0.1425 (0.1338) Data: 0.0070 (0.0148) Loss: 0.5661 (0.4003)
[2022/12/29 01:35] | TRAIN(069): [450/879] Batch: 0.1339 (0.1334) Data: 0.0089 (0.0141) Loss: 0.2653 (0.3987)
[2022/12/29 01:35] | TRAIN(069): [500/879] Batch: 0.1223 (0.1331) Data: 0.0081 (0.0136) Loss: 0.4481 (0.3982)
[2022/12/29 01:36] | TRAIN(069): [550/879] Batch: 0.1256 (0.1329) Data: 0.0080 (0.0131) Loss: 0.5747 (0.3988)
[2022/12/29 01:36] | TRAIN(069): [600/879] Batch: 0.1210 (0.1328) Data: 0.0079 (0.0128) Loss: 0.1897 (0.3968)
[2022/12/29 01:36] | TRAIN(069): [650/879] Batch: 0.1315 (0.1326) Data: 0.0115 (0.0125) Loss: 0.4465 (0.3962)
[2022/12/29 01:36] | TRAIN(069): [700/879] Batch: 0.1426 (0.1325) Data: 0.0099 (0.0122) Loss: 0.3755 (0.3968)
[2022/12/29 01:36] | TRAIN(069): [750/879] Batch: 0.1198 (0.1328) Data: 0.0081 (0.0121) Loss: 0.3518 (0.3955)
[2022/12/29 01:36] | TRAIN(069): [800/879] Batch: 0.1473 (0.1327) Data: 0.0107 (0.0119) Loss: 0.4346 (0.3964)
[2022/12/29 01:36] | TRAIN(069): [850/879] Batch: 0.1214 (0.1325) Data: 0.0080 (0.0118) Loss: 0.2395 (0.3965)
[2022/12/29 01:36] | ------------------------------------------------------------
[2022/12/29 01:36] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:36] | ------------------------------------------------------------
[2022/12/29 01:36] |    TRAIN(69)     0:01:56     0:00:10     0:01:46      0.3974
[2022/12/29 01:36] | ------------------------------------------------------------
[2022/12/29 01:36] | VALID(069): [ 50/220] Batch: 0.0477 (0.0725) Data: 0.0233 (0.0603) Loss: 0.6590 (0.6438)
[2022/12/29 01:36] | VALID(069): [100/220] Batch: 0.0444 (0.0569) Data: 0.0303 (0.0457) Loss: 1.4197 (0.6903)
[2022/12/29 01:36] | VALID(069): [150/220] Batch: 0.0486 (0.0518) Data: 0.0332 (0.0409) Loss: 0.6565 (0.6934)
[2022/12/29 01:36] | VALID(069): [200/220] Batch: 0.0424 (0.0493) Data: 0.0151 (0.0381) Loss: 0.4992 (0.6898)
[2022/12/29 01:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:36] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:36] |    VALID(69)      0.6886      0.8138      0.8718      0.8138      0.8138      0.8138      0.9535
[2022/12/29 01:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:36] | ####################################################################################################
[2022/12/29 01:37] | TRAIN(070): [ 50/879] Batch: 0.1379 (0.1763) Data: 0.0081 (0.0509) Loss: 0.1225 (0.3785)
[2022/12/29 01:37] | TRAIN(070): [100/879] Batch: 0.1418 (0.1564) Data: 0.0103 (0.0308) Loss: 0.4420 (0.3780)
[2022/12/29 01:37] | TRAIN(070): [150/879] Batch: 0.1224 (0.1471) Data: 0.0082 (0.0235) Loss: 0.5982 (0.3865)
[2022/12/29 01:37] | TRAIN(070): [200/879] Batch: 0.1201 (0.1431) Data: 0.0187 (0.0199) Loss: 0.3598 (0.3924)
[2022/12/29 01:37] | TRAIN(070): [250/879] Batch: 0.1432 (0.1408) Data: 0.0080 (0.0177) Loss: 0.5453 (0.3967)
[2022/12/29 01:37] | TRAIN(070): [300/879] Batch: 0.1254 (0.1392) Data: 0.0084 (0.0162) Loss: 0.4884 (0.3982)
[2022/12/29 01:37] | TRAIN(070): [350/879] Batch: 0.0781 (0.1367) Data: 0.0089 (0.0151) Loss: 0.3459 (0.4014)
[2022/12/29 01:37] | TRAIN(070): [400/879] Batch: 0.1384 (0.1348) Data: 0.0183 (0.0143) Loss: 0.2782 (0.4016)
[2022/12/29 01:37] | TRAIN(070): [450/879] Batch: 0.1404 (0.1346) Data: 0.0101 (0.0138) Loss: 0.2156 (0.4001)
[2022/12/29 01:38] | TRAIN(070): [500/879] Batch: 0.1245 (0.1319) Data: 0.0081 (0.0133) Loss: 0.3973 (0.4001)
[2022/12/29 01:38] | TRAIN(070): [550/879] Batch: 0.1278 (0.1323) Data: 0.0092 (0.0130) Loss: 0.5564 (0.3999)
[2022/12/29 01:38] | TRAIN(070): [600/879] Batch: 0.1217 (0.1321) Data: 0.0079 (0.0126) Loss: 0.7332 (0.3995)
[2022/12/29 01:38] | TRAIN(070): [650/879] Batch: 0.1219 (0.1320) Data: 0.0081 (0.0124) Loss: 0.3323 (0.3964)
[2022/12/29 01:38] | TRAIN(070): [700/879] Batch: 0.1377 (0.1318) Data: 0.0073 (0.0121) Loss: 0.2721 (0.3954)
[2022/12/29 01:38] | TRAIN(070): [750/879] Batch: 0.1221 (0.1317) Data: 0.0104 (0.0119) Loss: 0.2906 (0.3929)
[2022/12/29 01:38] | TRAIN(070): [800/879] Batch: 0.1213 (0.1315) Data: 0.0080 (0.0116) Loss: 0.5543 (0.3931)
[2022/12/29 01:38] | TRAIN(070): [850/879] Batch: 0.1424 (0.1314) Data: 0.0074 (0.0114) Loss: 0.2145 (0.3938)
[2022/12/29 01:38] | ------------------------------------------------------------
[2022/12/29 01:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:38] | ------------------------------------------------------------
[2022/12/29 01:38] |    TRAIN(70)     0:01:55     0:00:09     0:01:45      0.3935
[2022/12/29 01:38] | ------------------------------------------------------------
[2022/12/29 01:38] | VALID(070): [ 50/220] Batch: 0.0321 (0.0737) Data: 0.0145 (0.0581) Loss: 0.6743 (0.6288)
[2022/12/29 01:38] | VALID(070): [100/220] Batch: 0.0492 (0.0569) Data: 0.0290 (0.0420) Loss: 1.2748 (0.6784)
[2022/12/29 01:38] | VALID(070): [150/220] Batch: 0.0488 (0.0509) Data: 0.0222 (0.0366) Loss: 0.6030 (0.6846)
[2022/12/29 01:38] | VALID(070): [200/220] Batch: 0.0354 (0.0483) Data: 0.0383 (0.0339) Loss: 0.5380 (0.6864)
[2022/12/29 01:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:39] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:39] |    VALID(70)      0.6849      0.8123      0.8758      0.8123      0.8123      0.8123      0.9531
[2022/12/29 01:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:39] | ####################################################################################################
[2022/12/29 01:39] | TRAIN(071): [ 50/879] Batch: 0.1407 (0.1700) Data: 0.0080 (0.0496) Loss: 0.4284 (0.4009)
[2022/12/29 01:39] | TRAIN(071): [100/879] Batch: 0.1206 (0.1514) Data: 0.0081 (0.0295) Loss: 0.2994 (0.3986)
[2022/12/29 01:39] | TRAIN(071): [150/879] Batch: 0.1186 (0.1446) Data: 0.0085 (0.0226) Loss: 0.4806 (0.4077)
[2022/12/29 01:39] | TRAIN(071): [200/879] Batch: 0.1444 (0.1407) Data: 0.0074 (0.0190) Loss: 0.4995 (0.4150)
[2022/12/29 01:39] | TRAIN(071): [250/879] Batch: 0.1248 (0.1385) Data: 0.0082 (0.0169) Loss: 0.3496 (0.4068)
[2022/12/29 01:39] | TRAIN(071): [300/879] Batch: 0.1302 (0.1381) Data: 0.0100 (0.0157) Loss: 0.3095 (0.4030)
[2022/12/29 01:39] | TRAIN(071): [350/879] Batch: 0.1220 (0.1376) Data: 0.0082 (0.0148) Loss: 0.4130 (0.4026)
[2022/12/29 01:39] | TRAIN(071): [400/879] Batch: 0.1425 (0.1366) Data: 0.0076 (0.0140) Loss: 0.1267 (0.3964)
[2022/12/29 01:40] | TRAIN(071): [450/879] Batch: 0.1223 (0.1357) Data: 0.0090 (0.0133) Loss: 0.1664 (0.3929)
[2022/12/29 01:40] | TRAIN(071): [500/879] Batch: 0.1425 (0.1355) Data: 0.0074 (0.0129) Loss: 0.5432 (0.3912)
[2022/12/29 01:40] | TRAIN(071): [550/879] Batch: 0.1227 (0.1348) Data: 0.0098 (0.0125) Loss: 0.4376 (0.3934)
[2022/12/29 01:40] | TRAIN(071): [600/879] Batch: 0.1402 (0.1351) Data: 0.0107 (0.0123) Loss: 0.4323 (0.3917)
[2022/12/29 01:40] | TRAIN(071): [650/879] Batch: 0.1346 (0.1352) Data: 0.0082 (0.0121) Loss: 0.5426 (0.3938)
[2022/12/29 01:40] | TRAIN(071): [700/879] Batch: 0.1214 (0.1349) Data: 0.0080 (0.0118) Loss: 0.5454 (0.3923)
[2022/12/29 01:40] | TRAIN(071): [750/879] Batch: 0.1275 (0.1345) Data: 0.0109 (0.0116) Loss: 0.3206 (0.3910)
[2022/12/29 01:40] | TRAIN(071): [800/879] Batch: 0.1249 (0.1333) Data: 0.0088 (0.0115) Loss: 0.3334 (0.3910)
[2022/12/29 01:40] | TRAIN(071): [850/879] Batch: 0.1407 (0.1332) Data: 0.0094 (0.0113) Loss: 0.5702 (0.3896)
[2022/12/29 01:40] | ------------------------------------------------------------
[2022/12/29 01:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:40] | ------------------------------------------------------------
[2022/12/29 01:40] |    TRAIN(71)     0:01:56     0:00:09     0:01:47      0.3895
[2022/12/29 01:40] | ------------------------------------------------------------
[2022/12/29 01:41] | VALID(071): [ 50/220] Batch: 0.0280 (0.0666) Data: 0.0328 (0.0528) Loss: 0.5718 (0.6294)
[2022/12/29 01:41] | VALID(071): [100/220] Batch: 0.0473 (0.0544) Data: 0.0358 (0.0421) Loss: 1.3459 (0.6876)
[2022/12/29 01:41] | VALID(071): [150/220] Batch: 0.0437 (0.0502) Data: 0.0380 (0.0384) Loss: 0.7759 (0.6965)
[2022/12/29 01:41] | VALID(071): [200/220] Batch: 0.0431 (0.0481) Data: 0.0330 (0.0366) Loss: 0.6943 (0.7044)
[2022/12/29 01:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:41] |    VALID(71)      0.7038      0.8123      0.8683      0.8123      0.8123      0.8123      0.9531
[2022/12/29 01:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:41] | ####################################################################################################
[2022/12/29 01:41] | TRAIN(072): [ 50/879] Batch: 0.1311 (0.1736) Data: 0.0085 (0.0498) Loss: 0.5419 (0.3690)
[2022/12/29 01:41] | TRAIN(072): [100/879] Batch: 0.1270 (0.1538) Data: 0.0091 (0.0301) Loss: 0.6118 (0.3617)
[2022/12/29 01:41] | TRAIN(072): [150/879] Batch: 0.1432 (0.1466) Data: 0.0076 (0.0233) Loss: 0.5120 (0.3714)
[2022/12/29 01:41] | TRAIN(072): [200/879] Batch: 0.1222 (0.1432) Data: 0.0178 (0.0197) Loss: 0.2421 (0.3658)
[2022/12/29 01:41] | TRAIN(072): [250/879] Batch: 0.1306 (0.1417) Data: 0.0097 (0.0178) Loss: 0.3255 (0.3662)
[2022/12/29 01:41] | TRAIN(072): [300/879] Batch: 0.1308 (0.1403) Data: 0.0108 (0.0164) Loss: 0.2680 (0.3694)
[2022/12/29 01:41] | TRAIN(072): [350/879] Batch: 0.1390 (0.1394) Data: 0.0097 (0.0155) Loss: 0.4056 (0.3713)
[2022/12/29 01:42] | TRAIN(072): [400/879] Batch: 0.1214 (0.1385) Data: 0.0080 (0.0147) Loss: 0.4435 (0.3716)
[2022/12/29 01:42] | TRAIN(072): [450/879] Batch: 0.1513 (0.1378) Data: 0.0078 (0.0140) Loss: 0.5117 (0.3720)
[2022/12/29 01:42] | TRAIN(072): [500/879] Batch: 0.1232 (0.1370) Data: 0.0081 (0.0134) Loss: 0.4280 (0.3717)
[2022/12/29 01:42] | TRAIN(072): [550/879] Batch: 0.1211 (0.1366) Data: 0.0079 (0.0130) Loss: 0.3931 (0.3753)
[2022/12/29 01:42] | TRAIN(072): [600/879] Batch: 0.1386 (0.1360) Data: 0.0100 (0.0126) Loss: 0.2796 (0.3749)
[2022/12/29 01:42] | TRAIN(072): [650/879] Batch: 0.1247 (0.1358) Data: 0.0088 (0.0124) Loss: 0.4125 (0.3740)
[2022/12/29 01:42] | TRAIN(072): [700/879] Batch: 0.1299 (0.1352) Data: 0.0109 (0.0121) Loss: 0.4493 (0.3775)
[2022/12/29 01:42] | TRAIN(072): [750/879] Batch: 0.1292 (0.1353) Data: 0.0089 (0.0119) Loss: 0.4497 (0.3765)
[2022/12/29 01:42] | TRAIN(072): [800/879] Batch: 0.1429 (0.1351) Data: 0.0097 (0.0117) Loss: 0.1516 (0.3761)
[2022/12/29 01:43] | TRAIN(072): [850/879] Batch: 0.1213 (0.1352) Data: 0.0086 (0.0116) Loss: 0.3161 (0.3773)
[2022/12/29 01:43] | ------------------------------------------------------------
[2022/12/29 01:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:43] | ------------------------------------------------------------
[2022/12/29 01:43] |    TRAIN(72)     0:01:58     0:00:10     0:01:48      0.3777
[2022/12/29 01:43] | ------------------------------------------------------------
[2022/12/29 01:43] | VALID(072): [ 50/220] Batch: 0.0433 (0.0708) Data: 0.0385 (0.0593) Loss: 0.6791 (0.6590)
[2022/12/29 01:43] | VALID(072): [100/220] Batch: 0.0437 (0.0562) Data: 0.0375 (0.0454) Loss: 1.2729 (0.7019)
[2022/12/29 01:43] | VALID(072): [150/220] Batch: 0.0452 (0.0513) Data: 0.0339 (0.0406) Loss: 0.6494 (0.7012)
[2022/12/29 01:43] | VALID(072): [200/220] Batch: 0.0479 (0.0490) Data: 0.0264 (0.0383) Loss: 0.4378 (0.7012)
[2022/12/29 01:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:43] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:43] |    VALID(72)      0.7011      0.8057      0.8720      0.8057      0.8057      0.8057      0.9514
[2022/12/29 01:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:43] | ####################################################################################################
[2022/12/29 01:43] | TRAIN(073): [ 50/879] Batch: 0.1219 (0.1703) Data: 0.0112 (0.0481) Loss: 0.2917 (0.3919)
[2022/12/29 01:43] | TRAIN(073): [100/879] Batch: 0.1445 (0.1500) Data: 0.0071 (0.0285) Loss: 0.4796 (0.3775)
[2022/12/29 01:43] | TRAIN(073): [150/879] Batch: 0.1260 (0.1430) Data: 0.0080 (0.0218) Loss: 0.2856 (0.3866)
[2022/12/29 01:43] | TRAIN(073): [200/879] Batch: 0.0853 (0.1387) Data: 0.0089 (0.0187) Loss: 0.3008 (0.3808)
[2022/12/29 01:43] | TRAIN(073): [250/879] Batch: 0.1443 (0.1362) Data: 0.0099 (0.0169) Loss: 0.6012 (0.3815)
[2022/12/29 01:43] | TRAIN(073): [300/879] Batch: 0.1008 (0.1355) Data: 0.0074 (0.0156) Loss: 0.2480 (0.3814)
[2022/12/29 01:44] | TRAIN(073): [350/879] Batch: 0.1249 (0.1307) Data: 0.0081 (0.0145) Loss: 0.3781 (0.3781)
[2022/12/29 01:44] | TRAIN(073): [400/879] Batch: 0.1183 (0.1307) Data: 0.0087 (0.0138) Loss: 0.5447 (0.3748)
[2022/12/29 01:44] | TRAIN(073): [450/879] Batch: 0.1381 (0.1305) Data: 0.0105 (0.0131) Loss: 0.2918 (0.3752)
[2022/12/29 01:44] | TRAIN(073): [500/879] Batch: 0.1379 (0.1300) Data: 0.0078 (0.0127) Loss: 0.6825 (0.3781)
[2022/12/29 01:44] | TRAIN(073): [550/879] Batch: 0.1405 (0.1298) Data: 0.0106 (0.0123) Loss: 0.4753 (0.3792)
[2022/12/29 01:44] | TRAIN(073): [600/879] Batch: 0.1331 (0.1301) Data: 0.0114 (0.0121) Loss: 0.4428 (0.3784)
[2022/12/29 01:44] | TRAIN(073): [650/879] Batch: 0.1201 (0.1304) Data: 0.0081 (0.0118) Loss: 0.3945 (0.3778)
[2022/12/29 01:44] | TRAIN(073): [700/879] Batch: 0.1235 (0.1303) Data: 0.0088 (0.0116) Loss: 0.5173 (0.3773)
[2022/12/29 01:44] | TRAIN(073): [750/879] Batch: 0.1395 (0.1302) Data: 0.0078 (0.0114) Loss: 0.3544 (0.3764)
[2022/12/29 01:45] | TRAIN(073): [800/879] Batch: 0.1179 (0.1302) Data: 0.0104 (0.0112) Loss: 0.4769 (0.3785)
[2022/12/29 01:45] | TRAIN(073): [850/879] Batch: 0.1274 (0.1301) Data: 0.0099 (0.0111) Loss: 0.3168 (0.3777)
[2022/12/29 01:45] | ------------------------------------------------------------
[2022/12/29 01:45] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:45] | ------------------------------------------------------------
[2022/12/29 01:45] |    TRAIN(73)     0:01:54     0:00:09     0:01:44      0.3789
[2022/12/29 01:45] | ------------------------------------------------------------
[2022/12/29 01:45] | VALID(073): [ 50/220] Batch: 0.0316 (0.0705) Data: 0.0123 (0.0573) Loss: 0.6520 (0.6776)
[2022/12/29 01:45] | VALID(073): [100/220] Batch: 0.0289 (0.0558) Data: 0.0129 (0.0437) Loss: 1.3236 (0.7168)
[2022/12/29 01:45] | VALID(073): [150/220] Batch: 0.0488 (0.0507) Data: 0.0343 (0.0390) Loss: 0.7671 (0.7181)
[2022/12/29 01:45] | VALID(073): [200/220] Batch: 0.0482 (0.0484) Data: 0.0336 (0.0370) Loss: 0.4749 (0.7207)
[2022/12/29 01:45] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:45] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:45] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:45] |    VALID(73)      0.7173      0.8066      0.8704      0.8066      0.8066      0.8066      0.9516
[2022/12/29 01:45] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:45] | ####################################################################################################
[2022/12/29 01:45] | TRAIN(074): [ 50/879] Batch: 0.1440 (0.1744) Data: 0.0167 (0.0524) Loss: 0.2084 (0.3693)
[2022/12/29 01:45] | TRAIN(074): [100/879] Batch: 0.1323 (0.1538) Data: 0.0082 (0.0315) Loss: 0.1686 (0.3568)
[2022/12/29 01:45] | TRAIN(074): [150/879] Batch: 0.1389 (0.1473) Data: 0.0174 (0.0248) Loss: 0.4168 (0.3600)
[2022/12/29 01:45] | TRAIN(074): [200/879] Batch: 0.1385 (0.1441) Data: 0.0093 (0.0212) Loss: 0.3994 (0.3704)
[2022/12/29 01:45] | TRAIN(074): [250/879] Batch: 0.1390 (0.1422) Data: 0.0109 (0.0191) Loss: 0.4654 (0.3693)
[2022/12/29 01:46] | TRAIN(074): [300/879] Batch: 0.1513 (0.1412) Data: 0.0090 (0.0178) Loss: 0.4583 (0.3739)
[2022/12/29 01:46] | TRAIN(074): [350/879] Batch: 0.1339 (0.1398) Data: 0.0093 (0.0167) Loss: 0.5050 (0.3750)
[2022/12/29 01:46] | TRAIN(074): [400/879] Batch: 0.1294 (0.1392) Data: 0.0085 (0.0159) Loss: 0.4422 (0.3757)
[2022/12/29 01:46] | TRAIN(074): [450/879] Batch: 0.1167 (0.1389) Data: 0.0080 (0.0152) Loss: 0.5311 (0.3737)
[2022/12/29 01:46] | TRAIN(074): [500/879] Batch: 0.1172 (0.1382) Data: 0.0081 (0.0147) Loss: 0.1695 (0.3735)
[2022/12/29 01:46] | TRAIN(074): [550/879] Batch: 0.1380 (0.1381) Data: 0.0075 (0.0142) Loss: 0.2037 (0.3737)
[2022/12/29 01:46] | TRAIN(074): [600/879] Batch: 0.1213 (0.1376) Data: 0.0074 (0.0138) Loss: 0.2502 (0.3738)
[2022/12/29 01:46] | TRAIN(074): [650/879] Batch: 0.1520 (0.1357) Data: 0.0102 (0.0135) Loss: 0.1787 (0.3725)
[2022/12/29 01:46] | TRAIN(074): [700/879] Batch: 0.1332 (0.1359) Data: 0.0087 (0.0132) Loss: 0.2779 (0.3727)
[2022/12/29 01:47] | TRAIN(074): [750/879] Batch: 0.1331 (0.1343) Data: 0.0088 (0.0130) Loss: 0.1914 (0.3732)
[2022/12/29 01:47] | TRAIN(074): [800/879] Batch: 0.1344 (0.1340) Data: 0.0092 (0.0128) Loss: 0.4295 (0.3745)
[2022/12/29 01:47] | TRAIN(074): [850/879] Batch: 0.1227 (0.1337) Data: 0.0085 (0.0125) Loss: 0.3255 (0.3744)
[2022/12/29 01:47] | ------------------------------------------------------------
[2022/12/29 01:47] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:47] | ------------------------------------------------------------
[2022/12/29 01:47] |    TRAIN(74)     0:01:57     0:00:10     0:01:46      0.3740
[2022/12/29 01:47] | ------------------------------------------------------------
[2022/12/29 01:47] | VALID(074): [ 50/220] Batch: 0.0435 (0.0715) Data: 0.0321 (0.0590) Loss: 0.5894 (0.6623)
[2022/12/29 01:47] | VALID(074): [100/220] Batch: 0.0500 (0.0564) Data: 0.0326 (0.0451) Loss: 1.2023 (0.6971)
[2022/12/29 01:47] | VALID(074): [150/220] Batch: 0.0437 (0.0515) Data: 0.0371 (0.0405) Loss: 0.9110 (0.7056)
[2022/12/29 01:47] | VALID(074): [200/220] Batch: 0.0484 (0.0491) Data: 0.0226 (0.0381) Loss: 0.5550 (0.7077)
[2022/12/29 01:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:47] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:47] |    VALID(74)      0.7072      0.8066      0.8701      0.8066      0.8066      0.8066      0.9516
[2022/12/29 01:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:47] | ####################################################################################################
[2022/12/29 01:47] | TRAIN(075): [ 50/879] Batch: 0.1427 (0.1679) Data: 0.0077 (0.0466) Loss: 0.5326 (0.3681)
[2022/12/29 01:47] | TRAIN(075): [100/879] Batch: 0.1346 (0.1512) Data: 0.0101 (0.0283) Loss: 0.1812 (0.3663)
[2022/12/29 01:47] | TRAIN(075): [150/879] Batch: 0.1180 (0.1448) Data: 0.0080 (0.0221) Loss: 0.3203 (0.3711)
[2022/12/29 01:47] | TRAIN(075): [200/879] Batch: 0.1433 (0.1410) Data: 0.0074 (0.0188) Loss: 0.3708 (0.3764)
[2022/12/29 01:48] | TRAIN(075): [250/879] Batch: 0.1229 (0.1386) Data: 0.0100 (0.0167) Loss: 0.2762 (0.3760)
[2022/12/29 01:48] | TRAIN(075): [300/879] Batch: 0.1182 (0.1370) Data: 0.0082 (0.0154) Loss: 0.2113 (0.3749)
[2022/12/29 01:48] | TRAIN(075): [350/879] Batch: 0.1224 (0.1360) Data: 0.0080 (0.0144) Loss: 0.3569 (0.3739)
[2022/12/29 01:48] | TRAIN(075): [400/879] Batch: 0.1417 (0.1352) Data: 0.0095 (0.0136) Loss: 0.2605 (0.3759)
[2022/12/29 01:48] | TRAIN(075): [450/879] Batch: 0.1436 (0.1354) Data: 0.0093 (0.0132) Loss: 0.3269 (0.3765)
[2022/12/29 01:48] | TRAIN(075): [500/879] Batch: 0.1407 (0.1351) Data: 0.0071 (0.0128) Loss: 0.7005 (0.3761)
[2022/12/29 01:48] | TRAIN(075): [550/879] Batch: 0.1426 (0.1351) Data: 0.0075 (0.0125) Loss: 0.3029 (0.3759)
[2022/12/29 01:48] | TRAIN(075): [600/879] Batch: 0.1302 (0.1348) Data: 0.0113 (0.0121) Loss: 0.4022 (0.3743)
[2022/12/29 01:48] | TRAIN(075): [650/879] Batch: 0.1222 (0.1344) Data: 0.0081 (0.0119) Loss: 0.3140 (0.3730)
[2022/12/29 01:49] | TRAIN(075): [700/879] Batch: 0.1183 (0.1344) Data: 0.0080 (0.0117) Loss: 0.5514 (0.3718)
[2022/12/29 01:49] | TRAIN(075): [750/879] Batch: 0.1432 (0.1343) Data: 0.0092 (0.0115) Loss: 0.2805 (0.3752)
[2022/12/29 01:49] | TRAIN(075): [800/879] Batch: 0.1426 (0.1340) Data: 0.0071 (0.0113) Loss: 0.2994 (0.3740)
[2022/12/29 01:49] | TRAIN(075): [850/879] Batch: 0.1443 (0.1337) Data: 0.0076 (0.0111) Loss: 0.3389 (0.3745)
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] |    TRAIN(75)     0:01:57     0:00:09     0:01:47      0.3740
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] | VALID(075): [ 50/220] Batch: 0.0435 (0.0726) Data: 0.0279 (0.0591) Loss: 0.6444 (0.6446)
[2022/12/29 01:49] | VALID(075): [100/220] Batch: 0.0465 (0.0571) Data: 0.0238 (0.0445) Loss: 1.2561 (0.6894)
[2022/12/29 01:49] | VALID(075): [150/220] Batch: 0.0391 (0.0521) Data: 0.0190 (0.0392) Loss: 0.8806 (0.6946)
[2022/12/29 01:49] | VALID(075): [200/220] Batch: 0.0440 (0.0490) Data: 0.0331 (0.0367) Loss: 0.5464 (0.6933)
[2022/12/29 01:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:49] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:49] |    VALID(75)      0.6922      0.8080      0.8717      0.8080      0.8080      0.8080      0.9520
[2022/12/29 01:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:49] | ####################################################################################################
[2022/12/29 01:49] | TRAIN(076): [ 50/879] Batch: 0.0880 (0.1557) Data: 0.0073 (0.0497) Loss: 0.2919 (0.3382)
[2022/12/29 01:49] | TRAIN(076): [100/879] Batch: 0.1288 (0.1437) Data: 0.0086 (0.0302) Loss: 0.2548 (0.3629)
[2022/12/29 01:50] | TRAIN(076): [150/879] Batch: 0.0852 (0.1379) Data: 0.0094 (0.0235) Loss: 0.3211 (0.3555)
[2022/12/29 01:50] | TRAIN(076): [200/879] Batch: 0.1240 (0.1335) Data: 0.0124 (0.0201) Loss: 0.3416 (0.3581)
[2022/12/29 01:50] | TRAIN(076): [250/879] Batch: 0.1352 (0.1329) Data: 0.0082 (0.0181) Loss: 0.3317 (0.3538)
[2022/12/29 01:50] | TRAIN(076): [300/879] Batch: 0.1538 (0.1328) Data: 0.0091 (0.0166) Loss: 0.2660 (0.3555)
[2022/12/29 01:50] | TRAIN(076): [350/879] Batch: 0.1287 (0.1326) Data: 0.0103 (0.0155) Loss: 0.1565 (0.3552)
[2022/12/29 01:50] | TRAIN(076): [400/879] Batch: 0.1298 (0.1325) Data: 0.0081 (0.0147) Loss: 0.2725 (0.3629)
[2022/12/29 01:50] | TRAIN(076): [450/879] Batch: 0.1398 (0.1326) Data: 0.0088 (0.0141) Loss: 0.3405 (0.3635)
[2022/12/29 01:50] | TRAIN(076): [500/879] Batch: 0.1185 (0.1323) Data: 0.0081 (0.0136) Loss: 0.5492 (0.3618)
[2022/12/29 01:50] | TRAIN(076): [550/879] Batch: 0.1186 (0.1319) Data: 0.0080 (0.0131) Loss: 0.2382 (0.3591)
[2022/12/29 01:50] | TRAIN(076): [600/879] Batch: 0.1418 (0.1322) Data: 0.0093 (0.0128) Loss: 0.2475 (0.3592)
[2022/12/29 01:51] | TRAIN(076): [650/879] Batch: 0.1485 (0.1322) Data: 0.0116 (0.0125) Loss: 0.2775 (0.3591)
[2022/12/29 01:51] | TRAIN(076): [700/879] Batch: 0.1181 (0.1321) Data: 0.0081 (0.0123) Loss: 0.2993 (0.3588)
[2022/12/29 01:51] | TRAIN(076): [750/879] Batch: 0.1441 (0.1320) Data: 0.0078 (0.0120) Loss: 0.3420 (0.3597)
[2022/12/29 01:51] | TRAIN(076): [800/879] Batch: 0.1492 (0.1323) Data: 0.0109 (0.0119) Loss: 0.4449 (0.3599)
[2022/12/29 01:51] | TRAIN(076): [850/879] Batch: 0.1455 (0.1325) Data: 0.0106 (0.0117) Loss: 0.3655 (0.3612)
[2022/12/29 01:51] | ------------------------------------------------------------
[2022/12/29 01:51] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:51] | ------------------------------------------------------------
[2022/12/29 01:51] |    TRAIN(76)     0:01:56     0:00:10     0:01:46      0.3611
[2022/12/29 01:51] | ------------------------------------------------------------
[2022/12/29 01:51] | VALID(076): [ 50/220] Batch: 0.0449 (0.0726) Data: 0.0263 (0.0614) Loss: 0.6895 (0.6710)
[2022/12/29 01:51] | VALID(076): [100/220] Batch: 0.0420 (0.0573) Data: 0.0135 (0.0441) Loss: 1.4254 (0.7206)
[2022/12/29 01:51] | VALID(076): [150/220] Batch: 0.0319 (0.0521) Data: 0.0288 (0.0383) Loss: 0.8597 (0.7254)
[2022/12/29 01:51] | VALID(076): [200/220] Batch: 0.0289 (0.0494) Data: 0.0374 (0.0353) Loss: 0.4618 (0.7257)
[2022/12/29 01:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:51] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:51] |    VALID(76)      0.7237      0.8137      0.8721      0.8137      0.8137      0.8137      0.9534
[2022/12/29 01:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:51] | ####################################################################################################
[2022/12/29 01:51] | TRAIN(077): [ 50/879] Batch: 0.1246 (0.1709) Data: 0.0086 (0.0484) Loss: 0.4157 (0.3884)
[2022/12/29 01:52] | TRAIN(077): [100/879] Batch: 0.1324 (0.1540) Data: 0.0095 (0.0293) Loss: 0.4037 (0.3695)
[2022/12/29 01:52] | TRAIN(077): [150/879] Batch: 0.1232 (0.1462) Data: 0.0080 (0.0226) Loss: 0.5029 (0.3740)
[2022/12/29 01:52] | TRAIN(077): [200/879] Batch: 0.1185 (0.1432) Data: 0.0081 (0.0193) Loss: 0.4172 (0.3715)
[2022/12/29 01:52] | TRAIN(077): [250/879] Batch: 0.1307 (0.1418) Data: 0.0098 (0.0174) Loss: 0.3196 (0.3664)
[2022/12/29 01:52] | TRAIN(077): [300/879] Batch: 0.1430 (0.1410) Data: 0.0099 (0.0162) Loss: 0.3712 (0.3722)
[2022/12/29 01:52] | TRAIN(077): [350/879] Batch: 0.1462 (0.1401) Data: 0.0099 (0.0153) Loss: 0.4250 (0.3664)
[2022/12/29 01:52] | TRAIN(077): [400/879] Batch: 0.1201 (0.1393) Data: 0.0082 (0.0144) Loss: 0.1879 (0.3672)
[2022/12/29 01:52] | TRAIN(077): [450/879] Batch: 0.1481 (0.1365) Data: 0.0097 (0.0138) Loss: 0.4779 (0.3635)
[2022/12/29 01:52] | TRAIN(077): [500/879] Batch: 0.1376 (0.1361) Data: 0.0106 (0.0134) Loss: 0.3929 (0.3632)
[2022/12/29 01:53] | TRAIN(077): [550/879] Batch: 0.0863 (0.1339) Data: 0.0096 (0.0130) Loss: 0.2346 (0.3623)
[2022/12/29 01:53] | TRAIN(077): [600/879] Batch: 0.1533 (0.1339) Data: 0.0099 (0.0128) Loss: 0.3862 (0.3616)
[2022/12/29 01:53] | TRAIN(077): [650/879] Batch: 0.1140 (0.1338) Data: 0.0079 (0.0125) Loss: 0.4142 (0.3601)
[2022/12/29 01:53] | TRAIN(077): [700/879] Batch: 0.1207 (0.1334) Data: 0.0079 (0.0122) Loss: 0.7072 (0.3601)
[2022/12/29 01:53] | TRAIN(077): [750/879] Batch: 0.1221 (0.1330) Data: 0.0082 (0.0120) Loss: 0.2099 (0.3591)
[2022/12/29 01:53] | TRAIN(077): [800/879] Batch: 0.1253 (0.1328) Data: 0.0080 (0.0118) Loss: 0.6853 (0.3607)
[2022/12/29 01:53] | TRAIN(077): [850/879] Batch: 0.1376 (0.1327) Data: 0.0091 (0.0116) Loss: 0.3866 (0.3584)
[2022/12/29 01:53] | ------------------------------------------------------------
[2022/12/29 01:53] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:53] | ------------------------------------------------------------
[2022/12/29 01:53] |    TRAIN(77)     0:01:56     0:00:10     0:01:46      0.3589
[2022/12/29 01:53] | ------------------------------------------------------------
[2022/12/29 01:53] | VALID(077): [ 50/220] Batch: 0.0448 (0.0721) Data: 0.0330 (0.0603) Loss: 0.5501 (0.6781)
[2022/12/29 01:53] | VALID(077): [100/220] Batch: 0.0477 (0.0569) Data: 0.0269 (0.0459) Loss: 1.4755 (0.7348)
[2022/12/29 01:53] | VALID(077): [150/220] Batch: 0.0439 (0.0518) Data: 0.0181 (0.0410) Loss: 0.9433 (0.7423)
[2022/12/29 01:53] | VALID(077): [200/220] Batch: 0.0369 (0.0492) Data: 0.0217 (0.0385) Loss: 0.5109 (0.7444)
[2022/12/29 01:53] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:53] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:53] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:53] |    VALID(77)      0.7410      0.8126      0.8704      0.8126      0.8126      0.8126      0.9531
[2022/12/29 01:53] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:53] | ####################################################################################################
[2022/12/29 01:54] | TRAIN(078): [ 50/879] Batch: 0.1212 (0.1673) Data: 0.0080 (0.0456) Loss: 0.3300 (0.3405)
[2022/12/29 01:54] | TRAIN(078): [100/879] Batch: 0.1323 (0.1496) Data: 0.0099 (0.0276) Loss: 0.3691 (0.3513)
[2022/12/29 01:54] | TRAIN(078): [150/879] Batch: 0.1355 (0.1434) Data: 0.0082 (0.0213) Loss: 0.4465 (0.3543)
[2022/12/29 01:54] | TRAIN(078): [200/879] Batch: 0.1255 (0.1400) Data: 0.0103 (0.0181) Loss: 0.2466 (0.3592)
[2022/12/29 01:54] | TRAIN(078): [250/879] Batch: 0.1227 (0.1379) Data: 0.0085 (0.0162) Loss: 0.1096 (0.3624)
[2022/12/29 01:54] | TRAIN(078): [300/879] Batch: 0.1384 (0.1366) Data: 0.0077 (0.0149) Loss: 0.2615 (0.3591)
[2022/12/29 01:54] | TRAIN(078): [350/879] Batch: 0.1352 (0.1360) Data: 0.0079 (0.0140) Loss: 0.3849 (0.3560)
[2022/12/29 01:54] | TRAIN(078): [400/879] Batch: 0.1402 (0.1354) Data: 0.0072 (0.0133) Loss: 0.1898 (0.3510)
[2022/12/29 01:54] | TRAIN(078): [450/879] Batch: 0.1307 (0.1348) Data: 0.0080 (0.0128) Loss: 0.1835 (0.3509)
[2022/12/29 01:55] | TRAIN(078): [500/879] Batch: 0.1321 (0.1350) Data: 0.0111 (0.0125) Loss: 0.1113 (0.3512)
[2022/12/29 01:55] | TRAIN(078): [550/879] Batch: 0.1206 (0.1345) Data: 0.0101 (0.0122) Loss: 0.4074 (0.3514)
[2022/12/29 01:55] | TRAIN(078): [600/879] Batch: 0.1420 (0.1342) Data: 0.0073 (0.0119) Loss: 0.6126 (0.3521)
[2022/12/29 01:55] | TRAIN(078): [650/879] Batch: 0.1343 (0.1338) Data: 0.0086 (0.0117) Loss: 0.5133 (0.3517)
[2022/12/29 01:55] | TRAIN(078): [700/879] Batch: 0.1422 (0.1335) Data: 0.0076 (0.0114) Loss: 0.1728 (0.3525)
[2022/12/29 01:55] | TRAIN(078): [750/879] Batch: 0.1023 (0.1334) Data: 0.0090 (0.0113) Loss: 0.5079 (0.3520)
[2022/12/29 01:55] | TRAIN(078): [800/879] Batch: 0.1294 (0.1333) Data: 0.0108 (0.0111) Loss: 0.2909 (0.3528)
[2022/12/29 01:55] | TRAIN(078): [850/879] Batch: 0.0809 (0.1328) Data: 0.0110 (0.0109) Loss: 0.6784 (0.3542)
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] |    TRAIN(78)     0:01:56     0:00:09     0:01:46      0.3548
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] | VALID(078): [ 50/220] Batch: 0.0402 (0.0735) Data: 0.0269 (0.0599) Loss: 0.5691 (0.6736)
[2022/12/29 01:55] | VALID(078): [100/220] Batch: 0.0443 (0.0582) Data: 0.0298 (0.0462) Loss: 1.4201 (0.7278)
[2022/12/29 01:55] | VALID(078): [150/220] Batch: 0.0456 (0.0530) Data: 0.0351 (0.0416) Loss: 0.8279 (0.7318)
[2022/12/29 01:56] | VALID(078): [200/220] Batch: 0.0239 (0.0487) Data: 0.0106 (0.0363) Loss: 0.5164 (0.7297)
[2022/12/29 01:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:56] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:56] |    VALID(78)      0.7274      0.7995      0.8689      0.7995      0.7995      0.7995      0.9499
[2022/12/29 01:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:56] | ####################################################################################################
[2022/12/29 01:56] | TRAIN(079): [ 50/879] Batch: 0.1400 (0.1743) Data: 0.0126 (0.0501) Loss: 0.2250 (0.3614)
[2022/12/29 01:56] | TRAIN(079): [100/879] Batch: 0.1214 (0.1522) Data: 0.0081 (0.0295) Loss: 0.2824 (0.3527)
[2022/12/29 01:56] | TRAIN(079): [150/879] Batch: 0.1209 (0.1444) Data: 0.0080 (0.0225) Loss: 0.3279 (0.3497)
[2022/12/29 01:56] | TRAIN(079): [200/879] Batch: 0.1200 (0.1405) Data: 0.0086 (0.0190) Loss: 0.2445 (0.3558)
[2022/12/29 01:56] | TRAIN(079): [250/879] Batch: 0.1296 (0.1394) Data: 0.0103 (0.0171) Loss: 0.2354 (0.3554)
[2022/12/29 01:56] | TRAIN(079): [300/879] Batch: 0.1451 (0.1391) Data: 0.0093 (0.0159) Loss: 0.4456 (0.3501)
[2022/12/29 01:56] | TRAIN(079): [350/879] Batch: 0.1281 (0.1386) Data: 0.0118 (0.0151) Loss: 0.3874 (0.3475)
[2022/12/29 01:56] | TRAIN(079): [400/879] Batch: 0.1338 (0.1376) Data: 0.0074 (0.0143) Loss: 0.3273 (0.3470)
[2022/12/29 01:57] | TRAIN(079): [450/879] Batch: 0.1198 (0.1366) Data: 0.0083 (0.0137) Loss: 0.2529 (0.3514)
[2022/12/29 01:57] | TRAIN(079): [500/879] Batch: 0.1475 (0.1357) Data: 0.0092 (0.0132) Loss: 0.5529 (0.3541)
[2022/12/29 01:57] | TRAIN(079): [550/879] Batch: 0.1301 (0.1353) Data: 0.0084 (0.0129) Loss: 0.4979 (0.3560)
[2022/12/29 01:57] | TRAIN(079): [600/879] Batch: 0.1244 (0.1349) Data: 0.0083 (0.0125) Loss: 0.4132 (0.3554)
[2022/12/29 01:57] | TRAIN(079): [650/879] Batch: 0.1269 (0.1343) Data: 0.0078 (0.0122) Loss: 0.2558 (0.3558)
[2022/12/29 01:57] | TRAIN(079): [700/879] Batch: 0.1243 (0.1339) Data: 0.0104 (0.0119) Loss: 0.4589 (0.3534)
[2022/12/29 01:57] | TRAIN(079): [750/879] Batch: 0.1465 (0.1338) Data: 0.0073 (0.0116) Loss: 0.1616 (0.3547)
[2022/12/29 01:57] | TRAIN(079): [800/879] Batch: 0.1419 (0.1335) Data: 0.0072 (0.0114) Loss: 0.3282 (0.3571)
[2022/12/29 01:57] | TRAIN(079): [850/879] Batch: 0.1230 (0.1333) Data: 0.0087 (0.0112) Loss: 0.3841 (0.3546)
[2022/12/29 01:57] | ------------------------------------------------------------
[2022/12/29 01:57] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:57] | ------------------------------------------------------------
[2022/12/29 01:57] |    TRAIN(79)     0:01:57     0:00:09     0:01:47      0.3552
[2022/12/29 01:57] | ------------------------------------------------------------
[2022/12/29 01:58] | VALID(079): [ 50/220] Batch: 0.0469 (0.0733) Data: 0.0241 (0.0618) Loss: 0.6517 (0.7030)
[2022/12/29 01:58] | VALID(079): [100/220] Batch: 0.0355 (0.0576) Data: 0.0226 (0.0457) Loss: 1.3747 (0.7559)
[2022/12/29 01:58] | VALID(079): [150/220] Batch: 0.0408 (0.0523) Data: 0.0144 (0.0405) Loss: 0.9801 (0.7641)
[2022/12/29 01:58] | VALID(079): [200/220] Batch: 0.0325 (0.0497) Data: 0.0333 (0.0372) Loss: 0.6023 (0.7626)
[2022/12/29 01:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:58] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:58] |    VALID(79)      0.7624      0.8079      0.8702      0.8079      0.8079      0.8079      0.9520
[2022/12/29 01:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:58] | ####################################################################################################
[2022/12/29 01:58] | TRAIN(080): [ 50/879] Batch: 0.1458 (0.1714) Data: 0.0094 (0.0510) Loss: 0.2319 (0.3569)
[2022/12/29 01:58] | TRAIN(080): [100/879] Batch: 0.1371 (0.1517) Data: 0.0096 (0.0304) Loss: 0.4905 (0.3564)
[2022/12/29 01:58] | TRAIN(080): [150/879] Batch: 0.1344 (0.1451) Data: 0.0078 (0.0233) Loss: 0.4075 (0.3551)
[2022/12/29 01:58] | TRAIN(080): [200/879] Batch: 0.1413 (0.1413) Data: 0.0076 (0.0197) Loss: 0.2524 (0.3579)
[2022/12/29 01:58] | TRAIN(080): [250/879] Batch: 0.1429 (0.1390) Data: 0.0070 (0.0175) Loss: 0.2205 (0.3564)
[2022/12/29 01:58] | TRAIN(080): [300/879] Batch: 0.0866 (0.1343) Data: 0.0098 (0.0161) Loss: 0.3836 (0.3589)
[2022/12/29 01:58] | TRAIN(080): [350/879] Batch: 0.1344 (0.1338) Data: 0.0095 (0.0151) Loss: 0.1853 (0.3564)
[2022/12/29 01:59] | TRAIN(080): [400/879] Batch: 0.0831 (0.1327) Data: 0.0104 (0.0143) Loss: 0.5118 (0.3550)
[2022/12/29 01:59] | TRAIN(080): [450/879] Batch: 0.1295 (0.1302) Data: 0.0094 (0.0137) Loss: 0.3580 (0.3544)
[2022/12/29 01:59] | TRAIN(080): [500/879] Batch: 0.1224 (0.1308) Data: 0.0080 (0.0133) Loss: 0.2759 (0.3558)
[2022/12/29 01:59] | TRAIN(080): [550/879] Batch: 0.1444 (0.1306) Data: 0.0073 (0.0129) Loss: 0.2154 (0.3557)
[2022/12/29 01:59] | TRAIN(080): [600/879] Batch: 0.1227 (0.1304) Data: 0.0107 (0.0125) Loss: 0.4104 (0.3558)
[2022/12/29 01:59] | TRAIN(080): [650/879] Batch: 0.1202 (0.1305) Data: 0.0079 (0.0122) Loss: 0.2257 (0.3573)
[2022/12/29 01:59] | TRAIN(080): [700/879] Batch: 0.1407 (0.1306) Data: 0.0076 (0.0120) Loss: 0.3566 (0.3549)
[2022/12/29 01:59] | TRAIN(080): [750/879] Batch: 0.1424 (0.1305) Data: 0.0071 (0.0118) Loss: 0.5471 (0.3549)
[2022/12/29 01:59] | TRAIN(080): [800/879] Batch: 0.1201 (0.1305) Data: 0.0079 (0.0116) Loss: 0.4428 (0.3544)
[2022/12/29 02:00] | TRAIN(080): [850/879] Batch: 0.1382 (0.1304) Data: 0.0094 (0.0114) Loss: 0.1912 (0.3525)
[2022/12/29 02:00] | ------------------------------------------------------------
[2022/12/29 02:00] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:00] | ------------------------------------------------------------
[2022/12/29 02:00] |    TRAIN(80)     0:01:54     0:00:09     0:01:44      0.3536
[2022/12/29 02:00] | ------------------------------------------------------------
[2022/12/29 02:00] | VALID(080): [ 50/220] Batch: 0.0447 (0.0712) Data: 0.0343 (0.0591) Loss: 0.6514 (0.7127)
[2022/12/29 02:00] | VALID(080): [100/220] Batch: 0.0484 (0.0564) Data: 0.0338 (0.0450) Loss: 1.4621 (0.7547)
[2022/12/29 02:00] | VALID(080): [150/220] Batch: 0.0444 (0.0515) Data: 0.0381 (0.0403) Loss: 0.8515 (0.7657)
[2022/12/29 02:00] | VALID(080): [200/220] Batch: 0.0434 (0.0489) Data: 0.0381 (0.0378) Loss: 0.5318 (0.7679)
[2022/12/29 02:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:00] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:00] |    VALID(80)      0.7652      0.8066      0.8710      0.8066      0.8066      0.8066      0.9516
[2022/12/29 02:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:00] | ####################################################################################################
[2022/12/29 02:00] | TRAIN(081): [ 50/879] Batch: 0.1423 (0.1728) Data: 0.0128 (0.0499) Loss: 0.1770 (0.3394)
[2022/12/29 02:00] | TRAIN(081): [100/879] Batch: 0.1214 (0.1513) Data: 0.0101 (0.0301) Loss: 0.2656 (0.3500)
[2022/12/29 02:00] | TRAIN(081): [150/879] Batch: 0.1408 (0.1447) Data: 0.0109 (0.0232) Loss: 0.4638 (0.3602)
[2022/12/29 02:00] | TRAIN(081): [200/879] Batch: 0.1203 (0.1411) Data: 0.0147 (0.0196) Loss: 0.5514 (0.3562)
[2022/12/29 02:00] | TRAIN(081): [250/879] Batch: 0.1120 (0.1385) Data: 0.0104 (0.0174) Loss: 0.4730 (0.3501)
[2022/12/29 02:00] | TRAIN(081): [300/879] Batch: 0.1327 (0.1370) Data: 0.0097 (0.0160) Loss: 0.1848 (0.3495)
[2022/12/29 02:01] | TRAIN(081): [350/879] Batch: 0.1336 (0.1362) Data: 0.0104 (0.0151) Loss: 0.2271 (0.3510)
[2022/12/29 02:01] | TRAIN(081): [400/879] Batch: 0.1201 (0.1350) Data: 0.0081 (0.0143) Loss: 0.3271 (0.3520)
[2022/12/29 02:01] | TRAIN(081): [450/879] Batch: 0.1227 (0.1342) Data: 0.0084 (0.0136) Loss: 0.3992 (0.3504)
[2022/12/29 02:01] | TRAIN(081): [500/879] Batch: 0.1385 (0.1337) Data: 0.0077 (0.0131) Loss: 0.4942 (0.3503)
[2022/12/29 02:01] | TRAIN(081): [550/879] Batch: 0.1200 (0.1332) Data: 0.0080 (0.0127) Loss: 0.3088 (0.3479)
[2022/12/29 02:01] | TRAIN(081): [600/879] Batch: 0.1396 (0.1328) Data: 0.0075 (0.0123) Loss: 0.2533 (0.3471)
[2022/12/29 02:01] | TRAIN(081): [650/879] Batch: 0.1179 (0.1327) Data: 0.0079 (0.0120) Loss: 0.2219 (0.3481)
[2022/12/29 02:01] | TRAIN(081): [700/879] Batch: 0.1244 (0.1325) Data: 0.0087 (0.0117) Loss: 0.3274 (0.3477)
[2022/12/29 02:01] | TRAIN(081): [750/879] Batch: 0.1288 (0.1309) Data: 0.0061 (0.0115) Loss: 0.2265 (0.3465)
[2022/12/29 02:02] | TRAIN(081): [800/879] Batch: 0.1372 (0.1309) Data: 0.0110 (0.0114) Loss: 0.2336 (0.3461)
[2022/12/29 02:02] | TRAIN(081): [850/879] Batch: 0.0849 (0.1304) Data: 0.0145 (0.0113) Loss: 0.7124 (0.3468)
[2022/12/29 02:02] | ------------------------------------------------------------
[2022/12/29 02:02] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:02] | ------------------------------------------------------------
[2022/12/29 02:02] |    TRAIN(81)     0:01:54     0:00:09     0:01:44      0.3476
[2022/12/29 02:02] | ------------------------------------------------------------
[2022/12/29 02:02] | VALID(081): [ 50/220] Batch: 0.0448 (0.0719) Data: 0.0373 (0.0585) Loss: 0.6162 (0.6866)
[2022/12/29 02:02] | VALID(081): [100/220] Batch: 0.0499 (0.0567) Data: 0.0315 (0.0444) Loss: 1.4834 (0.7394)
[2022/12/29 02:02] | VALID(081): [150/220] Batch: 0.0507 (0.0517) Data: 0.0341 (0.0389) Loss: 1.0133 (0.7537)
[2022/12/29 02:02] | VALID(081): [200/220] Batch: 0.0480 (0.0489) Data: 0.0229 (0.0366) Loss: 0.5821 (0.7578)
[2022/12/29 02:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:02] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:02] |    VALID(81)      0.7576      0.8090      0.8725      0.8090      0.8090      0.8090      0.9522
[2022/12/29 02:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:02] | ####################################################################################################
[2022/12/29 02:02] | TRAIN(082): [ 50/879] Batch: 0.1184 (0.1730) Data: 0.0086 (0.0507) Loss: 0.3798 (0.3248)
[2022/12/29 02:02] | TRAIN(082): [100/879] Batch: 0.1236 (0.1505) Data: 0.0089 (0.0297) Loss: 0.3654 (0.3303)
[2022/12/29 02:02] | TRAIN(082): [150/879] Batch: 0.1460 (0.1449) Data: 0.0095 (0.0230) Loss: 0.2517 (0.3297)
[2022/12/29 02:02] | TRAIN(082): [200/879] Batch: 0.1197 (0.1431) Data: 0.0101 (0.0198) Loss: 0.1575 (0.3411)
[2022/12/29 02:02] | TRAIN(082): [250/879] Batch: 0.1427 (0.1402) Data: 0.0072 (0.0175) Loss: 0.3361 (0.3439)
[2022/12/29 02:03] | TRAIN(082): [300/879] Batch: 0.1291 (0.1394) Data: 0.0082 (0.0162) Loss: 0.2186 (0.3456)
[2022/12/29 02:03] | TRAIN(082): [350/879] Batch: 0.1207 (0.1380) Data: 0.0080 (0.0151) Loss: 0.5650 (0.3485)
[2022/12/29 02:03] | TRAIN(082): [400/879] Batch: 0.1206 (0.1369) Data: 0.0083 (0.0142) Loss: 0.3944 (0.3465)
[2022/12/29 02:03] | TRAIN(082): [450/879] Batch: 0.1196 (0.1360) Data: 0.0080 (0.0136) Loss: 0.3196 (0.3464)
[2022/12/29 02:03] | TRAIN(082): [500/879] Batch: 0.1387 (0.1354) Data: 0.0078 (0.0131) Loss: 0.2908 (0.3476)
[2022/12/29 02:03] | TRAIN(082): [550/879] Batch: 0.1454 (0.1353) Data: 0.0095 (0.0128) Loss: 0.3608 (0.3478)
[2022/12/29 02:03] | TRAIN(082): [600/879] Batch: 0.1190 (0.1351) Data: 0.0082 (0.0125) Loss: 0.4447 (0.3472)
[2022/12/29 02:03] | TRAIN(082): [650/879] Batch: 0.1345 (0.1349) Data: 0.0081 (0.0122) Loss: 0.3355 (0.3441)
[2022/12/29 02:03] | TRAIN(082): [700/879] Batch: 0.1182 (0.1345) Data: 0.0081 (0.0120) Loss: 0.4177 (0.3444)
[2022/12/29 02:04] | TRAIN(082): [750/879] Batch: 0.1163 (0.1341) Data: 0.0103 (0.0118) Loss: 0.3327 (0.3438)
[2022/12/29 02:04] | TRAIN(082): [800/879] Batch: 0.1199 (0.1342) Data: 0.0086 (0.0116) Loss: 0.2603 (0.3428)
[2022/12/29 02:04] | TRAIN(082): [850/879] Batch: 0.1219 (0.1338) Data: 0.0088 (0.0114) Loss: 0.2881 (0.3442)
[2022/12/29 02:04] | ------------------------------------------------------------
[2022/12/29 02:04] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:04] | ------------------------------------------------------------
[2022/12/29 02:04] |    TRAIN(82)     0:01:57     0:00:09     0:01:47      0.3442
[2022/12/29 02:04] | ------------------------------------------------------------
[2022/12/29 02:04] | VALID(082): [ 50/220] Batch: 0.0280 (0.0734) Data: 0.0389 (0.0620) Loss: 0.6587 (0.7036)
[2022/12/29 02:04] | VALID(082): [100/220] Batch: 0.0389 (0.0576) Data: 0.0338 (0.0465) Loss: 1.5416 (0.7720)
[2022/12/29 02:04] | VALID(082): [150/220] Batch: 0.0402 (0.0522) Data: 0.0350 (0.0413) Loss: 0.9164 (0.7849)
[2022/12/29 02:04] | VALID(082): [200/220] Batch: 0.0488 (0.0496) Data: 0.0339 (0.0388) Loss: 0.4871 (0.7838)
[2022/12/29 02:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:04] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:04] |    VALID(82)      0.7808      0.8069      0.8697      0.8069      0.8069      0.8069      0.9517
[2022/12/29 02:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:04] | ####################################################################################################
[2022/12/29 02:04] | TRAIN(083): [ 50/879] Batch: 0.1409 (0.1707) Data: 0.0093 (0.0464) Loss: 0.4264 (0.3502)
[2022/12/29 02:04] | TRAIN(083): [100/879] Batch: 0.1385 (0.1499) Data: 0.0071 (0.0275) Loss: 0.1354 (0.3384)
[2022/12/29 02:04] | TRAIN(083): [150/879] Batch: 0.1404 (0.1428) Data: 0.0079 (0.0212) Loss: 0.2173 (0.3372)
[2022/12/29 02:04] | TRAIN(083): [200/879] Batch: 0.1438 (0.1348) Data: 0.0098 (0.0180) Loss: 0.2855 (0.3363)
[2022/12/29 02:05] | TRAIN(083): [250/879] Batch: 0.1264 (0.1339) Data: 0.0084 (0.0162) Loss: 0.1358 (0.3333)
[2022/12/29 02:05] | TRAIN(083): [300/879] Batch: 0.1172 (0.1292) Data: 0.0098 (0.0150) Loss: 0.7860 (0.3395)
[2022/12/29 02:05] | TRAIN(083): [350/879] Batch: 0.1323 (0.1293) Data: 0.0082 (0.0140) Loss: 0.4065 (0.3398)
[2022/12/29 02:05] | TRAIN(083): [400/879] Batch: 0.1267 (0.1293) Data: 0.0074 (0.0133) Loss: 0.3482 (0.3417)
[2022/12/29 02:05] | TRAIN(083): [450/879] Batch: 0.1332 (0.1299) Data: 0.0086 (0.0129) Loss: 0.2759 (0.3412)
[2022/12/29 02:05] | TRAIN(083): [500/879] Batch: 0.1241 (0.1299) Data: 0.0092 (0.0125) Loss: 0.1991 (0.3409)
[2022/12/29 02:05] | TRAIN(083): [550/879] Batch: 0.1202 (0.1297) Data: 0.0081 (0.0121) Loss: 0.3379 (0.3438)
[2022/12/29 02:05] | TRAIN(083): [600/879] Batch: 0.1161 (0.1301) Data: 0.0074 (0.0118) Loss: 0.4527 (0.3432)
[2022/12/29 02:05] | TRAIN(083): [650/879] Batch: 0.1357 (0.1298) Data: 0.0109 (0.0116) Loss: 0.2123 (0.3427)
[2022/12/29 02:06] | TRAIN(083): [700/879] Batch: 0.1213 (0.1300) Data: 0.0081 (0.0114) Loss: 0.3020 (0.3421)
[2022/12/29 02:06] | TRAIN(083): [750/879] Batch: 0.1425 (0.1300) Data: 0.0074 (0.0112) Loss: 0.2357 (0.3424)
[2022/12/29 02:06] | TRAIN(083): [800/879] Batch: 0.1224 (0.1300) Data: 0.0091 (0.0110) Loss: 0.2179 (0.3421)
[2022/12/29 02:06] | TRAIN(083): [850/879] Batch: 0.1426 (0.1300) Data: 0.0084 (0.0109) Loss: 0.4181 (0.3443)
[2022/12/29 02:06] | ------------------------------------------------------------
[2022/12/29 02:06] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:06] | ------------------------------------------------------------
[2022/12/29 02:06] |    TRAIN(83)     0:01:54     0:00:09     0:01:44      0.3429
[2022/12/29 02:06] | ------------------------------------------------------------
[2022/12/29 02:06] | VALID(083): [ 50/220] Batch: 0.0306 (0.0714) Data: 0.0338 (0.0585) Loss: 0.7108 (0.7222)
[2022/12/29 02:06] | VALID(083): [100/220] Batch: 0.0472 (0.0564) Data: 0.0224 (0.0443) Loss: 1.5660 (0.7690)
[2022/12/29 02:06] | VALID(083): [150/220] Batch: 0.0451 (0.0513) Data: 0.0221 (0.0397) Loss: 0.8725 (0.7830)
[2022/12/29 02:06] | VALID(083): [200/220] Batch: 0.0426 (0.0489) Data: 0.0192 (0.0375) Loss: 0.4739 (0.7802)
[2022/12/29 02:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:06] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:06] |    VALID(83)      0.7799      0.8081      0.8716      0.8081      0.8081      0.8081      0.9520
[2022/12/29 02:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:06] | ####################################################################################################
[2022/12/29 02:06] | TRAIN(084): [ 50/879] Batch: 0.1308 (0.1673) Data: 0.0087 (0.0461) Loss: 0.5110 (0.3655)
[2022/12/29 02:06] | TRAIN(084): [100/879] Batch: 0.1302 (0.1485) Data: 0.0084 (0.0274) Loss: 0.2053 (0.3586)
[2022/12/29 02:06] | TRAIN(084): [150/879] Batch: 0.1193 (0.1416) Data: 0.0080 (0.0210) Loss: 0.4177 (0.3524)
[2022/12/29 02:07] | TRAIN(084): [200/879] Batch: 0.1421 (0.1383) Data: 0.0075 (0.0178) Loss: 0.2685 (0.3502)
[2022/12/29 02:07] | TRAIN(084): [250/879] Batch: 0.1440 (0.1367) Data: 0.0070 (0.0159) Loss: 0.3257 (0.3522)
[2022/12/29 02:07] | TRAIN(084): [300/879] Batch: 0.1223 (0.1353) Data: 0.0101 (0.0147) Loss: 0.3567 (0.3515)
[2022/12/29 02:07] | TRAIN(084): [350/879] Batch: 0.1307 (0.1347) Data: 0.0087 (0.0138) Loss: 0.2992 (0.3461)
[2022/12/29 02:07] | TRAIN(084): [400/879] Batch: 0.1454 (0.1340) Data: 0.0076 (0.0131) Loss: 0.1067 (0.3428)
[2022/12/29 02:07] | TRAIN(084): [450/879] Batch: 0.1341 (0.1338) Data: 0.0105 (0.0127) Loss: 0.2502 (0.3415)
[2022/12/29 02:07] | TRAIN(084): [500/879] Batch: 0.1237 (0.1338) Data: 0.0080 (0.0124) Loss: 0.3465 (0.3424)
[2022/12/29 02:07] | TRAIN(084): [550/879] Batch: 0.1334 (0.1335) Data: 0.0084 (0.0121) Loss: 0.4612 (0.3401)
[2022/12/29 02:07] | TRAIN(084): [600/879] Batch: 0.1248 (0.1331) Data: 0.0075 (0.0118) Loss: 0.3983 (0.3401)
[2022/12/29 02:07] | TRAIN(084): [650/879] Batch: 0.1329 (0.1313) Data: 0.0100 (0.0115) Loss: 0.4584 (0.3388)
[2022/12/29 02:08] | TRAIN(084): [700/879] Batch: 0.1340 (0.1319) Data: 0.0122 (0.0115) Loss: 0.3979 (0.3396)
[2022/12/29 02:08] | TRAIN(084): [750/879] Batch: 0.1254 (0.1306) Data: 0.0095 (0.0113) Loss: 0.3373 (0.3395)
[2022/12/29 02:08] | TRAIN(084): [800/879] Batch: 0.1241 (0.1305) Data: 0.0082 (0.0112) Loss: 0.1696 (0.3385)
[2022/12/29 02:08] | TRAIN(084): [850/879] Batch: 0.1321 (0.1307) Data: 0.0101 (0.0110) Loss: 0.3480 (0.3377)
[2022/12/29 02:08] | ------------------------------------------------------------
[2022/12/29 02:08] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:08] | ------------------------------------------------------------
[2022/12/29 02:08] |    TRAIN(84)     0:01:54     0:00:09     0:01:45      0.3370
[2022/12/29 02:08] | ------------------------------------------------------------
[2022/12/29 02:08] | VALID(084): [ 50/220] Batch: 0.0447 (0.0706) Data: 0.0347 (0.0590) Loss: 0.7697 (0.7191)
[2022/12/29 02:08] | VALID(084): [100/220] Batch: 0.0481 (0.0561) Data: 0.0247 (0.0450) Loss: 1.5098 (0.7697)
[2022/12/29 02:08] | VALID(084): [150/220] Batch: 0.0470 (0.0512) Data: 0.0240 (0.0404) Loss: 0.9465 (0.7819)
[2022/12/29 02:08] | VALID(084): [200/220] Batch: 0.0372 (0.0485) Data: 0.0376 (0.0379) Loss: 0.5598 (0.7767)
[2022/12/29 02:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:08] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:08] |    VALID(84)      0.7768      0.8040      0.8705      0.8040      0.8040      0.8040      0.9510
[2022/12/29 02:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:08] | ####################################################################################################
[2022/12/29 02:08] | TRAIN(085): [ 50/879] Batch: 0.1382 (0.1731) Data: 0.0083 (0.0490) Loss: 0.2889 (0.3242)
[2022/12/29 02:08] | TRAIN(085): [100/879] Batch: 0.1291 (0.1518) Data: 0.0069 (0.0293) Loss: 0.4207 (0.3160)
[2022/12/29 02:09] | TRAIN(085): [150/879] Batch: 0.1241 (0.1435) Data: 0.0114 (0.0224) Loss: 0.3784 (0.3182)
[2022/12/29 02:09] | TRAIN(085): [200/879] Batch: 0.1185 (0.1401) Data: 0.0082 (0.0190) Loss: 0.5257 (0.3185)
[2022/12/29 02:09] | TRAIN(085): [250/879] Batch: 0.1267 (0.1378) Data: 0.0086 (0.0170) Loss: 0.3223 (0.3259)
[2022/12/29 02:09] | TRAIN(085): [300/879] Batch: 0.1505 (0.1366) Data: 0.0082 (0.0156) Loss: 0.2548 (0.3257)
[2022/12/29 02:09] | TRAIN(085): [350/879] Batch: 0.1201 (0.1353) Data: 0.0079 (0.0146) Loss: 0.3835 (0.3275)
[2022/12/29 02:09] | TRAIN(085): [400/879] Batch: 0.1361 (0.1346) Data: 0.0102 (0.0139) Loss: 0.3151 (0.3276)
[2022/12/29 02:09] | TRAIN(085): [450/879] Batch: 0.1366 (0.1349) Data: 0.0094 (0.0134) Loss: 0.5333 (0.3299)
[2022/12/29 02:09] | TRAIN(085): [500/879] Batch: 0.1435 (0.1352) Data: 0.0095 (0.0131) Loss: 0.1909 (0.3326)
[2022/12/29 02:09] | TRAIN(085): [550/879] Batch: 0.1388 (0.1353) Data: 0.0106 (0.0128) Loss: 0.4697 (0.3319)
[2022/12/29 02:10] | TRAIN(085): [600/879] Batch: 0.1215 (0.1354) Data: 0.0103 (0.0125) Loss: 0.3676 (0.3309)
[2022/12/29 02:10] | TRAIN(085): [650/879] Batch: 0.1401 (0.1350) Data: 0.0072 (0.0122) Loss: 0.6914 (0.3315)
[2022/12/29 02:10] | TRAIN(085): [700/879] Batch: 0.1390 (0.1347) Data: 0.0071 (0.0119) Loss: 0.8576 (0.3326)
[2022/12/29 02:10] | TRAIN(085): [750/879] Batch: 0.1225 (0.1343) Data: 0.0081 (0.0117) Loss: 0.3430 (0.3325)
[2022/12/29 02:10] | TRAIN(085): [800/879] Batch: 0.1395 (0.1340) Data: 0.0072 (0.0115) Loss: 0.3431 (0.3325)
[2022/12/29 02:10] | TRAIN(085): [850/879] Batch: 0.1419 (0.1340) Data: 0.0070 (0.0113) Loss: 0.1550 (0.3327)
[2022/12/29 02:10] | ------------------------------------------------------------
[2022/12/29 02:10] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:10] | ------------------------------------------------------------
[2022/12/29 02:10] |    TRAIN(85)     0:01:57     0:00:09     0:01:47      0.3331
[2022/12/29 02:10] | ------------------------------------------------------------
[2022/12/29 02:10] | VALID(085): [ 50/220] Batch: 0.0274 (0.0714) Data: 0.0383 (0.0601) Loss: 0.7178 (0.7490)
[2022/12/29 02:10] | VALID(085): [100/220] Batch: 0.0370 (0.0565) Data: 0.0380 (0.0453) Loss: 1.5945 (0.8005)
[2022/12/29 02:10] | VALID(085): [150/220] Batch: 0.0480 (0.0516) Data: 0.0346 (0.0406) Loss: 0.9590 (0.8108)
[2022/12/29 02:10] | VALID(085): [200/220] Batch: 0.0485 (0.0492) Data: 0.0295 (0.0378) Loss: 0.5975 (0.8082)
[2022/12/29 02:10] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:10] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:10] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:10] |    VALID(85)      0.8088      0.8064      0.8695      0.8064      0.8064      0.8064      0.9516
[2022/12/29 02:10] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:10] | ####################################################################################################
[2022/12/29 02:10] | TRAIN(086): [ 50/879] Batch: 0.0763 (0.1615) Data: 0.0157 (0.0455) Loss: 0.4411 (0.3394)
[2022/12/29 02:11] | TRAIN(086): [100/879] Batch: 0.1272 (0.1421) Data: 0.0076 (0.0279) Loss: 0.5415 (0.3515)
[2022/12/29 02:11] | TRAIN(086): [150/879] Batch: 0.1322 (0.1386) Data: 0.0077 (0.0217) Loss: 0.5166 (0.3456)
[2022/12/29 02:11] | TRAIN(086): [200/879] Batch: 0.1233 (0.1299) Data: 0.0077 (0.0185) Loss: 0.2700 (0.3432)
[2022/12/29 02:11] | TRAIN(086): [250/879] Batch: 0.1279 (0.1310) Data: 0.0101 (0.0167) Loss: 0.2823 (0.3394)
[2022/12/29 02:11] | TRAIN(086): [300/879] Batch: 0.1194 (0.1310) Data: 0.0080 (0.0154) Loss: 0.1608 (0.3408)
[2022/12/29 02:11] | TRAIN(086): [350/879] Batch: 0.1424 (0.1307) Data: 0.0076 (0.0144) Loss: 0.2233 (0.3416)
[2022/12/29 02:11] | TRAIN(086): [400/879] Batch: 0.1155 (0.1305) Data: 0.0074 (0.0137) Loss: 0.4798 (0.3418)
[2022/12/29 02:11] | TRAIN(086): [450/879] Batch: 0.1431 (0.1307) Data: 0.0071 (0.0132) Loss: 0.4344 (0.3433)
[2022/12/29 02:11] | TRAIN(086): [500/879] Batch: 0.1232 (0.1305) Data: 0.0088 (0.0127) Loss: 0.4539 (0.3427)
[2022/12/29 02:12] | TRAIN(086): [550/879] Batch: 0.1447 (0.1303) Data: 0.0086 (0.0124) Loss: 0.4680 (0.3424)
[2022/12/29 02:12] | TRAIN(086): [600/879] Batch: 0.1329 (0.1306) Data: 0.0094 (0.0121) Loss: 0.4065 (0.3416)
[2022/12/29 02:12] | TRAIN(086): [650/879] Batch: 0.1386 (0.1306) Data: 0.0083 (0.0119) Loss: 0.3693 (0.3411)
[2022/12/29 02:12] | TRAIN(086): [700/879] Batch: 0.1488 (0.1309) Data: 0.0094 (0.0117) Loss: 0.1908 (0.3377)
[2022/12/29 02:12] | TRAIN(086): [750/879] Batch: 0.1361 (0.1312) Data: 0.0097 (0.0116) Loss: 0.4485 (0.3360)
[2022/12/29 02:12] | TRAIN(086): [800/879] Batch: 0.1377 (0.1314) Data: 0.0076 (0.0114) Loss: 0.5471 (0.3363)
[2022/12/29 02:12] | TRAIN(086): [850/879] Batch: 0.1228 (0.1313) Data: 0.0091 (0.0113) Loss: 0.3022 (0.3348)
[2022/12/29 02:12] | ------------------------------------------------------------
[2022/12/29 02:12] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:12] | ------------------------------------------------------------
[2022/12/29 02:12] |    TRAIN(86)     0:01:55     0:00:09     0:01:45      0.3347
[2022/12/29 02:12] | ------------------------------------------------------------
[2022/12/29 02:12] | VALID(086): [ 50/220] Batch: 0.0479 (0.0709) Data: 0.0336 (0.0603) Loss: 0.7138 (0.7443)
[2022/12/29 02:12] | VALID(086): [100/220] Batch: 0.0489 (0.0561) Data: 0.0347 (0.0455) Loss: 1.6054 (0.7998)
[2022/12/29 02:12] | VALID(086): [150/220] Batch: 0.0439 (0.0512) Data: 0.0380 (0.0408) Loss: 0.9387 (0.8099)
[2022/12/29 02:12] | VALID(086): [200/220] Batch: 0.0444 (0.0489) Data: 0.0346 (0.0384) Loss: 0.5521 (0.8050)
[2022/12/29 02:12] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:12] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:12] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:12] |    VALID(86)      0.8054      0.8098      0.8701      0.8098      0.8098      0.8098      0.9525
[2022/12/29 02:12] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:12] | ####################################################################################################
[2022/12/29 02:13] | TRAIN(087): [ 50/879] Batch: 0.1414 (0.1716) Data: 0.0074 (0.0489) Loss: 0.2810 (0.3441)
[2022/12/29 02:13] | TRAIN(087): [100/879] Batch: 0.1264 (0.1513) Data: 0.0089 (0.0290) Loss: 0.2161 (0.3333)
[2022/12/29 02:13] | TRAIN(087): [150/879] Batch: 0.1160 (0.1444) Data: 0.0085 (0.0222) Loss: 0.2362 (0.3324)
[2022/12/29 02:13] | TRAIN(087): [200/879] Batch: 0.1277 (0.1406) Data: 0.0110 (0.0189) Loss: 0.4155 (0.3334)
[2022/12/29 02:13] | TRAIN(087): [250/879] Batch: 0.1493 (0.1394) Data: 0.0085 (0.0171) Loss: 0.2152 (0.3352)
[2022/12/29 02:13] | TRAIN(087): [300/879] Batch: 0.1234 (0.1380) Data: 0.0085 (0.0157) Loss: 0.4450 (0.3327)
[2022/12/29 02:13] | TRAIN(087): [350/879] Batch: 0.1378 (0.1367) Data: 0.0086 (0.0146) Loss: 0.6279 (0.3334)
[2022/12/29 02:13] | TRAIN(087): [400/879] Batch: 0.1385 (0.1359) Data: 0.0072 (0.0139) Loss: 0.2319 (0.3323)
[2022/12/29 02:13] | TRAIN(087): [450/879] Batch: 0.1240 (0.1351) Data: 0.0084 (0.0133) Loss: 0.5846 (0.3305)
[2022/12/29 02:14] | TRAIN(087): [500/879] Batch: 0.1360 (0.1325) Data: 0.0089 (0.0128) Loss: 0.6419 (0.3290)
[2022/12/29 02:14] | TRAIN(087): [550/879] Batch: 0.1365 (0.1325) Data: 0.0107 (0.0125) Loss: 0.3199 (0.3314)
[2022/12/29 02:14] | TRAIN(087): [600/879] Batch: 0.0850 (0.1310) Data: 0.0078 (0.0123) Loss: 0.1897 (0.3342)
[2022/12/29 02:14] | TRAIN(087): [650/879] Batch: 0.1532 (0.1312) Data: 0.0096 (0.0121) Loss: 0.2930 (0.3360)
[2022/12/29 02:14] | TRAIN(087): [700/879] Batch: 0.1235 (0.1312) Data: 0.0085 (0.0119) Loss: 0.1622 (0.3333)
[2022/12/29 02:14] | TRAIN(087): [750/879] Batch: 0.1269 (0.1311) Data: 0.0095 (0.0116) Loss: 0.1361 (0.3338)
[2022/12/29 02:14] | TRAIN(087): [800/879] Batch: 0.1536 (0.1311) Data: 0.0090 (0.0115) Loss: 0.3049 (0.3336)
[2022/12/29 02:14] | TRAIN(087): [850/879] Batch: 0.1167 (0.1312) Data: 0.0084 (0.0114) Loss: 0.1305 (0.3331)
[2022/12/29 02:14] | ------------------------------------------------------------
[2022/12/29 02:14] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:14] | ------------------------------------------------------------
[2022/12/29 02:14] |    TRAIN(87)     0:01:55     0:00:09     0:01:45      0.3335
[2022/12/29 02:14] | ------------------------------------------------------------
[2022/12/29 02:14] | VALID(087): [ 50/220] Batch: 0.0499 (0.0718) Data: 0.0214 (0.0593) Loss: 0.6760 (0.7335)
[2022/12/29 02:14] | VALID(087): [100/220] Batch: 0.0403 (0.0575) Data: 0.0384 (0.0452) Loss: 1.4884 (0.7771)
[2022/12/29 02:14] | VALID(087): [150/220] Batch: 0.0480 (0.0524) Data: 0.0359 (0.0398) Loss: 0.8397 (0.7824)
[2022/12/29 02:15] | VALID(087): [200/220] Batch: 0.0433 (0.0501) Data: 0.0406 (0.0374) Loss: 0.5944 (0.7789)
[2022/12/29 02:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:15] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:15] |    VALID(87)      0.7769      0.8079      0.8705      0.8079      0.8079      0.8079      0.9520
[2022/12/29 02:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:15] | ####################################################################################################
[2022/12/29 02:15] | TRAIN(088): [ 50/879] Batch: 0.1369 (0.1769) Data: 0.0171 (0.0528) Loss: 0.1758 (0.3071)
[2022/12/29 02:15] | TRAIN(088): [100/879] Batch: 0.1258 (0.1569) Data: 0.0138 (0.0328) Loss: 0.0885 (0.3124)
[2022/12/29 02:15] | TRAIN(088): [150/879] Batch: 0.1161 (0.1494) Data: 0.0190 (0.0262) Loss: 0.2626 (0.3220)
[2022/12/29 02:15] | TRAIN(088): [200/879] Batch: 0.1416 (0.1451) Data: 0.0070 (0.0225) Loss: 0.6665 (0.3253)
[2022/12/29 02:15] | TRAIN(088): [250/879] Batch: 0.1382 (0.1433) Data: 0.0088 (0.0206) Loss: 0.6428 (0.3279)
[2022/12/29 02:15] | TRAIN(088): [300/879] Batch: 0.1256 (0.1420) Data: 0.0196 (0.0191) Loss: 0.1534 (0.3328)
[2022/12/29 02:15] | TRAIN(088): [350/879] Batch: 0.1393 (0.1411) Data: 0.0132 (0.0181) Loss: 0.4865 (0.3265)
[2022/12/29 02:15] | TRAIN(088): [400/879] Batch: 0.1255 (0.1403) Data: 0.0182 (0.0173) Loss: 0.3124 (0.3287)
[2022/12/29 02:16] | TRAIN(088): [450/879] Batch: 0.1362 (0.1398) Data: 0.0117 (0.0168) Loss: 0.2574 (0.3280)
[2022/12/29 02:16] | TRAIN(088): [500/879] Batch: 0.1395 (0.1394) Data: 0.0098 (0.0163) Loss: 0.5027 (0.3266)
[2022/12/29 02:16] | TRAIN(088): [550/879] Batch: 0.1423 (0.1387) Data: 0.0130 (0.0159) Loss: 0.3837 (0.3289)
[2022/12/29 02:16] | TRAIN(088): [600/879] Batch: 0.1395 (0.1384) Data: 0.0104 (0.0155) Loss: 0.4375 (0.3300)
[2022/12/29 02:16] | TRAIN(088): [650/879] Batch: 0.1416 (0.1383) Data: 0.0113 (0.0153) Loss: 0.3686 (0.3295)
[2022/12/29 02:16] | TRAIN(088): [700/879] Batch: 0.1242 (0.1381) Data: 0.0136 (0.0151) Loss: 0.1420 (0.3284)
[2022/12/29 02:16] | TRAIN(088): [750/879] Batch: 0.1418 (0.1381) Data: 0.0135 (0.0149) Loss: 0.1758 (0.3300)
[2022/12/29 02:16] | TRAIN(088): [800/879] Batch: 0.1208 (0.1381) Data: 0.0115 (0.0148) Loss: 0.3404 (0.3285)
[2022/12/29 02:16] | TRAIN(088): [850/879] Batch: 0.1418 (0.1366) Data: 0.0144 (0.0146) Loss: 0.3074 (0.3288)
[2022/12/29 02:17] | ------------------------------------------------------------
[2022/12/29 02:17] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:17] | ------------------------------------------------------------
[2022/12/29 02:17] |    TRAIN(88)     0:01:59     0:00:12     0:01:47      0.3293
[2022/12/29 02:17] | ------------------------------------------------------------
[2022/12/29 02:17] | VALID(088): [ 50/220] Batch: 0.0378 (0.0729) Data: 0.0295 (0.0601) Loss: 0.6782 (0.7873)
[2022/12/29 02:17] | VALID(088): [100/220] Batch: 0.0257 (0.0542) Data: 0.0121 (0.0413) Loss: 1.7478 (0.8437)
[2022/12/29 02:17] | VALID(088): [150/220] Batch: 0.0296 (0.0455) Data: 0.0077 (0.0319) Loss: 0.9439 (0.8530)
[2022/12/29 02:17] | VALID(088): [200/220] Batch: 0.0556 (0.0439) Data: 0.0287 (0.0303) Loss: 0.6027 (0.8523)
[2022/12/29 02:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:17] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:17] |    VALID(88)      0.8495      0.8077      0.8672      0.8077      0.8077      0.8077      0.9519
[2022/12/29 02:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:17] | ####################################################################################################
[2022/12/29 02:17] | TRAIN(089): [ 50/879] Batch: 0.1337 (0.1749) Data: 0.0102 (0.0514) Loss: 0.3805 (0.3466)
[2022/12/29 02:17] | TRAIN(089): [100/879] Batch: 0.1209 (0.1549) Data: 0.0110 (0.0318) Loss: 0.2681 (0.3384)
[2022/12/29 02:17] | TRAIN(089): [150/879] Batch: 0.1196 (0.1477) Data: 0.0082 (0.0246) Loss: 0.6075 (0.3477)
[2022/12/29 02:17] | TRAIN(089): [200/879] Batch: 0.1188 (0.1441) Data: 0.0082 (0.0211) Loss: 0.3520 (0.3533)
[2022/12/29 02:17] | TRAIN(089): [250/879] Batch: 0.1439 (0.1418) Data: 0.0099 (0.0190) Loss: 0.2428 (0.3468)
[2022/12/29 02:17] | TRAIN(089): [300/879] Batch: 0.1384 (0.1402) Data: 0.0092 (0.0173) Loss: 0.2649 (0.3453)
[2022/12/29 02:18] | TRAIN(089): [350/879] Batch: 0.1497 (0.1396) Data: 0.0116 (0.0163) Loss: 0.7166 (0.3397)
[2022/12/29 02:18] | TRAIN(089): [400/879] Batch: 0.1392 (0.1388) Data: 0.0164 (0.0155) Loss: 0.2588 (0.3432)
[2022/12/29 02:18] | TRAIN(089): [450/879] Batch: 0.1351 (0.1383) Data: 0.0104 (0.0149) Loss: 0.4963 (0.3440)
[2022/12/29 02:18] | TRAIN(089): [500/879] Batch: 0.1207 (0.1381) Data: 0.0082 (0.0144) Loss: 0.2725 (0.3431)
[2022/12/29 02:18] | TRAIN(089): [550/879] Batch: 0.1251 (0.1378) Data: 0.0098 (0.0139) Loss: 0.1452 (0.3384)
[2022/12/29 02:18] | TRAIN(089): [600/879] Batch: 0.1350 (0.1375) Data: 0.0087 (0.0136) Loss: 0.1595 (0.3349)
[2022/12/29 02:18] | TRAIN(089): [650/879] Batch: 0.1175 (0.1369) Data: 0.0093 (0.0132) Loss: 0.2597 (0.3334)
[2022/12/29 02:18] | TRAIN(089): [700/879] Batch: 0.1288 (0.1371) Data: 0.0096 (0.0129) Loss: 0.5674 (0.3310)
[2022/12/29 02:18] | TRAIN(089): [750/879] Batch: 0.1349 (0.1371) Data: 0.0102 (0.0127) Loss: 0.3456 (0.3298)
[2022/12/29 02:19] | TRAIN(089): [800/879] Batch: 0.1398 (0.1368) Data: 0.0080 (0.0125) Loss: 0.3882 (0.3278)
[2022/12/29 02:19] | TRAIN(089): [850/879] Batch: 0.1128 (0.1365) Data: 0.0095 (0.0123) Loss: 0.2666 (0.3293)
[2022/12/29 02:19] | ------------------------------------------------------------
[2022/12/29 02:19] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:19] | ------------------------------------------------------------
[2022/12/29 02:19] |    TRAIN(89)     0:01:59     0:00:10     0:01:48      0.3298
[2022/12/29 02:19] | ------------------------------------------------------------
[2022/12/29 02:19] | VALID(089): [ 50/220] Batch: 0.0488 (0.0705) Data: 0.0354 (0.0565) Loss: 0.6849 (0.7435)
[2022/12/29 02:19] | VALID(089): [100/220] Batch: 0.0468 (0.0562) Data: 0.0247 (0.0439) Loss: 1.6602 (0.8098)
[2022/12/29 02:19] | VALID(089): [150/220] Batch: 0.0429 (0.0514) Data: 0.0229 (0.0388) Loss: 0.9413 (0.8171)
[2022/12/29 02:19] | VALID(089): [200/220] Batch: 0.0489 (0.0491) Data: 0.0291 (0.0361) Loss: 0.6082 (0.8174)
[2022/12/29 02:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:19] |    VALID(89)      0.8162      0.8113      0.8683      0.8113      0.8113      0.8113      0.9528
[2022/12/29 02:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:19] | ####################################################################################################
[2022/12/29 02:19] | TRAIN(090): [ 50/879] Batch: 0.1201 (0.1758) Data: 0.0083 (0.0529) Loss: 0.3013 (0.3379)
[2022/12/29 02:19] | TRAIN(090): [100/879] Batch: 0.1165 (0.1529) Data: 0.0083 (0.0311) Loss: 0.4029 (0.3180)
[2022/12/29 02:19] | TRAIN(090): [150/879] Batch: 0.1180 (0.1466) Data: 0.0085 (0.0240) Loss: 0.3291 (0.3155)
[2022/12/29 02:19] | TRAIN(090): [200/879] Batch: 0.0806 (0.1421) Data: 0.0086 (0.0206) Loss: 0.1946 (0.3159)
[2022/12/29 02:19] | TRAIN(090): [250/879] Batch: 0.1256 (0.1373) Data: 0.0073 (0.0183) Loss: 0.6642 (0.3210)
[2022/12/29 02:20] | TRAIN(090): [300/879] Batch: 0.1302 (0.1368) Data: 0.0097 (0.0168) Loss: 0.2904 (0.3231)
[2022/12/29 02:20] | TRAIN(090): [350/879] Batch: 0.1320 (0.1324) Data: 0.0097 (0.0157) Loss: 0.1832 (0.3240)
[2022/12/29 02:20] | TRAIN(090): [400/879] Batch: 0.1387 (0.1332) Data: 0.0083 (0.0149) Loss: 0.3634 (0.3250)
[2022/12/29 02:20] | TRAIN(090): [450/879] Batch: 0.1257 (0.1339) Data: 0.0093 (0.0143) Loss: 0.4183 (0.3255)
[2022/12/29 02:20] | TRAIN(090): [500/879] Batch: 0.1172 (0.1340) Data: 0.0088 (0.0138) Loss: 0.2777 (0.3247)
[2022/12/29 02:20] | TRAIN(090): [550/879] Batch: 0.1528 (0.1337) Data: 0.0094 (0.0133) Loss: 0.2964 (0.3249)
[2022/12/29 02:20] | TRAIN(090): [600/879] Batch: 0.1334 (0.1340) Data: 0.0101 (0.0130) Loss: 0.3893 (0.3257)
[2022/12/29 02:20] | TRAIN(090): [650/879] Batch: 0.1241 (0.1338) Data: 0.0079 (0.0127) Loss: 0.3228 (0.3252)
[2022/12/29 02:20] | TRAIN(090): [700/879] Batch: 0.1240 (0.1333) Data: 0.0085 (0.0124) Loss: 0.1905 (0.3247)
[2022/12/29 02:21] | TRAIN(090): [750/879] Batch: 0.1423 (0.1330) Data: 0.0078 (0.0121) Loss: 0.2853 (0.3229)
[2022/12/29 02:21] | TRAIN(090): [800/879] Batch: 0.1196 (0.1327) Data: 0.0091 (0.0118) Loss: 0.4547 (0.3243)
[2022/12/29 02:21] | TRAIN(090): [850/879] Batch: 0.1291 (0.1324) Data: 0.0091 (0.0116) Loss: 0.3533 (0.3240)
[2022/12/29 02:21] | ------------------------------------------------------------
[2022/12/29 02:21] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:21] | ------------------------------------------------------------
[2022/12/29 02:21] |    TRAIN(90)     0:01:56     0:00:10     0:01:46      0.3245
[2022/12/29 02:21] | ------------------------------------------------------------
[2022/12/29 02:21] | VALID(090): [ 50/220] Batch: 0.0338 (0.0730) Data: 0.0196 (0.0605) Loss: 0.7034 (0.7558)
[2022/12/29 02:21] | VALID(090): [100/220] Batch: 0.0285 (0.0572) Data: 0.0364 (0.0452) Loss: 1.6001 (0.8034)
[2022/12/29 02:21] | VALID(090): [150/220] Batch: 0.0268 (0.0520) Data: 0.0418 (0.0401) Loss: 0.8844 (0.8136)
[2022/12/29 02:21] | VALID(090): [200/220] Batch: 0.0493 (0.0494) Data: 0.0338 (0.0379) Loss: 0.6187 (0.8099)
[2022/12/29 02:21] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:21] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:21] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:21] |    VALID(90)      0.8079      0.8097      0.8697      0.8097      0.8097      0.8097      0.9524
[2022/12/29 02:21] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:21] | ####################################################################################################
[2022/12/29 02:21] | TRAIN(091): [ 50/879] Batch: 0.1222 (0.1688) Data: 0.0187 (0.0459) Loss: 0.2685 (0.3321)
[2022/12/29 02:21] | TRAIN(091): [100/879] Batch: 0.1376 (0.1507) Data: 0.0103 (0.0277) Loss: 0.4105 (0.3349)
[2022/12/29 02:21] | TRAIN(091): [150/879] Batch: 0.1407 (0.1464) Data: 0.0094 (0.0218) Loss: 0.4339 (0.3331)
[2022/12/29 02:21] | TRAIN(091): [200/879] Batch: 0.1172 (0.1434) Data: 0.0086 (0.0187) Loss: 0.5692 (0.3354)
[2022/12/29 02:22] | TRAIN(091): [250/879] Batch: 0.1288 (0.1407) Data: 0.0095 (0.0167) Loss: 0.2052 (0.3274)
[2022/12/29 02:22] | TRAIN(091): [300/879] Batch: 0.1234 (0.1388) Data: 0.0092 (0.0153) Loss: 0.2573 (0.3238)
[2022/12/29 02:22] | TRAIN(091): [350/879] Batch: 0.1182 (0.1373) Data: 0.0087 (0.0143) Loss: 0.5485 (0.3241)
[2022/12/29 02:22] | TRAIN(091): [400/879] Batch: 0.1193 (0.1369) Data: 0.0088 (0.0136) Loss: 0.4073 (0.3277)
[2022/12/29 02:22] | TRAIN(091): [450/879] Batch: 0.1407 (0.1361) Data: 0.0089 (0.0130) Loss: 0.3793 (0.3277)
[2022/12/29 02:22] | TRAIN(091): [500/879] Batch: 0.1343 (0.1358) Data: 0.0104 (0.0127) Loss: 0.4941 (0.3286)
[2022/12/29 02:22] | TRAIN(091): [550/879] Batch: 0.1354 (0.1355) Data: 0.0106 (0.0123) Loss: 0.1856 (0.3251)
[2022/12/29 02:22] | TRAIN(091): [600/879] Batch: 0.0811 (0.1349) Data: 0.0096 (0.0120) Loss: 0.4745 (0.3243)
[2022/12/29 02:22] | TRAIN(091): [650/879] Batch: 0.1276 (0.1335) Data: 0.0081 (0.0118) Loss: 0.4259 (0.3266)
[2022/12/29 02:23] | TRAIN(091): [700/879] Batch: 0.1338 (0.1332) Data: 0.0094 (0.0115) Loss: 0.3313 (0.3244)
[2022/12/29 02:23] | TRAIN(091): [750/879] Batch: 0.1288 (0.1316) Data: 0.0100 (0.0114) Loss: 0.3267 (0.3240)
[2022/12/29 02:23] | TRAIN(091): [800/879] Batch: 0.1466 (0.1320) Data: 0.0093 (0.0113) Loss: 0.2313 (0.3233)
[2022/12/29 02:23] | TRAIN(091): [850/879] Batch: 0.1262 (0.1319) Data: 0.0096 (0.0112) Loss: 0.2416 (0.3229)
[2022/12/29 02:23] | ------------------------------------------------------------
[2022/12/29 02:23] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:23] | ------------------------------------------------------------
[2022/12/29 02:23] |    TRAIN(91)     0:01:55     0:00:09     0:01:46      0.3226
[2022/12/29 02:23] | ------------------------------------------------------------
[2022/12/29 02:23] | VALID(091): [ 50/220] Batch: 0.0522 (0.0712) Data: 0.0339 (0.0585) Loss: 0.6441 (0.7295)
[2022/12/29 02:23] | VALID(091): [100/220] Batch: 0.0460 (0.0563) Data: 0.0338 (0.0448) Loss: 1.5305 (0.7865)
[2022/12/29 02:23] | VALID(091): [150/220] Batch: 0.0478 (0.0514) Data: 0.0335 (0.0402) Loss: 0.8803 (0.7975)
[2022/12/29 02:23] | VALID(091): [200/220] Batch: 0.0437 (0.0490) Data: 0.0335 (0.0368) Loss: 0.5059 (0.7917)
[2022/12/29 02:23] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:23] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:23] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:23] |    VALID(91)      0.7896      0.8124      0.8717      0.8124      0.8124      0.8124      0.9531
[2022/12/29 02:23] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:23] | ####################################################################################################
[2022/12/29 02:23] | TRAIN(092): [ 50/879] Batch: 0.1348 (0.1734) Data: 0.0099 (0.0464) Loss: 0.3970 (0.3222)
[2022/12/29 02:23] | TRAIN(092): [100/879] Batch: 0.1357 (0.1560) Data: 0.0117 (0.0285) Loss: 0.2879 (0.3213)
[2022/12/29 02:23] | TRAIN(092): [150/879] Batch: 0.1434 (0.1483) Data: 0.0074 (0.0220) Loss: 0.4868 (0.3229)
[2022/12/29 02:24] | TRAIN(092): [200/879] Batch: 0.1205 (0.1434) Data: 0.0082 (0.0186) Loss: 0.3854 (0.3197)
[2022/12/29 02:24] | TRAIN(092): [250/879] Batch: 0.1203 (0.1393) Data: 0.0083 (0.0165) Loss: 0.2824 (0.3201)
[2022/12/29 02:24] | TRAIN(092): [300/879] Batch: 0.1392 (0.1374) Data: 0.0076 (0.0151) Loss: 0.1894 (0.3195)
[2022/12/29 02:24] | TRAIN(092): [350/879] Batch: 0.1417 (0.1366) Data: 0.0157 (0.0143) Loss: 0.3702 (0.3156)
[2022/12/29 02:24] | TRAIN(092): [400/879] Batch: 0.1406 (0.1358) Data: 0.0070 (0.0136) Loss: 0.4882 (0.3164)
[2022/12/29 02:24] | TRAIN(092): [450/879] Batch: 0.1406 (0.1353) Data: 0.0073 (0.0131) Loss: 0.2232 (0.3178)
[2022/12/29 02:24] | TRAIN(092): [500/879] Batch: 0.1500 (0.1352) Data: 0.0093 (0.0127) Loss: 0.3960 (0.3157)
[2022/12/29 02:24] | TRAIN(092): [550/879] Batch: 0.1249 (0.1349) Data: 0.0082 (0.0124) Loss: 0.3727 (0.3157)
[2022/12/29 02:24] | TRAIN(092): [600/879] Batch: 0.1212 (0.1349) Data: 0.0083 (0.0121) Loss: 0.2537 (0.3152)
[2022/12/29 02:25] | TRAIN(092): [650/879] Batch: 0.1420 (0.1344) Data: 0.0073 (0.0119) Loss: 0.2700 (0.3192)
[2022/12/29 02:25] | TRAIN(092): [700/879] Batch: 0.1193 (0.1340) Data: 0.0087 (0.0116) Loss: 0.4933 (0.3194)
[2022/12/29 02:25] | TRAIN(092): [750/879] Batch: 0.1424 (0.1337) Data: 0.0075 (0.0114) Loss: 0.3478 (0.3213)
[2022/12/29 02:25] | TRAIN(092): [800/879] Batch: 0.1402 (0.1335) Data: 0.0084 (0.0113) Loss: 0.2916 (0.3203)
[2022/12/29 02:25] | TRAIN(092): [850/879] Batch: 0.1272 (0.1337) Data: 0.0099 (0.0112) Loss: 0.2051 (0.3219)
[2022/12/29 02:25] | ------------------------------------------------------------
[2022/12/29 02:25] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:25] | ------------------------------------------------------------
[2022/12/29 02:25] |    TRAIN(92)     0:01:57     0:00:09     0:01:47      0.3214
[2022/12/29 02:25] | ------------------------------------------------------------
[2022/12/29 02:25] | VALID(092): [ 50/220] Batch: 0.0497 (0.0718) Data: 0.0338 (0.0613) Loss: 0.6507 (0.7649)
[2022/12/29 02:25] | VALID(092): [100/220] Batch: 0.0436 (0.0569) Data: 0.0373 (0.0464) Loss: 1.5620 (0.8118)
[2022/12/29 02:25] | VALID(092): [150/220] Batch: 0.0483 (0.0519) Data: 0.0245 (0.0414) Loss: 0.9497 (0.8236)
[2022/12/29 02:25] | VALID(092): [200/220] Batch: 0.0424 (0.0494) Data: 0.0172 (0.0388) Loss: 0.6153 (0.8180)
[2022/12/29 02:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:25] |    VALID(92)      0.8165      0.8100      0.8691      0.8100      0.8100      0.8100      0.9525
[2022/12/29 02:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:25] | ####################################################################################################
[2022/12/29 02:25] | TRAIN(093): [ 50/879] Batch: 0.1486 (0.1553) Data: 0.0113 (0.0504) Loss: 0.2456 (0.3602)
[2022/12/29 02:25] | TRAIN(093): [100/879] Batch: 0.1350 (0.1437) Data: 0.0073 (0.0303) Loss: 0.1459 (0.3543)
[2022/12/29 02:26] | TRAIN(093): [150/879] Batch: 0.1442 (0.1318) Data: 0.0099 (0.0233) Loss: 0.2850 (0.3374)
[2022/12/29 02:26] | TRAIN(093): [200/879] Batch: 0.1356 (0.1333) Data: 0.0110 (0.0200) Loss: 0.1455 (0.3368)
[2022/12/29 02:26] | TRAIN(093): [250/879] Batch: 0.1369 (0.1341) Data: 0.0088 (0.0180) Loss: 0.1748 (0.3370)
[2022/12/29 02:26] | TRAIN(093): [300/879] Batch: 0.1415 (0.1344) Data: 0.0103 (0.0166) Loss: 0.3874 (0.3360)
[2022/12/29 02:26] | TRAIN(093): [350/879] Batch: 0.1204 (0.1346) Data: 0.0083 (0.0157) Loss: 0.3031 (0.3333)
[2022/12/29 02:26] | TRAIN(093): [400/879] Batch: 0.1421 (0.1349) Data: 0.0094 (0.0149) Loss: 0.4692 (0.3284)
[2022/12/29 02:26] | TRAIN(093): [450/879] Batch: 0.1332 (0.1349) Data: 0.0097 (0.0143) Loss: 0.2845 (0.3280)
[2022/12/29 02:26] | TRAIN(093): [500/879] Batch: 0.1473 (0.1348) Data: 0.0094 (0.0138) Loss: 0.1686 (0.3255)
[2022/12/29 02:26] | TRAIN(093): [550/879] Batch: 0.1451 (0.1351) Data: 0.0098 (0.0135) Loss: 0.2288 (0.3251)
[2022/12/29 02:27] | TRAIN(093): [600/879] Batch: 0.1388 (0.1352) Data: 0.0094 (0.0131) Loss: 0.1186 (0.3263)
[2022/12/29 02:27] | TRAIN(093): [650/879] Batch: 0.1240 (0.1350) Data: 0.0082 (0.0128) Loss: 0.2846 (0.3253)
[2022/12/29 02:27] | TRAIN(093): [700/879] Batch: 0.1402 (0.1345) Data: 0.0106 (0.0125) Loss: 0.4513 (0.3250)
[2022/12/29 02:27] | TRAIN(093): [750/879] Batch: 0.1337 (0.1349) Data: 0.0101 (0.0123) Loss: 0.4763 (0.3246)
[2022/12/29 02:27] | TRAIN(093): [800/879] Batch: 0.1403 (0.1346) Data: 0.0072 (0.0121) Loss: 0.5209 (0.3237)
[2022/12/29 02:27] | TRAIN(093): [850/879] Batch: 0.1208 (0.1344) Data: 0.0104 (0.0119) Loss: 0.4602 (0.3255)
[2022/12/29 02:27] | ------------------------------------------------------------
[2022/12/29 02:27] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:27] | ------------------------------------------------------------
[2022/12/29 02:27] |    TRAIN(93)     0:01:57     0:00:10     0:01:47      0.3264
[2022/12/29 02:27] | ------------------------------------------------------------
[2022/12/29 02:27] | VALID(093): [ 50/220] Batch: 0.0444 (0.0710) Data: 0.0189 (0.0561) Loss: 0.6854 (0.7422)
[2022/12/29 02:27] | VALID(093): [100/220] Batch: 0.0295 (0.0562) Data: 0.0389 (0.0436) Loss: 1.5246 (0.7936)
[2022/12/29 02:27] | VALID(093): [150/220] Batch: 0.0444 (0.0513) Data: 0.0352 (0.0394) Loss: 0.8672 (0.8045)
[2022/12/29 02:27] | VALID(093): [200/220] Batch: 0.0435 (0.0488) Data: 0.0344 (0.0372) Loss: 0.5315 (0.7968)
[2022/12/29 02:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:27] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:27] |    VALID(93)      0.7938      0.8070      0.8695      0.8070      0.8070      0.8070      0.9518
[2022/12/29 02:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:27] | ####################################################################################################
[2022/12/29 02:28] | TRAIN(094): [ 50/879] Batch: 0.1333 (0.1762) Data: 0.0099 (0.0488) Loss: 0.2797 (0.3490)
[2022/12/29 02:28] | TRAIN(094): [100/879] Batch: 0.1454 (0.1561) Data: 0.0094 (0.0297) Loss: 0.1620 (0.3254)
[2022/12/29 02:28] | TRAIN(094): [150/879] Batch: 0.1454 (0.1492) Data: 0.0094 (0.0230) Loss: 0.3604 (0.3232)
[2022/12/29 02:28] | TRAIN(094): [200/879] Batch: 0.1365 (0.1486) Data: 0.0117 (0.0198) Loss: 0.3418 (0.3162)
[2022/12/29 02:28] | TRAIN(094): [250/879] Batch: 0.1316 (0.1468) Data: 0.0092 (0.0177) Loss: 0.2205 (0.3181)
[2022/12/29 02:28] | TRAIN(094): [300/879] Batch: 0.1529 (0.1448) Data: 0.0097 (0.0162) Loss: 0.4587 (0.3181)
[2022/12/29 02:28] | TRAIN(094): [350/879] Batch: 0.1529 (0.1437) Data: 0.0094 (0.0153) Loss: 0.2040 (0.3207)
[2022/12/29 02:28] | TRAIN(094): [400/879] Batch: 0.1347 (0.1404) Data: 0.0104 (0.0146) Loss: 0.2696 (0.3213)
[2022/12/29 02:28] | TRAIN(094): [450/879] Batch: 0.1254 (0.1399) Data: 0.0111 (0.0141) Loss: 0.2752 (0.3196)
[2022/12/29 02:29] | TRAIN(094): [500/879] Batch: 0.1383 (0.1369) Data: 0.0100 (0.0137) Loss: 0.2668 (0.3192)
[2022/12/29 02:29] | TRAIN(094): [550/879] Batch: 0.1472 (0.1369) Data: 0.0096 (0.0133) Loss: 0.7437 (0.3211)
[2022/12/29 02:29] | TRAIN(094): [600/879] Batch: 0.1346 (0.1366) Data: 0.0093 (0.0130) Loss: 0.4004 (0.3225)
[2022/12/29 02:29] | TRAIN(094): [650/879] Batch: 0.1214 (0.1361) Data: 0.0106 (0.0127) Loss: 0.3429 (0.3226)
[2022/12/29 02:29] | TRAIN(094): [700/879] Batch: 0.1195 (0.1355) Data: 0.0083 (0.0123) Loss: 0.3238 (0.3223)
[2022/12/29 02:29] | TRAIN(094): [750/879] Batch: 0.1262 (0.1350) Data: 0.0092 (0.0121) Loss: 0.2832 (0.3236)
[2022/12/29 02:29] | TRAIN(094): [800/879] Batch: 0.1413 (0.1346) Data: 0.0075 (0.0118) Loss: 0.3164 (0.3216)
[2022/12/29 02:29] | TRAIN(094): [850/879] Batch: 0.1427 (0.1344) Data: 0.0074 (0.0116) Loss: 0.2124 (0.3223)
[2022/12/29 02:29] | ------------------------------------------------------------
[2022/12/29 02:29] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:29] | ------------------------------------------------------------
[2022/12/29 02:29] |    TRAIN(94)     0:01:57     0:00:10     0:01:47      0.3230
[2022/12/29 02:29] | ------------------------------------------------------------
[2022/12/29 02:29] | VALID(094): [ 50/220] Batch: 0.0435 (0.0704) Data: 0.0363 (0.0570) Loss: 0.7924 (0.7684)
[2022/12/29 02:29] | VALID(094): [100/220] Batch: 0.0466 (0.0560) Data: 0.0244 (0.0441) Loss: 1.5947 (0.8124)
[2022/12/29 02:29] | VALID(094): [150/220] Batch: 0.0323 (0.0511) Data: 0.0294 (0.0399) Loss: 0.8621 (0.8202)
[2022/12/29 02:30] | VALID(094): [200/220] Batch: 0.0283 (0.0487) Data: 0.0388 (0.0377) Loss: 0.5908 (0.8109)
[2022/12/29 02:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:30] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:30] |    VALID(94)      0.8098      0.8017      0.8682      0.8017      0.8017      0.8017      0.9504
[2022/12/29 02:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:30] | ####################################################################################################
[2022/12/29 02:30] | TRAIN(095): [ 50/879] Batch: 0.1391 (0.1787) Data: 0.0102 (0.0501) Loss: 0.3217 (0.3117)
[2022/12/29 02:30] | TRAIN(095): [100/879] Batch: 0.1263 (0.1587) Data: 0.0100 (0.0304) Loss: 0.4278 (0.3142)
[2022/12/29 02:30] | TRAIN(095): [150/879] Batch: 0.1418 (0.1499) Data: 0.0071 (0.0234) Loss: 0.2074 (0.3223)
[2022/12/29 02:30] | TRAIN(095): [200/879] Batch: 0.1167 (0.1456) Data: 0.0084 (0.0199) Loss: 0.3740 (0.3170)
[2022/12/29 02:30] | TRAIN(095): [250/879] Batch: 0.1229 (0.1424) Data: 0.0101 (0.0177) Loss: 0.2480 (0.3198)
[2022/12/29 02:30] | TRAIN(095): [300/879] Batch: 0.1441 (0.1399) Data: 0.0072 (0.0162) Loss: 0.4217 (0.3198)
[2022/12/29 02:30] | TRAIN(095): [350/879] Batch: 0.1264 (0.1381) Data: 0.0086 (0.0150) Loss: 0.4249 (0.3188)
[2022/12/29 02:30] | TRAIN(095): [400/879] Batch: 0.1299 (0.1369) Data: 0.0107 (0.0142) Loss: 0.5418 (0.3196)
[2022/12/29 02:31] | TRAIN(095): [450/879] Batch: 0.1231 (0.1359) Data: 0.0082 (0.0135) Loss: 0.1608 (0.3193)
[2022/12/29 02:31] | TRAIN(095): [500/879] Batch: 0.1289 (0.1352) Data: 0.0089 (0.0130) Loss: 0.2433 (0.3232)
[2022/12/29 02:31] | TRAIN(095): [550/879] Batch: 0.1401 (0.1346) Data: 0.0076 (0.0126) Loss: 0.3690 (0.3242)
[2022/12/29 02:31] | TRAIN(095): [600/879] Batch: 0.1240 (0.1341) Data: 0.0084 (0.0123) Loss: 0.4519 (0.3244)
[2022/12/29 02:31] | TRAIN(095): [650/879] Batch: 0.1258 (0.1337) Data: 0.0093 (0.0120) Loss: 0.1777 (0.3241)
[2022/12/29 02:31] | TRAIN(095): [700/879] Batch: 0.1221 (0.1334) Data: 0.0084 (0.0117) Loss: 0.3825 (0.3216)
[2022/12/29 02:31] | TRAIN(095): [750/879] Batch: 0.1526 (0.1335) Data: 0.0094 (0.0116) Loss: 0.4155 (0.3204)
[2022/12/29 02:31] | TRAIN(095): [800/879] Batch: 0.1398 (0.1325) Data: 0.0083 (0.0114) Loss: 0.2594 (0.3208)
[2022/12/29 02:31] | TRAIN(095): [850/879] Batch: 0.1410 (0.1329) Data: 0.0094 (0.0114) Loss: 0.1963 (0.3213)
[2022/12/29 02:31] | ------------------------------------------------------------
[2022/12/29 02:31] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:31] | ------------------------------------------------------------
[2022/12/29 02:31] |    TRAIN(95)     0:01:56     0:00:09     0:01:46      0.3227
[2022/12/29 02:31] | ------------------------------------------------------------
[2022/12/29 02:32] | VALID(095): [ 50/220] Batch: 0.0443 (0.0719) Data: 0.0353 (0.0604) Loss: 0.7053 (0.7777)
[2022/12/29 02:32] | VALID(095): [100/220] Batch: 0.0440 (0.0566) Data: 0.0330 (0.0459) Loss: 1.6600 (0.8316)
[2022/12/29 02:32] | VALID(095): [150/220] Batch: 0.0434 (0.0515) Data: 0.0331 (0.0409) Loss: 0.9561 (0.8418)
[2022/12/29 02:32] | VALID(095): [200/220] Batch: 0.0443 (0.0490) Data: 0.0350 (0.0385) Loss: 0.5916 (0.8381)
[2022/12/29 02:32] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:32] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:32] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:32] |    VALID(95)      0.8352      0.8098      0.8673      0.8098      0.8098      0.8098      0.9525
[2022/12/29 02:32] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:32] | ####################################################################################################
[2022/12/29 02:32] | TRAIN(096): [ 50/879] Batch: 0.1322 (0.1736) Data: 0.0102 (0.0480) Loss: 0.3679 (0.3273)
[2022/12/29 02:32] | TRAIN(096): [100/879] Batch: 0.1389 (0.1552) Data: 0.0101 (0.0292) Loss: 0.2340 (0.3227)
[2022/12/29 02:32] | TRAIN(096): [150/879] Batch: 0.1404 (0.1486) Data: 0.0074 (0.0228) Loss: 0.5508 (0.3277)
[2022/12/29 02:32] | TRAIN(096): [200/879] Batch: 0.1203 (0.1446) Data: 0.0104 (0.0193) Loss: 0.3831 (0.3227)
[2022/12/29 02:32] | TRAIN(096): [250/879] Batch: 0.1291 (0.1413) Data: 0.0080 (0.0171) Loss: 0.4258 (0.3249)
[2022/12/29 02:32] | TRAIN(096): [300/879] Batch: 0.1249 (0.1390) Data: 0.0082 (0.0156) Loss: 0.2827 (0.3281)
[2022/12/29 02:32] | TRAIN(096): [350/879] Batch: 0.1338 (0.1380) Data: 0.0103 (0.0147) Loss: 0.5103 (0.3304)
[2022/12/29 02:33] | TRAIN(096): [400/879] Batch: 0.1422 (0.1378) Data: 0.0099 (0.0141) Loss: 0.3328 (0.3281)
[2022/12/29 02:33] | TRAIN(096): [450/879] Batch: 0.1311 (0.1376) Data: 0.0098 (0.0136) Loss: 0.2702 (0.3296)
[2022/12/29 02:33] | TRAIN(096): [500/879] Batch: 0.1400 (0.1373) Data: 0.0099 (0.0132) Loss: 0.2711 (0.3298)
[2022/12/29 02:33] | TRAIN(096): [550/879] Batch: 0.1420 (0.1371) Data: 0.0072 (0.0129) Loss: 0.3037 (0.3261)
[2022/12/29 02:33] | TRAIN(096): [600/879] Batch: 0.1122 (0.1362) Data: 0.0109 (0.0125) Loss: 0.2493 (0.3274)
[2022/12/29 02:33] | TRAIN(096): [650/879] Batch: 0.1302 (0.1362) Data: 0.0096 (0.0123) Loss: 0.4453 (0.3278)
[2022/12/29 02:33] | TRAIN(096): [700/879] Batch: 0.1324 (0.1364) Data: 0.0112 (0.0121) Loss: 0.2093 (0.3260)
[2022/12/29 02:33] | TRAIN(096): [750/879] Batch: 0.1320 (0.1364) Data: 0.0105 (0.0120) Loss: 0.2299 (0.3244)
[2022/12/29 02:33] | TRAIN(096): [800/879] Batch: 0.1238 (0.1359) Data: 0.0104 (0.0118) Loss: 0.2192 (0.3239)
[2022/12/29 02:34] | TRAIN(096): [850/879] Batch: 0.1218 (0.1355) Data: 0.0076 (0.0116) Loss: 0.2351 (0.3240)
[2022/12/29 02:34] | ------------------------------------------------------------
[2022/12/29 02:34] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:34] | ------------------------------------------------------------
[2022/12/29 02:34] |    TRAIN(96)     0:01:59     0:00:10     0:01:48      0.3244
[2022/12/29 02:34] | ------------------------------------------------------------
[2022/12/29 02:34] | VALID(096): [ 50/220] Batch: 0.0435 (0.0702) Data: 0.0349 (0.0593) Loss: 0.6489 (0.7447)
[2022/12/29 02:34] | VALID(096): [100/220] Batch: 0.0303 (0.0555) Data: 0.0398 (0.0451) Loss: 1.5767 (0.7965)
[2022/12/29 02:34] | VALID(096): [150/220] Batch: 0.0335 (0.0509) Data: 0.0384 (0.0405) Loss: 0.9334 (0.8082)
[2022/12/29 02:34] | VALID(096): [200/220] Batch: 0.0347 (0.0484) Data: 0.0168 (0.0379) Loss: 0.5714 (0.8031)
[2022/12/29 02:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:34] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:34] |    VALID(96)      0.8023      0.8110      0.8694      0.8110      0.8110      0.8110      0.9527
[2022/12/29 02:34] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:34] | ####################################################################################################
[2022/12/29 02:34] | TRAIN(097): [ 50/879] Batch: 0.1278 (0.1776) Data: 0.0116 (0.0497) Loss: 0.3119 (0.3104)
[2022/12/29 02:34] | TRAIN(097): [100/879] Batch: 0.1323 (0.1569) Data: 0.0098 (0.0299) Loss: 0.1928 (0.3361)
[2022/12/29 02:34] | TRAIN(097): [150/879] Batch: 0.1429 (0.1505) Data: 0.0094 (0.0233) Loss: 0.2915 (0.3368)
[2022/12/29 02:34] | TRAIN(097): [200/879] Batch: 0.1415 (0.1411) Data: 0.0074 (0.0199) Loss: 0.1547 (0.3347)
[2022/12/29 02:34] | TRAIN(097): [250/879] Batch: 0.1355 (0.1398) Data: 0.0082 (0.0178) Loss: 0.1722 (0.3284)
[2022/12/29 02:34] | TRAIN(097): [300/879] Batch: 0.1406 (0.1340) Data: 0.0096 (0.0163) Loss: 0.3362 (0.3231)
[2022/12/29 02:35] | TRAIN(097): [350/879] Batch: 0.1377 (0.1344) Data: 0.0149 (0.0154) Loss: 0.2392 (0.3229)
[2022/12/29 02:35] | TRAIN(097): [400/879] Batch: 0.1524 (0.1346) Data: 0.0094 (0.0147) Loss: 0.2029 (0.3232)
[2022/12/29 02:35] | TRAIN(097): [450/879] Batch: 0.1274 (0.1348) Data: 0.0097 (0.0142) Loss: 0.2640 (0.3193)
[2022/12/29 02:35] | TRAIN(097): [500/879] Batch: 0.1398 (0.1346) Data: 0.0081 (0.0136) Loss: 0.3002 (0.3199)
[2022/12/29 02:35] | TRAIN(097): [550/879] Batch: 0.1335 (0.1347) Data: 0.0093 (0.0133) Loss: 0.5850 (0.3186)
[2022/12/29 02:35] | TRAIN(097): [600/879] Batch: 0.1273 (0.1347) Data: 0.0100 (0.0130) Loss: 0.1749 (0.3171)
[2022/12/29 02:35] | TRAIN(097): [650/879] Batch: 0.1273 (0.1344) Data: 0.0096 (0.0127) Loss: 0.2043 (0.3151)
[2022/12/29 02:35] | TRAIN(097): [700/879] Batch: 0.1094 (0.1342) Data: 0.0091 (0.0124) Loss: 0.1731 (0.3151)
[2022/12/29 02:35] | TRAIN(097): [750/879] Batch: 0.1365 (0.1337) Data: 0.0086 (0.0122) Loss: 0.4715 (0.3152)
[2022/12/29 02:36] | TRAIN(097): [800/879] Batch: 0.1209 (0.1333) Data: 0.0078 (0.0119) Loss: 0.4357 (0.3174)
[2022/12/29 02:36] | TRAIN(097): [850/879] Batch: 0.1422 (0.1330) Data: 0.0072 (0.0117) Loss: 0.3298 (0.3168)
[2022/12/29 02:36] | ------------------------------------------------------------
[2022/12/29 02:36] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:36] | ------------------------------------------------------------
[2022/12/29 02:36] |    TRAIN(97)     0:01:56     0:00:10     0:01:46      0.3161
[2022/12/29 02:36] | ------------------------------------------------------------
[2022/12/29 02:36] | VALID(097): [ 50/220] Batch: 0.0325 (0.0721) Data: 0.0319 (0.0597) Loss: 0.7132 (0.7549)
[2022/12/29 02:36] | VALID(097): [100/220] Batch: 0.0348 (0.0569) Data: 0.0218 (0.0438) Loss: 1.6159 (0.8067)
[2022/12/29 02:36] | VALID(097): [150/220] Batch: 0.0254 (0.0518) Data: 0.0306 (0.0381) Loss: 0.9086 (0.8143)
[2022/12/29 02:36] | VALID(097): [200/220] Batch: 0.0476 (0.0490) Data: 0.0294 (0.0350) Loss: 0.6453 (0.8096)
[2022/12/29 02:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:36] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:36] |    VALID(97)      0.8086      0.8079      0.8694      0.8079      0.8079      0.8079      0.9520
[2022/12/29 02:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:36] | ####################################################################################################
[2022/12/29 02:36] | TRAIN(098): [ 50/879] Batch: 0.1428 (0.1723) Data: 0.0094 (0.0488) Loss: 0.1842 (0.2994)
[2022/12/29 02:36] | TRAIN(098): [100/879] Batch: 0.1457 (0.1512) Data: 0.0074 (0.0290) Loss: 0.3539 (0.3171)
[2022/12/29 02:36] | TRAIN(098): [150/879] Batch: 0.1279 (0.1457) Data: 0.0099 (0.0227) Loss: 0.3779 (0.3094)
[2022/12/29 02:36] | TRAIN(098): [200/879] Batch: 0.1352 (0.1434) Data: 0.0102 (0.0195) Loss: 0.2886 (0.3117)
[2022/12/29 02:37] | TRAIN(098): [250/879] Batch: 0.1398 (0.1421) Data: 0.0102 (0.0176) Loss: 0.3217 (0.3130)
[2022/12/29 02:37] | TRAIN(098): [300/879] Batch: 0.1427 (0.1411) Data: 0.0092 (0.0163) Loss: 0.3318 (0.3156)
[2022/12/29 02:37] | TRAIN(098): [350/879] Batch: 0.1529 (0.1408) Data: 0.0095 (0.0154) Loss: 0.4058 (0.3130)
[2022/12/29 02:37] | TRAIN(098): [400/879] Batch: 0.1266 (0.1400) Data: 0.0091 (0.0147) Loss: 0.2636 (0.3153)
[2022/12/29 02:37] | TRAIN(098): [450/879] Batch: 0.1407 (0.1388) Data: 0.0073 (0.0140) Loss: 0.2935 (0.3186)
[2022/12/29 02:37] | TRAIN(098): [500/879] Batch: 0.1344 (0.1385) Data: 0.0104 (0.0135) Loss: 0.3344 (0.3185)
[2022/12/29 02:37] | TRAIN(098): [550/879] Batch: 0.1307 (0.1383) Data: 0.0103 (0.0132) Loss: 0.2209 (0.3203)
[2022/12/29 02:37] | TRAIN(098): [600/879] Batch: 0.1313 (0.1360) Data: 0.0088 (0.0128) Loss: 0.4887 (0.3219)
[2022/12/29 02:37] | TRAIN(098): [650/879] Batch: 0.1372 (0.1359) Data: 0.0092 (0.0126) Loss: 0.3416 (0.3214)
[2022/12/29 02:38] | TRAIN(098): [700/879] Batch: 0.1200 (0.1338) Data: 0.0105 (0.0123) Loss: 0.4512 (0.3233)
[2022/12/29 02:38] | TRAIN(098): [750/879] Batch: 0.1423 (0.1337) Data: 0.0073 (0.0121) Loss: 0.3017 (0.3231)
[2022/12/29 02:38] | TRAIN(098): [800/879] Batch: 0.1485 (0.1335) Data: 0.0105 (0.0119) Loss: 0.2371 (0.3220)
[2022/12/29 02:38] | TRAIN(098): [850/879] Batch: 0.1418 (0.1334) Data: 0.0075 (0.0117) Loss: 0.3873 (0.3195)
[2022/12/29 02:38] | ------------------------------------------------------------
[2022/12/29 02:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:38] | ------------------------------------------------------------
[2022/12/29 02:38] |    TRAIN(98)     0:01:57     0:00:10     0:01:46      0.3198
[2022/12/29 02:38] | ------------------------------------------------------------
[2022/12/29 02:38] | VALID(098): [ 50/220] Batch: 0.0322 (0.0708) Data: 0.0341 (0.0585) Loss: 0.6651 (0.7665)
[2022/12/29 02:38] | VALID(098): [100/220] Batch: 0.0274 (0.0561) Data: 0.0317 (0.0442) Loss: 1.6349 (0.8186)
[2022/12/29 02:38] | VALID(098): [150/220] Batch: 0.0390 (0.0512) Data: 0.0170 (0.0397) Loss: 0.9336 (0.8296)
[2022/12/29 02:38] | VALID(098): [200/220] Batch: 0.0442 (0.0486) Data: 0.0386 (0.0375) Loss: 0.5974 (0.8254)
[2022/12/29 02:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:38] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:38] |    VALID(98)      0.8249      0.8091      0.8680      0.8091      0.8091      0.8091      0.9523
[2022/12/29 02:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:38] | ####################################################################################################
[2022/12/29 02:38] | TRAIN(099): [ 50/879] Batch: 0.1395 (0.1755) Data: 0.0094 (0.0489) Loss: 0.1673 (0.3218)
[2022/12/29 02:38] | TRAIN(099): [100/879] Batch: 0.1426 (0.1566) Data: 0.0093 (0.0296) Loss: 0.3940 (0.3134)
[2022/12/29 02:38] | TRAIN(099): [150/879] Batch: 0.1389 (0.1503) Data: 0.0099 (0.0231) Loss: 0.4012 (0.3245)
[2022/12/29 02:39] | TRAIN(099): [200/879] Batch: 0.1550 (0.1472) Data: 0.0091 (0.0198) Loss: 0.3847 (0.3276)
[2022/12/29 02:39] | TRAIN(099): [250/879] Batch: 0.1408 (0.1437) Data: 0.0073 (0.0175) Loss: 0.3511 (0.3232)
[2022/12/29 02:39] | TRAIN(099): [300/879] Batch: 0.1227 (0.1410) Data: 0.0089 (0.0160) Loss: 0.1491 (0.3267)
[2022/12/29 02:39] | TRAIN(099): [350/879] Batch: 0.1338 (0.1393) Data: 0.0095 (0.0149) Loss: 0.4967 (0.3254)
[2022/12/29 02:39] | TRAIN(099): [400/879] Batch: 0.1271 (0.1390) Data: 0.0097 (0.0143) Loss: 0.4631 (0.3269)
[2022/12/29 02:39] | TRAIN(099): [450/879] Batch: 0.1425 (0.1385) Data: 0.0093 (0.0138) Loss: 0.1347 (0.3294)
[2022/12/29 02:39] | TRAIN(099): [500/879] Batch: 0.1283 (0.1384) Data: 0.0100 (0.0134) Loss: 0.2483 (0.3288)
[2022/12/29 02:39] | TRAIN(099): [550/879] Batch: 0.1404 (0.1375) Data: 0.0074 (0.0129) Loss: 0.3216 (0.3260)
[2022/12/29 02:39] | TRAIN(099): [600/879] Batch: 0.1211 (0.1367) Data: 0.0086 (0.0125) Loss: 0.2394 (0.3238)
[2022/12/29 02:40] | TRAIN(099): [650/879] Batch: 0.1227 (0.1361) Data: 0.0087 (0.0122) Loss: 0.3995 (0.3251)
[2022/12/29 02:40] | TRAIN(099): [700/879] Batch: 0.1232 (0.1356) Data: 0.0090 (0.0119) Loss: 0.4118 (0.3271)
[2022/12/29 02:40] | TRAIN(099): [750/879] Batch: 0.1431 (0.1355) Data: 0.0093 (0.0118) Loss: 0.3661 (0.3248)
[2022/12/29 02:40] | TRAIN(099): [800/879] Batch: 0.1248 (0.1352) Data: 0.0075 (0.0116) Loss: 0.1514 (0.3239)
[2022/12/29 02:40] | TRAIN(099): [850/879] Batch: 0.1226 (0.1348) Data: 0.0086 (0.0114) Loss: 0.3717 (0.3249)
[2022/12/29 02:40] | ------------------------------------------------------------
[2022/12/29 02:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:40] | ------------------------------------------------------------
[2022/12/29 02:40] |    TRAIN(99)     0:01:58     0:00:09     0:01:48      0.3246
[2022/12/29 02:40] | ------------------------------------------------------------
[2022/12/29 02:40] | VALID(099): [ 50/220] Batch: 0.0444 (0.0716) Data: 0.0190 (0.0600) Loss: 0.6777 (0.7415)
[2022/12/29 02:40] | VALID(099): [100/220] Batch: 0.0425 (0.0565) Data: 0.0186 (0.0456) Loss: 1.5608 (0.7924)
[2022/12/29 02:40] | VALID(099): [150/220] Batch: 0.0442 (0.0514) Data: 0.0188 (0.0406) Loss: 0.9246 (0.8027)
[2022/12/29 02:40] | VALID(099): [200/220] Batch: 0.0436 (0.0488) Data: 0.0377 (0.0382) Loss: 0.6019 (0.7986)
[2022/12/29 02:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:40] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:40] |    VALID(99)      0.7981      0.8090      0.8690      0.8090      0.8090      0.8090      0.9522
[2022/12/29 02:40] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:40] | ####################################################################################################
