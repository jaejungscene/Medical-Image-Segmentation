[2022/12/28 20:50] | Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/densenet121_ra-50efcf5c.pth)
[2022/12/28 20:50] | ---------------------------------------------------------------------------------
[2022/12/28 20:50] |                                    INFORMATION
[2022/12/28 20:50] | ---------------------------------------------------------------------------------
[2022/12/28 20:50] | Project Name              | MECLA
[2022/12/28 20:50] | Project Administrator     | jaejung
[2022/12/28 20:50] | Experiment Name           | eyepacs_v1_densenet121_v1
[2022/12/28 20:50] | Experiment Start Time     | 2022-12-28 20:50:00
[2022/12/28 20:50] | Experiment Model Name     | densenet121
[2022/12/28 20:50] | Experiment Log Directory  | log/eyepacs_v1_densenet121_v1
[2022/12/28 20:50] | ---------------------------------------------------------------------------------
[2022/12/28 20:50] |                                 EXPERIMENT SETUP
[2022/12/28 20:50] | ---------------------------------------------------------------------------------
[2022/12/28 20:50] | train_size                | (224, 224)
[2022/12/28 20:50] | test_size                 | (224, 224)
[2022/12/28 20:50] | center_crop_ptr           | 0.875
[2022/12/28 20:50] | interpolation             | bicubic
[2022/12/28 20:50] | mean                      | (0.485, 0.456, 0.406)
[2022/12/28 20:50] | std                       | (0.229, 0.224, 0.225)
[2022/12/28 20:50] | hflip                     | 0.5
[2022/12/28 20:50] | auto_aug                  | False
[2022/12/28 20:50] | cutmix                    | None
[2022/12/28 20:50] | mixup                     | None
[2022/12/28 20:50] | remode                    | 0.2
[2022/12/28 20:50] | model_name                | densenet121
[2022/12/28 20:50] | lr                        | 0.001
[2022/12/28 20:50] | epoch                     | 2
[2022/12/28 20:50] | criterion                 | ce
[2022/12/28 20:50] | optimizer                 | adamw
[2022/12/28 20:50] | weight_decay              | 0.0001
[2022/12/28 20:50] | scheduler                 | cosine
[2022/12/28 20:50] | warmup_epoch              | 1
[2022/12/28 20:50] | batch_size                | 32
[2022/12/28 20:50] | ---------------------------------------------------------------------------------
[2022/12/28 20:50] |                                   DATA & MODEL
[2022/12/28 20:50] | ---------------------------------------------------------------------------------
[2022/12/28 20:50] | Model Parameters(M)       | 6958981
[2022/12/28 20:50] | Number of Train Examples  | 28100
[2022/12/28 20:50] | Number of Valid Examples  | 7026
[2022/12/28 20:50] | Number of Class           | 5
[2022/12/28 20:50] | Task                      | multiclass
[2022/12/28 20:50] | ---------------------------------------------------------------------------------
[2022/12/28 20:50] | TRAIN(000): [ 50/879] Batch: 0.1306 (0.1650) Data: 0.0099 (0.0472) Loss: 1.1446 (1.4294)
[2022/12/28 20:50] | TRAIN(000): [100/879] Batch: 0.1225 (0.1457) Data: 0.0096 (0.0288) Loss: 0.7064 (1.1815)
[2022/12/28 20:50] | TRAIN(000): [150/879] Batch: 0.1350 (0.1407) Data: 0.0090 (0.0225) Loss: 0.8517 (1.0795)
[2022/12/28 20:50] | TRAIN(000): [200/879] Batch: 0.1212 (0.1369) Data: 0.0101 (0.0193) Loss: 0.8591 (1.0284)
[2022/12/28 20:50] | TRAIN(000): [250/879] Batch: 0.1202 (0.1344) Data: 0.0097 (0.0174) Loss: 0.8527 (0.9893)
[2022/12/28 20:50] | TRAIN(000): [300/879] Batch: 0.1526 (0.1323) Data: 0.0120 (0.0161) Loss: 0.8687 (0.9503)
[2022/12/28 20:50] | TRAIN(000): [350/879] Batch: 0.1195 (0.1309) Data: 0.0089 (0.0152) Loss: 0.8545 (0.9247)
[2022/12/28 20:50] | TRAIN(000): [400/879] Batch: 0.1208 (0.1297) Data: 0.0105 (0.0145) Loss: 1.1277 (0.9100)
[2022/12/28 20:51] | TRAIN(000): [450/879] Batch: 0.1202 (0.1290) Data: 0.0099 (0.0140) Loss: 1.0238 (0.8973)
[2022/12/28 20:51] | TRAIN(000): [500/879] Batch: 0.1222 (0.1282) Data: 0.0097 (0.0136) Loss: 0.7993 (0.8875)
[2022/12/28 20:51] | TRAIN(000): [550/879] Batch: 0.1158 (0.1275) Data: 0.0091 (0.0132) Loss: 1.2882 (0.8786)
[2022/12/28 20:51] | TRAIN(000): [600/879] Batch: 0.1297 (0.1273) Data: 0.0093 (0.0129) Loss: 0.6132 (0.8696)
[2022/12/28 20:51] | TRAIN(000): [650/879] Batch: 0.1754 (0.1304) Data: 0.0116 (0.0128) Loss: 0.6659 (0.8623)
[2022/12/28 20:51] | TRAIN(000): [700/879] Batch: 0.1219 (0.1329) Data: 0.0100 (0.0127) Loss: 0.7778 (0.8580)
[2022/12/28 20:51] | TRAIN(000): [750/879] Batch: 0.1261 (0.1323) Data: 0.0088 (0.0125) Loss: 0.8525 (0.8534)
[2022/12/28 20:51] | TRAIN(000): [800/879] Batch: 0.1201 (0.1317) Data: 0.0102 (0.0124) Loss: 0.9011 (0.8486)
[2022/12/28 20:51] | TRAIN(000): [850/879] Batch: 0.1203 (0.1311) Data: 0.0094 (0.0122) Loss: 0.9227 (0.8432)
[2022/12/28 20:52] | ------------------------------------------------------------
[2022/12/28 20:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 20:52] | ------------------------------------------------------------
[2022/12/28 20:52] |     TRAIN(0)     0:01:55     0:00:10     0:01:44      0.8415
[2022/12/28 20:52] | ------------------------------------------------------------
[2022/12/28 20:52] | VALID(000): [ 50/220] Batch: 0.0293 (0.0632) Data: 0.0083 (0.0392) Loss: 0.6955 (0.7485)
[2022/12/28 20:52] | VALID(000): [100/220] Batch: 0.0297 (0.0490) Data: 0.0091 (0.0246) Loss: 0.9994 (0.7662)
[2022/12/28 20:52] | VALID(000): [150/220] Batch: 0.0457 (0.0465) Data: 0.0112 (0.0199) Loss: 0.7733 (0.7564)
[2022/12/28 20:52] | VALID(000): [200/220] Batch: 0.0456 (0.0464) Data: 0.0114 (0.0178) Loss: 0.4042 (0.7633)
[2022/12/28 20:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 20:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:52] |     VALID(0)      0.7629      0.7481      0.7501      0.7481      0.7481      0.7481      0.9370
[2022/12/28 20:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:52] | ####################################################################################################
[2022/12/28 20:52] | TRAIN(001): [ 50/879] Batch: 0.1173 (0.1621) Data: 0.0102 (0.0487) Loss: 0.8124 (0.7982)
[2022/12/28 20:52] | TRAIN(001): [100/879] Batch: 0.1209 (0.1440) Data: 0.0097 (0.0295) Loss: 0.5267 (0.8203)
[2022/12/28 20:52] | TRAIN(001): [150/879] Batch: 0.1290 (0.1370) Data: 0.0103 (0.0229) Loss: 0.8925 (0.8157)
[2022/12/28 20:52] | TRAIN(001): [200/879] Batch: 0.1209 (0.1334) Data: 0.0095 (0.0196) Loss: 0.6794 (0.8045)
[2022/12/28 20:52] | TRAIN(001): [250/879] Batch: 0.1208 (0.1313) Data: 0.0097 (0.0177) Loss: 0.4935 (0.7923)
[2022/12/28 20:52] | TRAIN(001): [300/879] Batch: 0.1208 (0.1297) Data: 0.0092 (0.0164) Loss: 0.9180 (0.7855)
[2022/12/28 20:52] | TRAIN(001): [350/879] Batch: 0.1216 (0.1285) Data: 0.0095 (0.0154) Loss: 0.6102 (0.7794)
[2022/12/28 20:53] | TRAIN(001): [400/879] Batch: 0.1730 (0.1280) Data: 0.0115 (0.0147) Loss: 0.7541 (0.7756)
[2022/12/28 20:53] | TRAIN(001): [450/879] Batch: 0.1202 (0.1282) Data: 0.0094 (0.0142) Loss: 0.6998 (0.7701)
[2022/12/28 20:53] | TRAIN(001): [500/879] Batch: 0.1203 (0.1280) Data: 0.0093 (0.0138) Loss: 0.7189 (0.7641)
[2022/12/28 20:53] | TRAIN(001): [550/879] Batch: 0.1286 (0.1276) Data: 0.0096 (0.0134) Loss: 0.5880 (0.7606)
[2022/12/28 20:53] | TRAIN(001): [600/879] Batch: 0.1198 (0.1272) Data: 0.0094 (0.0131) Loss: 0.7384 (0.7576)
[2022/12/28 20:53] | TRAIN(001): [650/879] Batch: 0.1283 (0.1268) Data: 0.0101 (0.0128) Loss: 0.6634 (0.7566)
[2022/12/28 20:53] | TRAIN(001): [700/879] Batch: 0.1194 (0.1267) Data: 0.0097 (0.0126) Loss: 0.8563 (0.7537)
[2022/12/28 20:53] | TRAIN(001): [750/879] Batch: 0.1211 (0.1264) Data: 0.0095 (0.0124) Loss: 0.6236 (0.7509)
[2022/12/28 20:53] | TRAIN(001): [800/879] Batch: 0.1198 (0.1261) Data: 0.0088 (0.0122) Loss: 0.9243 (0.7495)
[2022/12/28 20:54] | TRAIN(001): [850/879] Batch: 0.1207 (0.1260) Data: 0.0100 (0.0121) Loss: 0.7402 (0.7440)
[2022/12/28 20:54] | ------------------------------------------------------------
[2022/12/28 20:54] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 20:54] | ------------------------------------------------------------
[2022/12/28 20:54] |     TRAIN(1)     0:01:50     0:00:10     0:01:39      0.7421
[2022/12/28 20:54] | ------------------------------------------------------------
[2022/12/28 20:54] | VALID(001): [ 50/220] Batch: 0.0452 (0.0716) Data: 0.0116 (0.0398) Loss: 0.5924 (0.6291)
[2022/12/28 20:54] | VALID(001): [100/220] Batch: 0.0451 (0.0589) Data: 0.0113 (0.0259) Loss: 0.8291 (0.6459)
[2022/12/28 20:54] | VALID(001): [150/220] Batch: 0.0461 (0.0546) Data: 0.0115 (0.0212) Loss: 0.5508 (0.6402)
[2022/12/28 20:54] | VALID(001): [200/220] Batch: 0.0462 (0.0524) Data: 0.0111 (0.0188) Loss: 0.3150 (0.6429)
[2022/12/28 20:54] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:54] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 20:54] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:54] |     VALID(1)      0.6403      0.7834      0.8293      0.7834      0.7834      0.7834      0.9458
[2022/12/28 20:54] | ------------------------------------------------------------------------------------------------
[2022/12/28 20:54] | ####################################################################################################
