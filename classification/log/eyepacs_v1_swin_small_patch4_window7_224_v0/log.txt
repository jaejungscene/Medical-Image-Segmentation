[2022/12/28 23:16] | Loading pretrained weights from url (https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth)
[2022/12/28 23:16] | load model weight from data/pretrained/swin_small_patch4_window7_224.pth
[2022/12/28 23:16] | popping out head
[2022/12/28 23:16] | ---------------------------------------------------------------------------------
[2022/12/28 23:16] |                                    INFORMATION
[2022/12/28 23:16] | ---------------------------------------------------------------------------------
[2022/12/28 23:16] | Project Name              | MECLA
[2022/12/28 23:16] | Project Administrator     | jaejung
[2022/12/28 23:16] | Experiment Name           | eyepacs_v1_swin_small_patch4_window7_224_v0
[2022/12/28 23:16] | Experiment Start Time     | 2022-12-28 23:16:37
[2022/12/28 23:16] | Experiment Model Name     | swin_small_patch4_window7_224
[2022/12/28 23:16] | Experiment Log Directory  | log/eyepacs_v1_swin_small_patch4_window7_224_v0
[2022/12/28 23:16] | ---------------------------------------------------------------------------------
[2022/12/28 23:16] |                                 EXPERIMENT SETUP
[2022/12/28 23:16] | ---------------------------------------------------------------------------------
[2022/12/28 23:16] | train_size                | (224, 224)
[2022/12/28 23:16] | test_size                 | (224, 224)
[2022/12/28 23:16] | center_crop_ptr           | 0.875
[2022/12/28 23:16] | interpolation             | bicubic
[2022/12/28 23:16] | mean                      | (0.485, 0.456, 0.406)
[2022/12/28 23:16] | std                       | (0.229, 0.224, 0.225)
[2022/12/28 23:16] | hflip                     | 0.5
[2022/12/28 23:16] | auto_aug                  | False
[2022/12/28 23:16] | cutmix                    | None
[2022/12/28 23:16] | mixup                     | None
[2022/12/28 23:16] | remode                    | 0.2
[2022/12/28 23:16] | model_name                | swin_small_patch4_window7_224
[2022/12/28 23:16] | lr                        | 0.001
[2022/12/28 23:16] | epoch                     | 100
[2022/12/28 23:16] | criterion                 | ce
[2022/12/28 23:16] | optimizer                 | adamw
[2022/12/28 23:16] | weight_decay              | 0.0001
[2022/12/28 23:16] | scheduler                 | cosine
[2022/12/28 23:16] | warmup_epoch              | 1
[2022/12/28 23:16] | batch_size                | 32
[2022/12/28 23:16] | ---------------------------------------------------------------------------------
[2022/12/28 23:16] |                                   DATA & MODEL
[2022/12/28 23:16] | ---------------------------------------------------------------------------------
[2022/12/28 23:16] | Model Parameters(M)       | 48841103
[2022/12/28 23:16] | Number of Train Examples  | 28100
[2022/12/28 23:16] | Number of Valid Examples  | 7026
[2022/12/28 23:16] | Number of Class           | 5
[2022/12/28 23:16] | Task                      | multiclass
[2022/12/28 23:16] | ---------------------------------------------------------------------------------
[2022/12/28 23:16] | TRAIN(000): [ 50/879] Batch: 0.1523 (0.1989) Data: 0.0094 (0.0399) Loss: 1.0677 (1.0523)
[2022/12/28 23:17] | TRAIN(000): [100/879] Batch: 0.1653 (0.1832) Data: 0.0096 (0.0247) Loss: 0.8040 (0.9316)
[2022/12/28 23:17] | TRAIN(000): [150/879] Batch: 0.1683 (0.1781) Data: 0.0089 (0.0196) Loss: 0.8481 (0.8956)
[2022/12/28 23:17] | TRAIN(000): [200/879] Batch: 0.1662 (0.1755) Data: 0.0085 (0.0170) Loss: 0.4683 (0.8782)
[2022/12/28 23:17] | TRAIN(000): [250/879] Batch: 0.1746 (0.1743) Data: 0.0087 (0.0155) Loss: 0.6019 (0.8695)
[2022/12/28 23:17] | TRAIN(000): [300/879] Batch: 0.1711 (0.1734) Data: 0.0099 (0.0144) Loss: 0.9700 (0.8595)
[2022/12/28 23:17] | TRAIN(000): [350/879] Batch: 0.1716 (0.1727) Data: 0.0082 (0.0137) Loss: 0.9766 (0.8524)
[2022/12/28 23:17] | TRAIN(000): [400/879] Batch: 0.1640 (0.1723) Data: 0.0085 (0.0131) Loss: 0.5436 (0.8523)
[2022/12/28 23:18] | TRAIN(000): [450/879] Batch: 0.1729 (0.1721) Data: 0.0088 (0.0126) Loss: 0.7743 (0.8439)
[2022/12/28 23:18] | TRAIN(000): [500/879] Batch: 0.1707 (0.1719) Data: 0.0114 (0.0123) Loss: 1.0149 (0.8425)
[2022/12/28 23:18] | TRAIN(000): [550/879] Batch: 0.1669 (0.1716) Data: 0.0090 (0.0120) Loss: 1.0815 (0.8481)
[2022/12/28 23:18] | TRAIN(000): [600/879] Batch: 0.1712 (0.1717) Data: 0.0093 (0.0118) Loss: 1.1411 (0.8522)
[2022/12/28 23:18] | TRAIN(000): [650/879] Batch: 0.1656 (0.1717) Data: 0.0092 (0.0117) Loss: 0.8242 (0.8581)
[2022/12/28 23:18] | TRAIN(000): [700/879] Batch: 0.1663 (0.1716) Data: 0.0098 (0.0115) Loss: 0.9463 (0.8600)
[2022/12/28 23:18] | TRAIN(000): [750/879] Batch: 0.1732 (0.1714) Data: 0.0100 (0.0114) Loss: 0.8809 (0.8614)
[2022/12/28 23:19] | TRAIN(000): [800/879] Batch: 0.1701 (0.1715) Data: 0.0099 (0.0113) Loss: 1.1103 (0.8611)
[2022/12/28 23:19] | TRAIN(000): [850/879] Batch: 0.1699 (0.1713) Data: 0.0087 (0.0111) Loss: 1.1477 (0.8613)
[2022/12/28 23:19] | ------------------------------------------------------------
[2022/12/28 23:19] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:19] | ------------------------------------------------------------
[2022/12/28 23:19] |     TRAIN(0)     0:02:30     0:00:09     0:02:20      0.8628
[2022/12/28 23:19] | ------------------------------------------------------------
[2022/12/28 23:19] | VALID(000): [ 50/220] Batch: 0.0581 (0.0781) Data: 0.0347 (0.0550) Loss: 0.8383 (0.8606)
[2022/12/28 23:19] | VALID(000): [100/220] Batch: 0.0590 (0.0675) Data: 0.0368 (0.0453) Loss: 1.0910 (0.8811)
[2022/12/28 23:19] | VALID(000): [150/220] Batch: 0.0542 (0.0639) Data: 0.0358 (0.0421) Loss: 0.7872 (0.8741)
[2022/12/28 23:19] | VALID(000): [200/220] Batch: 0.0585 (0.0623) Data: 0.0325 (0.0400) Loss: 0.4908 (0.8808)
[2022/12/28 23:19] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:19] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:19] |     VALID(0)      0.8801      0.7347      0.5229      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:19] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:19] | ####################################################################################################
[2022/12/28 23:19] | TRAIN(001): [ 50/879] Batch: 0.1738 (0.2056) Data: 0.0150 (0.0444) Loss: 0.5114 (0.8770)
[2022/12/28 23:19] | TRAIN(001): [100/879] Batch: 0.1760 (0.1885) Data: 0.0095 (0.0274) Loss: 0.6973 (0.8798)
[2022/12/28 23:19] | TRAIN(001): [150/879] Batch: 0.1759 (0.1817) Data: 0.0108 (0.0214) Loss: 0.5566 (0.8726)
[2022/12/28 23:20] | TRAIN(001): [200/879] Batch: 0.1717 (0.1786) Data: 0.0143 (0.0185) Loss: 0.9196 (0.8672)
[2022/12/28 23:20] | TRAIN(001): [250/879] Batch: 0.1948 (0.1773) Data: 0.0109 (0.0168) Loss: 0.7230 (0.8664)
[2022/12/28 23:20] | TRAIN(001): [300/879] Batch: 0.1941 (0.1767) Data: 0.0108 (0.0157) Loss: 0.8475 (0.8766)
[2022/12/28 23:20] | TRAIN(001): [350/879] Batch: 0.1659 (0.1762) Data: 0.0088 (0.0148) Loss: 0.8586 (0.8775)
[2022/12/28 23:20] | TRAIN(001): [400/879] Batch: 0.1796 (0.1758) Data: 0.0079 (0.0143) Loss: 0.9507 (0.8746)
[2022/12/28 23:20] | TRAIN(001): [450/879] Batch: 0.1843 (0.1753) Data: 0.0110 (0.0138) Loss: 0.9378 (0.8746)
[2022/12/28 23:20] | TRAIN(001): [500/879] Batch: 0.1720 (0.1750) Data: 0.0087 (0.0134) Loss: 0.7993 (0.8724)
[2022/12/28 23:21] | TRAIN(001): [550/879] Batch: 0.1640 (0.1745) Data: 0.0092 (0.0130) Loss: 0.6708 (0.8712)
[2022/12/28 23:21] | TRAIN(001): [600/879] Batch: 0.1691 (0.1742) Data: 0.0093 (0.0127) Loss: 0.6525 (0.8711)
[2022/12/28 23:21] | TRAIN(001): [650/879] Batch: 0.1688 (0.1738) Data: 0.0083 (0.0125) Loss: 0.6677 (0.8699)
[2022/12/28 23:21] | TRAIN(001): [700/879] Batch: 0.1643 (0.1735) Data: 0.0105 (0.0123) Loss: 1.0097 (0.8710)
[2022/12/28 23:21] | TRAIN(001): [750/879] Batch: 0.1715 (0.1732) Data: 0.0107 (0.0121) Loss: 0.6764 (0.8707)
[2022/12/28 23:21] | TRAIN(001): [800/879] Batch: 0.1765 (0.1730) Data: 0.0112 (0.0120) Loss: 1.1888 (0.8723)
[2022/12/28 23:21] | TRAIN(001): [850/879] Batch: 0.1749 (0.1729) Data: 0.0086 (0.0118) Loss: 0.8425 (0.8741)
[2022/12/28 23:22] | ------------------------------------------------------------
[2022/12/28 23:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:22] | ------------------------------------------------------------
[2022/12/28 23:22] |     TRAIN(1)     0:02:31     0:00:10     0:02:21      0.8742
[2022/12/28 23:22] | ------------------------------------------------------------
[2022/12/28 23:22] | VALID(001): [ 50/220] Batch: 0.0543 (0.0812) Data: 0.0355 (0.0564) Loss: 0.7852 (0.8488)
[2022/12/28 23:22] | VALID(001): [100/220] Batch: 0.0542 (0.0693) Data: 0.0324 (0.0454) Loss: 1.1030 (0.8728)
[2022/12/28 23:22] | VALID(001): [150/220] Batch: 0.0583 (0.0652) Data: 0.0360 (0.0417) Loss: 0.7515 (0.8643)
[2022/12/28 23:22] | VALID(001): [200/220] Batch: 0.0574 (0.0631) Data: 0.0335 (0.0398) Loss: 0.5029 (0.8717)
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] |     VALID(1)      0.8708      0.7347      0.5226      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:22] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:22] | ####################################################################################################
[2022/12/28 23:22] | TRAIN(002): [ 50/879] Batch: 0.1696 (0.2041) Data: 0.0100 (0.0427) Loss: 0.7402 (0.8955)
[2022/12/28 23:22] | TRAIN(002): [100/879] Batch: 0.1721 (0.1867) Data: 0.0092 (0.0265) Loss: 0.8788 (0.8620)
[2022/12/28 23:22] | TRAIN(002): [150/879] Batch: 0.1742 (0.1807) Data: 0.0106 (0.0211) Loss: 1.1704 (0.8615)
[2022/12/28 23:22] | TRAIN(002): [200/879] Batch: 0.1753 (0.1783) Data: 0.0102 (0.0185) Loss: 0.6299 (0.8691)
[2022/12/28 23:23] | TRAIN(002): [250/879] Batch: 0.1653 (0.1769) Data: 0.0095 (0.0170) Loss: 0.5436 (0.8715)
[2022/12/28 23:23] | TRAIN(002): [300/879] Batch: 0.1736 (0.1757) Data: 0.0098 (0.0159) Loss: 1.0392 (0.8693)
[2022/12/28 23:23] | TRAIN(002): [350/879] Batch: 0.1671 (0.1751) Data: 0.0105 (0.0151) Loss: 0.5649 (0.8735)
[2022/12/28 23:23] | TRAIN(002): [400/879] Batch: 0.1694 (0.1744) Data: 0.0099 (0.0145) Loss: 1.1341 (0.8777)
[2022/12/28 23:23] | TRAIN(002): [450/879] Batch: 0.1663 (0.1739) Data: 0.0088 (0.0141) Loss: 0.6165 (0.8718)
[2022/12/28 23:23] | TRAIN(002): [500/879] Batch: 0.1900 (0.1738) Data: 0.0094 (0.0137) Loss: 0.6594 (0.8724)
[2022/12/28 23:23] | TRAIN(002): [550/879] Batch: 0.1734 (0.1737) Data: 0.0100 (0.0134) Loss: 0.4645 (0.8709)
[2022/12/28 23:23] | TRAIN(002): [600/879] Batch: 0.1649 (0.1733) Data: 0.0114 (0.0132) Loss: 0.9367 (0.8742)
[2022/12/28 23:24] | TRAIN(002): [650/879] Batch: 0.1677 (0.1730) Data: 0.0100 (0.0129) Loss: 0.6886 (0.8745)
[2022/12/28 23:24] | TRAIN(002): [700/879] Batch: 0.1640 (0.1728) Data: 0.0097 (0.0128) Loss: 0.9157 (0.8747)
[2022/12/28 23:24] | TRAIN(002): [750/879] Batch: 0.1695 (0.1725) Data: 0.0105 (0.0126) Loss: 0.6978 (0.8743)
[2022/12/28 23:24] | TRAIN(002): [800/879] Batch: 0.1732 (0.1723) Data: 0.0103 (0.0125) Loss: 0.8885 (0.8728)
[2022/12/28 23:24] | TRAIN(002): [850/879] Batch: 0.1726 (0.1721) Data: 0.0093 (0.0124) Loss: 0.7984 (0.8730)
[2022/12/28 23:24] | ------------------------------------------------------------
[2022/12/28 23:24] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:24] | ------------------------------------------------------------
[2022/12/28 23:24] |     TRAIN(2)     0:02:31     0:00:10     0:02:20      0.8728
[2022/12/28 23:24] | ------------------------------------------------------------
[2022/12/28 23:24] | VALID(002): [ 50/220] Batch: 0.0587 (0.0798) Data: 0.0394 (0.0571) Loss: 0.7814 (0.8457)
[2022/12/28 23:24] | VALID(002): [100/220] Batch: 0.0574 (0.0683) Data: 0.0361 (0.0455) Loss: 1.1163 (0.8692)
[2022/12/28 23:24] | VALID(002): [150/220] Batch: 0.0567 (0.0646) Data: 0.0340 (0.0418) Loss: 0.7602 (0.8609)
[2022/12/28 23:24] | VALID(002): [200/220] Batch: 0.0520 (0.0626) Data: 0.0337 (0.0401) Loss: 0.5025 (0.8686)
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] |     VALID(2)      0.8683      0.7347      0.5451      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:25] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:25] | ####################################################################################################
[2022/12/28 23:25] | TRAIN(003): [ 50/879] Batch: 0.1755 (0.2131) Data: 0.0082 (0.0455) Loss: 0.9549 (0.8911)
[2022/12/28 23:25] | TRAIN(003): [100/879] Batch: 0.1727 (0.1909) Data: 0.0153 (0.0276) Loss: 0.8823 (0.8850)
[2022/12/28 23:25] | TRAIN(003): [150/879] Batch: 0.1636 (0.1842) Data: 0.0094 (0.0214) Loss: 0.8284 (0.8730)
[2022/12/28 23:25] | TRAIN(003): [200/879] Batch: 0.1741 (0.1803) Data: 0.0089 (0.0183) Loss: 0.9134 (0.8683)
[2022/12/28 23:25] | TRAIN(003): [250/879] Batch: 0.1643 (0.1785) Data: 0.0089 (0.0165) Loss: 0.9719 (0.8642)
[2022/12/28 23:25] | TRAIN(003): [300/879] Batch: 0.1698 (0.1770) Data: 0.0082 (0.0153) Loss: 1.0313 (0.8661)
[2022/12/28 23:26] | TRAIN(003): [350/879] Batch: 0.1674 (0.1758) Data: 0.0082 (0.0144) Loss: 1.0285 (0.8737)
[2022/12/28 23:26] | TRAIN(003): [400/879] Batch: 0.1664 (0.1748) Data: 0.0093 (0.0137) Loss: 0.8380 (0.8742)
[2022/12/28 23:26] | TRAIN(003): [450/879] Batch: 0.1739 (0.1741) Data: 0.0092 (0.0131) Loss: 0.8310 (0.8747)
[2022/12/28 23:26] | TRAIN(003): [500/879] Batch: 0.1721 (0.1736) Data: 0.0088 (0.0127) Loss: 0.9390 (0.8739)
[2022/12/28 23:26] | TRAIN(003): [550/879] Batch: 0.1651 (0.1733) Data: 0.0087 (0.0124) Loss: 0.5332 (0.8748)
[2022/12/28 23:26] | TRAIN(003): [600/879] Batch: 0.1723 (0.1729) Data: 0.0088 (0.0121) Loss: 0.6455 (0.8696)
[2022/12/28 23:26] | TRAIN(003): [650/879] Batch: 0.1676 (0.1727) Data: 0.0088 (0.0119) Loss: 0.9251 (0.8681)
[2022/12/28 23:27] | TRAIN(003): [700/879] Batch: 0.1740 (0.1726) Data: 0.0091 (0.0118) Loss: 0.7994 (0.8668)
[2022/12/28 23:27] | TRAIN(003): [750/879] Batch: 0.1669 (0.1724) Data: 0.0095 (0.0116) Loss: 0.9818 (0.8655)
[2022/12/28 23:27] | TRAIN(003): [800/879] Batch: 0.1721 (0.1723) Data: 0.0093 (0.0115) Loss: 0.9780 (0.8682)
[2022/12/28 23:27] | TRAIN(003): [850/879] Batch: 0.1645 (0.1721) Data: 0.0091 (0.0113) Loss: 1.1837 (0.8697)
[2022/12/28 23:27] | ------------------------------------------------------------
[2022/12/28 23:27] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:27] | ------------------------------------------------------------
[2022/12/28 23:27] |     TRAIN(3)     0:02:31     0:00:09     0:02:21      0.8693
[2022/12/28 23:27] | ------------------------------------------------------------
[2022/12/28 23:27] | VALID(003): [ 50/220] Batch: 0.0552 (0.0822) Data: 0.0347 (0.0583) Loss: 0.8259 (0.8526)
[2022/12/28 23:27] | VALID(003): [100/220] Batch: 0.0566 (0.0701) Data: 0.0321 (0.0456) Loss: 1.0783 (0.8728)
[2022/12/28 23:27] | VALID(003): [150/220] Batch: 0.0580 (0.0663) Data: 0.0307 (0.0409) Loss: 0.7751 (0.8653)
[2022/12/28 23:27] | VALID(003): [200/220] Batch: 0.0595 (0.0641) Data: 0.0357 (0.0385) Loss: 0.5546 (0.8712)
[2022/12/28 23:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:27] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:27] |     VALID(3)      0.8715      0.7347      0.5552      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:27] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:27] | ####################################################################################################
[2022/12/28 23:27] | TRAIN(004): [ 50/879] Batch: 0.1699 (0.2046) Data: 0.0090 (0.0428) Loss: 0.8362 (0.8764)
[2022/12/28 23:28] | TRAIN(004): [100/879] Batch: 0.1708 (0.1878) Data: 0.0093 (0.0265) Loss: 0.7301 (0.8661)
[2022/12/28 23:28] | TRAIN(004): [150/879] Batch: 0.1774 (0.1817) Data: 0.0100 (0.0210) Loss: 0.7520 (0.8714)
[2022/12/28 23:28] | TRAIN(004): [200/879] Batch: 0.1721 (0.1785) Data: 0.0099 (0.0181) Loss: 0.9479 (0.8747)
[2022/12/28 23:28] | TRAIN(004): [250/879] Batch: 0.1645 (0.1765) Data: 0.0095 (0.0164) Loss: 1.0212 (0.8745)
[2022/12/28 23:28] | TRAIN(004): [300/879] Batch: 0.1629 (0.1751) Data: 0.0087 (0.0153) Loss: 0.9139 (0.8768)
[2022/12/28 23:28] | TRAIN(004): [350/879] Batch: 0.1648 (0.1742) Data: 0.0088 (0.0145) Loss: 0.7605 (0.8744)
[2022/12/28 23:28] | TRAIN(004): [400/879] Batch: 0.1703 (0.1736) Data: 0.0097 (0.0139) Loss: 0.7937 (0.8704)
[2022/12/28 23:29] | TRAIN(004): [450/879] Batch: 0.1731 (0.1733) Data: 0.0093 (0.0134) Loss: 1.1098 (0.8718)
[2022/12/28 23:29] | TRAIN(004): [500/879] Batch: 0.1689 (0.1729) Data: 0.0089 (0.0131) Loss: 1.0357 (0.8716)
[2022/12/28 23:29] | TRAIN(004): [550/879] Batch: 0.1657 (0.1728) Data: 0.0098 (0.0128) Loss: 0.8373 (0.8756)
[2022/12/28 23:29] | TRAIN(004): [600/879] Batch: 0.1658 (0.1726) Data: 0.0099 (0.0126) Loss: 1.0475 (0.8754)
[2022/12/28 23:29] | TRAIN(004): [650/879] Batch: 0.1680 (0.1726) Data: 0.0090 (0.0124) Loss: 0.6538 (0.8715)
[2022/12/28 23:29] | TRAIN(004): [700/879] Batch: 0.1644 (0.1724) Data: 0.0093 (0.0122) Loss: 0.8518 (0.8706)
[2022/12/28 23:29] | TRAIN(004): [750/879] Batch: 0.1662 (0.1722) Data: 0.0087 (0.0120) Loss: 0.6408 (0.8710)
[2022/12/28 23:30] | TRAIN(004): [800/879] Batch: 0.1690 (0.1721) Data: 0.0094 (0.0118) Loss: 1.2082 (0.8741)
[2022/12/28 23:30] | TRAIN(004): [850/879] Batch: 0.1844 (0.1720) Data: 0.0103 (0.0117) Loss: 1.0975 (0.8755)
[2022/12/28 23:30] | ------------------------------------------------------------
[2022/12/28 23:30] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:30] | ------------------------------------------------------------
[2022/12/28 23:30] |     TRAIN(4)     0:02:30     0:00:10     0:02:20      0.8754
[2022/12/28 23:30] | ------------------------------------------------------------
[2022/12/28 23:30] | VALID(004): [ 50/220] Batch: 0.0635 (0.0786) Data: 0.0311 (0.0544) Loss: 0.7909 (0.8476)
[2022/12/28 23:30] | VALID(004): [100/220] Batch: 0.0590 (0.0677) Data: 0.0363 (0.0444) Loss: 1.1148 (0.8697)
[2022/12/28 23:30] | VALID(004): [150/220] Batch: 0.0571 (0.0640) Data: 0.0354 (0.0413) Loss: 0.7610 (0.8618)
[2022/12/28 23:30] | VALID(004): [200/220] Batch: 0.0577 (0.0622) Data: 0.0378 (0.0396) Loss: 0.5148 (0.8690)
[2022/12/28 23:30] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:30] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:30] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:30] |     VALID(4)      0.8691      0.7347      0.5094      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:30] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:30] | ####################################################################################################
[2022/12/28 23:30] | TRAIN(005): [ 50/879] Batch: 0.1705 (0.2053) Data: 0.0096 (0.0446) Loss: 1.0027 (0.8529)
[2022/12/28 23:30] | TRAIN(005): [100/879] Batch: 0.1659 (0.1882) Data: 0.0088 (0.0276) Loss: 0.8326 (0.8221)
[2022/12/28 23:30] | TRAIN(005): [150/879] Batch: 0.1771 (0.1821) Data: 0.0117 (0.0219) Loss: 1.0782 (0.8415)
[2022/12/28 23:31] | TRAIN(005): [200/879] Batch: 0.1726 (0.1803) Data: 0.0093 (0.0193) Loss: 1.0684 (0.8524)
[2022/12/28 23:31] | TRAIN(005): [250/879] Batch: 0.1719 (0.1790) Data: 0.0094 (0.0176) Loss: 0.8186 (0.8585)
[2022/12/28 23:31] | TRAIN(005): [300/879] Batch: 0.1691 (0.1784) Data: 0.0096 (0.0164) Loss: 0.8966 (0.8680)
[2022/12/28 23:31] | TRAIN(005): [350/879] Batch: 0.1747 (0.1771) Data: 0.0115 (0.0155) Loss: 0.7141 (0.8686)
[2022/12/28 23:31] | TRAIN(005): [400/879] Batch: 0.1709 (0.1764) Data: 0.0098 (0.0148) Loss: 0.8450 (0.8708)
[2022/12/28 23:31] | TRAIN(005): [450/879] Batch: 0.1728 (0.1756) Data: 0.0088 (0.0142) Loss: 1.2583 (0.8728)
[2022/12/28 23:31] | TRAIN(005): [500/879] Batch: 0.1633 (0.1750) Data: 0.0085 (0.0138) Loss: 1.0543 (0.8780)
[2022/12/28 23:32] | TRAIN(005): [550/879] Batch: 0.1623 (0.1743) Data: 0.0086 (0.0134) Loss: 0.4838 (0.8767)
[2022/12/28 23:32] | TRAIN(005): [600/879] Batch: 0.1720 (0.1739) Data: 0.0104 (0.0131) Loss: 0.8390 (0.8766)
[2022/12/28 23:32] | TRAIN(005): [650/879] Batch: 0.1643 (0.1734) Data: 0.0087 (0.0128) Loss: 0.8326 (0.8758)
[2022/12/28 23:32] | TRAIN(005): [700/879] Batch: 0.1622 (0.1730) Data: 0.0096 (0.0126) Loss: 1.1225 (0.8756)
[2022/12/28 23:32] | TRAIN(005): [750/879] Batch: 0.1824 (0.1727) Data: 0.0115 (0.0124) Loss: 0.8932 (0.8729)
[2022/12/28 23:32] | TRAIN(005): [800/879] Batch: 0.1655 (0.1724) Data: 0.0101 (0.0122) Loss: 0.9202 (0.8747)
[2022/12/28 23:32] | TRAIN(005): [850/879] Batch: 0.1665 (0.1723) Data: 0.0090 (0.0121) Loss: 0.7149 (0.8725)
[2022/12/28 23:33] | ------------------------------------------------------------
[2022/12/28 23:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:33] | ------------------------------------------------------------
[2022/12/28 23:33] |     TRAIN(5)     0:02:31     0:00:10     0:02:20      0.8721
[2022/12/28 23:33] | ------------------------------------------------------------
[2022/12/28 23:33] | VALID(005): [ 50/220] Batch: 0.0545 (0.0801) Data: 0.0317 (0.0574) Loss: 0.7920 (0.8471)
[2022/12/28 23:33] | VALID(005): [100/220] Batch: 0.0570 (0.0682) Data: 0.0374 (0.0462) Loss: 1.1058 (0.8693)
[2022/12/28 23:33] | VALID(005): [150/220] Batch: 0.0546 (0.0642) Data: 0.0332 (0.0423) Loss: 0.7642 (0.8615)
[2022/12/28 23:33] | VALID(005): [200/220] Batch: 0.0585 (0.0622) Data: 0.0377 (0.0404) Loss: 0.4998 (0.8691)
[2022/12/28 23:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:33] |     VALID(5)      0.8690      0.7347      0.5196      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:33] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:33] | ####################################################################################################
[2022/12/28 23:33] | TRAIN(006): [ 50/879] Batch: 0.1638 (0.2033) Data: 0.0087 (0.0432) Loss: 0.7653 (0.8813)
[2022/12/28 23:33] | TRAIN(006): [100/879] Batch: 0.1628 (0.1863) Data: 0.0090 (0.0266) Loss: 0.8633 (0.8954)
[2022/12/28 23:33] | TRAIN(006): [150/879] Batch: 0.1661 (0.1801) Data: 0.0099 (0.0209) Loss: 0.8009 (0.8732)
[2022/12/28 23:33] | TRAIN(006): [200/879] Batch: 0.1695 (0.1767) Data: 0.0092 (0.0180) Loss: 0.9358 (0.8870)
[2022/12/28 23:34] | TRAIN(006): [250/879] Batch: 0.1653 (0.1745) Data: 0.0094 (0.0163) Loss: 0.9414 (0.8823)
[2022/12/28 23:34] | TRAIN(006): [300/879] Batch: 0.1687 (0.1736) Data: 0.0090 (0.0152) Loss: 1.2675 (0.8852)
[2022/12/28 23:34] | TRAIN(006): [350/879] Batch: 0.1823 (0.1728) Data: 0.0107 (0.0143) Loss: 0.9970 (0.8789)
[2022/12/28 23:34] | TRAIN(006): [400/879] Batch: 0.1656 (0.1723) Data: 0.0094 (0.0138) Loss: 1.0109 (0.8765)
[2022/12/28 23:34] | TRAIN(006): [450/879] Batch: 0.1659 (0.1718) Data: 0.0089 (0.0133) Loss: 0.8890 (0.8809)
[2022/12/28 23:34] | TRAIN(006): [500/879] Batch: 0.1633 (0.1713) Data: 0.0088 (0.0129) Loss: 0.8836 (0.8806)
[2022/12/28 23:34] | TRAIN(006): [550/879] Batch: 0.1622 (0.1709) Data: 0.0093 (0.0126) Loss: 1.2291 (0.8810)
[2022/12/28 23:34] | TRAIN(006): [600/879] Batch: 0.1639 (0.1709) Data: 0.0088 (0.0123) Loss: 0.5729 (0.8776)
[2022/12/28 23:35] | TRAIN(006): [650/879] Batch: 0.1634 (0.1706) Data: 0.0091 (0.0121) Loss: 0.6772 (0.8786)
[2022/12/28 23:35] | TRAIN(006): [700/879] Batch: 0.1727 (0.1703) Data: 0.0093 (0.0119) Loss: 0.9272 (0.8763)
[2022/12/28 23:35] | TRAIN(006): [750/879] Batch: 0.1647 (0.1701) Data: 0.0094 (0.0117) Loss: 0.4285 (0.8748)
[2022/12/28 23:35] | TRAIN(006): [800/879] Batch: 0.1738 (0.1699) Data: 0.0092 (0.0116) Loss: 0.9990 (0.8723)
[2022/12/28 23:35] | TRAIN(006): [850/879] Batch: 0.1745 (0.1698) Data: 0.0116 (0.0114) Loss: 0.9898 (0.8738)
[2022/12/28 23:35] | ------------------------------------------------------------
[2022/12/28 23:35] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:35] | ------------------------------------------------------------
[2022/12/28 23:35] |     TRAIN(6)     0:02:29     0:00:09     0:02:19      0.8721
[2022/12/28 23:35] | ------------------------------------------------------------
[2022/12/28 23:35] | VALID(006): [ 50/220] Batch: 0.0543 (0.0797) Data: 0.0326 (0.0564) Loss: 0.7849 (0.8478)
[2022/12/28 23:35] | VALID(006): [100/220] Batch: 0.0535 (0.0680) Data: 0.0368 (0.0446) Loss: 1.1183 (0.8710)
[2022/12/28 23:35] | VALID(006): [150/220] Batch: 0.0572 (0.0641) Data: 0.0337 (0.0411) Loss: 0.7594 (0.8630)
[2022/12/28 23:35] | VALID(006): [200/220] Batch: 0.0550 (0.0620) Data: 0.0334 (0.0393) Loss: 0.4901 (0.8711)
[2022/12/28 23:35] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:35] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:35] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:35] |     VALID(6)      0.8708      0.7347      0.5046      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:35] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:35] | ####################################################################################################
[2022/12/28 23:36] | TRAIN(007): [ 50/879] Batch: 0.1599 (0.2079) Data: 0.0091 (0.0461) Loss: 0.7688 (0.8912)
[2022/12/28 23:36] | TRAIN(007): [100/879] Batch: 0.1633 (0.1891) Data: 0.0086 (0.0284) Loss: 0.7079 (0.8622)
[2022/12/28 23:36] | TRAIN(007): [150/879] Batch: 0.1848 (0.1844) Data: 0.0178 (0.0228) Loss: 0.8540 (0.8696)
[2022/12/28 23:36] | TRAIN(007): [200/879] Batch: 0.1743 (0.1814) Data: 0.0143 (0.0198) Loss: 1.1340 (0.8726)
[2022/12/28 23:36] | TRAIN(007): [250/879] Batch: 0.1651 (0.1798) Data: 0.0102 (0.0180) Loss: 0.7944 (0.8721)
[2022/12/28 23:36] | TRAIN(007): [300/879] Batch: 0.1636 (0.1785) Data: 0.0171 (0.0169) Loss: 1.0759 (0.8709)
[2022/12/28 23:37] | TRAIN(007): [350/879] Batch: 0.1783 (0.1772) Data: 0.0145 (0.0159) Loss: 1.1599 (0.8701)
[2022/12/28 23:37] | TRAIN(007): [400/879] Batch: 0.1703 (0.1765) Data: 0.0147 (0.0153) Loss: 0.8181 (0.8720)
[2022/12/28 23:37] | TRAIN(007): [450/879] Batch: 0.1706 (0.1764) Data: 0.0099 (0.0148) Loss: 0.9257 (0.8735)
[2022/12/28 23:37] | TRAIN(007): [500/879] Batch: 0.1665 (0.1759) Data: 0.0088 (0.0145) Loss: 0.6718 (0.8710)
[2022/12/28 23:37] | TRAIN(007): [550/879] Batch: 0.1736 (0.1761) Data: 0.0091 (0.0142) Loss: 0.8086 (0.8714)
[2022/12/28 23:37] | TRAIN(007): [600/879] Batch: 0.1684 (0.1754) Data: 0.0093 (0.0138) Loss: 0.5730 (0.8683)
[2022/12/28 23:37] | TRAIN(007): [650/879] Batch: 0.1637 (0.1747) Data: 0.0089 (0.0135) Loss: 0.8840 (0.8670)
[2022/12/28 23:38] | TRAIN(007): [700/879] Batch: 0.1640 (0.1742) Data: 0.0091 (0.0132) Loss: 1.0657 (0.8678)
[2022/12/28 23:38] | TRAIN(007): [750/879] Batch: 0.1878 (0.1737) Data: 0.0116 (0.0129) Loss: 1.1968 (0.8694)
[2022/12/28 23:38] | TRAIN(007): [800/879] Batch: 0.1613 (0.1734) Data: 0.0082 (0.0127) Loss: 0.8325 (0.8689)
[2022/12/28 23:38] | TRAIN(007): [850/879] Batch: 0.1667 (0.1730) Data: 0.0106 (0.0125) Loss: 1.0339 (0.8705)
[2022/12/28 23:38] | ------------------------------------------------------------
[2022/12/28 23:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:38] | ------------------------------------------------------------
[2022/12/28 23:38] |     TRAIN(7)     0:02:31     0:00:10     0:02:20      0.8720
[2022/12/28 23:38] | ------------------------------------------------------------
[2022/12/28 23:38] | VALID(007): [ 50/220] Batch: 0.0527 (0.0802) Data: 0.0374 (0.0564) Loss: 0.7948 (0.8526)
[2022/12/28 23:38] | VALID(007): [100/220] Batch: 0.0578 (0.0685) Data: 0.0344 (0.0457) Loss: 1.0894 (0.8725)
[2022/12/28 23:38] | VALID(007): [150/220] Batch: 0.0577 (0.0646) Data: 0.0303 (0.0411) Loss: 0.7754 (0.8656)
[2022/12/28 23:38] | VALID(007): [200/220] Batch: 0.0581 (0.0626) Data: 0.0376 (0.0388) Loss: 0.5542 (0.8725)
[2022/12/28 23:38] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:38] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:38] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:38] |     VALID(7)      0.8723      0.7347      0.5156      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:38] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:38] | ####################################################################################################
[2022/12/28 23:38] | TRAIN(008): [ 50/879] Batch: 0.1725 (0.2042) Data: 0.0088 (0.0445) Loss: 1.0888 (0.8678)
[2022/12/28 23:39] | TRAIN(008): [100/879] Batch: 0.1649 (0.1860) Data: 0.0091 (0.0272) Loss: 0.7632 (0.8587)
[2022/12/28 23:39] | TRAIN(008): [150/879] Batch: 0.1709 (0.1802) Data: 0.0154 (0.0215) Loss: 0.8980 (0.8691)
[2022/12/28 23:39] | TRAIN(008): [200/879] Batch: 0.1698 (0.1771) Data: 0.0085 (0.0185) Loss: 0.7728 (0.8671)
[2022/12/28 23:39] | TRAIN(008): [250/879] Batch: 0.1671 (0.1750) Data: 0.0085 (0.0166) Loss: 0.6986 (0.8644)
[2022/12/28 23:39] | TRAIN(008): [300/879] Batch: 0.1841 (0.1738) Data: 0.0112 (0.0154) Loss: 1.0316 (0.8641)
[2022/12/28 23:39] | TRAIN(008): [350/879] Batch: 0.1603 (0.1750) Data: 0.0088 (0.0148) Loss: 0.8827 (0.8671)
[2022/12/28 23:39] | TRAIN(008): [400/879] Batch: 0.1657 (0.1746) Data: 0.0086 (0.0142) Loss: 0.8691 (0.8710)
[2022/12/28 23:40] | TRAIN(008): [450/879] Batch: 0.1653 (0.1737) Data: 0.0097 (0.0136) Loss: 1.2075 (0.8702)
[2022/12/28 23:40] | TRAIN(008): [500/879] Batch: 0.1736 (0.1732) Data: 0.0092 (0.0132) Loss: 0.8912 (0.8674)
[2022/12/28 23:40] | TRAIN(008): [550/879] Batch: 0.1642 (0.1729) Data: 0.0090 (0.0129) Loss: 1.1118 (0.8656)
[2022/12/28 23:40] | TRAIN(008): [600/879] Batch: 0.1769 (0.1725) Data: 0.0098 (0.0126) Loss: 0.7622 (0.8707)
[2022/12/28 23:40] | TRAIN(008): [650/879] Batch: 0.1681 (0.1726) Data: 0.0086 (0.0124) Loss: 0.9141 (0.8707)
[2022/12/28 23:40] | TRAIN(008): [700/879] Batch: 0.1728 (0.1723) Data: 0.0090 (0.0122) Loss: 0.9154 (0.8697)
[2022/12/28 23:40] | TRAIN(008): [750/879] Batch: 0.1636 (0.1719) Data: 0.0083 (0.0119) Loss: 1.0369 (0.8726)
[2022/12/28 23:41] | TRAIN(008): [800/879] Batch: 0.1731 (0.1716) Data: 0.0105 (0.0118) Loss: 0.9358 (0.8731)
[2022/12/28 23:41] | TRAIN(008): [850/879] Batch: 0.1671 (0.1713) Data: 0.0087 (0.0116) Loss: 1.0448 (0.8715)
[2022/12/28 23:41] | ------------------------------------------------------------
[2022/12/28 23:41] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:41] | ------------------------------------------------------------
[2022/12/28 23:41] |     TRAIN(8)     0:02:30     0:00:10     0:02:20      0.8713
[2022/12/28 23:41] | ------------------------------------------------------------
[2022/12/28 23:41] | VALID(008): [ 50/220] Batch: 0.0539 (0.0797) Data: 0.0372 (0.0575) Loss: 0.7802 (0.8464)
[2022/12/28 23:41] | VALID(008): [100/220] Batch: 0.0576 (0.0680) Data: 0.0357 (0.0462) Loss: 1.1111 (0.8689)
[2022/12/28 23:41] | VALID(008): [150/220] Batch: 0.0547 (0.0641) Data: 0.0330 (0.0420) Loss: 0.7589 (0.8611)
[2022/12/28 23:41] | VALID(008): [200/220] Batch: 0.0572 (0.0621) Data: 0.0369 (0.0402) Loss: 0.5141 (0.8688)
[2022/12/28 23:41] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:41] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:41] |     VALID(8)      0.8685      0.7347      0.5232      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:41] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:41] | ####################################################################################################
[2022/12/28 23:41] | TRAIN(009): [ 50/879] Batch: 0.1611 (0.2016) Data: 0.0085 (0.0401) Loss: 1.1214 (0.8954)
[2022/12/28 23:41] | TRAIN(009): [100/879] Batch: 0.1749 (0.1854) Data: 0.0089 (0.0251) Loss: 0.7370 (0.8927)
[2022/12/28 23:41] | TRAIN(009): [150/879] Batch: 0.1715 (0.1795) Data: 0.0103 (0.0200) Loss: 1.0179 (0.8930)
[2022/12/28 23:42] | TRAIN(009): [200/879] Batch: 0.1726 (0.1766) Data: 0.0089 (0.0173) Loss: 0.8563 (0.8809)
[2022/12/28 23:42] | TRAIN(009): [250/879] Batch: 0.1724 (0.1748) Data: 0.0095 (0.0157) Loss: 0.8132 (0.8707)
[2022/12/28 23:42] | TRAIN(009): [300/879] Batch: 0.1674 (0.1741) Data: 0.0093 (0.0146) Loss: 0.6775 (0.8670)
[2022/12/28 23:42] | TRAIN(009): [350/879] Batch: 0.1760 (0.1732) Data: 0.0110 (0.0139) Loss: 0.7595 (0.8783)
[2022/12/28 23:42] | TRAIN(009): [400/879] Batch: 0.1641 (0.1725) Data: 0.0091 (0.0133) Loss: 0.8555 (0.8724)
[2022/12/28 23:42] | TRAIN(009): [450/879] Batch: 0.1680 (0.1721) Data: 0.0097 (0.0128) Loss: 0.6471 (0.8751)
[2022/12/28 23:42] | TRAIN(009): [500/879] Batch: 0.1755 (0.1718) Data: 0.0091 (0.0125) Loss: 1.1141 (0.8752)
[2022/12/28 23:43] | TRAIN(009): [550/879] Batch: 0.1691 (0.1714) Data: 0.0094 (0.0122) Loss: 0.4858 (0.8742)
[2022/12/28 23:43] | TRAIN(009): [600/879] Batch: 0.1666 (0.1710) Data: 0.0088 (0.0120) Loss: 0.9184 (0.8747)
[2022/12/28 23:43] | TRAIN(009): [650/879] Batch: 0.1688 (0.1710) Data: 0.0093 (0.0118) Loss: 0.5598 (0.8731)
[2022/12/28 23:43] | TRAIN(009): [700/879] Batch: 0.1632 (0.1709) Data: 0.0094 (0.0116) Loss: 1.0774 (0.8747)
[2022/12/28 23:43] | TRAIN(009): [750/879] Batch: 0.1649 (0.1708) Data: 0.0096 (0.0115) Loss: 0.8446 (0.8733)
[2022/12/28 23:43] | TRAIN(009): [800/879] Batch: 0.1698 (0.1707) Data: 0.0086 (0.0114) Loss: 0.7815 (0.8712)
[2022/12/28 23:43] | TRAIN(009): [850/879] Batch: 0.1850 (0.1706) Data: 0.0134 (0.0112) Loss: 1.0300 (0.8700)
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:43] |     TRAIN(9)     0:02:29     0:00:09     0:02:19      0.8710
[2022/12/28 23:43] | ------------------------------------------------------------
[2022/12/28 23:44] | VALID(009): [ 50/220] Batch: 0.0543 (0.0811) Data: 0.0354 (0.0563) Loss: 0.7798 (0.8465)
[2022/12/28 23:44] | VALID(009): [100/220] Batch: 0.0574 (0.0687) Data: 0.0349 (0.0448) Loss: 1.1139 (0.8703)
[2022/12/28 23:44] | VALID(009): [150/220] Batch: 0.0534 (0.0646) Data: 0.0387 (0.0414) Loss: 0.7573 (0.8619)
[2022/12/28 23:44] | VALID(009): [200/220] Batch: 0.0583 (0.0627) Data: 0.0352 (0.0397) Loss: 0.4965 (0.8698)
[2022/12/28 23:44] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:44] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:44] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:44] |     VALID(9)      0.8694      0.7347      0.5097      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:44] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:44] | ####################################################################################################
[2022/12/28 23:44] | TRAIN(010): [ 50/879] Batch: 0.1737 (0.2028) Data: 0.0105 (0.0430) Loss: 1.0238 (0.8449)
[2022/12/28 23:44] | TRAIN(010): [100/879] Batch: 0.1756 (0.1854) Data: 0.0085 (0.0264) Loss: 0.9127 (0.8671)
[2022/12/28 23:44] | TRAIN(010): [150/879] Batch: 0.1669 (0.1797) Data: 0.0092 (0.0207) Loss: 1.0001 (0.8647)
[2022/12/28 23:44] | TRAIN(010): [200/879] Batch: 0.1634 (0.1767) Data: 0.0105 (0.0180) Loss: 0.5490 (0.8631)
[2022/12/28 23:44] | TRAIN(010): [250/879] Batch: 0.1723 (0.1749) Data: 0.0101 (0.0163) Loss: 0.7425 (0.8766)
[2022/12/28 23:45] | TRAIN(010): [300/879] Batch: 0.1691 (0.1739) Data: 0.0110 (0.0152) Loss: 0.7058 (0.8861)
[2022/12/28 23:45] | TRAIN(010): [350/879] Batch: 0.1644 (0.1730) Data: 0.0096 (0.0144) Loss: 0.9751 (0.8816)
[2022/12/28 23:45] | TRAIN(010): [400/879] Batch: 0.1633 (0.1722) Data: 0.0100 (0.0137) Loss: 0.9395 (0.8765)
[2022/12/28 23:45] | TRAIN(010): [450/879] Batch: 0.1679 (0.1716) Data: 0.0084 (0.0132) Loss: 0.9873 (0.8741)
[2022/12/28 23:45] | TRAIN(010): [500/879] Batch: 0.1702 (0.1712) Data: 0.0087 (0.0128) Loss: 1.3606 (0.8742)
[2022/12/28 23:45] | TRAIN(010): [550/879] Batch: 0.1611 (0.1707) Data: 0.0099 (0.0125) Loss: 1.1520 (0.8769)
[2022/12/28 23:45] | TRAIN(010): [600/879] Batch: 0.1622 (0.1703) Data: 0.0096 (0.0123) Loss: 0.8094 (0.8757)
[2022/12/28 23:46] | TRAIN(010): [650/879] Batch: 0.1740 (0.1703) Data: 0.0095 (0.0121) Loss: 0.8250 (0.8747)
[2022/12/28 23:46] | TRAIN(010): [700/879] Batch: 0.1703 (0.1701) Data: 0.0092 (0.0119) Loss: 1.0019 (0.8737)
[2022/12/28 23:46] | TRAIN(010): [750/879] Batch: 0.1630 (0.1701) Data: 0.0092 (0.0118) Loss: 0.9786 (0.8742)
[2022/12/28 23:46] | TRAIN(010): [800/879] Batch: 0.1650 (0.1701) Data: 0.0103 (0.0117) Loss: 0.7005 (0.8728)
[2022/12/28 23:46] | TRAIN(010): [850/879] Batch: 0.1626 (0.1700) Data: 0.0091 (0.0116) Loss: 0.8988 (0.8737)
[2022/12/28 23:46] | ------------------------------------------------------------
[2022/12/28 23:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:46] | ------------------------------------------------------------
[2022/12/28 23:46] |    TRAIN(10)     0:02:29     0:00:10     0:02:19      0.8723
[2022/12/28 23:46] | ------------------------------------------------------------
[2022/12/28 23:46] | VALID(010): [ 50/220] Batch: 0.0580 (0.0807) Data: 0.0308 (0.0552) Loss: 0.7893 (0.8479)
[2022/12/28 23:46] | VALID(010): [100/220] Batch: 0.0572 (0.0687) Data: 0.0282 (0.0424) Loss: 1.1198 (0.8703)
[2022/12/28 23:46] | VALID(010): [150/220] Batch: 0.0559 (0.0644) Data: 0.0318 (0.0392) Loss: 0.7600 (0.8625)
[2022/12/28 23:46] | VALID(010): [200/220] Batch: 0.0574 (0.0622) Data: 0.0324 (0.0376) Loss: 0.4958 (0.8704)
[2022/12/28 23:46] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:46] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:46] |    VALID(10)      0.8703      0.7347      0.4987      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:46] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:46] | ####################################################################################################
[2022/12/28 23:47] | TRAIN(011): [ 50/879] Batch: 0.1797 (0.2080) Data: 0.0096 (0.0450) Loss: 0.7466 (0.8804)
[2022/12/28 23:47] | TRAIN(011): [100/879] Batch: 0.1641 (0.1884) Data: 0.0173 (0.0281) Loss: 1.0632 (0.8872)
[2022/12/28 23:47] | TRAIN(011): [150/879] Batch: 0.1772 (0.1828) Data: 0.0163 (0.0225) Loss: 1.1179 (0.8788)
[2022/12/28 23:47] | TRAIN(011): [200/879] Batch: 0.1652 (0.1801) Data: 0.0096 (0.0196) Loss: 0.4985 (0.8738)
[2022/12/28 23:47] | TRAIN(011): [250/879] Batch: 0.1668 (0.1780) Data: 0.0120 (0.0178) Loss: 0.7362 (0.8696)
[2022/12/28 23:47] | TRAIN(011): [300/879] Batch: 0.1660 (0.1765) Data: 0.0093 (0.0166) Loss: 0.7981 (0.8602)
[2022/12/28 23:47] | TRAIN(011): [350/879] Batch: 0.1756 (0.1761) Data: 0.0163 (0.0158) Loss: 0.7080 (0.8637)
[2022/12/28 23:48] | TRAIN(011): [400/879] Batch: 0.1650 (0.1752) Data: 0.0085 (0.0151) Loss: 0.9537 (0.8674)
[2022/12/28 23:48] | TRAIN(011): [450/879] Batch: 0.1737 (0.1745) Data: 0.0101 (0.0145) Loss: 0.7864 (0.8688)
[2022/12/28 23:48] | TRAIN(011): [500/879] Batch: 0.1823 (0.1744) Data: 0.0115 (0.0142) Loss: 0.9053 (0.8677)
[2022/12/28 23:48] | TRAIN(011): [550/879] Batch: 0.1634 (0.1740) Data: 0.0102 (0.0138) Loss: 0.8025 (0.8696)
[2022/12/28 23:48] | TRAIN(011): [600/879] Batch: 0.1644 (0.1734) Data: 0.0091 (0.0134) Loss: 1.0070 (0.8711)
[2022/12/28 23:48] | TRAIN(011): [650/879] Batch: 0.1701 (0.1729) Data: 0.0091 (0.0131) Loss: 1.0762 (0.8708)
[2022/12/28 23:48] | TRAIN(011): [700/879] Batch: 0.1734 (0.1725) Data: 0.0090 (0.0129) Loss: 0.9531 (0.8717)
[2022/12/28 23:49] | TRAIN(011): [750/879] Batch: 0.1715 (0.1720) Data: 0.0101 (0.0127) Loss: 1.0902 (0.8736)
[2022/12/28 23:49] | TRAIN(011): [800/879] Batch: 0.1638 (0.1717) Data: 0.0109 (0.0125) Loss: 0.7538 (0.8743)
[2022/12/28 23:49] | TRAIN(011): [850/879] Batch: 0.1628 (0.1715) Data: 0.0091 (0.0123) Loss: 0.9546 (0.8716)
[2022/12/28 23:49] | ------------------------------------------------------------
[2022/12/28 23:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:49] | ------------------------------------------------------------
[2022/12/28 23:49] |    TRAIN(11)     0:02:30     0:00:10     0:02:19      0.8709
[2022/12/28 23:49] | ------------------------------------------------------------
[2022/12/28 23:49] | VALID(011): [ 50/220] Batch: 0.0566 (0.0794) Data: 0.0324 (0.0536) Loss: 0.7787 (0.8486)
[2022/12/28 23:49] | VALID(011): [100/220] Batch: 0.0560 (0.0674) Data: 0.0325 (0.0437) Loss: 1.1181 (0.8733)
[2022/12/28 23:49] | VALID(011): [150/220] Batch: 0.0571 (0.0635) Data: 0.0346 (0.0407) Loss: 0.7557 (0.8644)
[2022/12/28 23:49] | VALID(011): [200/220] Batch: 0.0568 (0.0615) Data: 0.0363 (0.0392) Loss: 0.4896 (0.8727)
[2022/12/28 23:49] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:49] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:49] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:49] |    VALID(11)      0.8720      0.7347      0.4999      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:49] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:49] | ####################################################################################################
[2022/12/28 23:49] | TRAIN(012): [ 50/879] Batch: 0.1614 (0.2049) Data: 0.0086 (0.0447) Loss: 1.1666 (0.8797)
[2022/12/28 23:49] | TRAIN(012): [100/879] Batch: 0.1694 (0.1859) Data: 0.0081 (0.0273) Loss: 0.4930 (0.8600)
[2022/12/28 23:50] | TRAIN(012): [150/879] Batch: 0.1848 (0.1790) Data: 0.0096 (0.0213) Loss: 1.0781 (0.8558)
[2022/12/28 23:50] | TRAIN(012): [200/879] Batch: 0.1615 (0.1755) Data: 0.0088 (0.0182) Loss: 0.8359 (0.8562)
[2022/12/28 23:50] | TRAIN(012): [250/879] Batch: 0.1662 (0.1738) Data: 0.0096 (0.0164) Loss: 1.0269 (0.8635)
[2022/12/28 23:50] | TRAIN(012): [300/879] Batch: 0.1693 (0.1725) Data: 0.0090 (0.0152) Loss: 0.7265 (0.8590)
[2022/12/28 23:50] | TRAIN(012): [350/879] Batch: 0.1606 (0.1718) Data: 0.0086 (0.0144) Loss: 0.7955 (0.8577)
[2022/12/28 23:50] | TRAIN(012): [400/879] Batch: 0.1635 (0.1711) Data: 0.0094 (0.0138) Loss: 0.7838 (0.8576)
[2022/12/28 23:50] | TRAIN(012): [450/879] Batch: 0.1630 (0.1707) Data: 0.0093 (0.0133) Loss: 0.5258 (0.8575)
[2022/12/28 23:51] | TRAIN(012): [500/879] Batch: 0.1797 (0.1718) Data: 0.0106 (0.0131) Loss: 1.0590 (0.8602)
[2022/12/28 23:51] | TRAIN(012): [550/879] Batch: 0.1618 (0.1722) Data: 0.0086 (0.0128) Loss: 1.1380 (0.8649)
[2022/12/28 23:51] | TRAIN(012): [600/879] Batch: 0.1712 (0.1717) Data: 0.0091 (0.0125) Loss: 0.8632 (0.8644)
[2022/12/28 23:51] | TRAIN(012): [650/879] Batch: 0.1611 (0.1713) Data: 0.0092 (0.0123) Loss: 1.1921 (0.8660)
[2022/12/28 23:51] | TRAIN(012): [700/879] Batch: 0.1614 (0.1709) Data: 0.0089 (0.0121) Loss: 0.9880 (0.8684)
[2022/12/28 23:51] | TRAIN(012): [750/879] Batch: 0.1618 (0.1705) Data: 0.0093 (0.0119) Loss: 0.9457 (0.8711)
[2022/12/28 23:51] | TRAIN(012): [800/879] Batch: 0.1720 (0.1705) Data: 0.0101 (0.0118) Loss: 1.0275 (0.8699)
[2022/12/28 23:52] | TRAIN(012): [850/879] Batch: 0.1680 (0.1702) Data: 0.0101 (0.0116) Loss: 1.0513 (0.8707)
[2022/12/28 23:52] | ------------------------------------------------------------
[2022/12/28 23:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:52] | ------------------------------------------------------------
[2022/12/28 23:52] |    TRAIN(12)     0:02:29     0:00:10     0:02:19      0.8705
[2022/12/28 23:52] | ------------------------------------------------------------
[2022/12/28 23:52] | VALID(012): [ 50/220] Batch: 0.0541 (0.0784) Data: 0.0328 (0.0574) Loss: 0.8003 (0.8509)
[2022/12/28 23:52] | VALID(012): [100/220] Batch: 0.0501 (0.0669) Data: 0.0350 (0.0452) Loss: 1.0925 (0.8705)
[2022/12/28 23:52] | VALID(012): [150/220] Batch: 0.0530 (0.0630) Data: 0.0353 (0.0413) Loss: 0.7740 (0.8638)
[2022/12/28 23:52] | VALID(012): [200/220] Batch: 0.0569 (0.0610) Data: 0.0338 (0.0393) Loss: 0.5385 (0.8707)
[2022/12/28 23:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:52] |    VALID(12)      0.8707      0.7347      0.5083      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:52] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:52] | ####################################################################################################
[2022/12/28 23:52] | TRAIN(013): [ 50/879] Batch: 0.1648 (0.2018) Data: 0.0085 (0.0425) Loss: 0.5972 (0.8740)
[2022/12/28 23:52] | TRAIN(013): [100/879] Batch: 0.1659 (0.1843) Data: 0.0091 (0.0261) Loss: 1.2005 (0.8805)
[2022/12/28 23:52] | TRAIN(013): [150/879] Batch: 0.1635 (0.1783) Data: 0.0088 (0.0206) Loss: 0.7242 (0.8695)
[2022/12/28 23:52] | TRAIN(013): [200/879] Batch: 0.1643 (0.1750) Data: 0.0087 (0.0178) Loss: 0.7715 (0.8598)
[2022/12/28 23:53] | TRAIN(013): [250/879] Batch: 0.1613 (0.1729) Data: 0.0113 (0.0161) Loss: 1.0142 (0.8675)
[2022/12/28 23:53] | TRAIN(013): [300/879] Batch: 0.1599 (0.1718) Data: 0.0086 (0.0150) Loss: 1.2712 (0.8651)
[2022/12/28 23:53] | TRAIN(013): [350/879] Batch: 0.1618 (0.1709) Data: 0.0085 (0.0142) Loss: 1.0259 (0.8692)
[2022/12/28 23:53] | TRAIN(013): [400/879] Batch: 0.1656 (0.1702) Data: 0.0087 (0.0135) Loss: 1.1579 (0.8673)
[2022/12/28 23:53] | TRAIN(013): [450/879] Batch: 0.1742 (0.1698) Data: 0.0091 (0.0131) Loss: 0.8063 (0.8675)
[2022/12/28 23:53] | TRAIN(013): [500/879] Batch: 0.1684 (0.1693) Data: 0.0087 (0.0127) Loss: 0.9085 (0.8675)
[2022/12/28 23:53] | TRAIN(013): [550/879] Batch: 0.1720 (0.1689) Data: 0.0090 (0.0124) Loss: 0.9142 (0.8627)
[2022/12/28 23:54] | TRAIN(013): [600/879] Batch: 0.1637 (0.1685) Data: 0.0090 (0.0121) Loss: 1.1787 (0.8672)
[2022/12/28 23:54] | TRAIN(013): [650/879] Batch: 0.1666 (0.1682) Data: 0.0106 (0.0119) Loss: 1.0127 (0.8705)
[2022/12/28 23:54] | TRAIN(013): [700/879] Batch: 0.1620 (0.1680) Data: 0.0086 (0.0117) Loss: 0.9219 (0.8709)
[2022/12/28 23:54] | TRAIN(013): [750/879] Batch: 0.1624 (0.1679) Data: 0.0088 (0.0115) Loss: 0.9370 (0.8712)
[2022/12/28 23:54] | TRAIN(013): [800/879] Batch: 0.1614 (0.1679) Data: 0.0090 (0.0114) Loss: 0.9651 (0.8715)
[2022/12/28 23:54] | TRAIN(013): [850/879] Batch: 0.1663 (0.1678) Data: 0.0083 (0.0113) Loss: 0.5301 (0.8701)
[2022/12/28 23:54] | ------------------------------------------------------------
[2022/12/28 23:54] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:54] | ------------------------------------------------------------
[2022/12/28 23:54] |    TRAIN(13)     0:02:27     0:00:09     0:02:17      0.8714
[2022/12/28 23:54] | ------------------------------------------------------------
[2022/12/28 23:54] | VALID(013): [ 50/220] Batch: 0.0549 (0.0771) Data: 0.0307 (0.0545) Loss: 0.7877 (0.8496)
[2022/12/28 23:54] | VALID(013): [100/220] Batch: 0.0583 (0.0663) Data: 0.0305 (0.0441) Loss: 1.1028 (0.8704)
[2022/12/28 23:54] | VALID(013): [150/220] Batch: 0.0580 (0.0628) Data: 0.0355 (0.0396) Loss: 0.7644 (0.8629)
[2022/12/28 23:55] | VALID(013): [200/220] Batch: 0.0572 (0.0610) Data: 0.0368 (0.0383) Loss: 0.5474 (0.8699)
[2022/12/28 23:55] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:55] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:55] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:55] |    VALID(13)      0.8698      0.7347      0.5033      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:55] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:55] | ####################################################################################################
[2022/12/28 23:55] | TRAIN(014): [ 50/879] Batch: 0.1805 (0.2032) Data: 0.0114 (0.0427) Loss: 0.9751 (0.8584)
[2022/12/28 23:55] | TRAIN(014): [100/879] Batch: 0.1683 (0.1850) Data: 0.0090 (0.0265) Loss: 0.8421 (0.8568)
[2022/12/28 23:55] | TRAIN(014): [150/879] Batch: 0.1704 (0.1789) Data: 0.0097 (0.0209) Loss: 0.7977 (0.8576)
[2022/12/28 23:55] | TRAIN(014): [200/879] Batch: 0.1638 (0.1759) Data: 0.0084 (0.0181) Loss: 0.8991 (0.8573)
[2022/12/28 23:55] | TRAIN(014): [250/879] Batch: 0.1666 (0.1740) Data: 0.0086 (0.0163) Loss: 0.7297 (0.8697)
[2022/12/28 23:55] | TRAIN(014): [300/879] Batch: 0.1637 (0.1725) Data: 0.0092 (0.0152) Loss: 0.8670 (0.8698)
[2022/12/28 23:56] | TRAIN(014): [350/879] Batch: 0.1768 (0.1716) Data: 0.0089 (0.0143) Loss: 0.6697 (0.8630)
[2022/12/28 23:56] | TRAIN(014): [400/879] Batch: 0.1605 (0.1710) Data: 0.0088 (0.0137) Loss: 0.6782 (0.8661)
[2022/12/28 23:56] | TRAIN(014): [450/879] Batch: 0.1668 (0.1704) Data: 0.0089 (0.0132) Loss: 0.7515 (0.8696)
[2022/12/28 23:56] | TRAIN(014): [500/879] Batch: 0.1748 (0.1700) Data: 0.0092 (0.0128) Loss: 0.6988 (0.8684)
[2022/12/28 23:56] | TRAIN(014): [550/879] Batch: 0.1623 (0.1696) Data: 0.0087 (0.0125) Loss: 0.8737 (0.8705)
[2022/12/28 23:56] | TRAIN(014): [600/879] Batch: 0.1596 (0.1693) Data: 0.0089 (0.0123) Loss: 1.0319 (0.8704)
[2022/12/28 23:56] | TRAIN(014): [650/879] Batch: 0.1633 (0.1692) Data: 0.0090 (0.0121) Loss: 1.1952 (0.8687)
[2022/12/28 23:57] | TRAIN(014): [700/879] Batch: 0.1661 (0.1690) Data: 0.0093 (0.0119) Loss: 1.0060 (0.8685)
[2022/12/28 23:57] | TRAIN(014): [750/879] Batch: 0.1621 (0.1688) Data: 0.0086 (0.0117) Loss: 0.4627 (0.8695)
[2022/12/28 23:57] | TRAIN(014): [800/879] Batch: 0.1601 (0.1687) Data: 0.0086 (0.0116) Loss: 1.2211 (0.8722)
[2022/12/28 23:57] | TRAIN(014): [850/879] Batch: 0.1613 (0.1686) Data: 0.0085 (0.0114) Loss: 0.8216 (0.8694)
[2022/12/28 23:57] | ------------------------------------------------------------
[2022/12/28 23:57] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/28 23:57] | ------------------------------------------------------------
[2022/12/28 23:57] |    TRAIN(14)     0:02:28     0:00:09     0:02:18      0.8700
[2022/12/28 23:57] | ------------------------------------------------------------
[2022/12/28 23:57] | VALID(014): [ 50/220] Batch: 0.0571 (0.0778) Data: 0.0335 (0.0549) Loss: 0.7721 (0.8468)
[2022/12/28 23:57] | VALID(014): [100/220] Batch: 0.0556 (0.0667) Data: 0.0357 (0.0448) Loss: 1.1181 (0.8708)
[2022/12/28 23:57] | VALID(014): [150/220] Batch: 0.0554 (0.0631) Data: 0.0319 (0.0413) Loss: 0.7533 (0.8621)
[2022/12/28 23:57] | VALID(014): [200/220] Batch: 0.0527 (0.0613) Data: 0.0376 (0.0397) Loss: 0.5138 (0.8700)
[2022/12/28 23:57] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:57] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/28 23:57] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:57] |    VALID(14)      0.8696      0.7347      0.4998      0.7347      0.7347      0.7347      0.9337
[2022/12/28 23:57] | ------------------------------------------------------------------------------------------------
[2022/12/28 23:57] | ####################################################################################################
[2022/12/28 23:57] | TRAIN(015): [ 50/879] Batch: 0.1673 (0.1973) Data: 0.0098 (0.0395) Loss: 0.7332 (0.8482)
[2022/12/28 23:58] | TRAIN(015): [100/879] Batch: 0.1625 (0.1823) Data: 0.0095 (0.0247) Loss: 0.7628 (0.8503)
[2022/12/28 23:58] | TRAIN(015): [150/879] Batch: 0.1693 (0.1767) Data: 0.0098 (0.0196) Loss: 0.9732 (0.8640)
[2022/12/28 23:58] | TRAIN(015): [200/879] Batch: 0.1679 (0.1747) Data: 0.0099 (0.0172) Loss: 0.8041 (0.8689)
[2022/12/28 23:58] | TRAIN(015): [250/879] Batch: 0.1664 (0.1732) Data: 0.0095 (0.0157) Loss: 0.7368 (0.8637)
[2022/12/28 23:58] | TRAIN(015): [300/879] Batch: 0.1616 (0.1720) Data: 0.0089 (0.0147) Loss: 0.9877 (0.8650)
[2022/12/28 23:58] | TRAIN(015): [350/879] Batch: 0.1647 (0.1716) Data: 0.0101 (0.0139) Loss: 1.1250 (0.8659)
[2022/12/28 23:58] | TRAIN(015): [400/879] Batch: 0.1634 (0.1708) Data: 0.0107 (0.0134) Loss: 0.8141 (0.8678)
[2022/12/28 23:59] | TRAIN(015): [450/879] Batch: 0.1648 (0.1702) Data: 0.0104 (0.0129) Loss: 0.6630 (0.8656)
[2022/12/28 23:59] | TRAIN(015): [500/879] Batch: 0.1657 (0.1699) Data: 0.0088 (0.0126) Loss: 0.9281 (0.8667)
[2022/12/28 23:59] | TRAIN(015): [550/879] Batch: 0.1721 (0.1695) Data: 0.0088 (0.0123) Loss: 0.6908 (0.8696)
[2022/12/28 23:59] | TRAIN(015): [600/879] Batch: 0.1688 (0.1692) Data: 0.0089 (0.0120) Loss: 0.8084 (0.8685)
[2022/12/28 23:59] | TRAIN(015): [650/879] Batch: 0.1609 (0.1690) Data: 0.0088 (0.0118) Loss: 1.0895 (0.8712)
[2022/12/28 23:59] | TRAIN(015): [700/879] Batch: 0.1619 (0.1687) Data: 0.0104 (0.0116) Loss: 0.7567 (0.8722)
[2022/12/28 23:59] | TRAIN(015): [750/879] Batch: 0.1685 (0.1685) Data: 0.0086 (0.0114) Loss: 1.0289 (0.8748)
[2022/12/29 00:00] | TRAIN(015): [800/879] Batch: 0.1702 (0.1682) Data: 0.0092 (0.0113) Loss: 1.0832 (0.8736)
[2022/12/29 00:00] | TRAIN(015): [850/879] Batch: 0.1644 (0.1681) Data: 0.0094 (0.0112) Loss: 1.1179 (0.8712)
[2022/12/29 00:00] | ------------------------------------------------------------
[2022/12/29 00:00] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:00] | ------------------------------------------------------------
[2022/12/29 00:00] |    TRAIN(15)     0:02:27     0:00:09     0:02:17      0.8712
[2022/12/29 00:00] | ------------------------------------------------------------
[2022/12/29 00:00] | VALID(015): [ 50/220] Batch: 0.0536 (0.0794) Data: 0.0361 (0.0543) Loss: 0.7972 (0.8524)
[2022/12/29 00:00] | VALID(015): [100/220] Batch: 0.0528 (0.0677) Data: 0.0363 (0.0444) Loss: 1.0927 (0.8724)
[2022/12/29 00:00] | VALID(015): [150/220] Batch: 0.0539 (0.0637) Data: 0.0364 (0.0410) Loss: 0.7721 (0.8652)
[2022/12/29 00:00] | VALID(015): [200/220] Batch: 0.0527 (0.0617) Data: 0.0363 (0.0393) Loss: 0.5530 (0.8719)
[2022/12/29 00:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:00] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:00] |    VALID(15)      0.8718      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:00] | ####################################################################################################
[2022/12/29 00:00] | TRAIN(016): [ 50/879] Batch: 0.1734 (0.2032) Data: 0.0161 (0.0422) Loss: 0.6644 (0.8493)
[2022/12/29 00:00] | TRAIN(016): [100/879] Batch: 0.1714 (0.1861) Data: 0.0101 (0.0262) Loss: 0.6498 (0.8565)
[2022/12/29 00:00] | TRAIN(016): [150/879] Batch: 0.1635 (0.1798) Data: 0.0115 (0.0207) Loss: 0.8013 (0.8664)
[2022/12/29 00:01] | TRAIN(016): [200/879] Batch: 0.1653 (0.1765) Data: 0.0091 (0.0179) Loss: 0.7963 (0.8603)
[2022/12/29 00:01] | TRAIN(016): [250/879] Batch: 0.1606 (0.1757) Data: 0.0086 (0.0164) Loss: 0.8761 (0.8664)
[2022/12/29 00:01] | TRAIN(016): [300/879] Batch: 0.1904 (0.1752) Data: 0.0112 (0.0155) Loss: 0.6533 (0.8655)
[2022/12/29 00:01] | TRAIN(016): [350/879] Batch: 0.1704 (0.1746) Data: 0.0093 (0.0147) Loss: 0.7833 (0.8697)
[2022/12/29 00:01] | TRAIN(016): [400/879] Batch: 0.1627 (0.1734) Data: 0.0090 (0.0140) Loss: 1.0645 (0.8685)
[2022/12/29 00:01] | TRAIN(016): [450/879] Batch: 0.1657 (0.1726) Data: 0.0092 (0.0135) Loss: 0.9035 (0.8675)
[2022/12/29 00:01] | TRAIN(016): [500/879] Batch: 0.1795 (0.1720) Data: 0.0092 (0.0131) Loss: 0.9827 (0.8668)
[2022/12/29 00:02] | TRAIN(016): [550/879] Batch: 0.1650 (0.1714) Data: 0.0086 (0.0128) Loss: 0.9988 (0.8678)
[2022/12/29 00:02] | TRAIN(016): [600/879] Batch: 0.1610 (0.1709) Data: 0.0087 (0.0125) Loss: 0.9286 (0.8677)
[2022/12/29 00:02] | TRAIN(016): [650/879] Batch: 0.1654 (0.1704) Data: 0.0097 (0.0122) Loss: 0.9753 (0.8698)
[2022/12/29 00:02] | TRAIN(016): [700/879] Batch: 0.1644 (0.1702) Data: 0.0088 (0.0120) Loss: 0.5702 (0.8711)
[2022/12/29 00:02] | TRAIN(016): [750/879] Batch: 0.1624 (0.1701) Data: 0.0101 (0.0119) Loss: 0.9588 (0.8735)
[2022/12/29 00:02] | TRAIN(016): [800/879] Batch: 0.1680 (0.1703) Data: 0.0089 (0.0118) Loss: 0.7827 (0.8720)
[2022/12/29 00:02] | TRAIN(016): [850/879] Batch: 0.1801 (0.1702) Data: 0.0115 (0.0117) Loss: 0.7767 (0.8713)
[2022/12/29 00:02] | ------------------------------------------------------------
[2022/12/29 00:02] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:02] | ------------------------------------------------------------
[2022/12/29 00:02] |    TRAIN(16)     0:02:29     0:00:10     0:02:19      0.8714
[2022/12/29 00:02] | ------------------------------------------------------------
[2022/12/29 00:03] | VALID(016): [ 50/220] Batch: 0.0565 (0.0786) Data: 0.0354 (0.0570) Loss: 0.7833 (0.8474)
[2022/12/29 00:03] | VALID(016): [100/220] Batch: 0.0536 (0.0671) Data: 0.0361 (0.0457) Loss: 1.1183 (0.8693)
[2022/12/29 00:03] | VALID(016): [150/220] Batch: 0.0594 (0.0634) Data: 0.0284 (0.0414) Loss: 0.7570 (0.8614)
[2022/12/29 00:03] | VALID(016): [200/220] Batch: 0.0538 (0.0616) Data: 0.0349 (0.0385) Loss: 0.5283 (0.8687)
[2022/12/29 00:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:03] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:03] |    VALID(16)      0.8688      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:03] | ####################################################################################################
[2022/12/29 00:03] | TRAIN(017): [ 50/879] Batch: 0.1692 (0.2076) Data: 0.0162 (0.0462) Loss: 0.9426 (0.8429)
[2022/12/29 00:03] | TRAIN(017): [100/879] Batch: 0.1696 (0.1876) Data: 0.0089 (0.0282) Loss: 0.7138 (0.8453)
[2022/12/29 00:03] | TRAIN(017): [150/879] Batch: 0.1690 (0.1805) Data: 0.0085 (0.0220) Loss: 0.8066 (0.8562)
[2022/12/29 00:03] | TRAIN(017): [200/879] Batch: 0.1647 (0.1774) Data: 0.0107 (0.0190) Loss: 0.9108 (0.8637)
[2022/12/29 00:03] | TRAIN(017): [250/879] Batch: 0.1701 (0.1748) Data: 0.0090 (0.0171) Loss: 1.0298 (0.8579)
[2022/12/29 00:04] | TRAIN(017): [300/879] Batch: 0.1678 (0.1733) Data: 0.0102 (0.0158) Loss: 0.7347 (0.8528)
[2022/12/29 00:04] | TRAIN(017): [350/879] Batch: 0.1802 (0.1725) Data: 0.0089 (0.0150) Loss: 0.7510 (0.8584)
[2022/12/29 00:04] | TRAIN(017): [400/879] Batch: 0.1654 (0.1725) Data: 0.0090 (0.0143) Loss: 0.8127 (0.8599)
[2022/12/29 00:04] | TRAIN(017): [450/879] Batch: 0.1723 (0.1721) Data: 0.0091 (0.0138) Loss: 0.8731 (0.8577)
[2022/12/29 00:04] | TRAIN(017): [500/879] Batch: 0.1674 (0.1716) Data: 0.0088 (0.0134) Loss: 0.8478 (0.8608)
[2022/12/29 00:04] | TRAIN(017): [550/879] Batch: 0.1709 (0.1711) Data: 0.0088 (0.0130) Loss: 0.6001 (0.8608)
[2022/12/29 00:04] | TRAIN(017): [600/879] Batch: 0.1606 (0.1706) Data: 0.0083 (0.0127) Loss: 0.6302 (0.8618)
[2022/12/29 00:05] | TRAIN(017): [650/879] Batch: 0.1631 (0.1701) Data: 0.0088 (0.0124) Loss: 0.6595 (0.8642)
[2022/12/29 00:05] | TRAIN(017): [700/879] Batch: 0.1643 (0.1698) Data: 0.0091 (0.0122) Loss: 0.6190 (0.8657)
[2022/12/29 00:05] | TRAIN(017): [750/879] Batch: 0.1646 (0.1695) Data: 0.0089 (0.0120) Loss: 1.0129 (0.8674)
[2022/12/29 00:05] | TRAIN(017): [800/879] Batch: 0.1755 (0.1692) Data: 0.0099 (0.0118) Loss: 1.0106 (0.8700)
[2022/12/29 00:05] | TRAIN(017): [850/879] Batch: 0.1599 (0.1691) Data: 0.0092 (0.0116) Loss: 1.0998 (0.8708)
[2022/12/29 00:05] | ------------------------------------------------------------
[2022/12/29 00:05] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:05] | ------------------------------------------------------------
[2022/12/29 00:05] |    TRAIN(17)     0:02:28     0:00:10     0:02:18      0.8702
[2022/12/29 00:05] | ------------------------------------------------------------
[2022/12/29 00:05] | VALID(017): [ 50/220] Batch: 0.0570 (0.0786) Data: 0.0367 (0.0555) Loss: 0.7808 (0.8468)
[2022/12/29 00:05] | VALID(017): [100/220] Batch: 0.0543 (0.0672) Data: 0.0325 (0.0451) Loss: 1.1177 (0.8702)
[2022/12/29 00:05] | VALID(017): [150/220] Batch: 0.0565 (0.0635) Data: 0.0332 (0.0418) Loss: 0.7569 (0.8620)
[2022/12/29 00:05] | VALID(017): [200/220] Batch: 0.0543 (0.0616) Data: 0.0331 (0.0400) Loss: 0.4965 (0.8700)
[2022/12/29 00:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:05] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:05] |    VALID(17)      0.8697      0.7347      0.5005      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:05] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:05] | ####################################################################################################
[2022/12/29 00:06] | TRAIN(018): [ 50/879] Batch: 0.1854 (0.2040) Data: 0.0117 (0.0439) Loss: 0.8190 (0.8777)
[2022/12/29 00:06] | TRAIN(018): [100/879] Batch: 0.1612 (0.1873) Data: 0.0082 (0.0271) Loss: 0.8164 (0.8983)
[2022/12/29 00:06] | TRAIN(018): [150/879] Batch: 0.1617 (0.1812) Data: 0.0101 (0.0215) Loss: 0.8819 (0.8932)
[2022/12/29 00:06] | TRAIN(018): [200/879] Batch: 0.1630 (0.1779) Data: 0.0091 (0.0186) Loss: 0.9486 (0.8814)
[2022/12/29 00:06] | TRAIN(018): [250/879] Batch: 0.1726 (0.1763) Data: 0.0090 (0.0168) Loss: 0.7332 (0.8821)
[2022/12/29 00:06] | TRAIN(018): [300/879] Batch: 0.1621 (0.1748) Data: 0.0097 (0.0156) Loss: 0.8937 (0.8802)
[2022/12/29 00:06] | TRAIN(018): [350/879] Batch: 0.1612 (0.1735) Data: 0.0094 (0.0147) Loss: 1.1169 (0.8792)
[2022/12/29 00:07] | TRAIN(018): [400/879] Batch: 0.1614 (0.1724) Data: 0.0094 (0.0140) Loss: 0.9913 (0.8756)
[2022/12/29 00:07] | TRAIN(018): [450/879] Batch: 0.1613 (0.1715) Data: 0.0099 (0.0134) Loss: 0.9596 (0.8738)
[2022/12/29 00:07] | TRAIN(018): [500/879] Batch: 0.1635 (0.1713) Data: 0.0099 (0.0131) Loss: 0.7142 (0.8738)
[2022/12/29 00:07] | TRAIN(018): [550/879] Batch: 0.1640 (0.1710) Data: 0.0137 (0.0127) Loss: 0.6083 (0.8728)
[2022/12/29 00:07] | TRAIN(018): [600/879] Batch: 0.1655 (0.1708) Data: 0.0107 (0.0125) Loss: 0.6564 (0.8737)
[2022/12/29 00:07] | TRAIN(018): [650/879] Batch: 0.1751 (0.1705) Data: 0.0098 (0.0123) Loss: 0.7997 (0.8737)
[2022/12/29 00:07] | TRAIN(018): [700/879] Batch: 0.1668 (0.1703) Data: 0.0091 (0.0121) Loss: 0.7149 (0.8727)
[2022/12/29 00:07] | TRAIN(018): [750/879] Batch: 0.1639 (0.1700) Data: 0.0099 (0.0119) Loss: 0.7754 (0.8734)
[2022/12/29 00:08] | TRAIN(018): [800/879] Batch: 0.1618 (0.1698) Data: 0.0096 (0.0117) Loss: 0.9520 (0.8725)
[2022/12/29 00:08] | TRAIN(018): [850/879] Batch: 0.1694 (0.1696) Data: 0.0095 (0.0116) Loss: 0.7896 (0.8723)
[2022/12/29 00:08] | ------------------------------------------------------------
[2022/12/29 00:08] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:08] | ------------------------------------------------------------
[2022/12/29 00:08] |    TRAIN(18)     0:02:28     0:00:10     0:02:18      0.8708
[2022/12/29 00:08] | ------------------------------------------------------------
[2022/12/29 00:08] | VALID(018): [ 50/220] Batch: 0.0535 (0.0793) Data: 0.0302 (0.0564) Loss: 0.7802 (0.8485)
[2022/12/29 00:08] | VALID(018): [100/220] Batch: 0.0536 (0.0678) Data: 0.0322 (0.0452) Loss: 1.1277 (0.8729)
[2022/12/29 00:08] | VALID(018): [150/220] Batch: 0.0558 (0.0640) Data: 0.0277 (0.0410) Loss: 0.7545 (0.8642)
[2022/12/29 00:08] | VALID(018): [200/220] Batch: 0.0552 (0.0622) Data: 0.0332 (0.0383) Loss: 0.4871 (0.8724)
[2022/12/29 00:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:08] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:08] |    VALID(18)      0.8720      0.7347      0.4983      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:08] | ####################################################################################################
[2022/12/29 00:08] | TRAIN(019): [ 50/879] Batch: 0.1715 (0.2007) Data: 0.0085 (0.0426) Loss: 0.7311 (0.8618)
[2022/12/29 00:08] | TRAIN(019): [100/879] Batch: 0.1699 (0.1845) Data: 0.0084 (0.0263) Loss: 0.8278 (0.8724)
[2022/12/29 00:09] | TRAIN(019): [150/879] Batch: 0.1648 (0.1785) Data: 0.0085 (0.0206) Loss: 1.4359 (0.8636)
[2022/12/29 00:09] | TRAIN(019): [200/879] Batch: 0.1648 (0.1753) Data: 0.0093 (0.0178) Loss: 0.7760 (0.8544)
[2022/12/29 00:09] | TRAIN(019): [250/879] Batch: 0.1662 (0.1737) Data: 0.0102 (0.0161) Loss: 0.8586 (0.8580)
[2022/12/29 00:09] | TRAIN(019): [300/879] Batch: 0.1638 (0.1723) Data: 0.0088 (0.0150) Loss: 0.6027 (0.8671)
[2022/12/29 00:09] | TRAIN(019): [350/879] Batch: 0.1618 (0.1714) Data: 0.0087 (0.0141) Loss: 0.9366 (0.8627)
[2022/12/29 00:09] | TRAIN(019): [400/879] Batch: 0.1645 (0.1707) Data: 0.0086 (0.0135) Loss: 0.9598 (0.8635)
[2022/12/29 00:09] | TRAIN(019): [450/879] Batch: 0.1656 (0.1704) Data: 0.0088 (0.0130) Loss: 0.7472 (0.8680)
[2022/12/29 00:10] | TRAIN(019): [500/879] Batch: 0.1620 (0.1703) Data: 0.0086 (0.0127) Loss: 0.8644 (0.8677)
[2022/12/29 00:10] | TRAIN(019): [550/879] Batch: 0.1832 (0.1704) Data: 0.0112 (0.0124) Loss: 0.8834 (0.8665)
[2022/12/29 00:10] | TRAIN(019): [600/879] Batch: 0.1622 (0.1704) Data: 0.0103 (0.0122) Loss: 0.8093 (0.8685)
[2022/12/29 00:10] | TRAIN(019): [650/879] Batch: 0.1730 (0.1701) Data: 0.0095 (0.0120) Loss: 1.1388 (0.8684)
[2022/12/29 00:10] | TRAIN(019): [700/879] Batch: 0.1614 (0.1698) Data: 0.0103 (0.0118) Loss: 0.8542 (0.8676)
[2022/12/29 00:10] | TRAIN(019): [750/879] Batch: 0.1692 (0.1698) Data: 0.0098 (0.0117) Loss: 0.7670 (0.8662)
[2022/12/29 00:10] | TRAIN(019): [800/879] Batch: 0.1635 (0.1694) Data: 0.0101 (0.0115) Loss: 1.1335 (0.8675)
[2022/12/29 00:10] | TRAIN(019): [850/879] Batch: 0.1655 (0.1692) Data: 0.0110 (0.0114) Loss: 0.9927 (0.8689)
[2022/12/29 00:11] | ------------------------------------------------------------
[2022/12/29 00:11] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:11] | ------------------------------------------------------------
[2022/12/29 00:11] |    TRAIN(19)     0:02:28     0:00:09     0:02:18      0.8703
[2022/12/29 00:11] | ------------------------------------------------------------
[2022/12/29 00:11] | VALID(019): [ 50/220] Batch: 0.0571 (0.0789) Data: 0.0359 (0.0551) Loss: 0.7930 (0.8515)
[2022/12/29 00:11] | VALID(019): [100/220] Batch: 0.0585 (0.0675) Data: 0.0308 (0.0449) Loss: 1.0928 (0.8716)
[2022/12/29 00:11] | VALID(019): [150/220] Batch: 0.0583 (0.0636) Data: 0.0367 (0.0414) Loss: 0.7725 (0.8646)
[2022/12/29 00:11] | VALID(019): [200/220] Batch: 0.0562 (0.0617) Data: 0.0367 (0.0397) Loss: 0.5526 (0.8715)
[2022/12/29 00:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:11] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:11] |    VALID(19)      0.8714      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:11] | ####################################################################################################
[2022/12/29 00:11] | TRAIN(020): [ 50/879] Batch: 0.1633 (0.1959) Data: 0.0091 (0.0395) Loss: 0.6511 (0.8901)
[2022/12/29 00:11] | TRAIN(020): [100/879] Batch: 0.1602 (0.1809) Data: 0.0086 (0.0245) Loss: 1.0852 (0.8759)
[2022/12/29 00:11] | TRAIN(020): [150/879] Batch: 0.1628 (0.1764) Data: 0.0111 (0.0197) Loss: 0.8731 (0.8575)
[2022/12/29 00:11] | TRAIN(020): [200/879] Batch: 0.1628 (0.1738) Data: 0.0095 (0.0171) Loss: 0.6377 (0.8602)
[2022/12/29 00:12] | TRAIN(020): [250/879] Batch: 0.1704 (0.1723) Data: 0.0096 (0.0156) Loss: 0.8583 (0.8656)
[2022/12/29 00:12] | TRAIN(020): [300/879] Batch: 0.1694 (0.1714) Data: 0.0090 (0.0146) Loss: 0.7422 (0.8570)
[2022/12/29 00:12] | TRAIN(020): [350/879] Batch: 0.1701 (0.1706) Data: 0.0085 (0.0138) Loss: 1.0185 (0.8597)
[2022/12/29 00:12] | TRAIN(020): [400/879] Batch: 0.1638 (0.1699) Data: 0.0091 (0.0132) Loss: 0.9786 (0.8586)
[2022/12/29 00:12] | TRAIN(020): [450/879] Batch: 0.1646 (0.1694) Data: 0.0090 (0.0128) Loss: 0.7122 (0.8599)
[2022/12/29 00:12] | TRAIN(020): [500/879] Batch: 0.1712 (0.1690) Data: 0.0099 (0.0124) Loss: 1.3561 (0.8619)
[2022/12/29 00:12] | TRAIN(020): [550/879] Batch: 0.1666 (0.1687) Data: 0.0092 (0.0122) Loss: 0.9816 (0.8619)
[2022/12/29 00:12] | TRAIN(020): [600/879] Batch: 0.1641 (0.1685) Data: 0.0092 (0.0119) Loss: 0.8479 (0.8607)
[2022/12/29 00:13] | TRAIN(020): [650/879] Batch: 0.1706 (0.1683) Data: 0.0102 (0.0117) Loss: 0.8725 (0.8612)
[2022/12/29 00:13] | TRAIN(020): [700/879] Batch: 0.1719 (0.1682) Data: 0.0092 (0.0115) Loss: 1.0984 (0.8670)
[2022/12/29 00:13] | TRAIN(020): [750/879] Batch: 0.1923 (0.1682) Data: 0.0118 (0.0114) Loss: 0.6439 (0.8658)
[2022/12/29 00:13] | TRAIN(020): [800/879] Batch: 0.1660 (0.1682) Data: 0.0097 (0.0113) Loss: 0.7973 (0.8672)
[2022/12/29 00:13] | TRAIN(020): [850/879] Batch: 0.1655 (0.1683) Data: 0.0102 (0.0112) Loss: 0.8104 (0.8689)
[2022/12/29 00:13] | ------------------------------------------------------------
[2022/12/29 00:13] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:13] | ------------------------------------------------------------
[2022/12/29 00:13] |    TRAIN(20)     0:02:27     0:00:09     0:02:18      0.8697
[2022/12/29 00:13] | ------------------------------------------------------------
[2022/12/29 00:13] | VALID(020): [ 50/220] Batch: 0.0562 (0.0779) Data: 0.0345 (0.0557) Loss: 0.7861 (0.8481)
[2022/12/29 00:13] | VALID(020): [100/220] Batch: 0.0529 (0.0671) Data: 0.0369 (0.0446) Loss: 1.1015 (0.8696)
[2022/12/29 00:13] | VALID(020): [150/220] Batch: 0.0543 (0.0634) Data: 0.0316 (0.0409) Loss: 0.7625 (0.8618)
[2022/12/29 00:13] | VALID(020): [200/220] Batch: 0.0579 (0.0616) Data: 0.0337 (0.0391) Loss: 0.5341 (0.8690)
[2022/12/29 00:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:13] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:13] |    VALID(20)      0.8688      0.7347      0.4934      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:13] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:13] | ####################################################################################################
[2022/12/29 00:14] | TRAIN(021): [ 50/879] Batch: 0.1735 (0.2051) Data: 0.0110 (0.0454) Loss: 0.7631 (0.8446)
[2022/12/29 00:14] | TRAIN(021): [100/879] Batch: 0.1636 (0.1871) Data: 0.0100 (0.0284) Loss: 0.7453 (0.8627)
[2022/12/29 00:14] | TRAIN(021): [150/879] Batch: 0.1725 (0.1810) Data: 0.0100 (0.0226) Loss: 0.7630 (0.8751)
[2022/12/29 00:14] | TRAIN(021): [200/879] Batch: 0.1690 (0.1778) Data: 0.0100 (0.0198) Loss: 0.8130 (0.8691)
[2022/12/29 00:14] | TRAIN(021): [250/879] Batch: 0.1640 (0.1758) Data: 0.0108 (0.0180) Loss: 1.1459 (0.8684)
[2022/12/29 00:14] | TRAIN(021): [300/879] Batch: 0.1643 (0.1745) Data: 0.0113 (0.0168) Loss: 0.8163 (0.8714)
[2022/12/29 00:14] | TRAIN(021): [350/879] Batch: 0.1659 (0.1736) Data: 0.0119 (0.0160) Loss: 0.6845 (0.8718)
[2022/12/29 00:15] | TRAIN(021): [400/879] Batch: 0.1702 (0.1729) Data: 0.0101 (0.0153) Loss: 1.0536 (0.8690)
[2022/12/29 00:15] | TRAIN(021): [450/879] Batch: 0.1689 (0.1726) Data: 0.0124 (0.0149) Loss: 0.8881 (0.8675)
[2022/12/29 00:15] | TRAIN(021): [500/879] Batch: 0.1698 (0.1722) Data: 0.0112 (0.0145) Loss: 1.2519 (0.8731)
[2022/12/29 00:15] | TRAIN(021): [550/879] Batch: 0.1629 (0.1719) Data: 0.0120 (0.0142) Loss: 0.9543 (0.8733)
[2022/12/29 00:15] | TRAIN(021): [600/879] Batch: 0.1709 (0.1718) Data: 0.0099 (0.0139) Loss: 0.5147 (0.8716)
[2022/12/29 00:15] | TRAIN(021): [650/879] Batch: 0.1644 (0.1714) Data: 0.0104 (0.0136) Loss: 1.0912 (0.8724)
[2022/12/29 00:15] | TRAIN(021): [700/879] Batch: 0.1730 (0.1715) Data: 0.0102 (0.0134) Loss: 0.6273 (0.8738)
[2022/12/29 00:16] | TRAIN(021): [750/879] Batch: 0.1812 (0.1715) Data: 0.0105 (0.0133) Loss: 0.8545 (0.8746)
[2022/12/29 00:16] | TRAIN(021): [800/879] Batch: 0.1648 (0.1717) Data: 0.0107 (0.0132) Loss: 1.0300 (0.8713)
[2022/12/29 00:16] | TRAIN(021): [850/879] Batch: 0.1658 (0.1715) Data: 0.0103 (0.0131) Loss: 0.8604 (0.8698)
[2022/12/29 00:16] | ------------------------------------------------------------
[2022/12/29 00:16] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:16] | ------------------------------------------------------------
[2022/12/29 00:16] |    TRAIN(21)     0:02:30     0:00:11     0:02:19      0.8702
[2022/12/29 00:16] | ------------------------------------------------------------
[2022/12/29 00:16] | VALID(021): [ 50/220] Batch: 0.0546 (0.0795) Data: 0.0284 (0.0558) Loss: 0.8023 (0.8498)
[2022/12/29 00:16] | VALID(021): [100/220] Batch: 0.0545 (0.0680) Data: 0.0322 (0.0445) Loss: 1.1017 (0.8704)
[2022/12/29 00:16] | VALID(021): [150/220] Batch: 0.0552 (0.0641) Data: 0.0324 (0.0413) Loss: 0.7702 (0.8634)
[2022/12/29 00:16] | VALID(021): [200/220] Batch: 0.0585 (0.0622) Data: 0.0379 (0.0397) Loss: 0.5102 (0.8706)
[2022/12/29 00:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:16] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:16] |    VALID(21)      0.8707      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:16] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:16] | ####################################################################################################
[2022/12/29 00:16] | TRAIN(022): [ 50/879] Batch: 0.1672 (0.2022) Data: 0.0159 (0.0416) Loss: 1.0171 (0.8919)
[2022/12/29 00:17] | TRAIN(022): [100/879] Batch: 0.1718 (0.1864) Data: 0.0108 (0.0264) Loss: 0.8716 (0.8934)
[2022/12/29 00:17] | TRAIN(022): [150/879] Batch: 0.1627 (0.1817) Data: 0.0127 (0.0214) Loss: 1.0373 (0.8887)
[2022/12/29 00:17] | TRAIN(022): [200/879] Batch: 0.1728 (0.1795) Data: 0.0104 (0.0189) Loss: 0.6476 (0.8898)
[2022/12/29 00:17] | TRAIN(022): [250/879] Batch: 0.1721 (0.1776) Data: 0.0096 (0.0173) Loss: 1.0214 (0.8837)
[2022/12/29 00:17] | TRAIN(022): [300/879] Batch: 0.1672 (0.1763) Data: 0.0100 (0.0162) Loss: 1.0694 (0.8884)
[2022/12/29 00:17] | TRAIN(022): [350/879] Batch: 0.1682 (0.1753) Data: 0.0105 (0.0153) Loss: 0.9516 (0.8855)
[2022/12/29 00:17] | TRAIN(022): [400/879] Batch: 0.1921 (0.1746) Data: 0.0139 (0.0148) Loss: 0.6857 (0.8856)
[2022/12/29 00:18] | TRAIN(022): [450/879] Batch: 0.1734 (0.1741) Data: 0.0089 (0.0143) Loss: 0.6393 (0.8849)
[2022/12/29 00:18] | TRAIN(022): [500/879] Batch: 0.1664 (0.1737) Data: 0.0100 (0.0140) Loss: 0.4486 (0.8808)
[2022/12/29 00:18] | TRAIN(022): [550/879] Batch: 0.1665 (0.1734) Data: 0.0099 (0.0137) Loss: 0.6852 (0.8758)
[2022/12/29 00:18] | TRAIN(022): [600/879] Batch: 0.1733 (0.1731) Data: 0.0102 (0.0135) Loss: 0.6804 (0.8687)
[2022/12/29 00:18] | TRAIN(022): [650/879] Batch: 0.1842 (0.1729) Data: 0.0095 (0.0133) Loss: 0.7717 (0.8701)
[2022/12/29 00:18] | TRAIN(022): [700/879] Batch: 0.1612 (0.1727) Data: 0.0111 (0.0131) Loss: 0.7908 (0.8709)
[2022/12/29 00:18] | TRAIN(022): [750/879] Batch: 0.1822 (0.1726) Data: 0.0113 (0.0130) Loss: 0.8952 (0.8700)
[2022/12/29 00:19] | TRAIN(022): [800/879] Batch: 0.1711 (0.1725) Data: 0.0109 (0.0129) Loss: 1.0937 (0.8690)
[2022/12/29 00:19] | TRAIN(022): [850/879] Batch: 0.1726 (0.1725) Data: 0.0101 (0.0128) Loss: 0.6528 (0.8708)
[2022/12/29 00:19] | ------------------------------------------------------------
[2022/12/29 00:19] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:19] | ------------------------------------------------------------
[2022/12/29 00:19] |    TRAIN(22)     0:02:31     0:00:11     0:02:20      0.8697
[2022/12/29 00:19] | ------------------------------------------------------------
[2022/12/29 00:19] | VALID(022): [ 50/220] Batch: 0.0538 (0.0798) Data: 0.0323 (0.0576) Loss: 0.7672 (0.8526)
[2022/12/29 00:19] | VALID(022): [100/220] Batch: 0.0396 (0.0681) Data: 0.0307 (0.0445) Loss: 1.1484 (0.8789)
[2022/12/29 00:19] | VALID(022): [150/220] Batch: 0.0585 (0.0641) Data: 0.0379 (0.0400) Loss: 0.7503 (0.8695)
[2022/12/29 00:19] | VALID(022): [200/220] Batch: 0.0557 (0.0624) Data: 0.0261 (0.0377) Loss: 0.4915 (0.8782)
[2022/12/29 00:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:19] |    VALID(22)      0.8776      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:19] | ####################################################################################################
[2022/12/29 00:19] | TRAIN(023): [ 50/879] Batch: 0.1778 (0.2020) Data: 0.0125 (0.0452) Loss: 0.6309 (0.8926)
[2022/12/29 00:19] | TRAIN(023): [100/879] Batch: 0.1683 (0.1854) Data: 0.0102 (0.0286) Loss: 0.9813 (0.8582)
[2022/12/29 00:19] | TRAIN(023): [150/879] Batch: 0.1614 (0.1805) Data: 0.0100 (0.0228) Loss: 0.7316 (0.8643)
[2022/12/29 00:20] | TRAIN(023): [200/879] Batch: 0.1651 (0.1774) Data: 0.0153 (0.0199) Loss: 0.9084 (0.8628)
[2022/12/29 00:20] | TRAIN(023): [250/879] Batch: 0.1688 (0.1757) Data: 0.0104 (0.0181) Loss: 0.9924 (0.8639)
[2022/12/29 00:20] | TRAIN(023): [300/879] Batch: 0.1629 (0.1749) Data: 0.0103 (0.0170) Loss: 0.7680 (0.8635)
[2022/12/29 00:20] | TRAIN(023): [350/879] Batch: 0.1845 (0.1741) Data: 0.0116 (0.0162) Loss: 0.6803 (0.8663)
[2022/12/29 00:20] | TRAIN(023): [400/879] Batch: 0.1704 (0.1738) Data: 0.0164 (0.0156) Loss: 0.9039 (0.8643)
[2022/12/29 00:20] | TRAIN(023): [450/879] Batch: 0.1695 (0.1734) Data: 0.0102 (0.0151) Loss: 0.6617 (0.8643)
[2022/12/29 00:20] | TRAIN(023): [500/879] Batch: 0.1822 (0.1733) Data: 0.0126 (0.0147) Loss: 0.9415 (0.8641)
[2022/12/29 00:21] | TRAIN(023): [550/879] Batch: 0.1733 (0.1731) Data: 0.0102 (0.0144) Loss: 0.8084 (0.8619)
[2022/12/29 00:21] | TRAIN(023): [600/879] Batch: 0.1715 (0.1728) Data: 0.0175 (0.0141) Loss: 0.7011 (0.8613)
[2022/12/29 00:21] | TRAIN(023): [650/879] Batch: 0.1820 (0.1726) Data: 0.0130 (0.0139) Loss: 0.9884 (0.8631)
[2022/12/29 00:21] | TRAIN(023): [700/879] Batch: 0.1654 (0.1724) Data: 0.0104 (0.0137) Loss: 1.1448 (0.8658)
[2022/12/29 00:21] | TRAIN(023): [750/879] Batch: 0.1713 (0.1723) Data: 0.0125 (0.0135) Loss: 1.0137 (0.8654)
[2022/12/29 00:21] | TRAIN(023): [800/879] Batch: 0.1723 (0.1721) Data: 0.0104 (0.0133) Loss: 1.2028 (0.8650)
[2022/12/29 00:21] | TRAIN(023): [850/879] Batch: 0.1714 (0.1720) Data: 0.0103 (0.0132) Loss: 0.9199 (0.8676)
[2022/12/29 00:21] | ------------------------------------------------------------
[2022/12/29 00:21] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:21] | ------------------------------------------------------------
[2022/12/29 00:21] |    TRAIN(23)     0:02:31     0:00:11     0:02:19      0.8696
[2022/12/29 00:21] | ------------------------------------------------------------
[2022/12/29 00:22] | VALID(023): [ 50/220] Batch: 0.0545 (0.0815) Data: 0.0316 (0.0584) Loss: 0.7972 (0.8521)
[2022/12/29 00:22] | VALID(023): [100/220] Batch: 0.0471 (0.0689) Data: 0.0334 (0.0459) Loss: 1.0939 (0.8718)
[2022/12/29 00:22] | VALID(023): [150/220] Batch: 0.0637 (0.0646) Data: 0.0295 (0.0413) Loss: 0.7728 (0.8648)
[2022/12/29 00:22] | VALID(023): [200/220] Batch: 0.0495 (0.0625) Data: 0.0376 (0.0390) Loss: 0.5546 (0.8716)
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] |    VALID(23)      0.8716      0.7347      0.4998      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:22] | ####################################################################################################
[2022/12/29 00:22] | TRAIN(024): [ 50/879] Batch: 0.1764 (0.2085) Data: 0.0086 (0.0468) Loss: 0.9105 (0.8684)
[2022/12/29 00:22] | TRAIN(024): [100/879] Batch: 0.1712 (0.1892) Data: 0.0104 (0.0293) Loss: 0.6955 (0.8758)
[2022/12/29 00:22] | TRAIN(024): [150/879] Batch: 0.1636 (0.1842) Data: 0.0179 (0.0236) Loss: 1.0135 (0.8798)
[2022/12/29 00:22] | TRAIN(024): [200/879] Batch: 0.1756 (0.1812) Data: 0.0130 (0.0205) Loss: 0.8611 (0.8698)
[2022/12/29 00:22] | TRAIN(024): [250/879] Batch: 0.1789 (0.1789) Data: 0.0107 (0.0186) Loss: 0.8955 (0.8600)
[2022/12/29 00:23] | TRAIN(024): [300/879] Batch: 0.1627 (0.1774) Data: 0.0121 (0.0174) Loss: 0.6971 (0.8628)
[2022/12/29 00:23] | TRAIN(024): [350/879] Batch: 0.1830 (0.1765) Data: 0.0125 (0.0165) Loss: 0.8100 (0.8714)
[2022/12/29 00:23] | TRAIN(024): [400/879] Batch: 0.1655 (0.1756) Data: 0.0117 (0.0159) Loss: 0.7310 (0.8700)
[2022/12/29 00:23] | TRAIN(024): [450/879] Batch: 0.1648 (0.1749) Data: 0.0097 (0.0153) Loss: 0.9237 (0.8710)
[2022/12/29 00:23] | TRAIN(024): [500/879] Batch: 0.1661 (0.1745) Data: 0.0098 (0.0149) Loss: 0.8817 (0.8729)
[2022/12/29 00:23] | TRAIN(024): [550/879] Batch: 0.1632 (0.1740) Data: 0.0158 (0.0146) Loss: 1.1268 (0.8723)
[2022/12/29 00:23] | TRAIN(024): [600/879] Batch: 0.1725 (0.1739) Data: 0.0100 (0.0144) Loss: 0.8845 (0.8693)
[2022/12/29 00:24] | TRAIN(024): [650/879] Batch: 0.1650 (0.1735) Data: 0.0097 (0.0141) Loss: 0.9956 (0.8725)
[2022/12/29 00:24] | TRAIN(024): [700/879] Batch: 0.1653 (0.1734) Data: 0.0126 (0.0139) Loss: 0.8658 (0.8730)
[2022/12/29 00:24] | TRAIN(024): [750/879] Batch: 0.1864 (0.1733) Data: 0.0186 (0.0138) Loss: 0.8081 (0.8722)
[2022/12/29 00:24] | TRAIN(024): [800/879] Batch: 0.1651 (0.1732) Data: 0.0100 (0.0136) Loss: 0.7345 (0.8731)
[2022/12/29 00:24] | TRAIN(024): [850/879] Batch: 0.1739 (0.1731) Data: 0.0089 (0.0135) Loss: 0.9079 (0.8715)
[2022/12/29 00:24] | ------------------------------------------------------------
[2022/12/29 00:24] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:24] | ------------------------------------------------------------
[2022/12/29 00:24] |    TRAIN(24)     0:02:32     0:00:11     0:02:20      0.8694
[2022/12/29 00:24] | ------------------------------------------------------------
[2022/12/29 00:24] | VALID(024): [ 50/220] Batch: 0.0549 (0.0801) Data: 0.0315 (0.0554) Loss: 0.7760 (0.8474)
[2022/12/29 00:24] | VALID(024): [100/220] Batch: 0.0530 (0.0680) Data: 0.0333 (0.0439) Loss: 1.1314 (0.8709)
[2022/12/29 00:24] | VALID(024): [150/220] Batch: 0.0543 (0.0640) Data: 0.0299 (0.0402) Loss: 0.7513 (0.8624)
[2022/12/29 00:24] | VALID(024): [200/220] Batch: 0.0571 (0.0619) Data: 0.0367 (0.0385) Loss: 0.5141 (0.8702)
[2022/12/29 00:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:24] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:24] |    VALID(24)      0.8701      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:24] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:24] | ####################################################################################################
[2022/12/29 00:25] | TRAIN(025): [ 50/879] Batch: 0.1659 (0.2063) Data: 0.0113 (0.0481) Loss: 0.8869 (0.8481)
[2022/12/29 00:25] | TRAIN(025): [100/879] Batch: 0.1695 (0.1880) Data: 0.0111 (0.0297) Loss: 0.8699 (0.8431)
[2022/12/29 00:25] | TRAIN(025): [150/879] Batch: 0.1722 (0.1823) Data: 0.0096 (0.0235) Loss: 0.7049 (0.8557)
[2022/12/29 00:25] | TRAIN(025): [200/879] Batch: 0.1791 (0.1796) Data: 0.0119 (0.0205) Loss: 1.2868 (0.8614)
[2022/12/29 00:25] | TRAIN(025): [250/879] Batch: 0.1762 (0.1776) Data: 0.0102 (0.0186) Loss: 0.7338 (0.8690)
[2022/12/29 00:25] | TRAIN(025): [300/879] Batch: 0.1686 (0.1762) Data: 0.0087 (0.0173) Loss: 0.9939 (0.8682)
[2022/12/29 00:26] | TRAIN(025): [350/879] Batch: 0.1674 (0.1752) Data: 0.0098 (0.0164) Loss: 0.8782 (0.8681)
[2022/12/29 00:26] | TRAIN(025): [400/879] Batch: 0.1728 (0.1750) Data: 0.0107 (0.0158) Loss: 1.2272 (0.8684)
[2022/12/29 00:26] | TRAIN(025): [450/879] Batch: 0.1825 (0.1745) Data: 0.0132 (0.0153) Loss: 0.8065 (0.8630)
[2022/12/29 00:26] | TRAIN(025): [500/879] Batch: 0.1635 (0.1744) Data: 0.0086 (0.0148) Loss: 0.7984 (0.8650)
[2022/12/29 00:26] | TRAIN(025): [550/879] Batch: 0.1678 (0.1743) Data: 0.0076 (0.0144) Loss: 1.2891 (0.8684)
[2022/12/29 00:26] | TRAIN(025): [600/879] Batch: 0.1625 (0.1739) Data: 0.0080 (0.0139) Loss: 0.7964 (0.8715)
[2022/12/29 00:26] | TRAIN(025): [650/879] Batch: 0.1821 (0.1740) Data: 0.0167 (0.0136) Loss: 0.8714 (0.8722)
[2022/12/29 00:27] | TRAIN(025): [700/879] Batch: 0.1889 (0.1738) Data: 0.0081 (0.0133) Loss: 0.6518 (0.8705)
[2022/12/29 00:27] | TRAIN(025): [750/879] Batch: 0.1661 (0.1738) Data: 0.0077 (0.0130) Loss: 1.2479 (0.8713)
[2022/12/29 00:27] | TRAIN(025): [800/879] Batch: 0.1631 (0.1737) Data: 0.0077 (0.0128) Loss: 0.7601 (0.8686)
[2022/12/29 00:27] | TRAIN(025): [850/879] Batch: 0.1690 (0.1738) Data: 0.0077 (0.0126) Loss: 1.0219 (0.8695)
[2022/12/29 00:27] | ------------------------------------------------------------
[2022/12/29 00:27] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:27] | ------------------------------------------------------------
[2022/12/29 00:27] |    TRAIN(25)     0:02:32     0:00:10     0:02:21      0.8698
[2022/12/29 00:27] | ------------------------------------------------------------
[2022/12/29 00:27] | VALID(025): [ 50/220] Batch: 0.0540 (0.0830) Data: 0.0250 (0.0501) Loss: 0.7788 (0.8470)
[2022/12/29 00:27] | VALID(025): [100/220] Batch: 0.0619 (0.0693) Data: 0.0207 (0.0388) Loss: 1.1034 (0.8695)
[2022/12/29 00:27] | VALID(025): [150/220] Batch: 0.0569 (0.0646) Data: 0.0351 (0.0371) Loss: 0.7609 (0.8616)
[2022/12/29 00:27] | VALID(025): [200/220] Batch: 0.0558 (0.0623) Data: 0.0336 (0.0361) Loss: 0.5223 (0.8692)
[2022/12/29 00:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:27] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:27] |    VALID(25)      0.8688      0.7347      0.5065      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:27] | ####################################################################################################
[2022/12/29 00:27] | TRAIN(026): [ 50/879] Batch: 0.1980 (0.2136) Data: 0.0090 (0.0454) Loss: 0.8318 (0.8731)
[2022/12/29 00:28] | TRAIN(026): [100/879] Batch: 0.1910 (0.1958) Data: 0.0178 (0.0286) Loss: 0.7131 (0.8729)
[2022/12/29 00:28] | TRAIN(026): [150/879] Batch: 0.1679 (0.1888) Data: 0.0080 (0.0226) Loss: 0.7843 (0.8667)
[2022/12/29 00:28] | TRAIN(026): [200/879] Batch: 0.1710 (0.1844) Data: 0.0077 (0.0193) Loss: 0.7305 (0.8740)
[2022/12/29 00:28] | TRAIN(026): [250/879] Batch: 0.1828 (0.1817) Data: 0.0116 (0.0175) Loss: 0.8994 (0.8618)
[2022/12/29 00:28] | TRAIN(026): [300/879] Batch: 0.1765 (0.1803) Data: 0.0117 (0.0162) Loss: 0.7699 (0.8615)
[2022/12/29 00:28] | TRAIN(026): [350/879] Batch: 0.1946 (0.1789) Data: 0.0094 (0.0151) Loss: 0.8063 (0.8586)
[2022/12/29 00:28] | TRAIN(026): [400/879] Batch: 0.1862 (0.1781) Data: 0.0088 (0.0143) Loss: 1.1074 (0.8642)
[2022/12/29 00:29] | TRAIN(026): [450/879] Batch: 0.1631 (0.1772) Data: 0.0084 (0.0137) Loss: 0.7909 (0.8641)
[2022/12/29 00:29] | TRAIN(026): [500/879] Batch: 0.1625 (0.1764) Data: 0.0077 (0.0131) Loss: 0.9912 (0.8642)
[2022/12/29 00:29] | TRAIN(026): [550/879] Batch: 0.1653 (0.1757) Data: 0.0087 (0.0127) Loss: 0.7701 (0.8682)
[2022/12/29 00:29] | TRAIN(026): [600/879] Batch: 0.1645 (0.1750) Data: 0.0071 (0.0123) Loss: 0.6250 (0.8659)
[2022/12/29 00:29] | TRAIN(026): [650/879] Batch: 0.1638 (0.1745) Data: 0.0127 (0.0120) Loss: 1.2422 (0.8682)
[2022/12/29 00:29] | TRAIN(026): [700/879] Batch: 0.1663 (0.1742) Data: 0.0071 (0.0117) Loss: 0.9157 (0.8677)
[2022/12/29 00:29] | TRAIN(026): [750/879] Batch: 0.1625 (0.1740) Data: 0.0076 (0.0115) Loss: 1.2202 (0.8680)
[2022/12/29 00:30] | TRAIN(026): [800/879] Batch: 0.1619 (0.1736) Data: 0.0074 (0.0113) Loss: 0.5759 (0.8691)
[2022/12/29 00:30] | TRAIN(026): [850/879] Batch: 0.1642 (0.1732) Data: 0.0075 (0.0111) Loss: 0.7404 (0.8669)
[2022/12/29 00:30] | ------------------------------------------------------------
[2022/12/29 00:30] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:30] | ------------------------------------------------------------
[2022/12/29 00:30] |    TRAIN(26)     0:02:31     0:00:09     0:02:22      0.8699
[2022/12/29 00:30] | ------------------------------------------------------------
[2022/12/29 00:30] | VALID(026): [ 50/220] Batch: 0.0541 (0.0833) Data: 0.0319 (0.0599) Loss: 0.7838 (0.8468)
[2022/12/29 00:30] | VALID(026): [100/220] Batch: 0.0553 (0.0693) Data: 0.0344 (0.0467) Loss: 1.1039 (0.8695)
[2022/12/29 00:30] | VALID(026): [150/220] Batch: 0.0566 (0.0648) Data: 0.0272 (0.0411) Loss: 0.7611 (0.8616)
[2022/12/29 00:30] | VALID(026): [200/220] Batch: 0.0536 (0.0624) Data: 0.0308 (0.0376) Loss: 0.5055 (0.8693)
[2022/12/29 00:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:30] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:30] |    VALID(26)      0.8689      0.7347      0.5009      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:30] | ####################################################################################################
[2022/12/29 00:30] | TRAIN(027): [ 50/879] Batch: 0.1631 (0.2094) Data: 0.0072 (0.0448) Loss: 0.8803 (0.9061)
[2022/12/29 00:30] | TRAIN(027): [100/879] Batch: 0.1829 (0.1913) Data: 0.0078 (0.0280) Loss: 0.8051 (0.8882)
[2022/12/29 00:30] | TRAIN(027): [150/879] Batch: 0.1604 (0.1853) Data: 0.0071 (0.0218) Loss: 1.1041 (0.8919)
[2022/12/29 00:31] | TRAIN(027): [200/879] Batch: 0.1645 (0.1832) Data: 0.0079 (0.0187) Loss: 0.9112 (0.8905)
[2022/12/29 00:31] | TRAIN(027): [250/879] Batch: 0.1902 (0.1810) Data: 0.0180 (0.0169) Loss: 1.0597 (0.8935)
[2022/12/29 00:31] | TRAIN(027): [300/879] Batch: 0.1880 (0.1795) Data: 0.0074 (0.0158) Loss: 0.7651 (0.8903)
[2022/12/29 00:31] | TRAIN(027): [350/879] Batch: 0.1978 (0.1784) Data: 0.0098 (0.0148) Loss: 0.7558 (0.8934)
[2022/12/29 00:31] | TRAIN(027): [400/879] Batch: 0.1749 (0.1772) Data: 0.0076 (0.0141) Loss: 0.7442 (0.8907)
[2022/12/29 00:31] | TRAIN(027): [450/879] Batch: 0.1711 (0.1765) Data: 0.0074 (0.0136) Loss: 1.1126 (0.8884)
[2022/12/29 00:31] | TRAIN(027): [500/879] Batch: 0.1677 (0.1757) Data: 0.0073 (0.0131) Loss: 0.8396 (0.8849)
[2022/12/29 00:32] | TRAIN(027): [550/879] Batch: 0.1750 (0.1748) Data: 0.0073 (0.0127) Loss: 0.7842 (0.8778)
[2022/12/29 00:32] | TRAIN(027): [600/879] Batch: 0.1616 (0.1740) Data: 0.0075 (0.0123) Loss: 1.1816 (0.8773)
[2022/12/29 00:32] | TRAIN(027): [650/879] Batch: 0.1640 (0.1733) Data: 0.0071 (0.0120) Loss: 0.5869 (0.8731)
[2022/12/29 00:32] | TRAIN(027): [700/879] Batch: 0.1627 (0.1727) Data: 0.0072 (0.0117) Loss: 0.6534 (0.8704)
[2022/12/29 00:32] | TRAIN(027): [750/879] Batch: 0.1688 (0.1723) Data: 0.0071 (0.0115) Loss: 0.7140 (0.8691)
[2022/12/29 00:32] | TRAIN(027): [800/879] Batch: 0.1794 (0.1721) Data: 0.0096 (0.0113) Loss: 0.9839 (0.8678)
[2022/12/29 00:32] | TRAIN(027): [850/879] Batch: 0.1625 (0.1719) Data: 0.0076 (0.0111) Loss: 0.8781 (0.8686)
[2022/12/29 00:33] | ------------------------------------------------------------
[2022/12/29 00:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:33] | ------------------------------------------------------------
[2022/12/29 00:33] |    TRAIN(27)     0:02:30     0:00:09     0:02:21      0.8694
[2022/12/29 00:33] | ------------------------------------------------------------
[2022/12/29 00:33] | VALID(027): [ 50/220] Batch: 0.0535 (0.0827) Data: 0.0358 (0.0597) Loss: 0.7889 (0.8473)
[2022/12/29 00:33] | VALID(027): [100/220] Batch: 0.0540 (0.0688) Data: 0.0317 (0.0465) Loss: 1.1084 (0.8690)
[2022/12/29 00:33] | VALID(027): [150/220] Batch: 0.0560 (0.0642) Data: 0.0329 (0.0421) Loss: 0.7605 (0.8612)
[2022/12/29 00:33] | VALID(027): [200/220] Batch: 0.0573 (0.0620) Data: 0.0290 (0.0384) Loss: 0.5190 (0.8686)
[2022/12/29 00:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:33] |    VALID(27)      0.8685      0.7347      0.5079      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:33] | ####################################################################################################
[2022/12/29 00:33] | TRAIN(028): [ 50/879] Batch: 0.1691 (0.2081) Data: 0.0074 (0.0474) Loss: 0.7070 (0.8514)
[2022/12/29 00:33] | TRAIN(028): [100/879] Batch: 0.1633 (0.1881) Data: 0.0074 (0.0281) Loss: 0.8791 (0.8663)
[2022/12/29 00:33] | TRAIN(028): [150/879] Batch: 0.1594 (0.1808) Data: 0.0072 (0.0215) Loss: 0.7494 (0.8685)
[2022/12/29 00:33] | TRAIN(028): [200/879] Batch: 0.1674 (0.1769) Data: 0.0076 (0.0181) Loss: 0.7353 (0.8611)
[2022/12/29 00:34] | TRAIN(028): [250/879] Batch: 0.1728 (0.1747) Data: 0.0075 (0.0160) Loss: 0.9143 (0.8633)
[2022/12/29 00:34] | TRAIN(028): [300/879] Batch: 0.1723 (0.1731) Data: 0.0089 (0.0147) Loss: 1.0624 (0.8685)
[2022/12/29 00:34] | TRAIN(028): [350/879] Batch: 0.1612 (0.1720) Data: 0.0077 (0.0137) Loss: 0.5418 (0.8641)
[2022/12/29 00:34] | TRAIN(028): [400/879] Batch: 0.1666 (0.1715) Data: 0.0075 (0.0131) Loss: 0.8313 (0.8643)
[2022/12/29 00:34] | TRAIN(028): [450/879] Batch: 0.1732 (0.1711) Data: 0.0072 (0.0125) Loss: 0.9921 (0.8669)
[2022/12/29 00:34] | TRAIN(028): [500/879] Batch: 0.1664 (0.1709) Data: 0.0079 (0.0121) Loss: 0.7195 (0.8666)
[2022/12/29 00:34] | TRAIN(028): [550/879] Batch: 0.1664 (0.1704) Data: 0.0076 (0.0117) Loss: 0.8986 (0.8652)
[2022/12/29 00:34] | TRAIN(028): [600/879] Batch: 0.1620 (0.1701) Data: 0.0076 (0.0114) Loss: 0.6855 (0.8667)
[2022/12/29 00:35] | TRAIN(028): [650/879] Batch: 0.1726 (0.1698) Data: 0.0074 (0.0111) Loss: 0.7433 (0.8678)
[2022/12/29 00:35] | TRAIN(028): [700/879] Batch: 0.1619 (0.1696) Data: 0.0094 (0.0109) Loss: 0.8342 (0.8678)
[2022/12/29 00:35] | TRAIN(028): [750/879] Batch: 0.1615 (0.1695) Data: 0.0085 (0.0107) Loss: 1.1198 (0.8687)
[2022/12/29 00:35] | TRAIN(028): [800/879] Batch: 0.1652 (0.1694) Data: 0.0091 (0.0105) Loss: 0.8644 (0.8669)
[2022/12/29 00:35] | TRAIN(028): [850/879] Batch: 0.1621 (0.1692) Data: 0.0077 (0.0104) Loss: 0.7793 (0.8691)
[2022/12/29 00:35] | ------------------------------------------------------------
[2022/12/29 00:35] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:35] | ------------------------------------------------------------
[2022/12/29 00:35] |    TRAIN(28)     0:02:28     0:00:09     0:02:19      0.8693
[2022/12/29 00:35] | ------------------------------------------------------------
[2022/12/29 00:35] | VALID(028): [ 50/220] Batch: 0.0532 (0.0845) Data: 0.0382 (0.0616) Loss: 0.7831 (0.8468)
[2022/12/29 00:35] | VALID(028): [100/220] Batch: 0.0558 (0.0699) Data: 0.0330 (0.0476) Loss: 1.1046 (0.8688)
[2022/12/29 00:35] | VALID(028): [150/220] Batch: 0.0572 (0.0651) Data: 0.0338 (0.0431) Loss: 0.7604 (0.8610)
[2022/12/29 00:35] | VALID(028): [200/220] Batch: 0.0531 (0.0626) Data: 0.0364 (0.0408) Loss: 0.5248 (0.8684)
[2022/12/29 00:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:35] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:35] |    VALID(28)      0.8681      0.7347      0.4998      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:35] | ####################################################################################################
[2022/12/29 00:36] | TRAIN(029): [ 50/879] Batch: 0.1899 (0.2085) Data: 0.0144 (0.0487) Loss: 1.0965 (0.8818)
[2022/12/29 00:36] | TRAIN(029): [100/879] Batch: 0.1618 (0.1915) Data: 0.0077 (0.0297) Loss: 1.0781 (0.8868)
[2022/12/29 00:36] | TRAIN(029): [150/879] Batch: 0.1765 (0.1843) Data: 0.0095 (0.0233) Loss: 1.0332 (0.8843)
[2022/12/29 00:36] | TRAIN(029): [200/879] Batch: 0.1806 (0.1823) Data: 0.0096 (0.0200) Loss: 0.8380 (0.8814)
[2022/12/29 00:36] | TRAIN(029): [250/879] Batch: 0.1656 (0.1804) Data: 0.0092 (0.0182) Loss: 0.7331 (0.8693)
[2022/12/29 00:36] | TRAIN(029): [300/879] Batch: 0.1776 (0.1791) Data: 0.0078 (0.0168) Loss: 0.9507 (0.8701)
[2022/12/29 00:37] | TRAIN(029): [350/879] Batch: 0.1838 (0.1780) Data: 0.0075 (0.0158) Loss: 0.8743 (0.8711)
[2022/12/29 00:37] | TRAIN(029): [400/879] Batch: 0.1822 (0.1771) Data: 0.0074 (0.0151) Loss: 0.9687 (0.8684)
[2022/12/29 00:37] | TRAIN(029): [450/879] Batch: 0.1839 (0.1766) Data: 0.0095 (0.0145) Loss: 0.8372 (0.8693)
[2022/12/29 00:37] | TRAIN(029): [500/879] Batch: 0.1647 (0.1760) Data: 0.0074 (0.0139) Loss: 0.9134 (0.8708)
[2022/12/29 00:37] | TRAIN(029): [550/879] Batch: 0.1756 (0.1752) Data: 0.0105 (0.0134) Loss: 0.9493 (0.8680)
[2022/12/29 00:37] | TRAIN(029): [600/879] Batch: 0.1704 (0.1749) Data: 0.0100 (0.0131) Loss: 0.8905 (0.8655)
[2022/12/29 00:37] | TRAIN(029): [650/879] Batch: 0.1687 (0.1743) Data: 0.0076 (0.0127) Loss: 0.7602 (0.8672)
[2022/12/29 00:38] | TRAIN(029): [700/879] Batch: 0.1680 (0.1738) Data: 0.0074 (0.0124) Loss: 1.0344 (0.8704)
[2022/12/29 00:38] | TRAIN(029): [750/879] Batch: 0.1648 (0.1734) Data: 0.0076 (0.0121) Loss: 0.8754 (0.8718)
[2022/12/29 00:38] | TRAIN(029): [800/879] Batch: 0.1686 (0.1730) Data: 0.0088 (0.0119) Loss: 1.0856 (0.8706)
[2022/12/29 00:38] | TRAIN(029): [850/879] Batch: 0.1650 (0.1726) Data: 0.0072 (0.0116) Loss: 0.8857 (0.8690)
[2022/12/29 00:38] | ------------------------------------------------------------
[2022/12/29 00:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:38] | ------------------------------------------------------------
[2022/12/29 00:38] |    TRAIN(29)     0:02:31     0:00:10     0:02:21      0.8692
[2022/12/29 00:38] | ------------------------------------------------------------
[2022/12/29 00:38] | VALID(029): [ 50/220] Batch: 0.0561 (0.0819) Data: 0.0361 (0.0595) Loss: 0.7816 (0.8468)
[2022/12/29 00:38] | VALID(029): [100/220] Batch: 0.0545 (0.0688) Data: 0.0362 (0.0460) Loss: 1.1029 (0.8689)
[2022/12/29 00:38] | VALID(029): [150/220] Batch: 0.0523 (0.0644) Data: 0.0375 (0.0420) Loss: 0.7613 (0.8611)
[2022/12/29 00:38] | VALID(029): [200/220] Batch: 0.0537 (0.0622) Data: 0.0359 (0.0400) Loss: 0.5249 (0.8686)
[2022/12/29 00:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:38] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:38] |    VALID(29)      0.8683      0.7347      0.4937      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:38] | ####################################################################################################
[2022/12/29 00:38] | TRAIN(030): [ 50/879] Batch: 0.1733 (0.2096) Data: 0.0183 (0.0483) Loss: 0.7252 (0.8646)
[2022/12/29 00:39] | TRAIN(030): [100/879] Batch: 0.1804 (0.1920) Data: 0.0107 (0.0303) Loss: 0.6252 (0.8692)
[2022/12/29 00:39] | TRAIN(030): [150/879] Batch: 0.1768 (0.1854) Data: 0.0080 (0.0238) Loss: 0.7555 (0.8739)
[2022/12/29 00:39] | TRAIN(030): [200/879] Batch: 0.1654 (0.1814) Data: 0.0078 (0.0201) Loss: 0.6276 (0.8750)
[2022/12/29 00:39] | TRAIN(030): [250/879] Batch: 0.1763 (0.1791) Data: 0.0079 (0.0180) Loss: 0.8505 (0.8668)
[2022/12/29 00:39] | TRAIN(030): [300/879] Batch: 0.1782 (0.1779) Data: 0.0097 (0.0165) Loss: 0.9941 (0.8674)
[2022/12/29 00:39] | TRAIN(030): [350/879] Batch: 0.1897 (0.1770) Data: 0.0076 (0.0155) Loss: 0.8136 (0.8693)
[2022/12/29 00:39] | TRAIN(030): [400/879] Batch: 0.1820 (0.1767) Data: 0.0104 (0.0148) Loss: 0.7688 (0.8663)
[2022/12/29 00:40] | TRAIN(030): [450/879] Batch: 0.1767 (0.1762) Data: 0.0074 (0.0142) Loss: 0.7708 (0.8654)
[2022/12/29 00:40] | TRAIN(030): [500/879] Batch: 0.1749 (0.1754) Data: 0.0075 (0.0137) Loss: 1.0281 (0.8620)
[2022/12/29 00:40] | TRAIN(030): [550/879] Batch: 0.1756 (0.1755) Data: 0.0078 (0.0133) Loss: 0.6838 (0.8661)
[2022/12/29 00:40] | TRAIN(030): [600/879] Batch: 0.1792 (0.1752) Data: 0.0113 (0.0130) Loss: 0.9031 (0.8672)
[2022/12/29 00:40] | TRAIN(030): [650/879] Batch: 0.1602 (0.1747) Data: 0.0075 (0.0127) Loss: 0.5128 (0.8707)
[2022/12/29 00:40] | TRAIN(030): [700/879] Batch: 0.1754 (0.1745) Data: 0.0077 (0.0125) Loss: 0.8114 (0.8714)
[2022/12/29 00:40] | TRAIN(030): [750/879] Batch: 0.1810 (0.1745) Data: 0.0098 (0.0123) Loss: 1.0556 (0.8704)
[2022/12/29 00:41] | TRAIN(030): [800/879] Batch: 0.1703 (0.1743) Data: 0.0075 (0.0121) Loss: 1.0733 (0.8722)
[2022/12/29 00:41] | TRAIN(030): [850/879] Batch: 0.1769 (0.1740) Data: 0.0075 (0.0119) Loss: 0.7468 (0.8707)
[2022/12/29 00:41] | ------------------------------------------------------------
[2022/12/29 00:41] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:41] | ------------------------------------------------------------
[2022/12/29 00:41] |    TRAIN(30)     0:02:32     0:00:10     0:02:22      0.8692
[2022/12/29 00:41] | ------------------------------------------------------------
[2022/12/29 00:41] | VALID(030): [ 50/220] Batch: 0.0563 (0.0838) Data: 0.0336 (0.0612) Loss: 0.7710 (0.8473)
[2022/12/29 00:41] | VALID(030): [100/220] Batch: 0.0562 (0.0697) Data: 0.0328 (0.0476) Loss: 1.1222 (0.8718)
[2022/12/29 00:41] | VALID(030): [150/220] Batch: 0.0534 (0.0649) Data: 0.0368 (0.0431) Loss: 0.7523 (0.8629)
[2022/12/29 00:41] | VALID(030): [200/220] Batch: 0.0559 (0.0626) Data: 0.0361 (0.0408) Loss: 0.5063 (0.8710)
[2022/12/29 00:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:41] |    VALID(30)      0.8705      0.7347      0.4953      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:41] | ####################################################################################################
[2022/12/29 00:41] | TRAIN(031): [ 50/879] Batch: 0.1725 (0.2079) Data: 0.0072 (0.0443) Loss: 0.8268 (0.8740)
[2022/12/29 00:41] | TRAIN(031): [100/879] Batch: 0.1752 (0.1919) Data: 0.0113 (0.0278) Loss: 1.0269 (0.8802)
[2022/12/29 00:41] | TRAIN(031): [150/879] Batch: 0.1710 (0.1844) Data: 0.0080 (0.0217) Loss: 0.9255 (0.8733)
[2022/12/29 00:42] | TRAIN(031): [200/879] Batch: 0.1612 (0.1811) Data: 0.0075 (0.0187) Loss: 0.6560 (0.8699)
[2022/12/29 00:42] | TRAIN(031): [250/879] Batch: 0.1705 (0.1795) Data: 0.0125 (0.0171) Loss: 0.7749 (0.8634)
[2022/12/29 00:42] | TRAIN(031): [300/879] Batch: 0.1724 (0.1777) Data: 0.0172 (0.0157) Loss: 0.7860 (0.8659)
[2022/12/29 00:42] | TRAIN(031): [350/879] Batch: 0.1753 (0.1774) Data: 0.0088 (0.0148) Loss: 0.7430 (0.8625)
[2022/12/29 00:42] | TRAIN(031): [400/879] Batch: 0.1720 (0.1766) Data: 0.0105 (0.0142) Loss: 1.0141 (0.8623)
[2022/12/29 00:42] | TRAIN(031): [450/879] Batch: 0.1686 (0.1759) Data: 0.0075 (0.0137) Loss: 0.8338 (0.8634)
[2022/12/29 00:42] | TRAIN(031): [500/879] Batch: 0.1786 (0.1752) Data: 0.0108 (0.0133) Loss: 1.2507 (0.8648)
[2022/12/29 00:43] | TRAIN(031): [550/879] Batch: 0.1792 (0.1748) Data: 0.0079 (0.0129) Loss: 1.1140 (0.8684)
[2022/12/29 00:43] | TRAIN(031): [600/879] Batch: 0.1635 (0.1742) Data: 0.0077 (0.0125) Loss: 0.7843 (0.8678)
[2022/12/29 00:43] | TRAIN(031): [650/879] Batch: 0.1814 (0.1738) Data: 0.0073 (0.0122) Loss: 0.9247 (0.8670)
[2022/12/29 00:43] | TRAIN(031): [700/879] Batch: 0.1689 (0.1738) Data: 0.0079 (0.0119) Loss: 0.6916 (0.8676)
[2022/12/29 00:43] | TRAIN(031): [750/879] Batch: 0.1829 (0.1734) Data: 0.0079 (0.0117) Loss: 0.6123 (0.8683)
[2022/12/29 00:43] | TRAIN(031): [800/879] Batch: 0.1662 (0.1737) Data: 0.0073 (0.0115) Loss: 0.7751 (0.8695)
[2022/12/29 00:43] | TRAIN(031): [850/879] Batch: 0.1818 (0.1735) Data: 0.0096 (0.0113) Loss: 0.8029 (0.8676)
[2022/12/29 00:44] | ------------------------------------------------------------
[2022/12/29 00:44] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:44] | ------------------------------------------------------------
[2022/12/29 00:44] |    TRAIN(31)     0:02:32     0:00:09     0:02:22      0.8693
[2022/12/29 00:44] | ------------------------------------------------------------
[2022/12/29 00:44] | VALID(031): [ 50/220] Batch: 0.0579 (0.0834) Data: 0.0333 (0.0597) Loss: 0.7762 (0.8468)
[2022/12/29 00:44] | VALID(031): [100/220] Batch: 0.0564 (0.0695) Data: 0.0343 (0.0466) Loss: 1.1105 (0.8703)
[2022/12/29 00:44] | VALID(031): [150/220] Batch: 0.0569 (0.0649) Data: 0.0352 (0.0424) Loss: 0.7583 (0.8621)
[2022/12/29 00:44] | VALID(031): [200/220] Batch: 0.0540 (0.0626) Data: 0.0360 (0.0402) Loss: 0.5087 (0.8700)
[2022/12/29 00:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:44] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:44] |    VALID(31)      0.8695      0.7347      0.5108      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:44] | ####################################################################################################
[2022/12/29 00:44] | TRAIN(032): [ 50/879] Batch: 0.1731 (0.2058) Data: 0.0102 (0.0458) Loss: 0.7926 (0.8813)
[2022/12/29 00:44] | TRAIN(032): [100/879] Batch: 0.1832 (0.1897) Data: 0.0078 (0.0281) Loss: 0.8857 (0.8735)
[2022/12/29 00:44] | TRAIN(032): [150/879] Batch: 0.1895 (0.1830) Data: 0.0080 (0.0217) Loss: 0.8102 (0.8745)
[2022/12/29 00:44] | TRAIN(032): [200/879] Batch: 0.1641 (0.1795) Data: 0.0074 (0.0185) Loss: 0.6819 (0.8720)
[2022/12/29 00:45] | TRAIN(032): [250/879] Batch: 0.1617 (0.1775) Data: 0.0075 (0.0167) Loss: 0.8644 (0.8711)
[2022/12/29 00:45] | TRAIN(032): [300/879] Batch: 0.1640 (0.1762) Data: 0.0160 (0.0155) Loss: 0.8061 (0.8719)
[2022/12/29 00:45] | TRAIN(032): [350/879] Batch: 0.1614 (0.1756) Data: 0.0076 (0.0146) Loss: 0.9390 (0.8704)
[2022/12/29 00:45] | TRAIN(032): [400/879] Batch: 0.1664 (0.1747) Data: 0.0076 (0.0139) Loss: 0.6802 (0.8662)
[2022/12/29 00:45] | TRAIN(032): [450/879] Batch: 0.1694 (0.1742) Data: 0.0076 (0.0134) Loss: 0.7895 (0.8671)
[2022/12/29 00:45] | TRAIN(032): [500/879] Batch: 0.1637 (0.1736) Data: 0.0074 (0.0130) Loss: 0.4652 (0.8650)
[2022/12/29 00:45] | TRAIN(032): [550/879] Batch: 0.1658 (0.1733) Data: 0.0072 (0.0127) Loss: 0.8169 (0.8662)
[2022/12/29 00:46] | TRAIN(032): [600/879] Batch: 0.1673 (0.1733) Data: 0.0079 (0.0124) Loss: 1.1655 (0.8662)
[2022/12/29 00:46] | TRAIN(032): [650/879] Batch: 0.1732 (0.1735) Data: 0.0071 (0.0122) Loss: 0.6024 (0.8659)
[2022/12/29 00:46] | TRAIN(032): [700/879] Batch: 0.1734 (0.1733) Data: 0.0170 (0.0120) Loss: 0.7600 (0.8666)
[2022/12/29 00:46] | TRAIN(032): [750/879] Batch: 0.1809 (0.1731) Data: 0.0100 (0.0119) Loss: 0.6976 (0.8687)
[2022/12/29 00:46] | TRAIN(032): [800/879] Batch: 0.1647 (0.1730) Data: 0.0075 (0.0117) Loss: 0.7523 (0.8688)
[2022/12/29 00:46] | TRAIN(032): [850/879] Batch: 0.1637 (0.1732) Data: 0.0076 (0.0116) Loss: 0.8903 (0.8685)
[2022/12/29 00:46] | ------------------------------------------------------------
[2022/12/29 00:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:46] | ------------------------------------------------------------
[2022/12/29 00:46] |    TRAIN(32)     0:02:32     0:00:10     0:02:21      0.8691
[2022/12/29 00:46] | ------------------------------------------------------------
[2022/12/29 00:46] | VALID(032): [ 50/220] Batch: 0.0569 (0.0824) Data: 0.0367 (0.0590) Loss: 0.7938 (0.8501)
[2022/12/29 00:46] | VALID(032): [100/220] Batch: 0.0572 (0.0688) Data: 0.0348 (0.0463) Loss: 1.0991 (0.8702)
[2022/12/29 00:46] | VALID(032): [150/220] Batch: 0.0529 (0.0643) Data: 0.0328 (0.0419) Loss: 0.7690 (0.8631)
[2022/12/29 00:47] | VALID(032): [200/220] Batch: 0.0564 (0.0621) Data: 0.0303 (0.0395) Loss: 0.5467 (0.8700)
[2022/12/29 00:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:47] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:47] |    VALID(32)      0.8700      0.7347      0.5079      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:47] | ####################################################################################################
[2022/12/29 00:47] | TRAIN(033): [ 50/879] Batch: 0.1670 (0.2119) Data: 0.0095 (0.0472) Loss: 0.6954 (0.8521)
[2022/12/29 00:47] | TRAIN(033): [100/879] Batch: 0.1719 (0.1918) Data: 0.0082 (0.0287) Loss: 0.7352 (0.8632)
[2022/12/29 00:47] | TRAIN(033): [150/879] Batch: 0.1998 (0.1846) Data: 0.0097 (0.0222) Loss: 1.0176 (0.8701)
[2022/12/29 00:47] | TRAIN(033): [200/879] Batch: 0.1628 (0.1804) Data: 0.0073 (0.0189) Loss: 0.9061 (0.8730)
[2022/12/29 00:47] | TRAIN(033): [250/879] Batch: 0.1805 (0.1783) Data: 0.0097 (0.0168) Loss: 0.8635 (0.8772)
[2022/12/29 00:47] | TRAIN(033): [300/879] Batch: 0.1706 (0.1773) Data: 0.0076 (0.0154) Loss: 0.7516 (0.8750)
[2022/12/29 00:48] | TRAIN(033): [350/879] Batch: 0.1758 (0.1760) Data: 0.0081 (0.0144) Loss: 1.1224 (0.8828)
[2022/12/29 00:48] | TRAIN(033): [400/879] Batch: 0.1739 (0.1753) Data: 0.0077 (0.0137) Loss: 0.7300 (0.8816)
[2022/12/29 00:48] | TRAIN(033): [450/879] Batch: 0.1644 (0.1747) Data: 0.0072 (0.0131) Loss: 0.6756 (0.8747)
[2022/12/29 00:48] | TRAIN(033): [500/879] Batch: 0.1647 (0.1740) Data: 0.0076 (0.0126) Loss: 0.9454 (0.8709)
[2022/12/29 00:48] | TRAIN(033): [550/879] Batch: 0.1633 (0.1733) Data: 0.0076 (0.0122) Loss: 0.6973 (0.8674)
[2022/12/29 00:48] | TRAIN(033): [600/879] Batch: 0.1618 (0.1726) Data: 0.0075 (0.0118) Loss: 0.4275 (0.8666)
[2022/12/29 00:48] | TRAIN(033): [650/879] Batch: 0.1649 (0.1722) Data: 0.0082 (0.0116) Loss: 1.0129 (0.8694)
[2022/12/29 00:49] | TRAIN(033): [700/879] Batch: 0.1607 (0.1719) Data: 0.0076 (0.0113) Loss: 0.9289 (0.8709)
[2022/12/29 00:49] | TRAIN(033): [750/879] Batch: 0.1801 (0.1715) Data: 0.0095 (0.0111) Loss: 1.2246 (0.8723)
[2022/12/29 00:49] | TRAIN(033): [800/879] Batch: 0.1619 (0.1713) Data: 0.0113 (0.0109) Loss: 0.6427 (0.8702)
[2022/12/29 00:49] | TRAIN(033): [850/879] Batch: 0.1698 (0.1712) Data: 0.0090 (0.0108) Loss: 1.0136 (0.8707)
[2022/12/29 00:49] | ------------------------------------------------------------
[2022/12/29 00:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:49] | ------------------------------------------------------------
[2022/12/29 00:49] |    TRAIN(33)     0:02:30     0:00:09     0:02:20      0.8688
[2022/12/29 00:49] | ------------------------------------------------------------
[2022/12/29 00:49] | VALID(033): [ 50/220] Batch: 0.0550 (0.0812) Data: 0.0363 (0.0577) Loss: 0.7767 (0.8478)
[2022/12/29 00:49] | VALID(033): [100/220] Batch: 0.0534 (0.0683) Data: 0.0324 (0.0454) Loss: 1.1320 (0.8718)
[2022/12/29 00:49] | VALID(033): [150/220] Batch: 0.0553 (0.0640) Data: 0.0308 (0.0410) Loss: 0.7525 (0.8632)
[2022/12/29 00:49] | VALID(033): [200/220] Batch: 0.0541 (0.0618) Data: 0.0338 (0.0390) Loss: 0.4995 (0.8713)
[2022/12/29 00:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:49] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:49] |    VALID(33)      0.8711      0.7347      0.4971      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:49] | ####################################################################################################
[2022/12/29 00:49] | TRAIN(034): [ 50/879] Batch: 0.1680 (0.2126) Data: 0.0077 (0.0496) Loss: 0.7399 (0.8774)
[2022/12/29 00:50] | TRAIN(034): [100/879] Batch: 0.1641 (0.1921) Data: 0.0153 (0.0299) Loss: 0.9616 (0.8659)
[2022/12/29 00:50] | TRAIN(034): [150/879] Batch: 0.1698 (0.1853) Data: 0.0171 (0.0229) Loss: 0.7555 (0.8760)
[2022/12/29 00:50] | TRAIN(034): [200/879] Batch: 0.1664 (0.1819) Data: 0.0074 (0.0196) Loss: 0.8971 (0.8799)
[2022/12/29 00:50] | TRAIN(034): [250/879] Batch: 0.1680 (0.1793) Data: 0.0077 (0.0174) Loss: 0.8434 (0.8792)
[2022/12/29 00:50] | TRAIN(034): [300/879] Batch: 0.1739 (0.1770) Data: 0.0077 (0.0158) Loss: 0.7976 (0.8785)
[2022/12/29 00:50] | TRAIN(034): [350/879] Batch: 0.1626 (0.1756) Data: 0.0127 (0.0147) Loss: 0.5822 (0.8765)
[2022/12/29 00:50] | TRAIN(034): [400/879] Batch: 0.1630 (0.1744) Data: 0.0071 (0.0139) Loss: 0.8693 (0.8765)
[2022/12/29 00:51] | TRAIN(034): [450/879] Batch: 0.1685 (0.1737) Data: 0.0080 (0.0133) Loss: 0.6329 (0.8815)
[2022/12/29 00:51] | TRAIN(034): [500/879] Batch: 0.1628 (0.1729) Data: 0.0076 (0.0128) Loss: 1.1347 (0.8806)
[2022/12/29 00:51] | TRAIN(034): [550/879] Batch: 0.1609 (0.1723) Data: 0.0075 (0.0124) Loss: 0.9883 (0.8774)
[2022/12/29 00:51] | TRAIN(034): [600/879] Batch: 0.1858 (0.1719) Data: 0.0074 (0.0120) Loss: 0.9172 (0.8756)
[2022/12/29 00:51] | TRAIN(034): [650/879] Batch: 0.1782 (0.1714) Data: 0.0077 (0.0117) Loss: 1.3170 (0.8747)
[2022/12/29 00:51] | TRAIN(034): [700/879] Batch: 0.1635 (0.1712) Data: 0.0077 (0.0114) Loss: 1.2756 (0.8741)
[2022/12/29 00:51] | TRAIN(034): [750/879] Batch: 0.1610 (0.1709) Data: 0.0073 (0.0112) Loss: 0.6150 (0.8727)
[2022/12/29 00:52] | TRAIN(034): [800/879] Batch: 0.1711 (0.1706) Data: 0.0073 (0.0110) Loss: 0.9874 (0.8715)
[2022/12/29 00:52] | TRAIN(034): [850/879] Batch: 0.1620 (0.1704) Data: 0.0072 (0.0108) Loss: 0.6429 (0.8707)
[2022/12/29 00:52] | ------------------------------------------------------------
[2022/12/29 00:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:52] | ------------------------------------------------------------
[2022/12/29 00:52] |    TRAIN(34)     0:02:29     0:00:09     0:02:20      0.8693
[2022/12/29 00:52] | ------------------------------------------------------------
[2022/12/29 00:52] | VALID(034): [ 50/220] Batch: 0.0546 (0.0838) Data: 0.0315 (0.0592) Loss: 0.7831 (0.8471)
[2022/12/29 00:52] | VALID(034): [100/220] Batch: 0.0564 (0.0696) Data: 0.0333 (0.0462) Loss: 1.1187 (0.8691)
[2022/12/29 00:52] | VALID(034): [150/220] Batch: 0.0544 (0.0648) Data: 0.0353 (0.0418) Loss: 0.7571 (0.8612)
[2022/12/29 00:52] | VALID(034): [200/220] Batch: 0.0535 (0.0624) Data: 0.0327 (0.0393) Loss: 0.5242 (0.8686)
[2022/12/29 00:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:52] |    VALID(34)      0.8687      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:52] | ####################################################################################################
[2022/12/29 00:52] | TRAIN(035): [ 50/879] Batch: 0.1862 (0.2080) Data: 0.0167 (0.0470) Loss: 0.8693 (0.8932)
[2022/12/29 00:52] | TRAIN(035): [100/879] Batch: 0.1622 (0.1910) Data: 0.0144 (0.0292) Loss: 1.1988 (0.8817)
[2022/12/29 00:52] | TRAIN(035): [150/879] Batch: 0.1802 (0.1835) Data: 0.0078 (0.0226) Loss: 0.8987 (0.8856)
[2022/12/29 00:53] | TRAIN(035): [200/879] Batch: 0.1706 (0.1793) Data: 0.0171 (0.0192) Loss: 1.2602 (0.8936)
[2022/12/29 00:53] | TRAIN(035): [250/879] Batch: 0.1731 (0.1773) Data: 0.0073 (0.0171) Loss: 0.7547 (0.8871)
[2022/12/29 00:53] | TRAIN(035): [300/879] Batch: 0.1677 (0.1756) Data: 0.0076 (0.0157) Loss: 0.5862 (0.8832)
[2022/12/29 00:53] | TRAIN(035): [350/879] Batch: 0.1688 (0.1743) Data: 0.0089 (0.0146) Loss: 0.7135 (0.8791)
[2022/12/29 00:53] | TRAIN(035): [400/879] Batch: 0.1648 (0.1734) Data: 0.0180 (0.0138) Loss: 0.8261 (0.8797)
[2022/12/29 00:53] | TRAIN(035): [450/879] Batch: 0.1606 (0.1729) Data: 0.0073 (0.0133) Loss: 0.6207 (0.8789)
[2022/12/29 00:53] | TRAIN(035): [500/879] Batch: 0.1601 (0.1722) Data: 0.0104 (0.0128) Loss: 0.7192 (0.8755)
[2022/12/29 00:54] | TRAIN(035): [550/879] Batch: 0.1612 (0.1717) Data: 0.0096 (0.0124) Loss: 0.7702 (0.8771)
[2022/12/29 00:54] | TRAIN(035): [600/879] Batch: 0.1622 (0.1715) Data: 0.0108 (0.0121) Loss: 0.8563 (0.8794)
[2022/12/29 00:54] | TRAIN(035): [650/879] Batch: 0.1811 (0.1713) Data: 0.0095 (0.0118) Loss: 0.8923 (0.8788)
[2022/12/29 00:54] | TRAIN(035): [700/879] Batch: 0.1704 (0.1710) Data: 0.0174 (0.0116) Loss: 0.7993 (0.8777)
[2022/12/29 00:54] | TRAIN(035): [750/879] Batch: 0.1643 (0.1709) Data: 0.0086 (0.0114) Loss: 0.8486 (0.8755)
[2022/12/29 00:54] | TRAIN(035): [800/879] Batch: 0.1621 (0.1708) Data: 0.0077 (0.0113) Loss: 0.9082 (0.8724)
[2022/12/29 00:54] | TRAIN(035): [850/879] Batch: 0.1858 (0.1707) Data: 0.0093 (0.0111) Loss: 0.9265 (0.8700)
[2022/12/29 00:55] | ------------------------------------------------------------
[2022/12/29 00:55] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:55] | ------------------------------------------------------------
[2022/12/29 00:55] |    TRAIN(35)     0:02:29     0:00:09     0:02:20      0.8689
[2022/12/29 00:55] | ------------------------------------------------------------
[2022/12/29 00:55] | VALID(035): [ 50/220] Batch: 0.0557 (0.0828) Data: 0.0310 (0.0587) Loss: 0.7853 (0.8471)
[2022/12/29 00:55] | VALID(035): [100/220] Batch: 0.0543 (0.0690) Data: 0.0310 (0.0455) Loss: 1.1188 (0.8692)
[2022/12/29 00:55] | VALID(035): [150/220] Batch: 0.0581 (0.0644) Data: 0.0286 (0.0414) Loss: 0.7572 (0.8613)
[2022/12/29 00:55] | VALID(035): [200/220] Batch: 0.0557 (0.0621) Data: 0.0336 (0.0394) Loss: 0.5161 (0.8687)
[2022/12/29 00:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:55] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:55] |    VALID(35)      0.8688      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:55] | ####################################################################################################
[2022/12/29 00:55] | TRAIN(036): [ 50/879] Batch: 0.1662 (0.2099) Data: 0.0169 (0.0457) Loss: 1.0779 (0.8393)
[2022/12/29 00:55] | TRAIN(036): [100/879] Batch: 0.1700 (0.1925) Data: 0.0088 (0.0287) Loss: 0.9503 (0.8815)
[2022/12/29 00:55] | TRAIN(036): [150/879] Batch: 0.1883 (0.1863) Data: 0.0092 (0.0226) Loss: 0.6454 (0.8710)
[2022/12/29 00:55] | TRAIN(036): [200/879] Batch: 0.1669 (0.1837) Data: 0.0077 (0.0195) Loss: 0.7826 (0.8733)
[2022/12/29 00:55] | TRAIN(036): [250/879] Batch: 0.1705 (0.1815) Data: 0.0161 (0.0177) Loss: 0.7426 (0.8744)
[2022/12/29 00:56] | TRAIN(036): [300/879] Batch: 0.1694 (0.1798) Data: 0.0073 (0.0161) Loss: 0.8662 (0.8780)
[2022/12/29 00:56] | TRAIN(036): [350/879] Batch: 0.1654 (0.1786) Data: 0.0071 (0.0150) Loss: 0.9296 (0.8759)
[2022/12/29 00:56] | TRAIN(036): [400/879] Batch: 0.1629 (0.1770) Data: 0.0078 (0.0142) Loss: 0.9743 (0.8735)
[2022/12/29 00:56] | TRAIN(036): [450/879] Batch: 0.1622 (0.1757) Data: 0.0076 (0.0134) Loss: 0.5519 (0.8708)
[2022/12/29 00:56] | TRAIN(036): [500/879] Batch: 0.1803 (0.1751) Data: 0.0076 (0.0130) Loss: 0.5995 (0.8689)
[2022/12/29 00:56] | TRAIN(036): [550/879] Batch: 0.1723 (0.1748) Data: 0.0088 (0.0126) Loss: 0.7298 (0.8665)
[2022/12/29 00:56] | TRAIN(036): [600/879] Batch: 0.1833 (0.1747) Data: 0.0073 (0.0123) Loss: 0.8200 (0.8686)
[2022/12/29 00:57] | TRAIN(036): [650/879] Batch: 0.1610 (0.1744) Data: 0.0076 (0.0120) Loss: 1.0219 (0.8690)
[2022/12/29 00:57] | TRAIN(036): [700/879] Batch: 0.1642 (0.1739) Data: 0.0072 (0.0117) Loss: 0.8336 (0.8679)
[2022/12/29 00:57] | TRAIN(036): [750/879] Batch: 0.1615 (0.1735) Data: 0.0075 (0.0115) Loss: 0.9314 (0.8696)
[2022/12/29 00:57] | TRAIN(036): [800/879] Batch: 0.1656 (0.1730) Data: 0.0073 (0.0112) Loss: 0.6495 (0.8671)
[2022/12/29 00:57] | TRAIN(036): [850/879] Batch: 0.1719 (0.1726) Data: 0.0074 (0.0110) Loss: 0.6308 (0.8688)
[2022/12/29 00:57] | ------------------------------------------------------------
[2022/12/29 00:57] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 00:57] | ------------------------------------------------------------
[2022/12/29 00:57] |    TRAIN(36)     0:02:31     0:00:09     0:02:22      0.8691
[2022/12/29 00:57] | ------------------------------------------------------------
[2022/12/29 00:57] | VALID(036): [ 50/220] Batch: 0.0540 (0.0842) Data: 0.0327 (0.0605) Loss: 0.7926 (0.8485)
[2022/12/29 00:57] | VALID(036): [100/220] Batch: 0.0526 (0.0697) Data: 0.0317 (0.0470) Loss: 1.1056 (0.8692)
[2022/12/29 00:57] | VALID(036): [150/220] Batch: 0.0554 (0.0649) Data: 0.0320 (0.0425) Loss: 0.7645 (0.8619)
[2022/12/29 00:57] | VALID(036): [200/220] Batch: 0.0573 (0.0625) Data: 0.0339 (0.0402) Loss: 0.5315 (0.8690)
[2022/12/29 00:57] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:57] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 00:57] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:57] |    VALID(36)      0.8691      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 00:57] | ------------------------------------------------------------------------------------------------
[2022/12/29 00:57] | ####################################################################################################
[2022/12/29 00:58] | TRAIN(037): [ 50/879] Batch: 0.1892 (0.2122) Data: 0.0087 (0.0487) Loss: 0.7167 (0.8412)
[2022/12/29 00:58] | TRAIN(037): [100/879] Batch: 0.1620 (0.1956) Data: 0.0183 (0.0299) Loss: 0.7388 (0.8768)
[2022/12/29 00:58] | TRAIN(037): [150/879] Batch: 0.1721 (0.1883) Data: 0.0182 (0.0235) Loss: 0.8823 (0.8765)
[2022/12/29 00:58] | TRAIN(037): [200/879] Batch: 0.1716 (0.1847) Data: 0.0072 (0.0204) Loss: 0.7644 (0.8766)
[2022/12/29 00:58] | TRAIN(037): [250/879] Batch: 0.1702 (0.1822) Data: 0.0119 (0.0183) Loss: 0.8158 (0.8736)
[2022/12/29 00:58] | TRAIN(037): [300/879] Batch: 0.1740 (0.1804) Data: 0.0079 (0.0169) Loss: 0.9414 (0.8725)
[2022/12/29 00:59] | TRAIN(037): [350/879] Batch: 0.1687 (0.1790) Data: 0.0165 (0.0159) Loss: 0.7861 (0.8702)
[2022/12/29 00:59] | TRAIN(037): [400/879] Batch: 0.1628 (0.1784) Data: 0.0074 (0.0154) Loss: 0.7346 (0.8744)
[2022/12/29 00:59] | TRAIN(037): [450/879] Batch: 0.1761 (0.1779) Data: 0.0075 (0.0147) Loss: 0.7293 (0.8742)
[2022/12/29 00:59] | TRAIN(037): [500/879] Batch: 0.1844 (0.1780) Data: 0.0075 (0.0140) Loss: 0.9458 (0.8704)
[2022/12/29 00:59] | TRAIN(037): [550/879] Batch: 0.1798 (0.1776) Data: 0.0073 (0.0136) Loss: 0.7557 (0.8665)
[2022/12/29 00:59] | TRAIN(037): [600/879] Batch: 0.1620 (0.1772) Data: 0.0075 (0.0131) Loss: 1.0962 (0.8664)
[2022/12/29 00:59] | TRAIN(037): [650/879] Batch: 0.1720 (0.1765) Data: 0.0088 (0.0128) Loss: 0.8553 (0.8710)
[2022/12/29 01:00] | TRAIN(037): [700/879] Batch: 0.1657 (0.1761) Data: 0.0076 (0.0125) Loss: 0.6965 (0.8672)
[2022/12/29 01:00] | TRAIN(037): [750/879] Batch: 0.1670 (0.1763) Data: 0.0155 (0.0123) Loss: 0.7663 (0.8675)
[2022/12/29 01:00] | TRAIN(037): [800/879] Batch: 0.1613 (0.1757) Data: 0.0075 (0.0121) Loss: 0.4995 (0.8674)
[2022/12/29 01:00] | TRAIN(037): [850/879] Batch: 0.1690 (0.1752) Data: 0.0073 (0.0118) Loss: 1.0721 (0.8692)
[2022/12/29 01:00] | ------------------------------------------------------------
[2022/12/29 01:00] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:00] | ------------------------------------------------------------
[2022/12/29 01:00] |    TRAIN(37)     0:02:33     0:00:10     0:02:23      0.8690
[2022/12/29 01:00] | ------------------------------------------------------------
[2022/12/29 01:00] | VALID(037): [ 50/220] Batch: 0.0560 (0.0824) Data: 0.0316 (0.0576) Loss: 0.7858 (0.8478)
[2022/12/29 01:00] | VALID(037): [100/220] Batch: 0.0564 (0.0687) Data: 0.0353 (0.0455) Loss: 1.1180 (0.8712)
[2022/12/29 01:00] | VALID(037): [150/220] Batch: 0.0552 (0.0641) Data: 0.0326 (0.0413) Loss: 0.7572 (0.8628)
[2022/12/29 01:00] | VALID(037): [200/220] Batch: 0.0553 (0.0618) Data: 0.0353 (0.0395) Loss: 0.4935 (0.8707)
[2022/12/29 01:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:00] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:00] |    VALID(37)      0.8704      0.7347      0.5016      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:00] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:00] | ####################################################################################################
[2022/12/29 01:00] | TRAIN(038): [ 50/879] Batch: 0.1880 (0.2109) Data: 0.0101 (0.0499) Loss: 0.9789 (0.8282)
[2022/12/29 01:01] | TRAIN(038): [100/879] Batch: 0.1648 (0.1921) Data: 0.0175 (0.0302) Loss: 0.6605 (0.8499)
[2022/12/29 01:01] | TRAIN(038): [150/879] Batch: 0.1610 (0.1844) Data: 0.0075 (0.0232) Loss: 0.9007 (0.8683)
[2022/12/29 01:01] | TRAIN(038): [200/879] Batch: 0.1686 (0.1798) Data: 0.0071 (0.0196) Loss: 0.7423 (0.8688)
[2022/12/29 01:01] | TRAIN(038): [250/879] Batch: 0.1748 (0.1777) Data: 0.0185 (0.0174) Loss: 0.6671 (0.8693)
[2022/12/29 01:01] | TRAIN(038): [300/879] Batch: 0.1605 (0.1766) Data: 0.0075 (0.0158) Loss: 0.9005 (0.8645)
[2022/12/29 01:01] | TRAIN(038): [350/879] Batch: 0.1646 (0.1752) Data: 0.0083 (0.0148) Loss: 0.7051 (0.8658)
[2022/12/29 01:01] | TRAIN(038): [400/879] Batch: 0.1811 (0.1754) Data: 0.0072 (0.0140) Loss: 0.8987 (0.8643)
[2022/12/29 01:02] | TRAIN(038): [450/879] Batch: 0.1627 (0.1745) Data: 0.0175 (0.0134) Loss: 1.2788 (0.8661)
[2022/12/29 01:02] | TRAIN(038): [500/879] Batch: 0.1654 (0.1735) Data: 0.0072 (0.0129) Loss: 0.8946 (0.8654)
[2022/12/29 01:02] | TRAIN(038): [550/879] Batch: 0.1612 (0.1728) Data: 0.0076 (0.0124) Loss: 0.9283 (0.8618)
[2022/12/29 01:02] | TRAIN(038): [600/879] Batch: 0.1606 (0.1721) Data: 0.0078 (0.0120) Loss: 0.9837 (0.8648)
[2022/12/29 01:02] | TRAIN(038): [650/879] Batch: 0.1609 (0.1717) Data: 0.0082 (0.0117) Loss: 0.7680 (0.8671)
[2022/12/29 01:02] | TRAIN(038): [700/879] Batch: 0.1637 (0.1711) Data: 0.0075 (0.0115) Loss: 0.8656 (0.8670)
[2022/12/29 01:02] | TRAIN(038): [750/879] Batch: 0.2012 (0.1709) Data: 0.0080 (0.0112) Loss: 0.9940 (0.8674)
[2022/12/29 01:03] | TRAIN(038): [800/879] Batch: 0.1602 (0.1706) Data: 0.0073 (0.0110) Loss: 1.2467 (0.8655)
[2022/12/29 01:03] | TRAIN(038): [850/879] Batch: 0.1633 (0.1704) Data: 0.0077 (0.0108) Loss: 1.1512 (0.8662)
[2022/12/29 01:03] | ------------------------------------------------------------
[2022/12/29 01:03] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:03] | ------------------------------------------------------------
[2022/12/29 01:03] |    TRAIN(38)     0:02:29     0:00:09     0:02:20      0.8687
[2022/12/29 01:03] | ------------------------------------------------------------
[2022/12/29 01:03] | VALID(038): [ 50/220] Batch: 0.0534 (0.0826) Data: 0.0317 (0.0597) Loss: 0.7997 (0.8524)
[2022/12/29 01:03] | VALID(038): [100/220] Batch: 0.0568 (0.0688) Data: 0.0350 (0.0465) Loss: 1.0907 (0.8719)
[2022/12/29 01:03] | VALID(038): [150/220] Batch: 0.0540 (0.0642) Data: 0.0318 (0.0419) Loss: 0.7748 (0.8651)
[2022/12/29 01:03] | VALID(038): [200/220] Batch: 0.0540 (0.0619) Data: 0.0316 (0.0393) Loss: 0.5517 (0.8718)
[2022/12/29 01:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:03] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:03] |    VALID(38)      0.8718      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:03] | ####################################################################################################
[2022/12/29 01:03] | TRAIN(039): [ 50/879] Batch: 0.1639 (0.2079) Data: 0.0081 (0.0468) Loss: 0.6534 (0.8520)
[2022/12/29 01:03] | TRAIN(039): [100/879] Batch: 0.1631 (0.1895) Data: 0.0071 (0.0289) Loss: 0.9482 (0.8538)
[2022/12/29 01:03] | TRAIN(039): [150/879] Batch: 0.1769 (0.1832) Data: 0.0076 (0.0225) Loss: 1.1726 (0.8520)
[2022/12/29 01:04] | TRAIN(039): [200/879] Batch: 0.1833 (0.1800) Data: 0.0170 (0.0193) Loss: 0.7136 (0.8546)
[2022/12/29 01:04] | TRAIN(039): [250/879] Batch: 0.1757 (0.1786) Data: 0.0077 (0.0173) Loss: 0.9631 (0.8669)
[2022/12/29 01:04] | TRAIN(039): [300/879] Batch: 0.1838 (0.1775) Data: 0.0096 (0.0161) Loss: 0.9106 (0.8733)
[2022/12/29 01:04] | TRAIN(039): [350/879] Batch: 0.1659 (0.1765) Data: 0.0159 (0.0152) Loss: 0.5873 (0.8701)
[2022/12/29 01:04] | TRAIN(039): [400/879] Batch: 0.1648 (0.1759) Data: 0.0180 (0.0146) Loss: 0.9356 (0.8682)
[2022/12/29 01:04] | TRAIN(039): [450/879] Batch: 0.1684 (0.1755) Data: 0.0076 (0.0141) Loss: 0.8095 (0.8672)
[2022/12/29 01:04] | TRAIN(039): [500/879] Batch: 0.1625 (0.1751) Data: 0.0076 (0.0136) Loss: 0.9011 (0.8644)
[2022/12/29 01:05] | TRAIN(039): [550/879] Batch: 0.1830 (0.1749) Data: 0.0099 (0.0133) Loss: 0.9377 (0.8654)
[2022/12/29 01:05] | TRAIN(039): [600/879] Batch: 0.1643 (0.1747) Data: 0.0163 (0.0130) Loss: 0.6379 (0.8655)
[2022/12/29 01:05] | TRAIN(039): [650/879] Batch: 0.1696 (0.1742) Data: 0.0106 (0.0126) Loss: 1.1920 (0.8636)
[2022/12/29 01:05] | TRAIN(039): [700/879] Batch: 0.1615 (0.1740) Data: 0.0075 (0.0124) Loss: 0.9365 (0.8641)
[2022/12/29 01:05] | TRAIN(039): [750/879] Batch: 0.1636 (0.1737) Data: 0.0074 (0.0121) Loss: 0.9211 (0.8656)
[2022/12/29 01:05] | TRAIN(039): [800/879] Batch: 0.1603 (0.1733) Data: 0.0075 (0.0120) Loss: 0.9722 (0.8659)
[2022/12/29 01:05] | TRAIN(039): [850/879] Batch: 0.1669 (0.1732) Data: 0.0202 (0.0118) Loss: 1.0443 (0.8669)
[2022/12/29 01:06] | ------------------------------------------------------------
[2022/12/29 01:06] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:06] | ------------------------------------------------------------
[2022/12/29 01:06] |    TRAIN(39)     0:02:32     0:00:10     0:02:21      0.8689
[2022/12/29 01:06] | ------------------------------------------------------------
[2022/12/29 01:06] | VALID(039): [ 50/220] Batch: 0.0546 (0.0886) Data: 0.0325 (0.0619) Loss: 0.7892 (0.8494)
[2022/12/29 01:06] | VALID(039): [100/220] Batch: 0.0522 (0.0718) Data: 0.0367 (0.0473) Loss: 1.0969 (0.8707)
[2022/12/29 01:06] | VALID(039): [150/220] Batch: 0.0560 (0.0664) Data: 0.0232 (0.0421) Loss: 0.7653 (0.8630)
[2022/12/29 01:06] | VALID(039): [200/220] Batch: 0.0523 (0.0635) Data: 0.0310 (0.0384) Loss: 0.5379 (0.8701)
[2022/12/29 01:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:06] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:06] |    VALID(39)      0.8698      0.7347      0.4976      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:06] | ####################################################################################################
[2022/12/29 01:06] | TRAIN(040): [ 50/879] Batch: 0.1908 (0.2148) Data: 0.0165 (0.0513) Loss: 0.8326 (0.8918)
[2022/12/29 01:06] | TRAIN(040): [100/879] Batch: 0.1607 (0.1917) Data: 0.0073 (0.0302) Loss: 0.7912 (0.8927)
[2022/12/29 01:06] | TRAIN(040): [150/879] Batch: 0.1615 (0.1829) Data: 0.0073 (0.0228) Loss: 1.1489 (0.8802)
[2022/12/29 01:06] | TRAIN(040): [200/879] Batch: 0.1649 (0.1782) Data: 0.0074 (0.0191) Loss: 0.5788 (0.8730)
[2022/12/29 01:07] | TRAIN(040): [250/879] Batch: 0.1637 (0.1757) Data: 0.0072 (0.0169) Loss: 0.9178 (0.8673)
[2022/12/29 01:07] | TRAIN(040): [300/879] Batch: 0.1612 (0.1741) Data: 0.0074 (0.0154) Loss: 0.7267 (0.8741)
[2022/12/29 01:07] | TRAIN(040): [350/879] Batch: 0.1805 (0.1731) Data: 0.0168 (0.0144) Loss: 0.8911 (0.8681)
[2022/12/29 01:07] | TRAIN(040): [400/879] Batch: 0.1667 (0.1725) Data: 0.0080 (0.0136) Loss: 0.8358 (0.8704)
[2022/12/29 01:07] | TRAIN(040): [450/879] Batch: 0.1762 (0.1719) Data: 0.0096 (0.0130) Loss: 1.2143 (0.8710)
[2022/12/29 01:07] | TRAIN(040): [500/879] Batch: 0.1602 (0.1713) Data: 0.0083 (0.0125) Loss: 0.6942 (0.8701)
[2022/12/29 01:07] | TRAIN(040): [550/879] Batch: 0.1751 (0.1708) Data: 0.0072 (0.0120) Loss: 0.5624 (0.8677)
[2022/12/29 01:07] | TRAIN(040): [600/879] Batch: 0.1799 (0.1705) Data: 0.0075 (0.0117) Loss: 0.8119 (0.8655)
[2022/12/29 01:08] | TRAIN(040): [650/879] Batch: 0.1677 (0.1705) Data: 0.0074 (0.0114) Loss: 1.1235 (0.8683)
[2022/12/29 01:08] | TRAIN(040): [700/879] Batch: 0.1687 (0.1703) Data: 0.0077 (0.0112) Loss: 0.6597 (0.8696)
[2022/12/29 01:08] | TRAIN(040): [750/879] Batch: 0.1718 (0.1699) Data: 0.0075 (0.0109) Loss: 0.9018 (0.8675)
[2022/12/29 01:08] | TRAIN(040): [800/879] Batch: 0.1698 (0.1696) Data: 0.0071 (0.0107) Loss: 0.9482 (0.8676)
[2022/12/29 01:08] | TRAIN(040): [850/879] Batch: 0.1619 (0.1694) Data: 0.0078 (0.0105) Loss: 1.0538 (0.8692)
[2022/12/29 01:08] | ------------------------------------------------------------
[2022/12/29 01:08] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:08] | ------------------------------------------------------------
[2022/12/29 01:08] |    TRAIN(40)     0:02:28     0:00:09     0:02:19      0.8686
[2022/12/29 01:08] | ------------------------------------------------------------
[2022/12/29 01:08] | VALID(040): [ 50/220] Batch: 0.0569 (0.0833) Data: 0.0352 (0.0586) Loss: 0.7850 (0.8469)
[2022/12/29 01:08] | VALID(040): [100/220] Batch: 0.0566 (0.0694) Data: 0.0284 (0.0435) Loss: 1.1093 (0.8686)
[2022/12/29 01:08] | VALID(040): [150/220] Batch: 0.0558 (0.0648) Data: 0.0269 (0.0383) Loss: 0.7599 (0.8608)
[2022/12/29 01:08] | VALID(040): [200/220] Batch: 0.0570 (0.0624) Data: 0.0292 (0.0355) Loss: 0.5235 (0.8682)
[2022/12/29 01:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:08] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:08] |    VALID(40)      0.8681      0.7347      0.4969      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:08] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:08] | ####################################################################################################
[2022/12/29 01:09] | TRAIN(041): [ 50/879] Batch: 0.1596 (0.2082) Data: 0.0082 (0.0457) Loss: 1.2617 (0.9006)
[2022/12/29 01:09] | TRAIN(041): [100/879] Batch: 0.1637 (0.1877) Data: 0.0090 (0.0272) Loss: 1.0595 (0.8735)
[2022/12/29 01:09] | TRAIN(041): [150/879] Batch: 0.1609 (0.1820) Data: 0.0075 (0.0209) Loss: 0.8755 (0.8746)
[2022/12/29 01:09] | TRAIN(041): [200/879] Batch: 0.1619 (0.1782) Data: 0.0079 (0.0177) Loss: 1.1669 (0.8713)
[2022/12/29 01:09] | TRAIN(041): [250/879] Batch: 0.1617 (0.1761) Data: 0.0073 (0.0158) Loss: 1.2615 (0.8749)
[2022/12/29 01:09] | TRAIN(041): [300/879] Batch: 0.1672 (0.1748) Data: 0.0074 (0.0145) Loss: 1.2462 (0.8660)
[2022/12/29 01:10] | TRAIN(041): [350/879] Batch: 0.1629 (0.1736) Data: 0.0071 (0.0135) Loss: 1.0033 (0.8692)
[2022/12/29 01:10] | TRAIN(041): [400/879] Batch: 0.1622 (0.1727) Data: 0.0077 (0.0128) Loss: 0.9954 (0.8696)
[2022/12/29 01:10] | TRAIN(041): [450/879] Batch: 0.1619 (0.1718) Data: 0.0073 (0.0122) Loss: 0.8898 (0.8676)
[2022/12/29 01:10] | TRAIN(041): [500/879] Batch: 0.1627 (0.1713) Data: 0.0075 (0.0118) Loss: 1.1199 (0.8685)
[2022/12/29 01:10] | TRAIN(041): [550/879] Batch: 0.1604 (0.1707) Data: 0.0072 (0.0114) Loss: 1.3093 (0.8692)
[2022/12/29 01:10] | TRAIN(041): [600/879] Batch: 0.1638 (0.1706) Data: 0.0078 (0.0112) Loss: 1.1031 (0.8680)
[2022/12/29 01:10] | TRAIN(041): [650/879] Batch: 0.1693 (0.1704) Data: 0.0076 (0.0109) Loss: 0.7901 (0.8675)
[2022/12/29 01:10] | TRAIN(041): [700/879] Batch: 0.1732 (0.1700) Data: 0.0074 (0.0107) Loss: 0.7787 (0.8686)
[2022/12/29 01:11] | TRAIN(041): [750/879] Batch: 0.1806 (0.1697) Data: 0.0097 (0.0105) Loss: 0.7845 (0.8667)
[2022/12/29 01:11] | TRAIN(041): [800/879] Batch: 0.1614 (0.1695) Data: 0.0076 (0.0103) Loss: 0.8379 (0.8671)
[2022/12/29 01:11] | TRAIN(041): [850/879] Batch: 0.1641 (0.1693) Data: 0.0076 (0.0102) Loss: 1.1342 (0.8689)
[2022/12/29 01:11] | ------------------------------------------------------------
[2022/12/29 01:11] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:11] | ------------------------------------------------------------
[2022/12/29 01:11] |    TRAIN(41)     0:02:28     0:00:08     0:02:19      0.8688
[2022/12/29 01:11] | ------------------------------------------------------------
[2022/12/29 01:11] | VALID(041): [ 50/220] Batch: 0.0582 (0.0830) Data: 0.0334 (0.0566) Loss: 0.7850 (0.8478)
[2022/12/29 01:11] | VALID(041): [100/220] Batch: 0.0559 (0.0692) Data: 0.0338 (0.0450) Loss: 1.1049 (0.8693)
[2022/12/29 01:11] | VALID(041): [150/220] Batch: 0.0540 (0.0647) Data: 0.0324 (0.0413) Loss: 0.7612 (0.8615)
[2022/12/29 01:11] | VALID(041): [200/220] Batch: 0.0538 (0.0624) Data: 0.0263 (0.0387) Loss: 0.5356 (0.8687)
[2022/12/29 01:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:11] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:11] |    VALID(41)      0.8686      0.7347      0.5019      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:11] | ####################################################################################################
[2022/12/29 01:11] | TRAIN(042): [ 50/879] Batch: 0.1764 (0.2088) Data: 0.0087 (0.0470) Loss: 0.8716 (0.8583)
[2022/12/29 01:12] | TRAIN(042): [100/879] Batch: 0.1755 (0.1916) Data: 0.0177 (0.0290) Loss: 0.8358 (0.8492)
[2022/12/29 01:12] | TRAIN(042): [150/879] Batch: 0.1620 (0.1863) Data: 0.0085 (0.0228) Loss: 0.7317 (0.8539)
[2022/12/29 01:12] | TRAIN(042): [200/879] Batch: 0.1781 (0.1839) Data: 0.0121 (0.0196) Loss: 1.2138 (0.8692)
[2022/12/29 01:12] | TRAIN(042): [250/879] Batch: 0.1629 (0.1825) Data: 0.0080 (0.0177) Loss: 0.7574 (0.8757)
[2022/12/29 01:12] | TRAIN(042): [300/879] Batch: 0.1598 (0.1804) Data: 0.0091 (0.0164) Loss: 0.6223 (0.8691)
[2022/12/29 01:12] | TRAIN(042): [350/879] Batch: 0.1681 (0.1785) Data: 0.0078 (0.0154) Loss: 0.7723 (0.8705)
[2022/12/29 01:12] | TRAIN(042): [400/879] Batch: 0.1840 (0.1773) Data: 0.0077 (0.0147) Loss: 0.8949 (0.8698)
[2022/12/29 01:13] | TRAIN(042): [450/879] Batch: 0.1631 (0.1767) Data: 0.0134 (0.0141) Loss: 0.9522 (0.8705)
[2022/12/29 01:13] | TRAIN(042): [500/879] Batch: 0.1676 (0.1760) Data: 0.0073 (0.0136) Loss: 0.6052 (0.8712)
[2022/12/29 01:13] | TRAIN(042): [550/879] Batch: 0.1835 (0.1758) Data: 0.0079 (0.0132) Loss: 0.7834 (0.8745)
[2022/12/29 01:13] | TRAIN(042): [600/879] Batch: 0.1795 (0.1755) Data: 0.0143 (0.0129) Loss: 0.8477 (0.8758)
[2022/12/29 01:13] | TRAIN(042): [650/879] Batch: 0.1815 (0.1754) Data: 0.0099 (0.0127) Loss: 0.9393 (0.8714)
[2022/12/29 01:13] | TRAIN(042): [700/879] Batch: 0.1803 (0.1754) Data: 0.0076 (0.0126) Loss: 0.7057 (0.8696)
[2022/12/29 01:13] | TRAIN(042): [750/879] Batch: 0.1642 (0.1749) Data: 0.0080 (0.0123) Loss: 0.7031 (0.8718)
[2022/12/29 01:14] | TRAIN(042): [800/879] Batch: 0.1725 (0.1745) Data: 0.0154 (0.0121) Loss: 0.8969 (0.8704)
[2022/12/29 01:14] | TRAIN(042): [850/879] Batch: 0.1644 (0.1742) Data: 0.0073 (0.0119) Loss: 1.2523 (0.8686)
[2022/12/29 01:14] | ------------------------------------------------------------
[2022/12/29 01:14] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:14] | ------------------------------------------------------------
[2022/12/29 01:14] |    TRAIN(42)     0:02:32     0:00:10     0:02:22      0.8686
[2022/12/29 01:14] | ------------------------------------------------------------
[2022/12/29 01:14] | VALID(042): [ 50/220] Batch: 0.0573 (0.0830) Data: 0.0321 (0.0597) Loss: 0.7818 (0.8474)
[2022/12/29 01:14] | VALID(042): [100/220] Batch: 0.0542 (0.0693) Data: 0.0320 (0.0468) Loss: 1.1073 (0.8705)
[2022/12/29 01:14] | VALID(042): [150/220] Batch: 0.0533 (0.0647) Data: 0.0325 (0.0423) Loss: 0.7617 (0.8626)
[2022/12/29 01:14] | VALID(042): [200/220] Batch: 0.0565 (0.0624) Data: 0.0329 (0.0402) Loss: 0.4988 (0.8706)
[2022/12/29 01:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:14] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:14] |    VALID(42)      0.8701      0.7347      0.4958      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:14] | ####################################################################################################
[2022/12/29 01:14] | TRAIN(043): [ 50/879] Batch: 0.1698 (0.2082) Data: 0.0183 (0.0463) Loss: 0.9793 (0.9011)
[2022/12/29 01:14] | TRAIN(043): [100/879] Batch: 0.1647 (0.1904) Data: 0.0177 (0.0289) Loss: 0.9257 (0.8674)
[2022/12/29 01:14] | TRAIN(043): [150/879] Batch: 0.1667 (0.1847) Data: 0.0075 (0.0230) Loss: 0.5219 (0.8716)
[2022/12/29 01:15] | TRAIN(043): [200/879] Batch: 0.1806 (0.1816) Data: 0.0111 (0.0197) Loss: 0.7426 (0.8710)
[2022/12/29 01:15] | TRAIN(043): [250/879] Batch: 0.1703 (0.1802) Data: 0.0167 (0.0180) Loss: 0.7552 (0.8678)
[2022/12/29 01:15] | TRAIN(043): [300/879] Batch: 0.1694 (0.1788) Data: 0.0076 (0.0166) Loss: 1.0477 (0.8721)
[2022/12/29 01:15] | TRAIN(043): [350/879] Batch: 0.1691 (0.1781) Data: 0.0089 (0.0157) Loss: 0.6163 (0.8732)
[2022/12/29 01:15] | TRAIN(043): [400/879] Batch: 0.1801 (0.1770) Data: 0.0094 (0.0148) Loss: 0.6552 (0.8771)
[2022/12/29 01:15] | TRAIN(043): [450/879] Batch: 0.1625 (0.1767) Data: 0.0073 (0.0142) Loss: 0.8699 (0.8720)
[2022/12/29 01:15] | TRAIN(043): [500/879] Batch: 0.1726 (0.1757) Data: 0.0167 (0.0137) Loss: 0.9202 (0.8724)
[2022/12/29 01:16] | TRAIN(043): [550/879] Batch: 0.1805 (0.1753) Data: 0.0093 (0.0134) Loss: 0.8847 (0.8712)
[2022/12/29 01:16] | TRAIN(043): [600/879] Batch: 0.1704 (0.1747) Data: 0.0175 (0.0130) Loss: 0.8114 (0.8712)
[2022/12/29 01:16] | TRAIN(043): [650/879] Batch: 0.1684 (0.1742) Data: 0.0073 (0.0127) Loss: 1.1986 (0.8708)
[2022/12/29 01:16] | TRAIN(043): [700/879] Batch: 0.1646 (0.1741) Data: 0.0101 (0.0125) Loss: 0.8576 (0.8686)
[2022/12/29 01:16] | TRAIN(043): [750/879] Batch: 0.1827 (0.1739) Data: 0.0097 (0.0122) Loss: 0.6146 (0.8682)
[2022/12/29 01:16] | TRAIN(043): [800/879] Batch: 0.1676 (0.1736) Data: 0.0092 (0.0121) Loss: 0.8288 (0.8672)
[2022/12/29 01:16] | TRAIN(043): [850/879] Batch: 0.1674 (0.1736) Data: 0.0091 (0.0120) Loss: 0.7876 (0.8695)
[2022/12/29 01:17] | ------------------------------------------------------------
[2022/12/29 01:17] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:17] | ------------------------------------------------------------
[2022/12/29 01:17] |    TRAIN(43)     0:02:32     0:00:10     0:02:21      0.8689
[2022/12/29 01:17] | ------------------------------------------------------------
[2022/12/29 01:17] | VALID(043): [ 50/220] Batch: 0.0532 (0.0830) Data: 0.0322 (0.0592) Loss: 0.7837 (0.8462)
[2022/12/29 01:17] | VALID(043): [100/220] Batch: 0.0562 (0.0690) Data: 0.0324 (0.0457) Loss: 1.1112 (0.8684)
[2022/12/29 01:17] | VALID(043): [150/220] Batch: 0.0544 (0.0643) Data: 0.0314 (0.0416) Loss: 0.7584 (0.8606)
[2022/12/29 01:17] | VALID(043): [200/220] Batch: 0.0528 (0.0620) Data: 0.0353 (0.0393) Loss: 0.5124 (0.8681)
[2022/12/29 01:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:17] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:17] |    VALID(43)      0.8680      0.7347      0.4974      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:17] | ####################################################################################################
[2022/12/29 01:17] | TRAIN(044): [ 50/879] Batch: 0.1744 (0.2120) Data: 0.0119 (0.0475) Loss: 1.1155 (0.8538)
[2022/12/29 01:17] | TRAIN(044): [100/879] Batch: 0.1923 (0.1920) Data: 0.0169 (0.0291) Loss: 0.6888 (0.8629)
[2022/12/29 01:17] | TRAIN(044): [150/879] Batch: 0.1855 (0.1859) Data: 0.0101 (0.0225) Loss: 0.8190 (0.8586)
[2022/12/29 01:17] | TRAIN(044): [200/879] Batch: 0.1798 (0.1819) Data: 0.0077 (0.0190) Loss: 1.2636 (0.8646)
[2022/12/29 01:17] | TRAIN(044): [250/879] Batch: 0.1700 (0.1799) Data: 0.0073 (0.0172) Loss: 1.0380 (0.8681)
[2022/12/29 01:18] | TRAIN(044): [300/879] Batch: 0.1780 (0.1779) Data: 0.0077 (0.0158) Loss: 1.1419 (0.8706)
[2022/12/29 01:18] | TRAIN(044): [350/879] Batch: 0.1609 (0.1767) Data: 0.0073 (0.0148) Loss: 0.9614 (0.8714)
[2022/12/29 01:18] | TRAIN(044): [400/879] Batch: 0.1726 (0.1759) Data: 0.0123 (0.0141) Loss: 0.8322 (0.8674)
[2022/12/29 01:18] | TRAIN(044): [450/879] Batch: 0.1615 (0.1751) Data: 0.0075 (0.0135) Loss: 0.8367 (0.8708)
[2022/12/29 01:18] | TRAIN(044): [500/879] Batch: 0.1709 (0.1743) Data: 0.0087 (0.0130) Loss: 0.6091 (0.8712)
[2022/12/29 01:18] | TRAIN(044): [550/879] Batch: 0.1664 (0.1738) Data: 0.0083 (0.0126) Loss: 0.9220 (0.8728)
[2022/12/29 01:18] | TRAIN(044): [600/879] Batch: 0.1650 (0.1735) Data: 0.0075 (0.0123) Loss: 0.7280 (0.8709)
[2022/12/29 01:19] | TRAIN(044): [650/879] Batch: 0.1619 (0.1733) Data: 0.0074 (0.0120) Loss: 1.3108 (0.8711)
[2022/12/29 01:19] | TRAIN(044): [700/879] Batch: 0.1638 (0.1731) Data: 0.0075 (0.0118) Loss: 0.7653 (0.8724)
[2022/12/29 01:19] | TRAIN(044): [750/879] Batch: 0.1707 (0.1727) Data: 0.0081 (0.0116) Loss: 0.8177 (0.8714)
[2022/12/29 01:19] | TRAIN(044): [800/879] Batch: 0.1624 (0.1724) Data: 0.0165 (0.0114) Loss: 1.5726 (0.8697)
[2022/12/29 01:19] | TRAIN(044): [850/879] Batch: 0.1712 (0.1721) Data: 0.0075 (0.0112) Loss: 0.8245 (0.8690)
[2022/12/29 01:19] | ------------------------------------------------------------
[2022/12/29 01:19] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:19] | ------------------------------------------------------------
[2022/12/29 01:19] |    TRAIN(44)     0:02:31     0:00:09     0:02:21      0.8686
[2022/12/29 01:19] | ------------------------------------------------------------
[2022/12/29 01:19] | VALID(044): [ 50/220] Batch: 0.0556 (0.0843) Data: 0.0326 (0.0606) Loss: 0.7849 (0.8464)
[2022/12/29 01:19] | VALID(044): [100/220] Batch: 0.0530 (0.0698) Data: 0.0353 (0.0458) Loss: 1.1108 (0.8688)
[2022/12/29 01:19] | VALID(044): [150/220] Batch: 0.0570 (0.0651) Data: 0.0338 (0.0410) Loss: 0.7589 (0.8609)
[2022/12/29 01:19] | VALID(044): [200/220] Batch: 0.0523 (0.0626) Data: 0.0334 (0.0392) Loss: 0.5081 (0.8685)
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] |    VALID(44)      0.8683      0.7347      0.5019      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:19] | ####################################################################################################
[2022/12/29 01:20] | TRAIN(045): [ 50/879] Batch: 0.1649 (0.2094) Data: 0.0073 (0.0482) Loss: 0.5985 (0.8653)
[2022/12/29 01:20] | TRAIN(045): [100/879] Batch: 0.1806 (0.1892) Data: 0.0074 (0.0286) Loss: 0.4738 (0.8674)
[2022/12/29 01:20] | TRAIN(045): [150/879] Batch: 0.1697 (0.1817) Data: 0.0094 (0.0219) Loss: 0.8469 (0.8773)
[2022/12/29 01:20] | TRAIN(045): [200/879] Batch: 0.1612 (0.1785) Data: 0.0073 (0.0186) Loss: 0.9326 (0.8733)
[2022/12/29 01:20] | TRAIN(045): [250/879] Batch: 0.1659 (0.1764) Data: 0.0072 (0.0165) Loss: 0.8467 (0.8657)
[2022/12/29 01:20] | TRAIN(045): [300/879] Batch: 0.1803 (0.1756) Data: 0.0097 (0.0153) Loss: 0.9904 (0.8682)
[2022/12/29 01:21] | TRAIN(045): [350/879] Batch: 0.1714 (0.1746) Data: 0.0074 (0.0143) Loss: 0.7433 (0.8720)
[2022/12/29 01:21] | TRAIN(045): [400/879] Batch: 0.1659 (0.1738) Data: 0.0078 (0.0136) Loss: 0.9382 (0.8725)
[2022/12/29 01:21] | TRAIN(045): [450/879] Batch: 0.1812 (0.1733) Data: 0.0086 (0.0131) Loss: 0.7984 (0.8718)
[2022/12/29 01:21] | TRAIN(045): [500/879] Batch: 0.1720 (0.1730) Data: 0.0084 (0.0126) Loss: 0.7162 (0.8699)
[2022/12/29 01:21] | TRAIN(045): [550/879] Batch: 0.2075 (0.1728) Data: 0.0099 (0.0123) Loss: 0.6349 (0.8662)
[2022/12/29 01:21] | TRAIN(045): [600/879] Batch: 0.1627 (0.1727) Data: 0.0077 (0.0120) Loss: 1.1994 (0.8697)
[2022/12/29 01:21] | TRAIN(045): [650/879] Batch: 0.1722 (0.1723) Data: 0.0073 (0.0117) Loss: 0.7660 (0.8705)
[2022/12/29 01:22] | TRAIN(045): [700/879] Batch: 0.1708 (0.1722) Data: 0.0071 (0.0114) Loss: 0.7795 (0.8703)
[2022/12/29 01:22] | TRAIN(045): [750/879] Batch: 0.1683 (0.1721) Data: 0.0091 (0.0112) Loss: 0.5861 (0.8716)
[2022/12/29 01:22] | TRAIN(045): [800/879] Batch: 0.1685 (0.1717) Data: 0.0075 (0.0110) Loss: 1.2364 (0.8724)
[2022/12/29 01:22] | TRAIN(045): [850/879] Batch: 0.1714 (0.1714) Data: 0.0073 (0.0108) Loss: 0.6179 (0.8710)
[2022/12/29 01:22] | ------------------------------------------------------------
[2022/12/29 01:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:22] | ------------------------------------------------------------
[2022/12/29 01:22] |    TRAIN(45)     0:02:30     0:00:09     0:02:21      0.8706
[2022/12/29 01:22] | ------------------------------------------------------------
[2022/12/29 01:22] | VALID(045): [ 50/220] Batch: 0.0580 (0.0830) Data: 0.0319 (0.0534) Loss: 0.7789 (0.8464)
[2022/12/29 01:22] | VALID(045): [100/220] Batch: 0.0565 (0.0693) Data: 0.0303 (0.0411) Loss: 1.1145 (0.8697)
[2022/12/29 01:22] | VALID(045): [150/220] Batch: 0.0554 (0.0647) Data: 0.0268 (0.0360) Loss: 0.7555 (0.8614)
[2022/12/29 01:22] | VALID(045): [200/220] Batch: 0.0535 (0.0624) Data: 0.0305 (0.0339) Loss: 0.5054 (0.8692)
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] |    VALID(45)      0.8688      0.7347      0.4986      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:22] | ####################################################################################################
[2022/12/29 01:22] | TRAIN(046): [ 50/879] Batch: 0.1864 (0.2082) Data: 0.0076 (0.0442) Loss: 0.8712 (0.8953)
[2022/12/29 01:23] | TRAIN(046): [100/879] Batch: 0.1639 (0.1901) Data: 0.0073 (0.0266) Loss: 0.6198 (0.8852)
[2022/12/29 01:23] | TRAIN(046): [150/879] Batch: 0.1748 (0.1834) Data: 0.0078 (0.0209) Loss: 0.9755 (0.8778)
[2022/12/29 01:23] | TRAIN(046): [200/879] Batch: 0.1697 (0.1800) Data: 0.0071 (0.0180) Loss: 0.7453 (0.8801)
[2022/12/29 01:23] | TRAIN(046): [250/879] Batch: 0.1787 (0.1788) Data: 0.0165 (0.0162) Loss: 0.5794 (0.8781)
[2022/12/29 01:23] | TRAIN(046): [300/879] Batch: 0.1622 (0.1767) Data: 0.0072 (0.0148) Loss: 0.9547 (0.8735)
[2022/12/29 01:23] | TRAIN(046): [350/879] Batch: 0.1628 (0.1755) Data: 0.0076 (0.0139) Loss: 0.4785 (0.8724)
[2022/12/29 01:23] | TRAIN(046): [400/879] Batch: 0.1704 (0.1751) Data: 0.0097 (0.0132) Loss: 0.7171 (0.8726)
[2022/12/29 01:24] | TRAIN(046): [450/879] Batch: 0.1613 (0.1745) Data: 0.0070 (0.0127) Loss: 0.7559 (0.8708)
[2022/12/29 01:24] | TRAIN(046): [500/879] Batch: 0.1638 (0.1742) Data: 0.0078 (0.0122) Loss: 0.5775 (0.8717)
[2022/12/29 01:24] | TRAIN(046): [550/879] Batch: 0.1735 (0.1736) Data: 0.0077 (0.0119) Loss: 0.6557 (0.8734)
[2022/12/29 01:24] | TRAIN(046): [600/879] Batch: 0.1687 (0.1733) Data: 0.0089 (0.0116) Loss: 1.0126 (0.8746)
[2022/12/29 01:24] | TRAIN(046): [650/879] Batch: 0.1693 (0.1727) Data: 0.0087 (0.0113) Loss: 0.7592 (0.8724)
[2022/12/29 01:24] | TRAIN(046): [700/879] Batch: 0.1726 (0.1723) Data: 0.0078 (0.0110) Loss: 1.0048 (0.8722)
[2022/12/29 01:24] | TRAIN(046): [750/879] Batch: 0.1619 (0.1721) Data: 0.0090 (0.0108) Loss: 1.1349 (0.8711)
[2022/12/29 01:25] | TRAIN(046): [800/879] Batch: 0.1598 (0.1719) Data: 0.0077 (0.0107) Loss: 0.9101 (0.8719)
[2022/12/29 01:25] | TRAIN(046): [850/879] Batch: 0.1721 (0.1718) Data: 0.0072 (0.0105) Loss: 0.7657 (0.8707)
[2022/12/29 01:25] | ------------------------------------------------------------
[2022/12/29 01:25] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:25] | ------------------------------------------------------------
[2022/12/29 01:25] |    TRAIN(46)     0:02:30     0:00:09     0:02:21      0.8696
[2022/12/29 01:25] | ------------------------------------------------------------
[2022/12/29 01:25] | VALID(046): [ 50/220] Batch: 0.0560 (0.0825) Data: 0.0285 (0.0558) Loss: 0.7839 (0.8467)
[2022/12/29 01:25] | VALID(046): [100/220] Batch: 0.0582 (0.0688) Data: 0.0348 (0.0446) Loss: 1.1176 (0.8695)
[2022/12/29 01:25] | VALID(046): [150/220] Batch: 0.0531 (0.0643) Data: 0.0326 (0.0409) Loss: 0.7573 (0.8614)
[2022/12/29 01:25] | VALID(046): [200/220] Batch: 0.0564 (0.0621) Data: 0.0348 (0.0390) Loss: 0.5020 (0.8692)
[2022/12/29 01:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:25] |    VALID(46)      0.8691      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:25] | ####################################################################################################
[2022/12/29 01:25] | TRAIN(047): [ 50/879] Batch: 0.1906 (0.2111) Data: 0.0076 (0.0481) Loss: 0.8890 (0.8422)
[2022/12/29 01:25] | TRAIN(047): [100/879] Batch: 0.1702 (0.1920) Data: 0.0075 (0.0293) Loss: 1.0451 (0.8681)
[2022/12/29 01:25] | TRAIN(047): [150/879] Batch: 0.1679 (0.1860) Data: 0.0072 (0.0230) Loss: 1.0313 (0.8698)
[2022/12/29 01:26] | TRAIN(047): [200/879] Batch: 0.1655 (0.1833) Data: 0.0158 (0.0198) Loss: 0.8579 (0.8770)
[2022/12/29 01:26] | TRAIN(047): [250/879] Batch: 0.1932 (0.1816) Data: 0.0095 (0.0178) Loss: 1.0034 (0.8775)
[2022/12/29 01:26] | TRAIN(047): [300/879] Batch: 0.1616 (0.1802) Data: 0.0101 (0.0164) Loss: 0.6169 (0.8824)
[2022/12/29 01:26] | TRAIN(047): [350/879] Batch: 0.1766 (0.1790) Data: 0.0072 (0.0154) Loss: 0.7555 (0.8777)
[2022/12/29 01:26] | TRAIN(047): [400/879] Batch: 0.1739 (0.1778) Data: 0.0106 (0.0146) Loss: 0.5227 (0.8740)
[2022/12/29 01:26] | TRAIN(047): [450/879] Batch: 0.1782 (0.1769) Data: 0.0078 (0.0140) Loss: 0.6501 (0.8763)
[2022/12/29 01:26] | TRAIN(047): [500/879] Batch: 0.1724 (0.1766) Data: 0.0087 (0.0135) Loss: 0.4801 (0.8730)
[2022/12/29 01:27] | TRAIN(047): [550/879] Batch: 0.1816 (0.1764) Data: 0.0089 (0.0132) Loss: 0.9342 (0.8706)
[2022/12/29 01:27] | TRAIN(047): [600/879] Batch: 0.1683 (0.1759) Data: 0.0074 (0.0128) Loss: 0.6393 (0.8738)
[2022/12/29 01:27] | TRAIN(047): [650/879] Batch: 0.1631 (0.1753) Data: 0.0077 (0.0126) Loss: 0.8355 (0.8728)
[2022/12/29 01:27] | TRAIN(047): [700/879] Batch: 0.1672 (0.1749) Data: 0.0071 (0.0123) Loss: 0.5810 (0.8700)
[2022/12/29 01:27] | TRAIN(047): [750/879] Batch: 0.1755 (0.1745) Data: 0.0073 (0.0120) Loss: 1.0406 (0.8698)
[2022/12/29 01:27] | TRAIN(047): [800/879] Batch: 0.1678 (0.1743) Data: 0.0079 (0.0119) Loss: 0.6823 (0.8700)
[2022/12/29 01:27] | TRAIN(047): [850/879] Batch: 0.1627 (0.1741) Data: 0.0069 (0.0117) Loss: 0.9191 (0.8702)
[2022/12/29 01:28] | ------------------------------------------------------------
[2022/12/29 01:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:28] | ------------------------------------------------------------
[2022/12/29 01:28] |    TRAIN(47)     0:02:32     0:00:10     0:02:22      0.8692
[2022/12/29 01:28] | ------------------------------------------------------------
[2022/12/29 01:28] | VALID(047): [ 50/220] Batch: 0.0551 (0.0826) Data: 0.0344 (0.0597) Loss: 0.7788 (0.8461)
[2022/12/29 01:28] | VALID(047): [100/220] Batch: 0.0578 (0.0692) Data: 0.0303 (0.0455) Loss: 1.1130 (0.8692)
[2022/12/29 01:28] | VALID(047): [150/220] Batch: 0.0586 (0.0646) Data: 0.0359 (0.0402) Loss: 0.7561 (0.8610)
[2022/12/29 01:28] | VALID(047): [200/220] Batch: 0.0598 (0.0624) Data: 0.0303 (0.0384) Loss: 0.5089 (0.8687)
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] |    VALID(47)      0.8684      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:28] | ####################################################################################################
[2022/12/29 01:28] | TRAIN(048): [ 50/879] Batch: 0.1610 (0.2069) Data: 0.0079 (0.0450) Loss: 0.9033 (0.8413)
[2022/12/29 01:28] | TRAIN(048): [100/879] Batch: 0.1605 (0.1866) Data: 0.0085 (0.0270) Loss: 0.6290 (0.8583)
[2022/12/29 01:28] | TRAIN(048): [150/879] Batch: 0.1735 (0.1802) Data: 0.0097 (0.0208) Loss: 0.7391 (0.8568)
[2022/12/29 01:28] | TRAIN(048): [200/879] Batch: 0.1635 (0.1774) Data: 0.0085 (0.0177) Loss: 0.9696 (0.8711)
[2022/12/29 01:28] | TRAIN(048): [250/879] Batch: 0.1650 (0.1757) Data: 0.0074 (0.0159) Loss: 1.0025 (0.8737)
[2022/12/29 01:29] | TRAIN(048): [300/879] Batch: 0.1617 (0.1743) Data: 0.0083 (0.0146) Loss: 1.1133 (0.8757)
[2022/12/29 01:29] | TRAIN(048): [350/879] Batch: 0.1613 (0.1732) Data: 0.0070 (0.0136) Loss: 0.7549 (0.8768)
[2022/12/29 01:29] | TRAIN(048): [400/879] Batch: 0.1631 (0.1726) Data: 0.0100 (0.0130) Loss: 1.2358 (0.8727)
[2022/12/29 01:29] | TRAIN(048): [450/879] Batch: 0.1650 (0.1722) Data: 0.0098 (0.0125) Loss: 0.9463 (0.8733)
[2022/12/29 01:29] | TRAIN(048): [500/879] Batch: 0.1645 (0.1718) Data: 0.0072 (0.0121) Loss: 0.9497 (0.8698)
[2022/12/29 01:29] | TRAIN(048): [550/879] Batch: 0.1705 (0.1715) Data: 0.0075 (0.0117) Loss: 0.9425 (0.8678)
[2022/12/29 01:29] | TRAIN(048): [600/879] Batch: 0.1687 (0.1716) Data: 0.0081 (0.0115) Loss: 1.0464 (0.8690)
[2022/12/29 01:30] | TRAIN(048): [650/879] Batch: 0.1782 (0.1718) Data: 0.0075 (0.0112) Loss: 1.0873 (0.8684)
[2022/12/29 01:30] | TRAIN(048): [700/879] Batch: 0.1644 (0.1721) Data: 0.0080 (0.0110) Loss: 0.9889 (0.8674)
[2022/12/29 01:30] | TRAIN(048): [750/879] Batch: 0.1801 (0.1722) Data: 0.0093 (0.0109) Loss: 0.8738 (0.8692)
[2022/12/29 01:30] | TRAIN(048): [800/879] Batch: 0.1637 (0.1724) Data: 0.0077 (0.0107) Loss: 0.9013 (0.8681)
[2022/12/29 01:30] | TRAIN(048): [850/879] Batch: 0.1638 (0.1723) Data: 0.0074 (0.0106) Loss: 0.4091 (0.8681)
[2022/12/29 01:30] | ------------------------------------------------------------
[2022/12/29 01:30] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:30] | ------------------------------------------------------------
[2022/12/29 01:30] |    TRAIN(48)     0:02:31     0:00:09     0:02:21      0.8689
[2022/12/29 01:30] | ------------------------------------------------------------
[2022/12/29 01:30] | VALID(048): [ 50/220] Batch: 0.0560 (0.0815) Data: 0.0356 (0.0588) Loss: 0.7913 (0.8505)
[2022/12/29 01:30] | VALID(048): [100/220] Batch: 0.0542 (0.0682) Data: 0.0313 (0.0458) Loss: 1.1049 (0.8707)
[2022/12/29 01:30] | VALID(048): [150/220] Batch: 0.0566 (0.0638) Data: 0.0355 (0.0417) Loss: 0.7671 (0.8635)
[2022/12/29 01:30] | VALID(048): [200/220] Batch: 0.0538 (0.0616) Data: 0.0320 (0.0395) Loss: 0.5516 (0.8704)
[2022/12/29 01:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:31] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:31] |    VALID(48)      0.8705      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:31] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:31] | ####################################################################################################
[2022/12/29 01:31] | TRAIN(049): [ 50/879] Batch: 0.1836 (0.2086) Data: 0.0076 (0.0455) Loss: 0.9606 (0.9181)
[2022/12/29 01:31] | TRAIN(049): [100/879] Batch: 0.1621 (0.1900) Data: 0.0071 (0.0272) Loss: 0.8300 (0.9174)
[2022/12/29 01:31] | TRAIN(049): [150/879] Batch: 0.1615 (0.1821) Data: 0.0076 (0.0209) Loss: 0.7072 (0.9035)
[2022/12/29 01:31] | TRAIN(049): [200/879] Batch: 0.1638 (0.1779) Data: 0.0072 (0.0176) Loss: 0.7378 (0.8886)
[2022/12/29 01:31] | TRAIN(049): [250/879] Batch: 0.1623 (0.1765) Data: 0.0074 (0.0158) Loss: 0.6807 (0.8766)
[2022/12/29 01:31] | TRAIN(049): [300/879] Batch: 0.1682 (0.1748) Data: 0.0071 (0.0145) Loss: 1.0928 (0.8755)
[2022/12/29 01:32] | TRAIN(049): [350/879] Batch: 0.1674 (0.1737) Data: 0.0072 (0.0135) Loss: 1.0022 (0.8742)
[2022/12/29 01:32] | TRAIN(049): [400/879] Batch: 0.1610 (0.1730) Data: 0.0073 (0.0128) Loss: 0.7195 (0.8712)
[2022/12/29 01:32] | TRAIN(049): [450/879] Batch: 0.1604 (0.1722) Data: 0.0082 (0.0123) Loss: 1.0176 (0.8702)
[2022/12/29 01:32] | TRAIN(049): [500/879] Batch: 0.1663 (0.1715) Data: 0.0086 (0.0118) Loss: 0.8944 (0.8717)
[2022/12/29 01:32] | TRAIN(049): [550/879] Batch: 0.1822 (0.1718) Data: 0.0094 (0.0116) Loss: 0.8321 (0.8750)
[2022/12/29 01:32] | TRAIN(049): [600/879] Batch: 0.1656 (0.1722) Data: 0.0078 (0.0114) Loss: 0.6670 (0.8770)
[2022/12/29 01:32] | TRAIN(049): [650/879] Batch: 0.1601 (0.1716) Data: 0.0076 (0.0111) Loss: 0.9712 (0.8755)
[2022/12/29 01:33] | TRAIN(049): [700/879] Batch: 0.1636 (0.1712) Data: 0.0090 (0.0108) Loss: 0.9255 (0.8718)
[2022/12/29 01:33] | TRAIN(049): [750/879] Batch: 0.1698 (0.1709) Data: 0.0075 (0.0107) Loss: 0.8085 (0.8683)
[2022/12/29 01:33] | TRAIN(049): [800/879] Batch: 0.1633 (0.1707) Data: 0.0073 (0.0105) Loss: 0.9838 (0.8666)
[2022/12/29 01:33] | TRAIN(049): [850/879] Batch: 0.1649 (0.1704) Data: 0.0072 (0.0103) Loss: 0.8312 (0.8691)
[2022/12/29 01:33] | ------------------------------------------------------------
[2022/12/29 01:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:33] | ------------------------------------------------------------
[2022/12/29 01:33] |    TRAIN(49)     0:02:29     0:00:09     0:02:20      0.8686
[2022/12/29 01:33] | ------------------------------------------------------------
[2022/12/29 01:33] | VALID(049): [ 50/220] Batch: 0.0506 (0.0844) Data: 0.0307 (0.0590) Loss: 0.7785 (0.8465)
[2022/12/29 01:33] | VALID(049): [100/220] Batch: 0.0562 (0.0696) Data: 0.0327 (0.0458) Loss: 1.1080 (0.8693)
[2022/12/29 01:33] | VALID(049): [150/220] Batch: 0.0546 (0.0647) Data: 0.0364 (0.0415) Loss: 0.7583 (0.8612)
[2022/12/29 01:33] | VALID(049): [200/220] Batch: 0.0560 (0.0624) Data: 0.0274 (0.0383) Loss: 0.5186 (0.8689)
[2022/12/29 01:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:33] |    VALID(49)      0.8685      0.7347      0.4995      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:33] | ####################################################################################################
[2022/12/29 01:33] | TRAIN(050): [ 50/879] Batch: 0.1736 (0.2111) Data: 0.0076 (0.0493) Loss: 0.6335 (0.8938)
[2022/12/29 01:34] | TRAIN(050): [100/879] Batch: 0.1611 (0.1913) Data: 0.0087 (0.0299) Loss: 0.7421 (0.8753)
[2022/12/29 01:34] | TRAIN(050): [150/879] Batch: 0.1622 (0.1845) Data: 0.0072 (0.0234) Loss: 0.9266 (0.8724)
[2022/12/29 01:34] | TRAIN(050): [200/879] Batch: 0.1786 (0.1813) Data: 0.0085 (0.0200) Loss: 0.8859 (0.8713)
[2022/12/29 01:34] | TRAIN(050): [250/879] Batch: 0.1713 (0.1792) Data: 0.0073 (0.0179) Loss: 0.7174 (0.8764)
[2022/12/29 01:34] | TRAIN(050): [300/879] Batch: 0.1720 (0.1772) Data: 0.0085 (0.0164) Loss: 1.1237 (0.8778)
[2022/12/29 01:34] | TRAIN(050): [350/879] Batch: 0.1617 (0.1764) Data: 0.0069 (0.0154) Loss: 0.7859 (0.8769)
[2022/12/29 01:34] | TRAIN(050): [400/879] Batch: 0.1586 (0.1754) Data: 0.0075 (0.0145) Loss: 0.8625 (0.8735)
[2022/12/29 01:35] | TRAIN(050): [450/879] Batch: 0.1602 (0.1746) Data: 0.0070 (0.0138) Loss: 0.9379 (0.8753)
[2022/12/29 01:35] | TRAIN(050): [500/879] Batch: 0.1645 (0.1736) Data: 0.0085 (0.0132) Loss: 0.6350 (0.8790)
[2022/12/29 01:35] | TRAIN(050): [550/879] Batch: 0.1780 (0.1729) Data: 0.0072 (0.0127) Loss: 1.0740 (0.8747)
[2022/12/29 01:35] | TRAIN(050): [600/879] Batch: 0.1668 (0.1724) Data: 0.0072 (0.0123) Loss: 0.7412 (0.8753)
[2022/12/29 01:35] | TRAIN(050): [650/879] Batch: 0.1593 (0.1719) Data: 0.0077 (0.0119) Loss: 0.6741 (0.8742)
[2022/12/29 01:35] | TRAIN(050): [700/879] Batch: 0.1617 (0.1714) Data: 0.0083 (0.0116) Loss: 0.6654 (0.8729)
[2022/12/29 01:35] | TRAIN(050): [750/879] Batch: 0.1651 (0.1710) Data: 0.0075 (0.0113) Loss: 0.7855 (0.8694)
[2022/12/29 01:36] | TRAIN(050): [800/879] Batch: 0.1720 (0.1708) Data: 0.0072 (0.0111) Loss: 0.9505 (0.8687)
[2022/12/29 01:36] | TRAIN(050): [850/879] Batch: 0.1641 (0.1704) Data: 0.0074 (0.0109) Loss: 0.7321 (0.8689)
[2022/12/29 01:36] | ------------------------------------------------------------
[2022/12/29 01:36] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:36] | ------------------------------------------------------------
[2022/12/29 01:36] |    TRAIN(50)     0:02:29     0:00:09     0:02:20      0.8690
[2022/12/29 01:36] | ------------------------------------------------------------
[2022/12/29 01:36] | VALID(050): [ 50/220] Batch: 0.0532 (0.0825) Data: 0.0356 (0.0596) Loss: 0.7837 (0.8469)
[2022/12/29 01:36] | VALID(050): [100/220] Batch: 0.0545 (0.0688) Data: 0.0364 (0.0465) Loss: 1.1158 (0.8688)
[2022/12/29 01:36] | VALID(050): [150/220] Batch: 0.0561 (0.0643) Data: 0.0361 (0.0422) Loss: 0.7577 (0.8609)
[2022/12/29 01:36] | VALID(050): [200/220] Batch: 0.0563 (0.0620) Data: 0.0287 (0.0390) Loss: 0.5227 (0.8684)
[2022/12/29 01:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:36] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:36] |    VALID(50)      0.8684      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:36] | ####################################################################################################
[2022/12/29 01:36] | TRAIN(051): [ 50/879] Batch: 0.1665 (0.2071) Data: 0.0075 (0.0470) Loss: 0.9928 (0.8672)
[2022/12/29 01:36] | TRAIN(051): [100/879] Batch: 0.1805 (0.1887) Data: 0.0096 (0.0284) Loss: 1.2791 (0.8695)
[2022/12/29 01:36] | TRAIN(051): [150/879] Batch: 0.1658 (0.1809) Data: 0.0073 (0.0217) Loss: 0.6023 (0.8541)
[2022/12/29 01:37] | TRAIN(051): [200/879] Batch: 0.1634 (0.1777) Data: 0.0076 (0.0183) Loss: 1.1685 (0.8568)
[2022/12/29 01:37] | TRAIN(051): [250/879] Batch: 0.1647 (0.1755) Data: 0.0130 (0.0162) Loss: 0.8061 (0.8659)
[2022/12/29 01:37] | TRAIN(051): [300/879] Batch: 0.1669 (0.1741) Data: 0.0077 (0.0149) Loss: 0.8139 (0.8686)
[2022/12/29 01:37] | TRAIN(051): [350/879] Batch: 0.1618 (0.1730) Data: 0.0072 (0.0138) Loss: 1.2480 (0.8707)
[2022/12/29 01:37] | TRAIN(051): [400/879] Batch: 0.1665 (0.1720) Data: 0.0077 (0.0131) Loss: 0.7337 (0.8665)
[2022/12/29 01:37] | TRAIN(051): [450/879] Batch: 0.1647 (0.1715) Data: 0.0076 (0.0125) Loss: 1.1573 (0.8711)
[2022/12/29 01:37] | TRAIN(051): [500/879] Batch: 0.1640 (0.1710) Data: 0.0095 (0.0120) Loss: 0.6750 (0.8724)
[2022/12/29 01:38] | TRAIN(051): [550/879] Batch: 0.1638 (0.1706) Data: 0.0070 (0.0116) Loss: 0.7603 (0.8713)
[2022/12/29 01:38] | TRAIN(051): [600/879] Batch: 0.1593 (0.1704) Data: 0.0073 (0.0113) Loss: 1.0491 (0.8698)
[2022/12/29 01:38] | TRAIN(051): [650/879] Batch: 0.1678 (0.1700) Data: 0.0081 (0.0110) Loss: 0.8972 (0.8696)
[2022/12/29 01:38] | TRAIN(051): [700/879] Batch: 0.1663 (0.1696) Data: 0.0095 (0.0107) Loss: 0.7122 (0.8700)
[2022/12/29 01:38] | TRAIN(051): [750/879] Batch: 0.1677 (0.1693) Data: 0.0097 (0.0105) Loss: 1.3233 (0.8717)
[2022/12/29 01:38] | TRAIN(051): [800/879] Batch: 0.1621 (0.1691) Data: 0.0071 (0.0103) Loss: 0.8979 (0.8709)
[2022/12/29 01:38] | TRAIN(051): [850/879] Batch: 0.1848 (0.1689) Data: 0.0092 (0.0102) Loss: 0.7398 (0.8695)
[2022/12/29 01:38] | ------------------------------------------------------------
[2022/12/29 01:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:38] | ------------------------------------------------------------
[2022/12/29 01:38] |    TRAIN(51)     0:02:28     0:00:08     0:02:19      0.8686
[2022/12/29 01:38] | ------------------------------------------------------------
[2022/12/29 01:39] | VALID(051): [ 50/220] Batch: 0.0524 (0.0834) Data: 0.0328 (0.0593) Loss: 0.7809 (0.8466)
[2022/12/29 01:39] | VALID(051): [100/220] Batch: 0.0536 (0.0693) Data: 0.0335 (0.0462) Loss: 1.1174 (0.8698)
[2022/12/29 01:39] | VALID(051): [150/220] Batch: 0.0544 (0.0647) Data: 0.0258 (0.0403) Loss: 0.7560 (0.8616)
[2022/12/29 01:39] | VALID(051): [200/220] Batch: 0.0577 (0.0623) Data: 0.0294 (0.0372) Loss: 0.5003 (0.8695)
[2022/12/29 01:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:39] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:39] |    VALID(51)      0.8692      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:39] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:39] | ####################################################################################################
[2022/12/29 01:39] | TRAIN(052): [ 50/879] Batch: 0.1656 (0.2094) Data: 0.0149 (0.0486) Loss: 0.9865 (0.8978)
[2022/12/29 01:39] | TRAIN(052): [100/879] Batch: 0.1628 (0.1914) Data: 0.0071 (0.0301) Loss: 1.3078 (0.8791)
[2022/12/29 01:39] | TRAIN(052): [150/879] Batch: 0.1614 (0.1845) Data: 0.0160 (0.0234) Loss: 0.9959 (0.8691)
[2022/12/29 01:39] | TRAIN(052): [200/879] Batch: 0.1814 (0.1820) Data: 0.0180 (0.0198) Loss: 0.7324 (0.8611)
[2022/12/29 01:39] | TRAIN(052): [250/879] Batch: 0.1640 (0.1801) Data: 0.0074 (0.0178) Loss: 0.8230 (0.8644)
[2022/12/29 01:40] | TRAIN(052): [300/879] Batch: 0.1622 (0.1786) Data: 0.0147 (0.0165) Loss: 0.9174 (0.8646)
[2022/12/29 01:40] | TRAIN(052): [350/879] Batch: 0.1720 (0.1779) Data: 0.0078 (0.0154) Loss: 0.9755 (0.8669)
[2022/12/29 01:40] | TRAIN(052): [400/879] Batch: 0.1638 (0.1766) Data: 0.0081 (0.0146) Loss: 0.8866 (0.8674)
[2022/12/29 01:40] | TRAIN(052): [450/879] Batch: 0.1622 (0.1758) Data: 0.0083 (0.0140) Loss: 0.5069 (0.8648)
[2022/12/29 01:40] | TRAIN(052): [500/879] Batch: 0.1631 (0.1752) Data: 0.0103 (0.0136) Loss: 1.0039 (0.8657)
[2022/12/29 01:40] | TRAIN(052): [550/879] Batch: 0.1781 (0.1752) Data: 0.0079 (0.0133) Loss: 0.7788 (0.8634)
[2022/12/29 01:40] | TRAIN(052): [600/879] Batch: 0.1827 (0.1751) Data: 0.0102 (0.0130) Loss: 0.9153 (0.8654)
[2022/12/29 01:41] | TRAIN(052): [650/879] Batch: 0.1813 (0.1749) Data: 0.0098 (0.0127) Loss: 1.0801 (0.8699)
[2022/12/29 01:41] | TRAIN(052): [700/879] Batch: 0.1801 (0.1748) Data: 0.0094 (0.0124) Loss: 0.8466 (0.8707)
[2022/12/29 01:41] | TRAIN(052): [750/879] Batch: 0.1706 (0.1743) Data: 0.0076 (0.0121) Loss: 0.7669 (0.8701)
[2022/12/29 01:41] | TRAIN(052): [800/879] Batch: 0.1655 (0.1739) Data: 0.0076 (0.0119) Loss: 0.6410 (0.8702)
[2022/12/29 01:41] | TRAIN(052): [850/879] Batch: 0.1690 (0.1734) Data: 0.0073 (0.0117) Loss: 0.7134 (0.8688)
[2022/12/29 01:41] | ------------------------------------------------------------
[2022/12/29 01:41] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:41] | ------------------------------------------------------------
[2022/12/29 01:41] |    TRAIN(52)     0:02:32     0:00:10     0:02:21      0.8687
[2022/12/29 01:41] | ------------------------------------------------------------
[2022/12/29 01:41] | VALID(052): [ 50/220] Batch: 0.0557 (0.0838) Data: 0.0365 (0.0589) Loss: 0.7805 (0.8464)
[2022/12/29 01:41] | VALID(052): [100/220] Batch: 0.0527 (0.0695) Data: 0.0366 (0.0460) Loss: 1.1135 (0.8687)
[2022/12/29 01:41] | VALID(052): [150/220] Batch: 0.0546 (0.0648) Data: 0.0315 (0.0418) Loss: 0.7578 (0.8609)
[2022/12/29 01:41] | VALID(052): [200/220] Batch: 0.0568 (0.0624) Data: 0.0345 (0.0398) Loss: 0.5179 (0.8685)
[2022/12/29 01:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:41] |    VALID(52)      0.8683      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:41] | ####################################################################################################
[2022/12/29 01:42] | TRAIN(053): [ 50/879] Batch: 0.1659 (0.2031) Data: 0.0075 (0.0446) Loss: 0.9165 (0.8571)
[2022/12/29 01:42] | TRAIN(053): [100/879] Batch: 0.1730 (0.1855) Data: 0.0076 (0.0265) Loss: 0.7381 (0.8436)
[2022/12/29 01:42] | TRAIN(053): [150/879] Batch: 0.1614 (0.1788) Data: 0.0097 (0.0203) Loss: 0.7241 (0.8474)
[2022/12/29 01:42] | TRAIN(053): [200/879] Batch: 0.1734 (0.1759) Data: 0.0075 (0.0173) Loss: 0.9856 (0.8475)
[2022/12/29 01:42] | TRAIN(053): [250/879] Batch: 0.1729 (0.1747) Data: 0.0079 (0.0155) Loss: 1.2494 (0.8522)
[2022/12/29 01:42] | TRAIN(053): [300/879] Batch: 0.1631 (0.1735) Data: 0.0077 (0.0142) Loss: 0.9223 (0.8558)
[2022/12/29 01:42] | TRAIN(053): [350/879] Batch: 0.1609 (0.1730) Data: 0.0071 (0.0134) Loss: 0.9408 (0.8579)
[2022/12/29 01:43] | TRAIN(053): [400/879] Batch: 0.1674 (0.1719) Data: 0.0078 (0.0127) Loss: 0.7112 (0.8614)
[2022/12/29 01:43] | TRAIN(053): [450/879] Batch: 0.1660 (0.1716) Data: 0.0075 (0.0122) Loss: 0.6267 (0.8602)
[2022/12/29 01:43] | TRAIN(053): [500/879] Batch: 0.1638 (0.1712) Data: 0.0079 (0.0118) Loss: 0.8151 (0.8658)
[2022/12/29 01:43] | TRAIN(053): [550/879] Batch: 0.1663 (0.1708) Data: 0.0075 (0.0114) Loss: 0.8536 (0.8686)
[2022/12/29 01:43] | TRAIN(053): [600/879] Batch: 0.1719 (0.1705) Data: 0.0073 (0.0111) Loss: 0.6075 (0.8665)
[2022/12/29 01:43] | TRAIN(053): [650/879] Batch: 0.1703 (0.1701) Data: 0.0076 (0.0109) Loss: 0.6941 (0.8664)
[2022/12/29 01:43] | TRAIN(053): [700/879] Batch: 0.1602 (0.1698) Data: 0.0075 (0.0106) Loss: 0.8172 (0.8666)
[2022/12/29 01:44] | TRAIN(053): [750/879] Batch: 0.1659 (0.1695) Data: 0.0076 (0.0105) Loss: 0.7009 (0.8678)
[2022/12/29 01:44] | TRAIN(053): [800/879] Batch: 0.1667 (0.1693) Data: 0.0075 (0.0103) Loss: 0.7194 (0.8685)
[2022/12/29 01:44] | TRAIN(053): [850/879] Batch: 0.1698 (0.1692) Data: 0.0073 (0.0101) Loss: 0.6533 (0.8687)
[2022/12/29 01:44] | ------------------------------------------------------------
[2022/12/29 01:44] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:44] | ------------------------------------------------------------
[2022/12/29 01:44] |    TRAIN(53)     0:02:28     0:00:08     0:02:19      0.8684
[2022/12/29 01:44] | ------------------------------------------------------------
[2022/12/29 01:44] | VALID(053): [ 50/220] Batch: 0.0572 (0.0806) Data: 0.0336 (0.0547) Loss: 0.7838 (0.8469)
[2022/12/29 01:44] | VALID(053): [100/220] Batch: 0.0532 (0.0677) Data: 0.0358 (0.0438) Loss: 1.1058 (0.8685)
[2022/12/29 01:44] | VALID(053): [150/220] Batch: 0.0552 (0.0635) Data: 0.0313 (0.0400) Loss: 0.7609 (0.8609)
[2022/12/29 01:44] | VALID(053): [200/220] Batch: 0.0571 (0.0613) Data: 0.0340 (0.0382) Loss: 0.5266 (0.8683)
[2022/12/29 01:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:44] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:44] |    VALID(53)      0.8681      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:44] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:44] | ####################################################################################################
[2022/12/29 01:44] | TRAIN(054): [ 50/879] Batch: 0.1834 (0.2097) Data: 0.0109 (0.0440) Loss: 0.7253 (0.8114)
[2022/12/29 01:44] | TRAIN(054): [100/879] Batch: 0.1608 (0.1882) Data: 0.0086 (0.0268) Loss: 0.7920 (0.8468)
[2022/12/29 01:45] | TRAIN(054): [150/879] Batch: 0.1606 (0.1812) Data: 0.0169 (0.0208) Loss: 0.5543 (0.8475)
[2022/12/29 01:45] | TRAIN(054): [200/879] Batch: 0.1825 (0.1780) Data: 0.0076 (0.0178) Loss: 1.1948 (0.8459)
[2022/12/29 01:45] | TRAIN(054): [250/879] Batch: 0.1698 (0.1764) Data: 0.0082 (0.0160) Loss: 0.9866 (0.8587)
[2022/12/29 01:45] | TRAIN(054): [300/879] Batch: 0.1640 (0.1749) Data: 0.0076 (0.0147) Loss: 0.6186 (0.8567)
[2022/12/29 01:45] | TRAIN(054): [350/879] Batch: 0.1632 (0.1734) Data: 0.0078 (0.0137) Loss: 0.6090 (0.8632)
[2022/12/29 01:45] | TRAIN(054): [400/879] Batch: 0.1623 (0.1724) Data: 0.0076 (0.0130) Loss: 1.3342 (0.8633)
[2022/12/29 01:45] | TRAIN(054): [450/879] Batch: 0.1638 (0.1719) Data: 0.0085 (0.0125) Loss: 0.8912 (0.8663)
[2022/12/29 01:46] | TRAIN(054): [500/879] Batch: 0.1708 (0.1714) Data: 0.0079 (0.0120) Loss: 0.8638 (0.8694)
[2022/12/29 01:46] | TRAIN(054): [550/879] Batch: 0.1727 (0.1709) Data: 0.0092 (0.0117) Loss: 1.0121 (0.8691)
[2022/12/29 01:46] | TRAIN(054): [600/879] Batch: 0.1638 (0.1703) Data: 0.0074 (0.0114) Loss: 0.9209 (0.8684)
[2022/12/29 01:46] | TRAIN(054): [650/879] Batch: 0.1644 (0.1699) Data: 0.0083 (0.0111) Loss: 0.8837 (0.8697)
[2022/12/29 01:46] | TRAIN(054): [700/879] Batch: 0.1613 (0.1696) Data: 0.0072 (0.0108) Loss: 1.0700 (0.8712)
[2022/12/29 01:46] | TRAIN(054): [750/879] Batch: 0.1599 (0.1692) Data: 0.0075 (0.0106) Loss: 0.9372 (0.8719)
[2022/12/29 01:46] | TRAIN(054): [800/879] Batch: 0.1619 (0.1690) Data: 0.0076 (0.0104) Loss: 0.7726 (0.8710)
[2022/12/29 01:47] | TRAIN(054): [850/879] Batch: 0.1704 (0.1688) Data: 0.0081 (0.0103) Loss: 0.9448 (0.8705)
[2022/12/29 01:47] | ------------------------------------------------------------
[2022/12/29 01:47] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:47] | ------------------------------------------------------------
[2022/12/29 01:47] |    TRAIN(54)     0:02:28     0:00:08     0:02:19      0.8686
[2022/12/29 01:47] | ------------------------------------------------------------
[2022/12/29 01:47] | VALID(054): [ 50/220] Batch: 0.0537 (0.0832) Data: 0.0317 (0.0582) Loss: 0.7789 (0.8466)
[2022/12/29 01:47] | VALID(054): [100/220] Batch: 0.0557 (0.0690) Data: 0.0319 (0.0452) Loss: 1.1141 (0.8701)
[2022/12/29 01:47] | VALID(054): [150/220] Batch: 0.0581 (0.0643) Data: 0.0342 (0.0410) Loss: 0.7566 (0.8618)
[2022/12/29 01:47] | VALID(054): [200/220] Batch: 0.0525 (0.0619) Data: 0.0367 (0.0389) Loss: 0.5014 (0.8697)
[2022/12/29 01:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:47] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:47] |    VALID(54)      0.8693      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:47] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:47] | ####################################################################################################
[2022/12/29 01:47] | TRAIN(055): [ 50/879] Batch: 0.1811 (0.2091) Data: 0.0093 (0.0459) Loss: 0.8054 (0.8474)
[2022/12/29 01:47] | TRAIN(055): [100/879] Batch: 0.1647 (0.1899) Data: 0.0114 (0.0280) Loss: 0.9474 (0.8548)
[2022/12/29 01:47] | TRAIN(055): [150/879] Batch: 0.1736 (0.1829) Data: 0.0108 (0.0221) Loss: 0.8157 (0.8609)
[2022/12/29 01:47] | TRAIN(055): [200/879] Batch: 0.1615 (0.1802) Data: 0.0085 (0.0190) Loss: 0.6549 (0.8712)
[2022/12/29 01:48] | TRAIN(055): [250/879] Batch: 0.1745 (0.1779) Data: 0.0080 (0.0171) Loss: 0.8302 (0.8699)
[2022/12/29 01:48] | TRAIN(055): [300/879] Batch: 0.1802 (0.1769) Data: 0.0169 (0.0158) Loss: 0.8122 (0.8784)
[2022/12/29 01:48] | TRAIN(055): [350/879] Batch: 0.1616 (0.1765) Data: 0.0077 (0.0150) Loss: 0.8229 (0.8724)
[2022/12/29 01:48] | TRAIN(055): [400/879] Batch: 0.1807 (0.1755) Data: 0.0077 (0.0142) Loss: 0.9731 (0.8680)
[2022/12/29 01:48] | TRAIN(055): [450/879] Batch: 0.1850 (0.1749) Data: 0.0098 (0.0136) Loss: 0.6502 (0.8674)
[2022/12/29 01:48] | TRAIN(055): [500/879] Batch: 0.1703 (0.1743) Data: 0.0178 (0.0132) Loss: 0.6514 (0.8656)
[2022/12/29 01:48] | TRAIN(055): [550/879] Batch: 0.1861 (0.1741) Data: 0.0098 (0.0128) Loss: 0.7762 (0.8656)
[2022/12/29 01:49] | TRAIN(055): [600/879] Batch: 0.1716 (0.1739) Data: 0.0072 (0.0124) Loss: 0.9928 (0.8639)
[2022/12/29 01:49] | TRAIN(055): [650/879] Batch: 0.1801 (0.1740) Data: 0.0094 (0.0121) Loss: 0.9048 (0.8664)
[2022/12/29 01:49] | TRAIN(055): [700/879] Batch: 0.1601 (0.1735) Data: 0.0075 (0.0119) Loss: 0.6575 (0.8719)
[2022/12/29 01:49] | TRAIN(055): [750/879] Batch: 0.1657 (0.1730) Data: 0.0087 (0.0116) Loss: 0.8372 (0.8717)
[2022/12/29 01:49] | TRAIN(055): [800/879] Batch: 0.1708 (0.1729) Data: 0.0072 (0.0115) Loss: 1.0335 (0.8701)
[2022/12/29 01:49] | TRAIN(055): [850/879] Batch: 0.1685 (0.1727) Data: 0.0078 (0.0113) Loss: 0.8254 (0.8689)
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] |    TRAIN(55)     0:02:31     0:00:09     0:02:21      0.8684
[2022/12/29 01:49] | ------------------------------------------------------------
[2022/12/29 01:49] | VALID(055): [ 50/220] Batch: 0.0566 (0.0836) Data: 0.0374 (0.0603) Loss: 0.7794 (0.8461)
[2022/12/29 01:49] | VALID(055): [100/220] Batch: 0.0528 (0.0694) Data: 0.0369 (0.0465) Loss: 1.1138 (0.8688)
[2022/12/29 01:50] | VALID(055): [150/220] Batch: 0.0565 (0.0647) Data: 0.0326 (0.0417) Loss: 0.7556 (0.8606)
[2022/12/29 01:50] | VALID(055): [200/220] Batch: 0.0560 (0.0623) Data: 0.0338 (0.0395) Loss: 0.5165 (0.8682)
[2022/12/29 01:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:50] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:50] |    VALID(55)      0.8680      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:50] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:50] | ####################################################################################################
[2022/12/29 01:50] | TRAIN(056): [ 50/879] Batch: 0.1803 (0.2107) Data: 0.0074 (0.0461) Loss: 0.8971 (0.8543)
[2022/12/29 01:50] | TRAIN(056): [100/879] Batch: 0.1598 (0.1924) Data: 0.0073 (0.0271) Loss: 0.5837 (0.8636)
[2022/12/29 01:50] | TRAIN(056): [150/879] Batch: 0.1622 (0.1832) Data: 0.0075 (0.0207) Loss: 0.9367 (0.8643)
[2022/12/29 01:50] | TRAIN(056): [200/879] Batch: 0.1611 (0.1788) Data: 0.0083 (0.0176) Loss: 1.2120 (0.8707)
[2022/12/29 01:50] | TRAIN(056): [250/879] Batch: 0.1634 (0.1766) Data: 0.0077 (0.0156) Loss: 0.8317 (0.8707)
[2022/12/29 01:50] | TRAIN(056): [300/879] Batch: 0.1641 (0.1749) Data: 0.0075 (0.0144) Loss: 0.6439 (0.8776)
[2022/12/29 01:51] | TRAIN(056): [350/879] Batch: 0.1653 (0.1736) Data: 0.0076 (0.0135) Loss: 0.8196 (0.8740)
[2022/12/29 01:51] | TRAIN(056): [400/879] Batch: 0.1638 (0.1727) Data: 0.0076 (0.0128) Loss: 1.0208 (0.8698)
[2022/12/29 01:51] | TRAIN(056): [450/879] Batch: 0.1633 (0.1719) Data: 0.0074 (0.0123) Loss: 0.7113 (0.8694)
[2022/12/29 01:51] | TRAIN(056): [500/879] Batch: 0.1686 (0.1717) Data: 0.0076 (0.0119) Loss: 1.1965 (0.8721)
[2022/12/29 01:51] | TRAIN(056): [550/879] Batch: 0.1637 (0.1715) Data: 0.0074 (0.0115) Loss: 0.7887 (0.8718)
[2022/12/29 01:51] | TRAIN(056): [600/879] Batch: 0.1817 (0.1712) Data: 0.0146 (0.0113) Loss: 1.1899 (0.8730)
[2022/12/29 01:51] | TRAIN(056): [650/879] Batch: 0.1661 (0.1709) Data: 0.0071 (0.0110) Loss: 0.8343 (0.8740)
[2022/12/29 01:52] | TRAIN(056): [700/879] Batch: 0.1724 (0.1706) Data: 0.0070 (0.0108) Loss: 0.9916 (0.8735)
[2022/12/29 01:52] | TRAIN(056): [750/879] Batch: 0.1640 (0.1701) Data: 0.0080 (0.0106) Loss: 1.0834 (0.8700)
[2022/12/29 01:52] | TRAIN(056): [800/879] Batch: 0.1637 (0.1699) Data: 0.0072 (0.0104) Loss: 0.7707 (0.8684)
[2022/12/29 01:52] | TRAIN(056): [850/879] Batch: 0.1623 (0.1695) Data: 0.0076 (0.0102) Loss: 1.0301 (0.8676)
[2022/12/29 01:52] | ------------------------------------------------------------
[2022/12/29 01:52] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:52] | ------------------------------------------------------------
[2022/12/29 01:52] |    TRAIN(56)     0:02:28     0:00:08     0:02:19      0.8685
[2022/12/29 01:52] | ------------------------------------------------------------
[2022/12/29 01:52] | VALID(056): [ 50/220] Batch: 0.0540 (0.0829) Data: 0.0307 (0.0592) Loss: 0.7853 (0.8468)
[2022/12/29 01:52] | VALID(056): [100/220] Batch: 0.0535 (0.0690) Data: 0.0357 (0.0461) Loss: 1.1057 (0.8684)
[2022/12/29 01:52] | VALID(056): [150/220] Batch: 0.0533 (0.0644) Data: 0.0266 (0.0411) Loss: 0.7608 (0.8607)
[2022/12/29 01:52] | VALID(056): [200/220] Batch: 0.0494 (0.0622) Data: 0.0250 (0.0378) Loss: 0.5246 (0.8681)
[2022/12/29 01:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:52] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:52] |    VALID(56)      0.8680      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:52] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:52] | ####################################################################################################
[2022/12/29 01:52] | TRAIN(057): [ 50/879] Batch: 0.1715 (0.2063) Data: 0.0076 (0.0461) Loss: 0.7994 (0.8501)
[2022/12/29 01:53] | TRAIN(057): [100/879] Batch: 0.1609 (0.1889) Data: 0.0075 (0.0283) Loss: 0.7228 (0.8572)
[2022/12/29 01:53] | TRAIN(057): [150/879] Batch: 0.1633 (0.1838) Data: 0.0077 (0.0222) Loss: 0.5663 (0.8589)
[2022/12/29 01:53] | TRAIN(057): [200/879] Batch: 0.1623 (0.1807) Data: 0.0089 (0.0190) Loss: 0.7929 (0.8593)
[2022/12/29 01:53] | TRAIN(057): [250/879] Batch: 0.1600 (0.1783) Data: 0.0072 (0.0170) Loss: 0.9210 (0.8571)
[2022/12/29 01:53] | TRAIN(057): [300/879] Batch: 0.1597 (0.1779) Data: 0.0073 (0.0155) Loss: 1.1334 (0.8640)
[2022/12/29 01:53] | TRAIN(057): [350/879] Batch: 0.1708 (0.1762) Data: 0.0075 (0.0145) Loss: 0.7250 (0.8677)
[2022/12/29 01:53] | TRAIN(057): [400/879] Batch: 0.1753 (0.1751) Data: 0.0059 (0.0137) Loss: 0.6328 (0.8639)
[2022/12/29 01:54] | TRAIN(057): [450/879] Batch: 0.1605 (0.1742) Data: 0.0076 (0.0131) Loss: 0.6794 (0.8663)
[2022/12/29 01:54] | TRAIN(057): [500/879] Batch: 0.1716 (0.1732) Data: 0.0076 (0.0125) Loss: 0.8055 (0.8694)
[2022/12/29 01:54] | TRAIN(057): [550/879] Batch: 0.1626 (0.1724) Data: 0.0074 (0.0121) Loss: 0.8704 (0.8695)
[2022/12/29 01:54] | TRAIN(057): [600/879] Batch: 0.1617 (0.1718) Data: 0.0076 (0.0117) Loss: 0.7965 (0.8683)
[2022/12/29 01:54] | TRAIN(057): [650/879] Batch: 0.1613 (0.1713) Data: 0.0071 (0.0114) Loss: 0.7241 (0.8662)
[2022/12/29 01:54] | TRAIN(057): [700/879] Batch: 0.1671 (0.1711) Data: 0.0075 (0.0112) Loss: 0.7218 (0.8685)
[2022/12/29 01:54] | TRAIN(057): [750/879] Batch: 0.1640 (0.1708) Data: 0.0077 (0.0110) Loss: 0.8828 (0.8669)
[2022/12/29 01:55] | TRAIN(057): [800/879] Batch: 0.1601 (0.1707) Data: 0.0098 (0.0109) Loss: 0.9425 (0.8688)
[2022/12/29 01:55] | TRAIN(057): [850/879] Batch: 0.1628 (0.1704) Data: 0.0077 (0.0107) Loss: 0.5872 (0.8693)
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] |    TRAIN(57)     0:02:29     0:00:09     0:02:20      0.8684
[2022/12/29 01:55] | ------------------------------------------------------------
[2022/12/29 01:55] | VALID(057): [ 50/220] Batch: 0.0542 (0.0824) Data: 0.0383 (0.0593) Loss: 0.7815 (0.8461)
[2022/12/29 01:55] | VALID(057): [100/220] Batch: 0.0525 (0.0688) Data: 0.0368 (0.0464) Loss: 1.1090 (0.8686)
[2022/12/29 01:55] | VALID(057): [150/220] Batch: 0.0533 (0.0643) Data: 0.0316 (0.0414) Loss: 0.7582 (0.8606)
[2022/12/29 01:55] | VALID(057): [200/220] Batch: 0.0555 (0.0621) Data: 0.0347 (0.0393) Loss: 0.5133 (0.8683)
[2022/12/29 01:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:55] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:55] |    VALID(57)      0.8680      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:55] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:55] | ####################################################################################################
[2022/12/29 01:55] | TRAIN(058): [ 50/879] Batch: 0.1925 (0.2108) Data: 0.0092 (0.0474) Loss: 0.5505 (0.8727)
[2022/12/29 01:55] | TRAIN(058): [100/879] Batch: 0.1597 (0.1935) Data: 0.0071 (0.0295) Loss: 1.0401 (0.9046)
[2022/12/29 01:55] | TRAIN(058): [150/879] Batch: 0.1619 (0.1869) Data: 0.0081 (0.0229) Loss: 0.9597 (0.8969)
[2022/12/29 01:56] | TRAIN(058): [200/879] Batch: 0.1644 (0.1835) Data: 0.0078 (0.0196) Loss: 0.8896 (0.8871)
[2022/12/29 01:56] | TRAIN(058): [250/879] Batch: 0.1829 (0.1806) Data: 0.0074 (0.0177) Loss: 0.9620 (0.8846)
[2022/12/29 01:56] | TRAIN(058): [300/879] Batch: 0.1622 (0.1786) Data: 0.0076 (0.0162) Loss: 0.9790 (0.8825)
[2022/12/29 01:56] | TRAIN(058): [350/879] Batch: 0.1698 (0.1771) Data: 0.0164 (0.0152) Loss: 1.0352 (0.8799)
[2022/12/29 01:56] | TRAIN(058): [400/879] Batch: 0.1648 (0.1757) Data: 0.0076 (0.0143) Loss: 0.8096 (0.8787)
[2022/12/29 01:56] | TRAIN(058): [450/879] Batch: 0.1881 (0.1749) Data: 0.0132 (0.0138) Loss: 0.6392 (0.8769)
[2022/12/29 01:56] | TRAIN(058): [500/879] Batch: 0.1724 (0.1746) Data: 0.0067 (0.0133) Loss: 0.8335 (0.8755)
[2022/12/29 01:57] | TRAIN(058): [550/879] Batch: 0.1663 (0.1738) Data: 0.0079 (0.0129) Loss: 0.9395 (0.8765)
[2022/12/29 01:57] | TRAIN(058): [600/879] Batch: 0.1704 (0.1735) Data: 0.0086 (0.0125) Loss: 0.6512 (0.8717)
[2022/12/29 01:57] | TRAIN(058): [650/879] Batch: 0.1733 (0.1730) Data: 0.0100 (0.0122) Loss: 1.0809 (0.8712)
[2022/12/29 01:57] | TRAIN(058): [700/879] Batch: 0.1667 (0.1728) Data: 0.0089 (0.0120) Loss: 0.6815 (0.8676)
[2022/12/29 01:57] | TRAIN(058): [750/879] Batch: 0.1621 (0.1723) Data: 0.0094 (0.0118) Loss: 0.8588 (0.8666)
[2022/12/29 01:57] | TRAIN(058): [800/879] Batch: 0.1649 (0.1721) Data: 0.0078 (0.0116) Loss: 0.8211 (0.8685)
[2022/12/29 01:57] | TRAIN(058): [850/879] Batch: 0.1643 (0.1718) Data: 0.0071 (0.0115) Loss: 0.7320 (0.8678)
[2022/12/29 01:58] | ------------------------------------------------------------
[2022/12/29 01:58] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 01:58] | ------------------------------------------------------------
[2022/12/29 01:58] |    TRAIN(58)     0:02:30     0:00:09     0:02:20      0.8681
[2022/12/29 01:58] | ------------------------------------------------------------
[2022/12/29 01:58] | VALID(058): [ 50/220] Batch: 0.0554 (0.0821) Data: 0.0345 (0.0595) Loss: 0.7849 (0.8466)
[2022/12/29 01:58] | VALID(058): [100/220] Batch: 0.0558 (0.0686) Data: 0.0304 (0.0461) Loss: 1.1088 (0.8685)
[2022/12/29 01:58] | VALID(058): [150/220] Batch: 0.0527 (0.0641) Data: 0.0299 (0.0396) Loss: 0.7598 (0.8608)
[2022/12/29 01:58] | VALID(058): [200/220] Batch: 0.0566 (0.0619) Data: 0.0296 (0.0365) Loss: 0.5181 (0.8683)
[2022/12/29 01:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:58] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 01:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:58] |    VALID(58)      0.8681      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 01:58] | ------------------------------------------------------------------------------------------------
[2022/12/29 01:58] | ####################################################################################################
[2022/12/29 01:58] | TRAIN(059): [ 50/879] Batch: 0.1655 (0.2072) Data: 0.0076 (0.0463) Loss: 0.6986 (0.8317)
[2022/12/29 01:58] | TRAIN(059): [100/879] Batch: 0.1796 (0.1892) Data: 0.0096 (0.0285) Loss: 0.5027 (0.8444)
[2022/12/29 01:58] | TRAIN(059): [150/879] Batch: 0.1621 (0.1834) Data: 0.0073 (0.0227) Loss: 0.9287 (0.8470)
[2022/12/29 01:58] | TRAIN(059): [200/879] Batch: 0.1639 (0.1807) Data: 0.0113 (0.0196) Loss: 0.8518 (0.8545)
[2022/12/29 01:59] | TRAIN(059): [250/879] Batch: 0.1745 (0.1783) Data: 0.0072 (0.0176) Loss: 0.8471 (0.8585)
[2022/12/29 01:59] | TRAIN(059): [300/879] Batch: 0.1604 (0.1769) Data: 0.0070 (0.0163) Loss: 1.2861 (0.8657)
[2022/12/29 01:59] | TRAIN(059): [350/879] Batch: 0.1602 (0.1757) Data: 0.0075 (0.0153) Loss: 0.5800 (0.8649)
[2022/12/29 01:59] | TRAIN(059): [400/879] Batch: 0.1818 (0.1746) Data: 0.0076 (0.0144) Loss: 0.8377 (0.8664)
[2022/12/29 01:59] | TRAIN(059): [450/879] Batch: 0.1754 (0.1740) Data: 0.0078 (0.0138) Loss: 1.2055 (0.8644)
[2022/12/29 01:59] | TRAIN(059): [500/879] Batch: 0.1792 (0.1739) Data: 0.0097 (0.0133) Loss: 0.7905 (0.8611)
[2022/12/29 01:59] | TRAIN(059): [550/879] Batch: 0.1636 (0.1734) Data: 0.0078 (0.0129) Loss: 1.0437 (0.8635)
[2022/12/29 02:00] | TRAIN(059): [600/879] Batch: 0.1754 (0.1729) Data: 0.0073 (0.0125) Loss: 0.9261 (0.8639)
[2022/12/29 02:00] | TRAIN(059): [650/879] Batch: 0.1704 (0.1728) Data: 0.0093 (0.0122) Loss: 0.7517 (0.8668)
[2022/12/29 02:00] | TRAIN(059): [700/879] Batch: 0.1635 (0.1724) Data: 0.0085 (0.0119) Loss: 0.8319 (0.8692)
[2022/12/29 02:00] | TRAIN(059): [750/879] Batch: 0.1835 (0.1721) Data: 0.0104 (0.0117) Loss: 0.6269 (0.8693)
[2022/12/29 02:00] | TRAIN(059): [800/879] Batch: 0.1769 (0.1720) Data: 0.0109 (0.0115) Loss: 0.7030 (0.8680)
[2022/12/29 02:00] | TRAIN(059): [850/879] Batch: 0.1623 (0.1716) Data: 0.0082 (0.0113) Loss: 1.1202 (0.8673)
[2022/12/29 02:00] | ------------------------------------------------------------
[2022/12/29 02:00] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:00] | ------------------------------------------------------------
[2022/12/29 02:00] |    TRAIN(59)     0:02:30     0:00:09     0:02:20      0.8681
[2022/12/29 02:00] | ------------------------------------------------------------
[2022/12/29 02:00] | VALID(059): [ 50/220] Batch: 0.0558 (0.0810) Data: 0.0345 (0.0577) Loss: 0.7842 (0.8468)
[2022/12/29 02:00] | VALID(059): [100/220] Batch: 0.0538 (0.0678) Data: 0.0310 (0.0454) Loss: 1.1034 (0.8687)
[2022/12/29 02:00] | VALID(059): [150/220] Batch: 0.0688 (0.0635) Data: 0.0201 (0.0398) Loss: 0.7609 (0.8609)
[2022/12/29 02:00] | VALID(059): [200/220] Batch: 0.0545 (0.0612) Data: 0.0318 (0.0380) Loss: 0.5240 (0.8683)
[2022/12/29 02:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:01] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:01] |    VALID(59)      0.8681      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:01] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:01] | ####################################################################################################
[2022/12/29 02:01] | TRAIN(060): [ 50/879] Batch: 0.1606 (0.2016) Data: 0.0074 (0.0439) Loss: 0.7827 (0.8739)
[2022/12/29 02:01] | TRAIN(060): [100/879] Batch: 0.1754 (0.1859) Data: 0.0084 (0.0266) Loss: 0.8426 (0.8685)
[2022/12/29 02:01] | TRAIN(060): [150/879] Batch: 0.1837 (0.1802) Data: 0.0075 (0.0207) Loss: 0.8492 (0.8804)
[2022/12/29 02:01] | TRAIN(060): [200/879] Batch: 0.1672 (0.1780) Data: 0.0076 (0.0176) Loss: 1.0369 (0.8803)
[2022/12/29 02:01] | TRAIN(060): [250/879] Batch: 0.1704 (0.1766) Data: 0.0089 (0.0158) Loss: 1.0252 (0.8804)
[2022/12/29 02:01] | TRAIN(060): [300/879] Batch: 0.1822 (0.1752) Data: 0.0094 (0.0147) Loss: 0.9750 (0.8891)
[2022/12/29 02:02] | TRAIN(060): [350/879] Batch: 0.1690 (0.1740) Data: 0.0071 (0.0139) Loss: 0.8301 (0.8770)
[2022/12/29 02:02] | TRAIN(060): [400/879] Batch: 0.1793 (0.1739) Data: 0.0074 (0.0132) Loss: 0.6712 (0.8755)
[2022/12/29 02:02] | TRAIN(060): [450/879] Batch: 0.1627 (0.1736) Data: 0.0079 (0.0128) Loss: 0.7217 (0.8708)
[2022/12/29 02:02] | TRAIN(060): [500/879] Batch: 0.1951 (0.1734) Data: 0.0079 (0.0124) Loss: 0.7034 (0.8732)
[2022/12/29 02:02] | TRAIN(060): [550/879] Batch: 0.1640 (0.1730) Data: 0.0098 (0.0120) Loss: 0.9275 (0.8754)
[2022/12/29 02:02] | TRAIN(060): [600/879] Batch: 0.1776 (0.1726) Data: 0.0087 (0.0117) Loss: 0.8080 (0.8714)
[2022/12/29 02:02] | TRAIN(060): [650/879] Batch: 0.1605 (0.1719) Data: 0.0073 (0.0114) Loss: 0.6011 (0.8713)
[2022/12/29 02:03] | TRAIN(060): [700/879] Batch: 0.1804 (0.1716) Data: 0.0075 (0.0112) Loss: 0.6197 (0.8711)
[2022/12/29 02:03] | TRAIN(060): [750/879] Batch: 0.1742 (0.1712) Data: 0.0073 (0.0110) Loss: 0.9530 (0.8690)
[2022/12/29 02:03] | TRAIN(060): [800/879] Batch: 0.1602 (0.1708) Data: 0.0072 (0.0108) Loss: 0.7546 (0.8693)
[2022/12/29 02:03] | TRAIN(060): [850/879] Batch: 0.1664 (0.1705) Data: 0.0085 (0.0107) Loss: 0.8078 (0.8670)
[2022/12/29 02:03] | ------------------------------------------------------------
[2022/12/29 02:03] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:03] | ------------------------------------------------------------
[2022/12/29 02:03] |    TRAIN(60)     0:02:29     0:00:09     0:02:20      0.8680
[2022/12/29 02:03] | ------------------------------------------------------------
[2022/12/29 02:03] | VALID(060): [ 50/220] Batch: 0.0525 (0.0814) Data: 0.0337 (0.0572) Loss: 0.7838 (0.8465)
[2022/12/29 02:03] | VALID(060): [100/220] Batch: 0.0519 (0.0681) Data: 0.0356 (0.0448) Loss: 1.1070 (0.8685)
[2022/12/29 02:03] | VALID(060): [150/220] Batch: 0.0537 (0.0637) Data: 0.0315 (0.0406) Loss: 0.7594 (0.8607)
[2022/12/29 02:03] | VALID(060): [200/220] Batch: 0.0577 (0.0615) Data: 0.0346 (0.0387) Loss: 0.5203 (0.8681)
[2022/12/29 02:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:03] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:03] |    VALID(60)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:03] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:03] | ####################################################################################################
[2022/12/29 02:03] | TRAIN(061): [ 50/879] Batch: 0.1699 (0.2062) Data: 0.0073 (0.0450) Loss: 0.5732 (0.8988)
[2022/12/29 02:04] | TRAIN(061): [100/879] Batch: 0.1787 (0.1889) Data: 0.0184 (0.0283) Loss: 0.6660 (0.8725)
[2022/12/29 02:04] | TRAIN(061): [150/879] Batch: 0.1632 (0.1832) Data: 0.0078 (0.0225) Loss: 0.8267 (0.8726)
[2022/12/29 02:04] | TRAIN(061): [200/879] Batch: 0.1657 (0.1791) Data: 0.0075 (0.0191) Loss: 0.8374 (0.8719)
[2022/12/29 02:04] | TRAIN(061): [250/879] Batch: 0.2002 (0.1778) Data: 0.0078 (0.0171) Loss: 0.5577 (0.8679)
[2022/12/29 02:04] | TRAIN(061): [300/879] Batch: 0.1666 (0.1770) Data: 0.0078 (0.0157) Loss: 0.7763 (0.8641)
[2022/12/29 02:04] | TRAIN(061): [350/879] Batch: 0.1639 (0.1751) Data: 0.0076 (0.0146) Loss: 1.1374 (0.8652)
[2022/12/29 02:04] | TRAIN(061): [400/879] Batch: 0.1734 (0.1738) Data: 0.0079 (0.0137) Loss: 0.5135 (0.8674)
[2022/12/29 02:05] | TRAIN(061): [450/879] Batch: 0.1700 (0.1727) Data: 0.0072 (0.0130) Loss: 0.9584 (0.8685)
[2022/12/29 02:05] | TRAIN(061): [500/879] Batch: 0.1589 (0.1723) Data: 0.0078 (0.0126) Loss: 0.8154 (0.8665)
[2022/12/29 02:05] | TRAIN(061): [550/879] Batch: 0.1642 (0.1716) Data: 0.0075 (0.0121) Loss: 0.8156 (0.8659)
[2022/12/29 02:05] | TRAIN(061): [600/879] Batch: 0.1627 (0.1712) Data: 0.0074 (0.0118) Loss: 0.8769 (0.8674)
[2022/12/29 02:05] | TRAIN(061): [650/879] Batch: 0.1715 (0.1706) Data: 0.0073 (0.0115) Loss: 0.7001 (0.8664)
[2022/12/29 02:05] | TRAIN(061): [700/879] Batch: 0.1607 (0.1701) Data: 0.0071 (0.0112) Loss: 1.0676 (0.8680)
[2022/12/29 02:05] | TRAIN(061): [750/879] Batch: 0.1628 (0.1697) Data: 0.0080 (0.0110) Loss: 1.1117 (0.8686)
[2022/12/29 02:05] | TRAIN(061): [800/879] Batch: 0.1718 (0.1696) Data: 0.0059 (0.0108) Loss: 0.9850 (0.8676)
[2022/12/29 02:06] | TRAIN(061): [850/879] Batch: 0.1631 (0.1693) Data: 0.0085 (0.0106) Loss: 1.2333 (0.8678)
[2022/12/29 02:06] | ------------------------------------------------------------
[2022/12/29 02:06] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:06] | ------------------------------------------------------------
[2022/12/29 02:06] |    TRAIN(61)     0:02:28     0:00:09     0:02:19      0.8680
[2022/12/29 02:06] | ------------------------------------------------------------
[2022/12/29 02:06] | VALID(061): [ 50/220] Batch: 0.0621 (0.0818) Data: 0.0234 (0.0567) Loss: 0.7859 (0.8468)
[2022/12/29 02:06] | VALID(061): [100/220] Batch: 0.0558 (0.0683) Data: 0.0332 (0.0415) Loss: 1.1058 (0.8683)
[2022/12/29 02:06] | VALID(061): [150/220] Batch: 0.0519 (0.0639) Data: 0.0305 (0.0379) Loss: 0.7612 (0.8607)
[2022/12/29 02:06] | VALID(061): [200/220] Batch: 0.0471 (0.0617) Data: 0.0262 (0.0351) Loss: 0.5248 (0.8681)
[2022/12/29 02:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:06] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:06] |    VALID(61)      0.8680      0.7347      0.5005      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:06] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:06] | ####################################################################################################
[2022/12/29 02:06] | TRAIN(062): [ 50/879] Batch: 0.1612 (0.2092) Data: 0.0162 (0.0476) Loss: 0.4855 (0.8773)
[2022/12/29 02:06] | TRAIN(062): [100/879] Batch: 0.1704 (0.1889) Data: 0.0140 (0.0290) Loss: 0.9104 (0.8714)
[2022/12/29 02:06] | TRAIN(062): [150/879] Batch: 0.1603 (0.1822) Data: 0.0069 (0.0225) Loss: 0.8778 (0.8858)
[2022/12/29 02:07] | TRAIN(062): [200/879] Batch: 0.1615 (0.1788) Data: 0.0078 (0.0193) Loss: 0.8146 (0.8818)
[2022/12/29 02:07] | TRAIN(062): [250/879] Batch: 0.1624 (0.1765) Data: 0.0076 (0.0174) Loss: 0.8103 (0.8805)
[2022/12/29 02:07] | TRAIN(062): [300/879] Batch: 0.1641 (0.1754) Data: 0.0072 (0.0162) Loss: 0.6058 (0.8806)
[2022/12/29 02:07] | TRAIN(062): [350/879] Batch: 0.1798 (0.1747) Data: 0.0101 (0.0153) Loss: 0.7642 (0.8801)
[2022/12/29 02:07] | TRAIN(062): [400/879] Batch: 0.1614 (0.1750) Data: 0.0075 (0.0146) Loss: 0.7708 (0.8794)
[2022/12/29 02:07] | TRAIN(062): [450/879] Batch: 0.1800 (0.1747) Data: 0.0157 (0.0140) Loss: 0.7154 (0.8830)
[2022/12/29 02:07] | TRAIN(062): [500/879] Batch: 0.1616 (0.1742) Data: 0.0071 (0.0135) Loss: 0.9349 (0.8797)
[2022/12/29 02:08] | TRAIN(062): [550/879] Batch: 0.1804 (0.1740) Data: 0.0097 (0.0131) Loss: 0.7008 (0.8771)
[2022/12/29 02:08] | TRAIN(062): [600/879] Batch: 0.1617 (0.1736) Data: 0.0079 (0.0127) Loss: 0.8193 (0.8744)
[2022/12/29 02:08] | TRAIN(062): [650/879] Batch: 0.1603 (0.1731) Data: 0.0099 (0.0124) Loss: 0.6129 (0.8717)
[2022/12/29 02:08] | TRAIN(062): [700/879] Batch: 0.1839 (0.1729) Data: 0.0075 (0.0121) Loss: 1.2060 (0.8727)
[2022/12/29 02:08] | TRAIN(062): [750/879] Batch: 0.1644 (0.1727) Data: 0.0076 (0.0119) Loss: 0.9125 (0.8730)
[2022/12/29 02:08] | TRAIN(062): [800/879] Batch: 0.1693 (0.1726) Data: 0.0085 (0.0117) Loss: 0.9709 (0.8706)
[2022/12/29 02:08] | TRAIN(062): [850/879] Batch: 0.1599 (0.1723) Data: 0.0183 (0.0115) Loss: 0.7142 (0.8698)
[2022/12/29 02:08] | ------------------------------------------------------------
[2022/12/29 02:08] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:08] | ------------------------------------------------------------
[2022/12/29 02:08] |    TRAIN(62)     0:02:31     0:00:10     0:02:21      0.8680
[2022/12/29 02:08] | ------------------------------------------------------------
[2022/12/29 02:09] | VALID(062): [ 50/220] Batch: 0.0545 (0.0829) Data: 0.0332 (0.0587) Loss: 0.7819 (0.8461)
[2022/12/29 02:09] | VALID(062): [100/220] Batch: 0.0536 (0.0689) Data: 0.0322 (0.0458) Loss: 1.1153 (0.8688)
[2022/12/29 02:09] | VALID(062): [150/220] Batch: 0.0540 (0.0643) Data: 0.0352 (0.0416) Loss: 0.7562 (0.8607)
[2022/12/29 02:09] | VALID(062): [200/220] Batch: 0.0534 (0.0620) Data: 0.0358 (0.0395) Loss: 0.5093 (0.8684)
[2022/12/29 02:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:09] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:09] |    VALID(62)      0.8682      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:09] | ####################################################################################################
[2022/12/29 02:09] | TRAIN(063): [ 50/879] Batch: 0.1670 (0.2055) Data: 0.0169 (0.0452) Loss: 0.9130 (0.8547)
[2022/12/29 02:09] | TRAIN(063): [100/879] Batch: 0.1604 (0.1891) Data: 0.0087 (0.0279) Loss: 0.8269 (0.8722)
[2022/12/29 02:09] | TRAIN(063): [150/879] Batch: 0.1780 (0.1828) Data: 0.0073 (0.0217) Loss: 1.0259 (0.8695)
[2022/12/29 02:09] | TRAIN(063): [200/879] Batch: 0.1893 (0.1801) Data: 0.0093 (0.0185) Loss: 1.1381 (0.8754)
[2022/12/29 02:09] | TRAIN(063): [250/879] Batch: 0.1682 (0.1773) Data: 0.0074 (0.0166) Loss: 0.9261 (0.8800)
[2022/12/29 02:10] | TRAIN(063): [300/879] Batch: 0.1640 (0.1763) Data: 0.0074 (0.0155) Loss: 1.0639 (0.8784)
[2022/12/29 02:10] | TRAIN(063): [350/879] Batch: 0.1693 (0.1754) Data: 0.0071 (0.0146) Loss: 1.0592 (0.8852)
[2022/12/29 02:10] | TRAIN(063): [400/879] Batch: 0.1745 (0.1744) Data: 0.0075 (0.0138) Loss: 0.9415 (0.8827)
[2022/12/29 02:10] | TRAIN(063): [450/879] Batch: 0.1716 (0.1739) Data: 0.0092 (0.0132) Loss: 1.1275 (0.8796)
[2022/12/29 02:10] | TRAIN(063): [500/879] Batch: 0.1616 (0.1732) Data: 0.0078 (0.0127) Loss: 0.9425 (0.8785)
[2022/12/29 02:10] | TRAIN(063): [550/879] Batch: 0.1629 (0.1725) Data: 0.0078 (0.0123) Loss: 0.9865 (0.8787)
[2022/12/29 02:10] | TRAIN(063): [600/879] Batch: 0.1644 (0.1724) Data: 0.0105 (0.0120) Loss: 0.8378 (0.8754)
[2022/12/29 02:11] | TRAIN(063): [650/879] Batch: 0.1667 (0.1721) Data: 0.0073 (0.0117) Loss: 0.9619 (0.8754)
[2022/12/29 02:11] | TRAIN(063): [700/879] Batch: 0.1631 (0.1720) Data: 0.0092 (0.0116) Loss: 0.5318 (0.8739)
[2022/12/29 02:11] | TRAIN(063): [750/879] Batch: 0.1600 (0.1719) Data: 0.0075 (0.0114) Loss: 1.0051 (0.8717)
[2022/12/29 02:11] | TRAIN(063): [800/879] Batch: 0.1627 (0.1715) Data: 0.0070 (0.0112) Loss: 0.6296 (0.8709)
[2022/12/29 02:11] | TRAIN(063): [850/879] Batch: 0.1632 (0.1711) Data: 0.0075 (0.0110) Loss: 1.2518 (0.8693)
[2022/12/29 02:11] | ------------------------------------------------------------
[2022/12/29 02:11] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:11] | ------------------------------------------------------------
[2022/12/29 02:11] |    TRAIN(63)     0:02:30     0:00:09     0:02:20      0.8680
[2022/12/29 02:11] | ------------------------------------------------------------
[2022/12/29 02:11] | VALID(063): [ 50/220] Batch: 0.0581 (0.0828) Data: 0.0365 (0.0594) Loss: 0.7859 (0.8466)
[2022/12/29 02:11] | VALID(063): [100/220] Batch: 0.0527 (0.0690) Data: 0.0361 (0.0464) Loss: 1.1118 (0.8686)
[2022/12/29 02:11] | VALID(063): [150/220] Batch: 0.0477 (0.0644) Data: 0.0377 (0.0418) Loss: 0.7587 (0.8608)
[2022/12/29 02:11] | VALID(063): [200/220] Batch: 0.0557 (0.0621) Data: 0.0315 (0.0397) Loss: 0.5139 (0.8683)
[2022/12/29 02:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:11] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:11] |    VALID(63)      0.8682      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:11] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:11] | ####################################################################################################
[2022/12/29 02:12] | TRAIN(064): [ 50/879] Batch: 0.1659 (0.2054) Data: 0.0081 (0.0465) Loss: 0.8685 (0.8823)
[2022/12/29 02:12] | TRAIN(064): [100/879] Batch: 0.1603 (0.1852) Data: 0.0081 (0.0276) Loss: 0.6622 (0.8802)
[2022/12/29 02:12] | TRAIN(064): [150/879] Batch: 0.1604 (0.1781) Data: 0.0075 (0.0212) Loss: 1.0964 (0.8794)
[2022/12/29 02:12] | TRAIN(064): [200/879] Batch: 0.1702 (0.1766) Data: 0.0082 (0.0182) Loss: 0.6299 (0.8715)
[2022/12/29 02:12] | TRAIN(064): [250/879] Batch: 0.1861 (0.1745) Data: 0.0091 (0.0162) Loss: 1.1841 (0.8798)
[2022/12/29 02:12] | TRAIN(064): [300/879] Batch: 0.1630 (0.1738) Data: 0.0072 (0.0149) Loss: 0.5369 (0.8790)
[2022/12/29 02:12] | TRAIN(064): [350/879] Batch: 0.1646 (0.1729) Data: 0.0169 (0.0140) Loss: 0.9017 (0.8736)
[2022/12/29 02:13] | TRAIN(064): [400/879] Batch: 0.1591 (0.1723) Data: 0.0078 (0.0133) Loss: 0.7568 (0.8713)
[2022/12/29 02:13] | TRAIN(064): [450/879] Batch: 0.1768 (0.1718) Data: 0.0080 (0.0128) Loss: 0.8893 (0.8736)
[2022/12/29 02:13] | TRAIN(064): [500/879] Batch: 0.1646 (0.1711) Data: 0.0072 (0.0124) Loss: 0.9583 (0.8723)
[2022/12/29 02:13] | TRAIN(064): [550/879] Batch: 0.1633 (0.1706) Data: 0.0095 (0.0120) Loss: 0.5798 (0.8725)
[2022/12/29 02:13] | TRAIN(064): [600/879] Batch: 0.1733 (0.1704) Data: 0.0072 (0.0118) Loss: 0.6659 (0.8707)
[2022/12/29 02:13] | TRAIN(064): [650/879] Batch: 0.1747 (0.1700) Data: 0.0088 (0.0115) Loss: 1.0810 (0.8657)
[2022/12/29 02:13] | TRAIN(064): [700/879] Batch: 0.1597 (0.1695) Data: 0.0070 (0.0113) Loss: 0.8400 (0.8671)
[2022/12/29 02:14] | TRAIN(064): [750/879] Batch: 0.1634 (0.1693) Data: 0.0179 (0.0111) Loss: 0.6263 (0.8661)
[2022/12/29 02:14] | TRAIN(064): [800/879] Batch: 0.1642 (0.1698) Data: 0.0094 (0.0110) Loss: 1.2474 (0.8703)
[2022/12/29 02:14] | TRAIN(064): [850/879] Batch: 0.1702 (0.1696) Data: 0.0097 (0.0108) Loss: 0.8179 (0.8678)
[2022/12/29 02:14] | ------------------------------------------------------------
[2022/12/29 02:14] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:14] | ------------------------------------------------------------
[2022/12/29 02:14] |    TRAIN(64)     0:02:28     0:00:09     0:02:19      0.8680
[2022/12/29 02:14] | ------------------------------------------------------------
[2022/12/29 02:14] | VALID(064): [ 50/220] Batch: 0.0516 (0.0791) Data: 0.0362 (0.0557) Loss: 0.7847 (0.8464)
[2022/12/29 02:14] | VALID(064): [100/220] Batch: 0.0565 (0.0670) Data: 0.0289 (0.0424) Loss: 1.1102 (0.8686)
[2022/12/29 02:14] | VALID(064): [150/220] Batch: 0.0550 (0.0629) Data: 0.0255 (0.0373) Loss: 0.7588 (0.8607)
[2022/12/29 02:14] | VALID(064): [200/220] Batch: 0.0546 (0.0608) Data: 0.0275 (0.0347) Loss: 0.5131 (0.8682)
[2022/12/29 02:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:14] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:14] |    VALID(64)      0.8681      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:14] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:14] | ####################################################################################################
[2022/12/29 02:14] | TRAIN(065): [ 50/879] Batch: 0.1660 (0.2036) Data: 0.0104 (0.0469) Loss: 0.7617 (0.8737)
[2022/12/29 02:14] | TRAIN(065): [100/879] Batch: 0.1753 (0.1872) Data: 0.0092 (0.0289) Loss: 0.8111 (0.8803)
[2022/12/29 02:15] | TRAIN(065): [150/879] Batch: 0.1744 (0.1819) Data: 0.0110 (0.0232) Loss: 1.0423 (0.8857)
[2022/12/29 02:15] | TRAIN(065): [200/879] Batch: 0.1616 (0.1796) Data: 0.0177 (0.0204) Loss: 0.9317 (0.8754)
[2022/12/29 02:15] | TRAIN(065): [250/879] Batch: 0.1714 (0.1777) Data: 0.0099 (0.0187) Loss: 0.7539 (0.8725)
[2022/12/29 02:15] | TRAIN(065): [300/879] Batch: 0.1707 (0.1763) Data: 0.0098 (0.0174) Loss: 0.7582 (0.8660)
[2022/12/29 02:15] | TRAIN(065): [350/879] Batch: 0.1664 (0.1754) Data: 0.0107 (0.0166) Loss: 1.0624 (0.8687)
[2022/12/29 02:15] | TRAIN(065): [400/879] Batch: 0.1663 (0.1745) Data: 0.0123 (0.0159) Loss: 0.7322 (0.8642)
[2022/12/29 02:15] | TRAIN(065): [450/879] Batch: 0.1822 (0.1744) Data: 0.0083 (0.0155) Loss: 0.9115 (0.8651)
[2022/12/29 02:16] | TRAIN(065): [500/879] Batch: 0.1714 (0.1734) Data: 0.0096 (0.0150) Loss: 1.0432 (0.8659)
[2022/12/29 02:16] | TRAIN(065): [550/879] Batch: 0.1575 (0.1729) Data: 0.0110 (0.0146) Loss: 0.7607 (0.8642)
[2022/12/29 02:16] | TRAIN(065): [600/879] Batch: 0.1763 (0.1727) Data: 0.0180 (0.0143) Loss: 0.7089 (0.8646)
[2022/12/29 02:16] | TRAIN(065): [650/879] Batch: 0.1577 (0.1724) Data: 0.0162 (0.0140) Loss: 0.8903 (0.8640)
[2022/12/29 02:16] | TRAIN(065): [700/879] Batch: 0.1604 (0.1721) Data: 0.0093 (0.0138) Loss: 1.0680 (0.8662)
[2022/12/29 02:16] | TRAIN(065): [750/879] Batch: 0.1869 (0.1719) Data: 0.0129 (0.0136) Loss: 0.8183 (0.8661)
[2022/12/29 02:16] | TRAIN(065): [800/879] Batch: 0.1802 (0.1717) Data: 0.0105 (0.0135) Loss: 0.8716 (0.8667)
[2022/12/29 02:17] | TRAIN(065): [850/879] Batch: 0.1770 (0.1716) Data: 0.0088 (0.0133) Loss: 0.8010 (0.8682)
[2022/12/29 02:17] | ------------------------------------------------------------
[2022/12/29 02:17] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:17] | ------------------------------------------------------------
[2022/12/29 02:17] |    TRAIN(65)     0:02:30     0:00:11     0:02:19      0.8680
[2022/12/29 02:17] | ------------------------------------------------------------
[2022/12/29 02:17] | VALID(065): [ 50/220] Batch: 0.0500 (0.0832) Data: 0.0217 (0.0553) Loss: 0.7849 (0.8468)
[2022/12/29 02:17] | VALID(065): [100/220] Batch: 0.0535 (0.0686) Data: 0.0309 (0.0411) Loss: 1.1046 (0.8685)
[2022/12/29 02:17] | VALID(065): [150/220] Batch: 0.0516 (0.0637) Data: 0.0331 (0.0380) Loss: 0.7610 (0.8608)
[2022/12/29 02:17] | VALID(065): [200/220] Batch: 0.0513 (0.0612) Data: 0.0265 (0.0358) Loss: 0.5245 (0.8682)
[2022/12/29 02:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:17] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:17] |    VALID(65)      0.8680      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:17] | ####################################################################################################
[2022/12/29 02:17] | TRAIN(066): [ 50/879] Batch: 0.1606 (0.1978) Data: 0.0083 (0.0430) Loss: 0.9450 (0.8756)
[2022/12/29 02:17] | TRAIN(066): [100/879] Batch: 0.1842 (0.1800) Data: 0.0116 (0.0263) Loss: 0.7117 (0.8984)
[2022/12/29 02:17] | TRAIN(066): [150/879] Batch: 0.1597 (0.1748) Data: 0.0074 (0.0205) Loss: 0.7742 (0.8887)
[2022/12/29 02:17] | TRAIN(066): [200/879] Batch: 0.1609 (0.1717) Data: 0.0100 (0.0175) Loss: 0.7232 (0.8836)
[2022/12/29 02:18] | TRAIN(066): [250/879] Batch: 0.1604 (0.1694) Data: 0.0093 (0.0158) Loss: 1.0301 (0.8777)
[2022/12/29 02:18] | TRAIN(066): [300/879] Batch: 0.1591 (0.1683) Data: 0.0082 (0.0147) Loss: 0.8973 (0.8757)
[2022/12/29 02:18] | TRAIN(066): [350/879] Batch: 0.1586 (0.1673) Data: 0.0086 (0.0139) Loss: 0.9489 (0.8685)
[2022/12/29 02:18] | TRAIN(066): [400/879] Batch: 0.1609 (0.1664) Data: 0.0077 (0.0133) Loss: 0.8870 (0.8622)
[2022/12/29 02:18] | TRAIN(066): [450/879] Batch: 0.1684 (0.1657) Data: 0.0103 (0.0128) Loss: 0.5180 (0.8651)
[2022/12/29 02:18] | TRAIN(066): [500/879] Batch: 0.1607 (0.1652) Data: 0.0090 (0.0124) Loss: 0.6190 (0.8669)
[2022/12/29 02:18] | TRAIN(066): [550/879] Batch: 0.1608 (0.1648) Data: 0.0078 (0.0120) Loss: 0.8223 (0.8643)
[2022/12/29 02:19] | TRAIN(066): [600/879] Batch: 0.1600 (0.1645) Data: 0.0093 (0.0118) Loss: 0.7823 (0.8624)
[2022/12/29 02:19] | TRAIN(066): [650/879] Batch: 0.1602 (0.1644) Data: 0.0082 (0.0116) Loss: 0.5912 (0.8672)
[2022/12/29 02:19] | TRAIN(066): [700/879] Batch: 0.1596 (0.1644) Data: 0.0090 (0.0114) Loss: 1.3131 (0.8678)
[2022/12/29 02:19] | TRAIN(066): [750/879] Batch: 0.1633 (0.1645) Data: 0.0070 (0.0112) Loss: 1.0608 (0.8677)
[2022/12/29 02:19] | TRAIN(066): [800/879] Batch: 0.1620 (0.1646) Data: 0.0077 (0.0111) Loss: 0.7504 (0.8678)
[2022/12/29 02:19] | TRAIN(066): [850/879] Batch: 0.1613 (0.1645) Data: 0.0097 (0.0110) Loss: 1.1368 (0.8668)
[2022/12/29 02:19] | ------------------------------------------------------------
[2022/12/29 02:19] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:19] | ------------------------------------------------------------
[2022/12/29 02:19] |    TRAIN(66)     0:02:24     0:00:09     0:02:14      0.8681
[2022/12/29 02:19] | ------------------------------------------------------------
[2022/12/29 02:19] | VALID(066): [ 50/220] Batch: 0.0571 (0.0810) Data: 0.0286 (0.0597) Loss: 0.7843 (0.8469)
[2022/12/29 02:19] | VALID(066): [100/220] Batch: 0.0551 (0.0674) Data: 0.0361 (0.0465) Loss: 1.1059 (0.8686)
[2022/12/29 02:19] | VALID(066): [150/220] Batch: 0.0543 (0.0629) Data: 0.0331 (0.0423) Loss: 0.7606 (0.8609)
[2022/12/29 02:19] | VALID(066): [200/220] Batch: 0.0531 (0.0606) Data: 0.0331 (0.0403) Loss: 0.5265 (0.8682)
[2022/12/29 02:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:19] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:19] |    VALID(66)      0.8681      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:19] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:19] | ####################################################################################################
[2022/12/29 02:20] | TRAIN(067): [ 50/879] Batch: 0.1606 (0.2012) Data: 0.0085 (0.0453) Loss: 0.7487 (0.8545)
[2022/12/29 02:20] | TRAIN(067): [100/879] Batch: 0.1646 (0.1810) Data: 0.0069 (0.0275) Loss: 1.0172 (0.8571)
[2022/12/29 02:20] | TRAIN(067): [150/879] Batch: 0.1589 (0.1745) Data: 0.0096 (0.0213) Loss: 0.8309 (0.8510)
[2022/12/29 02:20] | TRAIN(067): [200/879] Batch: 0.1684 (0.1711) Data: 0.0092 (0.0183) Loss: 0.8397 (0.8510)
[2022/12/29 02:20] | TRAIN(067): [250/879] Batch: 0.1610 (0.1693) Data: 0.0081 (0.0164) Loss: 0.9561 (0.8581)
[2022/12/29 02:20] | TRAIN(067): [300/879] Batch: 0.1589 (0.1679) Data: 0.0098 (0.0152) Loss: 1.1521 (0.8600)
[2022/12/29 02:20] | TRAIN(067): [350/879] Batch: 0.1591 (0.1672) Data: 0.0096 (0.0143) Loss: 0.8838 (0.8621)
[2022/12/29 02:21] | TRAIN(067): [400/879] Batch: 0.1604 (0.1665) Data: 0.0081 (0.0136) Loss: 0.7773 (0.8621)
[2022/12/29 02:21] | TRAIN(067): [450/879] Batch: 0.1592 (0.1659) Data: 0.0086 (0.0131) Loss: 0.6039 (0.8623)
[2022/12/29 02:21] | TRAIN(067): [500/879] Batch: 0.1694 (0.1659) Data: 0.0084 (0.0127) Loss: 0.5307 (0.8625)
[2022/12/29 02:21] | TRAIN(067): [550/879] Batch: 0.1585 (0.1656) Data: 0.0083 (0.0123) Loss: 0.7617 (0.8647)
[2022/12/29 02:21] | TRAIN(067): [600/879] Batch: 0.1605 (0.1653) Data: 0.0073 (0.0120) Loss: 0.8288 (0.8660)
[2022/12/29 02:21] | TRAIN(067): [650/879] Batch: 0.1607 (0.1649) Data: 0.0089 (0.0118) Loss: 0.9920 (0.8681)
[2022/12/29 02:21] | TRAIN(067): [700/879] Batch: 0.1622 (0.1646) Data: 0.0095 (0.0116) Loss: 0.6882 (0.8662)
[2022/12/29 02:22] | TRAIN(067): [750/879] Batch: 0.1594 (0.1644) Data: 0.0085 (0.0114) Loss: 0.9852 (0.8653)
[2022/12/29 02:22] | TRAIN(067): [800/879] Batch: 0.1589 (0.1642) Data: 0.0091 (0.0113) Loss: 0.9115 (0.8665)
[2022/12/29 02:22] | TRAIN(067): [850/879] Batch: 0.1690 (0.1640) Data: 0.0085 (0.0111) Loss: 0.8507 (0.8679)
[2022/12/29 02:22] | ------------------------------------------------------------
[2022/12/29 02:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:22] | ------------------------------------------------------------
[2022/12/29 02:22] |    TRAIN(67)     0:02:24     0:00:09     0:02:14      0.8679
[2022/12/29 02:22] | ------------------------------------------------------------
[2022/12/29 02:22] | VALID(067): [ 50/220] Batch: 0.0543 (0.0771) Data: 0.0348 (0.0567) Loss: 0.7861 (0.8471)
[2022/12/29 02:22] | VALID(067): [100/220] Batch: 0.0543 (0.0654) Data: 0.0361 (0.0453) Loss: 1.1047 (0.8685)
[2022/12/29 02:22] | VALID(067): [150/220] Batch: 0.0547 (0.0616) Data: 0.0330 (0.0415) Loss: 0.7624 (0.8610)
[2022/12/29 02:22] | VALID(067): [200/220] Batch: 0.0547 (0.0596) Data: 0.0351 (0.0397) Loss: 0.5267 (0.8683)
[2022/12/29 02:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:22] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:22] |    VALID(67)      0.8682      0.7347      0.5064      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:22] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:22] | ####################################################################################################
[2022/12/29 02:22] | TRAIN(068): [ 50/879] Batch: 0.1739 (0.2026) Data: 0.0079 (0.0453) Loss: 0.9744 (0.8863)
[2022/12/29 02:22] | TRAIN(068): [100/879] Batch: 0.1610 (0.1857) Data: 0.0108 (0.0278) Loss: 1.0116 (0.8678)
[2022/12/29 02:23] | TRAIN(068): [150/879] Batch: 0.1685 (0.1786) Data: 0.0079 (0.0221) Loss: 0.7742 (0.8605)
[2022/12/29 02:23] | TRAIN(068): [200/879] Batch: 0.1621 (0.1783) Data: 0.0095 (0.0190) Loss: 0.8748 (0.8654)
[2022/12/29 02:23] | TRAIN(068): [250/879] Batch: 0.1650 (0.1769) Data: 0.0101 (0.0172) Loss: 0.7435 (0.8663)
[2022/12/29 02:23] | TRAIN(068): [300/879] Batch: 0.1803 (0.1749) Data: 0.0107 (0.0159) Loss: 0.7409 (0.8678)
[2022/12/29 02:23] | TRAIN(068): [350/879] Batch: 0.1631 (0.1735) Data: 0.0166 (0.0151) Loss: 0.8236 (0.8671)
[2022/12/29 02:23] | TRAIN(068): [400/879] Batch: 0.1604 (0.1723) Data: 0.0084 (0.0144) Loss: 0.9426 (0.8599)
[2022/12/29 02:23] | TRAIN(068): [450/879] Batch: 0.1609 (0.1718) Data: 0.0082 (0.0139) Loss: 0.9217 (0.8627)
[2022/12/29 02:24] | TRAIN(068): [500/879] Batch: 0.1829 (0.1712) Data: 0.0083 (0.0135) Loss: 0.8196 (0.8640)
[2022/12/29 02:24] | TRAIN(068): [550/879] Batch: 0.1591 (0.1710) Data: 0.0074 (0.0131) Loss: 0.8074 (0.8653)
[2022/12/29 02:24] | TRAIN(068): [600/879] Batch: 0.1672 (0.1708) Data: 0.0099 (0.0128) Loss: 0.8433 (0.8646)
[2022/12/29 02:24] | TRAIN(068): [650/879] Batch: 0.1874 (0.1703) Data: 0.0125 (0.0126) Loss: 0.6601 (0.8623)
[2022/12/29 02:24] | TRAIN(068): [700/879] Batch: 0.1623 (0.1700) Data: 0.0081 (0.0124) Loss: 0.8584 (0.8643)
[2022/12/29 02:24] | TRAIN(068): [750/879] Batch: 0.1598 (0.1697) Data: 0.0103 (0.0122) Loss: 1.0179 (0.8666)
[2022/12/29 02:24] | TRAIN(068): [800/879] Batch: 0.1602 (0.1695) Data: 0.0095 (0.0121) Loss: 1.1710 (0.8651)
[2022/12/29 02:25] | TRAIN(068): [850/879] Batch: 0.1599 (0.1691) Data: 0.0100 (0.0119) Loss: 0.7071 (0.8674)
[2022/12/29 02:25] | ------------------------------------------------------------
[2022/12/29 02:25] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:25] | ------------------------------------------------------------
[2022/12/29 02:25] |    TRAIN(68)     0:02:28     0:00:10     0:02:18      0.8679
[2022/12/29 02:25] | ------------------------------------------------------------
[2022/12/29 02:25] | VALID(068): [ 50/220] Batch: 0.0553 (0.0800) Data: 0.0273 (0.0542) Loss: 0.7833 (0.8473)
[2022/12/29 02:25] | VALID(068): [100/220] Batch: 0.0548 (0.0669) Data: 0.0306 (0.0420) Loss: 1.1025 (0.8692)
[2022/12/29 02:25] | VALID(068): [150/220] Batch: 0.0543 (0.0625) Data: 0.0335 (0.0389) Loss: 0.7611 (0.8613)
[2022/12/29 02:25] | VALID(068): [200/220] Batch: 0.0516 (0.0603) Data: 0.0342 (0.0375) Loss: 0.5293 (0.8687)
[2022/12/29 02:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:25] |    VALID(68)      0.8684      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:25] | ####################################################################################################
[2022/12/29 02:25] | TRAIN(069): [ 50/879] Batch: 0.1600 (0.1985) Data: 0.0085 (0.0443) Loss: 0.8320 (0.8768)
[2022/12/29 02:25] | TRAIN(069): [100/879] Batch: 0.1603 (0.1802) Data: 0.0087 (0.0267) Loss: 0.8234 (0.8849)
[2022/12/29 02:25] | TRAIN(069): [150/879] Batch: 0.1632 (0.1735) Data: 0.0097 (0.0207) Loss: 0.8657 (0.8749)
[2022/12/29 02:25] | TRAIN(069): [200/879] Batch: 0.1616 (0.1707) Data: 0.0072 (0.0178) Loss: 0.6131 (0.8638)
[2022/12/29 02:26] | TRAIN(069): [250/879] Batch: 0.1656 (0.1690) Data: 0.0094 (0.0162) Loss: 0.9603 (0.8725)
[2022/12/29 02:26] | TRAIN(069): [300/879] Batch: 0.1731 (0.1685) Data: 0.0104 (0.0150) Loss: 0.9506 (0.8724)
[2022/12/29 02:26] | TRAIN(069): [350/879] Batch: 0.1616 (0.1674) Data: 0.0080 (0.0142) Loss: 0.8502 (0.8669)
[2022/12/29 02:26] | TRAIN(069): [400/879] Batch: 0.1597 (0.1667) Data: 0.0096 (0.0135) Loss: 0.9038 (0.8631)
[2022/12/29 02:26] | TRAIN(069): [450/879] Batch: 0.1599 (0.1661) Data: 0.0081 (0.0130) Loss: 1.0997 (0.8613)
[2022/12/29 02:26] | TRAIN(069): [500/879] Batch: 0.1613 (0.1656) Data: 0.0078 (0.0126) Loss: 1.0816 (0.8680)
[2022/12/29 02:26] | TRAIN(069): [550/879] Batch: 0.1607 (0.1651) Data: 0.0090 (0.0122) Loss: 0.7677 (0.8688)
[2022/12/29 02:26] | TRAIN(069): [600/879] Batch: 0.1601 (0.1648) Data: 0.0080 (0.0119) Loss: 0.5346 (0.8692)
[2022/12/29 02:27] | TRAIN(069): [650/879] Batch: 0.1600 (0.1644) Data: 0.0083 (0.0116) Loss: 0.9666 (0.8709)
[2022/12/29 02:27] | TRAIN(069): [700/879] Batch: 0.1620 (0.1642) Data: 0.0102 (0.0114) Loss: 0.8025 (0.8688)
[2022/12/29 02:27] | TRAIN(069): [750/879] Batch: 0.1582 (0.1640) Data: 0.0080 (0.0113) Loss: 1.0048 (0.8681)
[2022/12/29 02:27] | TRAIN(069): [800/879] Batch: 0.1609 (0.1637) Data: 0.0087 (0.0111) Loss: 0.8166 (0.8695)
[2022/12/29 02:27] | TRAIN(069): [850/879] Batch: 0.1618 (0.1635) Data: 0.0090 (0.0109) Loss: 0.9076 (0.8683)
[2022/12/29 02:27] | ------------------------------------------------------------
[2022/12/29 02:27] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:27] | ------------------------------------------------------------
[2022/12/29 02:27] |    TRAIN(69)     0:02:23     0:00:09     0:02:14      0.8679
[2022/12/29 02:27] | ------------------------------------------------------------
[2022/12/29 02:27] | VALID(069): [ 50/220] Batch: 0.0463 (0.0905) Data: 0.0335 (0.0686) Loss: 0.7813 (0.8462)
[2022/12/29 02:27] | VALID(069): [100/220] Batch: 0.0608 (0.0722) Data: 0.0240 (0.0487) Loss: 1.1114 (0.8687)
[2022/12/29 02:27] | VALID(069): [150/220] Batch: 0.0519 (0.0660) Data: 0.0348 (0.0431) Loss: 0.7576 (0.8607)
[2022/12/29 02:27] | VALID(069): [200/220] Batch: 0.0550 (0.0630) Data: 0.0231 (0.0395) Loss: 0.5158 (0.8683)
[2022/12/29 02:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:27] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:27] |    VALID(69)      0.8681      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:27] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:27] | ####################################################################################################
[2022/12/29 02:28] | TRAIN(070): [ 50/879] Batch: 0.1615 (0.1997) Data: 0.0086 (0.0457) Loss: 0.9843 (0.8731)
[2022/12/29 02:28] | TRAIN(070): [100/879] Batch: 0.1603 (0.1810) Data: 0.0089 (0.0273) Loss: 0.5377 (0.8777)
[2022/12/29 02:28] | TRAIN(070): [150/879] Batch: 0.1630 (0.1741) Data: 0.0078 (0.0212) Loss: 0.8212 (0.8697)
[2022/12/29 02:28] | TRAIN(070): [200/879] Batch: 0.1601 (0.1706) Data: 0.0076 (0.0181) Loss: 0.8170 (0.8848)
[2022/12/29 02:28] | TRAIN(070): [250/879] Batch: 0.1584 (0.1686) Data: 0.0103 (0.0163) Loss: 0.5503 (0.8790)
[2022/12/29 02:28] | TRAIN(070): [300/879] Batch: 0.1606 (0.1675) Data: 0.0096 (0.0151) Loss: 0.8332 (0.8758)
[2022/12/29 02:28] | TRAIN(070): [350/879] Batch: 0.1609 (0.1666) Data: 0.0088 (0.0143) Loss: 0.8649 (0.8736)
[2022/12/29 02:29] | TRAIN(070): [400/879] Batch: 0.1605 (0.1661) Data: 0.0092 (0.0137) Loss: 0.7031 (0.8730)
[2022/12/29 02:29] | TRAIN(070): [450/879] Batch: 0.1617 (0.1654) Data: 0.0095 (0.0132) Loss: 0.7683 (0.8695)
[2022/12/29 02:29] | TRAIN(070): [500/879] Batch: 0.1586 (0.1649) Data: 0.0101 (0.0128) Loss: 0.5546 (0.8659)
[2022/12/29 02:29] | TRAIN(070): [550/879] Batch: 0.1678 (0.1645) Data: 0.0090 (0.0124) Loss: 0.8566 (0.8654)
[2022/12/29 02:29] | TRAIN(070): [600/879] Batch: 0.1596 (0.1642) Data: 0.0075 (0.0122) Loss: 0.8469 (0.8635)
[2022/12/29 02:29] | TRAIN(070): [650/879] Batch: 0.1678 (0.1640) Data: 0.0087 (0.0119) Loss: 1.2036 (0.8666)
[2022/12/29 02:29] | TRAIN(070): [700/879] Batch: 0.1593 (0.1638) Data: 0.0082 (0.0117) Loss: 0.9569 (0.8661)
[2022/12/29 02:30] | TRAIN(070): [750/879] Batch: 0.1596 (0.1640) Data: 0.0093 (0.0116) Loss: 0.9224 (0.8671)
[2022/12/29 02:30] | TRAIN(070): [800/879] Batch: 0.1591 (0.1640) Data: 0.0092 (0.0114) Loss: 0.7046 (0.8681)
[2022/12/29 02:30] | TRAIN(070): [850/879] Batch: 0.1599 (0.1640) Data: 0.0098 (0.0113) Loss: 1.1425 (0.8678)
[2022/12/29 02:30] | ------------------------------------------------------------
[2022/12/29 02:30] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:30] | ------------------------------------------------------------
[2022/12/29 02:30] |    TRAIN(70)     0:02:23     0:00:09     0:02:14      0.8678
[2022/12/29 02:30] | ------------------------------------------------------------
[2022/12/29 02:30] | VALID(070): [ 50/220] Batch: 0.0558 (0.0810) Data: 0.0347 (0.0609) Loss: 0.7812 (0.8464)
[2022/12/29 02:30] | VALID(070): [100/220] Batch: 0.0505 (0.0673) Data: 0.0351 (0.0473) Loss: 1.1077 (0.8685)
[2022/12/29 02:30] | VALID(070): [150/220] Batch: 0.0512 (0.0627) Data: 0.0345 (0.0428) Loss: 0.7590 (0.8607)
[2022/12/29 02:30] | VALID(070): [200/220] Batch: 0.0534 (0.0604) Data: 0.0329 (0.0405) Loss: 0.5226 (0.8682)
[2022/12/29 02:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:30] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:30] |    VALID(70)      0.8679      0.7347      0.5001      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:30] | ####################################################################################################
[2022/12/29 02:30] | TRAIN(071): [ 50/879] Batch: 0.1600 (0.2048) Data: 0.0090 (0.0456) Loss: 0.6795 (0.8529)
[2022/12/29 02:30] | TRAIN(071): [100/879] Batch: 0.1583 (0.1825) Data: 0.0098 (0.0276) Loss: 1.1566 (0.8533)
[2022/12/29 02:31] | TRAIN(071): [150/879] Batch: 0.1599 (0.1751) Data: 0.0100 (0.0214) Loss: 0.7219 (0.8424)
[2022/12/29 02:31] | TRAIN(071): [200/879] Batch: 0.1614 (0.1715) Data: 0.0101 (0.0184) Loss: 0.8546 (0.8429)
[2022/12/29 02:31] | TRAIN(071): [250/879] Batch: 0.1601 (0.1692) Data: 0.0091 (0.0166) Loss: 1.0932 (0.8536)
[2022/12/29 02:31] | TRAIN(071): [300/879] Batch: 0.1645 (0.1678) Data: 0.0092 (0.0153) Loss: 0.9704 (0.8593)
[2022/12/29 02:31] | TRAIN(071): [350/879] Batch: 0.1603 (0.1669) Data: 0.0098 (0.0144) Loss: 0.9202 (0.8621)
[2022/12/29 02:31] | TRAIN(071): [400/879] Batch: 0.1594 (0.1662) Data: 0.0093 (0.0138) Loss: 0.9968 (0.8594)
[2022/12/29 02:31] | TRAIN(071): [450/879] Batch: 0.1602 (0.1657) Data: 0.0095 (0.0133) Loss: 0.6322 (0.8613)
[2022/12/29 02:31] | TRAIN(071): [500/879] Batch: 0.1602 (0.1655) Data: 0.0098 (0.0129) Loss: 1.0662 (0.8610)
[2022/12/29 02:32] | TRAIN(071): [550/879] Batch: 0.1704 (0.1656) Data: 0.0084 (0.0126) Loss: 0.7999 (0.8622)
[2022/12/29 02:32] | TRAIN(071): [600/879] Batch: 0.1605 (0.1654) Data: 0.0085 (0.0124) Loss: 0.6617 (0.8623)
[2022/12/29 02:32] | TRAIN(071): [650/879] Batch: 0.1604 (0.1652) Data: 0.0104 (0.0121) Loss: 0.8927 (0.8647)
[2022/12/29 02:32] | TRAIN(071): [700/879] Batch: 0.1618 (0.1652) Data: 0.0096 (0.0119) Loss: 0.8609 (0.8664)
[2022/12/29 02:32] | TRAIN(071): [750/879] Batch: 0.1758 (0.1650) Data: 0.0104 (0.0118) Loss: 0.7800 (0.8650)
[2022/12/29 02:32] | TRAIN(071): [800/879] Batch: 0.1601 (0.1649) Data: 0.0086 (0.0116) Loss: 1.0729 (0.8688)
[2022/12/29 02:32] | TRAIN(071): [850/879] Batch: 0.1775 (0.1647) Data: 0.0102 (0.0115) Loss: 0.6882 (0.8678)
[2022/12/29 02:32] | ------------------------------------------------------------
[2022/12/29 02:32] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:32] | ------------------------------------------------------------
[2022/12/29 02:32] |    TRAIN(71)     0:02:24     0:00:09     0:02:14      0.8677
[2022/12/29 02:32] | ------------------------------------------------------------
[2022/12/29 02:33] | VALID(071): [ 50/220] Batch: 0.0551 (0.0803) Data: 0.0339 (0.0590) Loss: 0.7825 (0.8467)
[2022/12/29 02:33] | VALID(071): [100/220] Batch: 0.0524 (0.0670) Data: 0.0361 (0.0463) Loss: 1.1081 (0.8687)
[2022/12/29 02:33] | VALID(071): [150/220] Batch: 0.0524 (0.0625) Data: 0.0345 (0.0422) Loss: 0.7586 (0.8608)
[2022/12/29 02:33] | VALID(071): [200/220] Batch: 0.0528 (0.0603) Data: 0.0325 (0.0400) Loss: 0.5254 (0.8682)
[2022/12/29 02:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:33] |    VALID(71)      0.8680      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:33] | ####################################################################################################
[2022/12/29 02:33] | TRAIN(072): [ 50/879] Batch: 0.1583 (0.2027) Data: 0.0083 (0.0478) Loss: 0.9370 (0.8918)
[2022/12/29 02:33] | TRAIN(072): [100/879] Batch: 0.1605 (0.1813) Data: 0.0107 (0.0286) Loss: 0.6568 (0.8796)
[2022/12/29 02:33] | TRAIN(072): [150/879] Batch: 0.1594 (0.1746) Data: 0.0094 (0.0223) Loss: 0.9890 (0.8839)
[2022/12/29 02:33] | TRAIN(072): [200/879] Batch: 0.1623 (0.1710) Data: 0.0089 (0.0190) Loss: 0.9087 (0.8655)
[2022/12/29 02:33] | TRAIN(072): [250/879] Batch: 0.1614 (0.1691) Data: 0.0101 (0.0170) Loss: 0.9261 (0.8696)
[2022/12/29 02:34] | TRAIN(072): [300/879] Batch: 0.1592 (0.1677) Data: 0.0088 (0.0157) Loss: 0.8406 (0.8739)
[2022/12/29 02:34] | TRAIN(072): [350/879] Batch: 0.1811 (0.1670) Data: 0.0114 (0.0147) Loss: 0.7144 (0.8690)
[2022/12/29 02:34] | TRAIN(072): [400/879] Batch: 0.1592 (0.1668) Data: 0.0065 (0.0141) Loss: 0.9244 (0.8646)
[2022/12/29 02:34] | TRAIN(072): [450/879] Batch: 0.1594 (0.1668) Data: 0.0086 (0.0136) Loss: 1.0142 (0.8654)
[2022/12/29 02:34] | TRAIN(072): [500/879] Batch: 0.1589 (0.1663) Data: 0.0095 (0.0131) Loss: 0.7579 (0.8613)
[2022/12/29 02:34] | TRAIN(072): [550/879] Batch: 0.1695 (0.1659) Data: 0.0091 (0.0128) Loss: 0.9992 (0.8620)
[2022/12/29 02:34] | TRAIN(072): [600/879] Batch: 0.1610 (0.1657) Data: 0.0084 (0.0125) Loss: 1.0725 (0.8625)
[2022/12/29 02:35] | TRAIN(072): [650/879] Batch: 0.1621 (0.1659) Data: 0.0098 (0.0123) Loss: 1.0389 (0.8611)
[2022/12/29 02:35] | TRAIN(072): [700/879] Batch: 0.1636 (0.1655) Data: 0.0101 (0.0121) Loss: 0.9233 (0.8636)
[2022/12/29 02:35] | TRAIN(072): [750/879] Batch: 0.1590 (0.1652) Data: 0.0087 (0.0119) Loss: 0.8909 (0.8647)
[2022/12/29 02:35] | TRAIN(072): [800/879] Batch: 0.1630 (0.1649) Data: 0.0091 (0.0117) Loss: 1.0637 (0.8666)
[2022/12/29 02:35] | TRAIN(072): [850/879] Batch: 0.1611 (0.1647) Data: 0.0107 (0.0115) Loss: 1.1036 (0.8657)
[2022/12/29 02:35] | ------------------------------------------------------------
[2022/12/29 02:35] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:35] | ------------------------------------------------------------
[2022/12/29 02:35] |    TRAIN(72)     0:02:24     0:00:10     0:02:14      0.8678
[2022/12/29 02:35] | ------------------------------------------------------------
[2022/12/29 02:35] | VALID(072): [ 50/220] Batch: 0.0557 (0.0817) Data: 0.0397 (0.0604) Loss: 0.7833 (0.8468)
[2022/12/29 02:35] | VALID(072): [100/220] Batch: 0.0556 (0.0677) Data: 0.0323 (0.0473) Loss: 1.1063 (0.8688)
[2022/12/29 02:35] | VALID(072): [150/220] Batch: 0.0562 (0.0631) Data: 0.0354 (0.0429) Loss: 0.7594 (0.8609)
[2022/12/29 02:35] | VALID(072): [200/220] Batch: 0.0518 (0.0607) Data: 0.0371 (0.0407) Loss: 0.5243 (0.8683)
[2022/12/29 02:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:35] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:35] |    VALID(72)      0.8681      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:35] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:35] | ####################################################################################################
[2022/12/29 02:36] | TRAIN(073): [ 50/879] Batch: 0.1596 (0.2003) Data: 0.0082 (0.0476) Loss: 0.6119 (0.8948)
[2022/12/29 02:36] | TRAIN(073): [100/879] Batch: 0.1594 (0.1821) Data: 0.0093 (0.0287) Loss: 0.6474 (0.8925)
[2022/12/29 02:36] | TRAIN(073): [150/879] Batch: 0.1853 (0.1757) Data: 0.0106 (0.0223) Loss: 0.6880 (0.8804)
[2022/12/29 02:36] | TRAIN(073): [200/879] Batch: 0.1598 (0.1733) Data: 0.0079 (0.0190) Loss: 1.0676 (0.8732)
[2022/12/29 02:36] | TRAIN(073): [250/879] Batch: 0.1611 (0.1715) Data: 0.0102 (0.0171) Loss: 0.8956 (0.8705)
[2022/12/29 02:36] | TRAIN(073): [300/879] Batch: 0.1652 (0.1698) Data: 0.0084 (0.0157) Loss: 0.7353 (0.8752)
[2022/12/29 02:36] | TRAIN(073): [350/879] Batch: 0.1599 (0.1686) Data: 0.0088 (0.0148) Loss: 1.0284 (0.8742)
[2022/12/29 02:36] | TRAIN(073): [400/879] Batch: 0.1598 (0.1676) Data: 0.0077 (0.0140) Loss: 0.7535 (0.8673)
[2022/12/29 02:37] | TRAIN(073): [450/879] Batch: 0.1612 (0.1668) Data: 0.0099 (0.0135) Loss: 1.0052 (0.8683)
[2022/12/29 02:37] | TRAIN(073): [500/879] Batch: 0.1612 (0.1662) Data: 0.0092 (0.0130) Loss: 0.8558 (0.8678)
[2022/12/29 02:37] | TRAIN(073): [550/879] Batch: 0.1590 (0.1657) Data: 0.0092 (0.0127) Loss: 0.9808 (0.8680)
[2022/12/29 02:37] | TRAIN(073): [600/879] Batch: 0.1593 (0.1653) Data: 0.0088 (0.0123) Loss: 0.7609 (0.8687)
[2022/12/29 02:37] | TRAIN(073): [650/879] Batch: 0.1624 (0.1651) Data: 0.0088 (0.0121) Loss: 0.4034 (0.8664)
[2022/12/29 02:37] | TRAIN(073): [700/879] Batch: 0.1803 (0.1651) Data: 0.0119 (0.0119) Loss: 0.8779 (0.8665)
[2022/12/29 02:37] | TRAIN(073): [750/879] Batch: 0.1627 (0.1650) Data: 0.0095 (0.0117) Loss: 0.7338 (0.8699)
[2022/12/29 02:38] | TRAIN(073): [800/879] Batch: 0.1608 (0.1649) Data: 0.0086 (0.0116) Loss: 1.2493 (0.8700)
[2022/12/29 02:38] | TRAIN(073): [850/879] Batch: 0.1604 (0.1647) Data: 0.0116 (0.0114) Loss: 0.9321 (0.8683)
[2022/12/29 02:38] | ------------------------------------------------------------
[2022/12/29 02:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:38] | ------------------------------------------------------------
[2022/12/29 02:38] |    TRAIN(73)     0:02:24     0:00:09     0:02:14      0.8676
[2022/12/29 02:38] | ------------------------------------------------------------
[2022/12/29 02:38] | VALID(073): [ 50/220] Batch: 0.0564 (0.0829) Data: 0.0297 (0.0569) Loss: 0.7841 (0.8465)
[2022/12/29 02:38] | VALID(073): [100/220] Batch: 0.0525 (0.0683) Data: 0.0313 (0.0429) Loss: 1.1103 (0.8684)
[2022/12/29 02:38] | VALID(073): [150/220] Batch: 0.0513 (0.0635) Data: 0.0268 (0.0386) Loss: 0.7594 (0.8607)
[2022/12/29 02:38] | VALID(073): [200/220] Batch: 0.0516 (0.0611) Data: 0.0355 (0.0357) Loss: 0.5180 (0.8682)
[2022/12/29 02:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:38] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:38] |    VALID(73)      0.8680      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:38] | ####################################################################################################
[2022/12/29 02:38] | TRAIN(074): [ 50/879] Batch: 0.1608 (0.2058) Data: 0.0138 (0.0443) Loss: 0.7675 (0.8989)
[2022/12/29 02:38] | TRAIN(074): [100/879] Batch: 0.1595 (0.1849) Data: 0.0100 (0.0268) Loss: 0.5315 (0.8758)
[2022/12/29 02:38] | TRAIN(074): [150/879] Batch: 0.1754 (0.1833) Data: 0.0086 (0.0208) Loss: 0.5280 (0.8855)
[2022/12/29 02:39] | TRAIN(074): [200/879] Batch: 0.1614 (0.1781) Data: 0.0084 (0.0179) Loss: 0.4820 (0.8774)
[2022/12/29 02:39] | TRAIN(074): [250/879] Batch: 0.1599 (0.1748) Data: 0.0095 (0.0161) Loss: 0.8327 (0.8662)
[2022/12/29 02:39] | TRAIN(074): [300/879] Batch: 0.1603 (0.1724) Data: 0.0083 (0.0149) Loss: 0.5039 (0.8669)
[2022/12/29 02:39] | TRAIN(074): [350/879] Batch: 0.1587 (0.1707) Data: 0.0104 (0.0140) Loss: 0.8758 (0.8614)
[2022/12/29 02:39] | TRAIN(074): [400/879] Batch: 0.1604 (0.1695) Data: 0.0078 (0.0134) Loss: 0.9850 (0.8596)
[2022/12/29 02:39] | TRAIN(074): [450/879] Batch: 0.1605 (0.1685) Data: 0.0078 (0.0129) Loss: 1.0496 (0.8618)
[2022/12/29 02:39] | TRAIN(074): [500/879] Batch: 0.1598 (0.1680) Data: 0.0090 (0.0125) Loss: 1.0262 (0.8669)
[2022/12/29 02:40] | TRAIN(074): [550/879] Batch: 0.1602 (0.1673) Data: 0.0087 (0.0122) Loss: 0.9098 (0.8667)
[2022/12/29 02:40] | TRAIN(074): [600/879] Batch: 0.1593 (0.1668) Data: 0.0087 (0.0119) Loss: 1.0553 (0.8703)
[2022/12/29 02:40] | TRAIN(074): [650/879] Batch: 0.1599 (0.1664) Data: 0.0085 (0.0116) Loss: 0.7301 (0.8707)
[2022/12/29 02:40] | TRAIN(074): [700/879] Batch: 0.1611 (0.1660) Data: 0.0080 (0.0114) Loss: 0.7448 (0.8719)
[2022/12/29 02:40] | TRAIN(074): [750/879] Batch: 0.1602 (0.1657) Data: 0.0069 (0.0112) Loss: 0.6038 (0.8708)
[2022/12/29 02:40] | TRAIN(074): [800/879] Batch: 0.1590 (0.1658) Data: 0.0098 (0.0111) Loss: 0.9774 (0.8710)
[2022/12/29 02:40] | TRAIN(074): [850/879] Batch: 0.1615 (0.1658) Data: 0.0072 (0.0110) Loss: 0.8650 (0.8707)
[2022/12/29 02:40] | ------------------------------------------------------------
[2022/12/29 02:40] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:40] | ------------------------------------------------------------
[2022/12/29 02:40] |    TRAIN(74)     0:02:25     0:00:09     0:02:16      0.8676
[2022/12/29 02:40] | ------------------------------------------------------------
[2022/12/29 02:40] | VALID(074): [ 50/220] Batch: 0.0546 (0.0812) Data: 0.0262 (0.0576) Loss: 0.7819 (0.8462)
[2022/12/29 02:41] | VALID(074): [100/220] Batch: 0.0557 (0.0676) Data: 0.0274 (0.0423) Loss: 1.1138 (0.8690)
[2022/12/29 02:41] | VALID(074): [150/220] Batch: 0.0568 (0.0631) Data: 0.0292 (0.0372) Loss: 0.7567 (0.8609)
[2022/12/29 02:41] | VALID(074): [200/220] Batch: 0.0550 (0.0608) Data: 0.0277 (0.0346) Loss: 0.5080 (0.8686)
[2022/12/29 02:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:41] |    VALID(74)      0.8684      0.7347      0.4999      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:41] | ####################################################################################################
[2022/12/29 02:41] | TRAIN(075): [ 50/879] Batch: 0.1602 (0.2014) Data: 0.0099 (0.0475) Loss: 0.7943 (0.8488)
[2022/12/29 02:41] | TRAIN(075): [100/879] Batch: 0.1633 (0.1818) Data: 0.0102 (0.0287) Loss: 0.7605 (0.8695)
[2022/12/29 02:41] | TRAIN(075): [150/879] Batch: 0.1642 (0.1763) Data: 0.0093 (0.0224) Loss: 0.6699 (0.8822)
[2022/12/29 02:41] | TRAIN(075): [200/879] Batch: 0.1614 (0.1730) Data: 0.0089 (0.0192) Loss: 0.8597 (0.8740)
[2022/12/29 02:41] | TRAIN(075): [250/879] Batch: 0.1604 (0.1712) Data: 0.0083 (0.0173) Loss: 0.6179 (0.8793)
[2022/12/29 02:41] | TRAIN(075): [300/879] Batch: 0.1631 (0.1700) Data: 0.0102 (0.0160) Loss: 0.8979 (0.8756)
[2022/12/29 02:42] | TRAIN(075): [350/879] Batch: 0.1606 (0.1691) Data: 0.0091 (0.0151) Loss: 0.9412 (0.8744)
[2022/12/29 02:42] | TRAIN(075): [400/879] Batch: 0.1596 (0.1685) Data: 0.0088 (0.0144) Loss: 0.6499 (0.8703)
[2022/12/29 02:42] | TRAIN(075): [450/879] Batch: 0.1609 (0.1679) Data: 0.0087 (0.0138) Loss: 0.8175 (0.8691)
[2022/12/29 02:42] | TRAIN(075): [500/879] Batch: 0.1597 (0.1675) Data: 0.0105 (0.0134) Loss: 0.7827 (0.8699)
[2022/12/29 02:42] | TRAIN(075): [550/879] Batch: 0.1586 (0.1671) Data: 0.0111 (0.0130) Loss: 0.8838 (0.8717)
[2022/12/29 02:42] | TRAIN(075): [600/879] Batch: 0.1637 (0.1669) Data: 0.0101 (0.0128) Loss: 0.6056 (0.8697)
[2022/12/29 02:42] | TRAIN(075): [650/879] Batch: 0.1633 (0.1668) Data: 0.0097 (0.0125) Loss: 0.6900 (0.8696)
[2022/12/29 02:43] | TRAIN(075): [700/879] Batch: 0.1723 (0.1665) Data: 0.0102 (0.0123) Loss: 0.8343 (0.8671)
[2022/12/29 02:43] | TRAIN(075): [750/879] Batch: 0.1602 (0.1662) Data: 0.0093 (0.0121) Loss: 0.6255 (0.8664)
[2022/12/29 02:43] | TRAIN(075): [800/879] Batch: 0.1580 (0.1662) Data: 0.0089 (0.0120) Loss: 1.3182 (0.8712)
[2022/12/29 02:43] | TRAIN(075): [850/879] Batch: 0.1658 (0.1660) Data: 0.0092 (0.0119) Loss: 0.8367 (0.8685)
[2022/12/29 02:43] | ------------------------------------------------------------
[2022/12/29 02:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:43] | ------------------------------------------------------------
[2022/12/29 02:43] |    TRAIN(75)     0:02:25     0:00:10     0:02:15      0.8677
[2022/12/29 02:43] | ------------------------------------------------------------
[2022/12/29 02:43] | VALID(075): [ 50/220] Batch: 0.0528 (0.0779) Data: 0.0325 (0.0569) Loss: 0.7830 (0.8462)
[2022/12/29 02:43] | VALID(075): [100/220] Batch: 0.0532 (0.0659) Data: 0.0396 (0.0452) Loss: 1.1099 (0.8685)
[2022/12/29 02:43] | VALID(075): [150/220] Batch: 0.0528 (0.0619) Data: 0.0362 (0.0415) Loss: 0.7581 (0.8605)
[2022/12/29 02:43] | VALID(075): [200/220] Batch: 0.0529 (0.0600) Data: 0.0314 (0.0388) Loss: 0.5148 (0.8681)
[2022/12/29 02:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:43] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:43] |    VALID(75)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:43] | ####################################################################################################
[2022/12/29 02:43] | TRAIN(076): [ 50/879] Batch: 0.1757 (0.2031) Data: 0.0155 (0.0467) Loss: 0.5108 (0.8609)
[2022/12/29 02:44] | TRAIN(076): [100/879] Batch: 0.1818 (0.1856) Data: 0.0119 (0.0290) Loss: 0.8906 (0.8532)
[2022/12/29 02:44] | TRAIN(076): [150/879] Batch: 0.1625 (0.1788) Data: 0.0084 (0.0230) Loss: 1.0711 (0.8537)
[2022/12/29 02:44] | TRAIN(076): [200/879] Batch: 0.1595 (0.1767) Data: 0.0099 (0.0200) Loss: 0.7392 (0.8539)
[2022/12/29 02:44] | TRAIN(076): [250/879] Batch: 0.1589 (0.1750) Data: 0.0090 (0.0183) Loss: 0.9365 (0.8632)
[2022/12/29 02:44] | TRAIN(076): [300/879] Batch: 0.1601 (0.1732) Data: 0.0088 (0.0169) Loss: 1.0052 (0.8628)
[2022/12/29 02:44] | TRAIN(076): [350/879] Batch: 0.1789 (0.1733) Data: 0.0092 (0.0160) Loss: 0.9041 (0.8619)
[2022/12/29 02:44] | TRAIN(076): [400/879] Batch: 0.1618 (0.1729) Data: 0.0159 (0.0154) Loss: 0.9407 (0.8605)
[2022/12/29 02:45] | TRAIN(076): [450/879] Batch: 0.1587 (0.1724) Data: 0.0090 (0.0150) Loss: 1.1403 (0.8678)
[2022/12/29 02:45] | TRAIN(076): [500/879] Batch: 0.1601 (0.1719) Data: 0.0089 (0.0146) Loss: 1.0365 (0.8694)
[2022/12/29 02:45] | TRAIN(076): [550/879] Batch: 0.1630 (0.1712) Data: 0.0105 (0.0142) Loss: 0.6835 (0.8667)
[2022/12/29 02:45] | TRAIN(076): [600/879] Batch: 0.1605 (0.1709) Data: 0.0106 (0.0140) Loss: 0.8661 (0.8646)
[2022/12/29 02:45] | TRAIN(076): [650/879] Batch: 0.1657 (0.1708) Data: 0.0094 (0.0138) Loss: 0.6140 (0.8660)
[2022/12/29 02:45] | TRAIN(076): [700/879] Batch: 0.1840 (0.1705) Data: 0.0092 (0.0136) Loss: 0.9656 (0.8663)
[2022/12/29 02:45] | TRAIN(076): [750/879] Batch: 0.1644 (0.1700) Data: 0.0148 (0.0134) Loss: 0.9068 (0.8685)
[2022/12/29 02:46] | TRAIN(076): [800/879] Batch: 0.1760 (0.1697) Data: 0.0209 (0.0133) Loss: 0.9223 (0.8685)
[2022/12/29 02:46] | TRAIN(076): [850/879] Batch: 0.1611 (0.1696) Data: 0.0087 (0.0131) Loss: 1.0170 (0.8688)
[2022/12/29 02:46] | ------------------------------------------------------------
[2022/12/29 02:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:46] | ------------------------------------------------------------
[2022/12/29 02:46] |    TRAIN(76)     0:02:28     0:00:11     0:02:17      0.8677
[2022/12/29 02:46] | ------------------------------------------------------------
[2022/12/29 02:46] | VALID(076): [ 50/220] Batch: 0.0508 (0.0824) Data: 0.0295 (0.0605) Loss: 0.7825 (0.8462)
[2022/12/29 02:46] | VALID(076): [100/220] Batch: 0.0507 (0.0680) Data: 0.0328 (0.0470) Loss: 1.1078 (0.8684)
[2022/12/29 02:46] | VALID(076): [150/220] Batch: 0.0526 (0.0633) Data: 0.0331 (0.0419) Loss: 0.7587 (0.8605)
[2022/12/29 02:46] | VALID(076): [200/220] Batch: 0.0568 (0.0611) Data: 0.0286 (0.0392) Loss: 0.5178 (0.8681)
[2022/12/29 02:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:46] |    VALID(76)      0.8678      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:46] | ####################################################################################################
[2022/12/29 02:46] | TRAIN(077): [ 50/879] Batch: 0.1571 (0.2041) Data: 0.0088 (0.0445) Loss: 0.7750 (0.8952)
[2022/12/29 02:46] | TRAIN(077): [100/879] Batch: 0.1695 (0.1879) Data: 0.0096 (0.0277) Loss: 0.8814 (0.8916)
[2022/12/29 02:46] | TRAIN(077): [150/879] Batch: 0.1599 (0.1803) Data: 0.0097 (0.0219) Loss: 1.0455 (0.8780)
[2022/12/29 02:47] | TRAIN(077): [200/879] Batch: 0.1609 (0.1756) Data: 0.0090 (0.0188) Loss: 0.8524 (0.8616)
[2022/12/29 02:47] | TRAIN(077): [250/879] Batch: 0.1609 (0.1725) Data: 0.0086 (0.0169) Loss: 1.1525 (0.8687)
[2022/12/29 02:47] | TRAIN(077): [300/879] Batch: 0.1631 (0.1704) Data: 0.0096 (0.0156) Loss: 0.8128 (0.8664)
[2022/12/29 02:47] | TRAIN(077): [350/879] Batch: 0.1622 (0.1692) Data: 0.0090 (0.0148) Loss: 1.0877 (0.8647)
[2022/12/29 02:47] | TRAIN(077): [400/879] Batch: 0.1606 (0.1680) Data: 0.0095 (0.0141) Loss: 1.0943 (0.8636)
[2022/12/29 02:47] | TRAIN(077): [450/879] Batch: 0.1625 (0.1671) Data: 0.0088 (0.0135) Loss: 0.8131 (0.8660)
[2022/12/29 02:47] | TRAIN(077): [500/879] Batch: 0.1605 (0.1666) Data: 0.0095 (0.0131) Loss: 0.8182 (0.8699)
[2022/12/29 02:48] | TRAIN(077): [550/879] Batch: 0.1581 (0.1661) Data: 0.0094 (0.0128) Loss: 0.8850 (0.8711)
[2022/12/29 02:48] | TRAIN(077): [600/879] Batch: 0.1577 (0.1656) Data: 0.0100 (0.0125) Loss: 1.2077 (0.8701)
[2022/12/29 02:48] | TRAIN(077): [650/879] Batch: 0.1584 (0.1652) Data: 0.0088 (0.0123) Loss: 0.9535 (0.8688)
[2022/12/29 02:48] | TRAIN(077): [700/879] Batch: 0.1612 (0.1648) Data: 0.0087 (0.0121) Loss: 0.6920 (0.8691)
[2022/12/29 02:48] | TRAIN(077): [750/879] Batch: 0.1603 (0.1645) Data: 0.0096 (0.0119) Loss: 0.8615 (0.8701)
[2022/12/29 02:48] | TRAIN(077): [800/879] Batch: 0.1602 (0.1645) Data: 0.0098 (0.0118) Loss: 0.4066 (0.8684)
[2022/12/29 02:48] | TRAIN(077): [850/879] Batch: 0.1597 (0.1645) Data: 0.0087 (0.0117) Loss: 0.8005 (0.8673)
[2022/12/29 02:48] | ------------------------------------------------------------
[2022/12/29 02:48] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:48] | ------------------------------------------------------------
[2022/12/29 02:48] |    TRAIN(77)     0:02:24     0:00:10     0:02:14      0.8677
[2022/12/29 02:48] | ------------------------------------------------------------
[2022/12/29 02:48] | VALID(077): [ 50/220] Batch: 0.0565 (0.0799) Data: 0.0278 (0.0558) Loss: 0.7829 (0.8462)
[2022/12/29 02:49] | VALID(077): [100/220] Batch: 0.0541 (0.0673) Data: 0.0256 (0.0419) Loss: 1.1095 (0.8685)
[2022/12/29 02:49] | VALID(077): [150/220] Batch: 0.0560 (0.0630) Data: 0.0266 (0.0371) Loss: 0.7584 (0.8606)
[2022/12/29 02:49] | VALID(077): [200/220] Batch: 0.0524 (0.0608) Data: 0.0334 (0.0350) Loss: 0.5151 (0.8682)
[2022/12/29 02:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:49] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:49] |    VALID(77)      0.8680      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:49] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:49] | ####################################################################################################
[2022/12/29 02:49] | TRAIN(078): [ 50/879] Batch: 0.1634 (0.1954) Data: 0.0096 (0.0427) Loss: 0.9978 (0.8337)
[2022/12/29 02:49] | TRAIN(078): [100/879] Batch: 0.1622 (0.1779) Data: 0.0093 (0.0263) Loss: 0.9009 (0.8521)
[2022/12/29 02:49] | TRAIN(078): [150/879] Batch: 0.1590 (0.1720) Data: 0.0089 (0.0208) Loss: 0.7766 (0.8470)
[2022/12/29 02:49] | TRAIN(078): [200/879] Batch: 0.1583 (0.1690) Data: 0.0093 (0.0179) Loss: 1.0582 (0.8714)
[2022/12/29 02:49] | TRAIN(078): [250/879] Batch: 0.1568 (0.1672) Data: 0.0090 (0.0162) Loss: 0.9917 (0.8650)
[2022/12/29 02:49] | TRAIN(078): [300/879] Batch: 0.1621 (0.1661) Data: 0.0095 (0.0151) Loss: 0.7966 (0.8615)
[2022/12/29 02:50] | TRAIN(078): [350/879] Batch: 0.1592 (0.1652) Data: 0.0090 (0.0143) Loss: 0.9706 (0.8625)
[2022/12/29 02:50] | TRAIN(078): [400/879] Batch: 0.1578 (0.1646) Data: 0.0095 (0.0137) Loss: 0.9966 (0.8618)
[2022/12/29 02:50] | TRAIN(078): [450/879] Batch: 0.1575 (0.1641) Data: 0.0093 (0.0132) Loss: 0.6857 (0.8612)
[2022/12/29 02:50] | TRAIN(078): [500/879] Batch: 0.1606 (0.1637) Data: 0.0097 (0.0129) Loss: 0.8791 (0.8631)
[2022/12/29 02:50] | TRAIN(078): [550/879] Batch: 0.1761 (0.1634) Data: 0.0112 (0.0126) Loss: 0.8642 (0.8640)
[2022/12/29 02:50] | TRAIN(078): [600/879] Batch: 0.1581 (0.1632) Data: 0.0088 (0.0123) Loss: 0.6294 (0.8632)
[2022/12/29 02:50] | TRAIN(078): [650/879] Batch: 0.1590 (0.1630) Data: 0.0093 (0.0121) Loss: 1.2296 (0.8661)
[2022/12/29 02:51] | TRAIN(078): [700/879] Batch: 0.1583 (0.1628) Data: 0.0089 (0.0119) Loss: 0.6836 (0.8679)
[2022/12/29 02:51] | TRAIN(078): [750/879] Batch: 0.1588 (0.1626) Data: 0.0092 (0.0117) Loss: 0.8769 (0.8657)
[2022/12/29 02:51] | TRAIN(078): [800/879] Batch: 0.1588 (0.1625) Data: 0.0097 (0.0116) Loss: 0.5798 (0.8657)
[2022/12/29 02:51] | TRAIN(078): [850/879] Batch: 0.1593 (0.1623) Data: 0.0097 (0.0114) Loss: 0.7507 (0.8656)
[2022/12/29 02:51] | ------------------------------------------------------------
[2022/12/29 02:51] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:51] | ------------------------------------------------------------
[2022/12/29 02:51] |    TRAIN(78)     0:02:22     0:00:09     0:02:12      0.8677
[2022/12/29 02:51] | ------------------------------------------------------------
[2022/12/29 02:51] | VALID(078): [ 50/220] Batch: 0.0531 (0.0772) Data: 0.0328 (0.0556) Loss: 0.7848 (0.8467)
[2022/12/29 02:51] | VALID(078): [100/220] Batch: 0.0550 (0.0654) Data: 0.0348 (0.0452) Loss: 1.1063 (0.8684)
[2022/12/29 02:51] | VALID(078): [150/220] Batch: 0.0518 (0.0615) Data: 0.0371 (0.0416) Loss: 0.7605 (0.8607)
[2022/12/29 02:51] | VALID(078): [200/220] Batch: 0.0540 (0.0596) Data: 0.0350 (0.0399) Loss: 0.5230 (0.8681)
[2022/12/29 02:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:51] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:51] |    VALID(78)      0.8680      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:51] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:51] | ####################################################################################################
[2022/12/29 02:51] | TRAIN(079): [ 50/879] Batch: 0.1627 (0.1925) Data: 0.0100 (0.0405) Loss: 0.9564 (0.8727)
[2022/12/29 02:52] | TRAIN(079): [100/879] Batch: 0.1614 (0.1762) Data: 0.0095 (0.0252) Loss: 0.6802 (0.8715)
[2022/12/29 02:52] | TRAIN(079): [150/879] Batch: 0.1624 (0.1709) Data: 0.0092 (0.0201) Loss: 0.9935 (0.8809)
[2022/12/29 02:52] | TRAIN(079): [200/879] Batch: 0.1604 (0.1682) Data: 0.0093 (0.0176) Loss: 0.8187 (0.8818)
[2022/12/29 02:52] | TRAIN(079): [250/879] Batch: 0.1590 (0.1665) Data: 0.0096 (0.0160) Loss: 1.1236 (0.8775)
[2022/12/29 02:52] | TRAIN(079): [300/879] Batch: 0.1591 (0.1654) Data: 0.0098 (0.0150) Loss: 0.8254 (0.8750)
[2022/12/29 02:52] | TRAIN(079): [350/879] Batch: 0.1589 (0.1646) Data: 0.0092 (0.0142) Loss: 0.7905 (0.8740)
[2022/12/29 02:52] | TRAIN(079): [400/879] Batch: 0.1583 (0.1642) Data: 0.0096 (0.0137) Loss: 1.0040 (0.8703)
[2022/12/29 02:52] | TRAIN(079): [450/879] Batch: 0.1595 (0.1639) Data: 0.0075 (0.0132) Loss: 0.8886 (0.8709)
[2022/12/29 02:53] | TRAIN(079): [500/879] Batch: 0.1586 (0.1636) Data: 0.0090 (0.0129) Loss: 0.8958 (0.8714)
[2022/12/29 02:53] | TRAIN(079): [550/879] Batch: 0.1584 (0.1633) Data: 0.0089 (0.0126) Loss: 0.8584 (0.8703)
[2022/12/29 02:53] | TRAIN(079): [600/879] Batch: 0.1591 (0.1630) Data: 0.0098 (0.0123) Loss: 0.8418 (0.8671)
[2022/12/29 02:53] | TRAIN(079): [650/879] Batch: 0.1612 (0.1628) Data: 0.0097 (0.0121) Loss: 0.8825 (0.8665)
[2022/12/29 02:53] | TRAIN(079): [700/879] Batch: 0.1615 (0.1627) Data: 0.0101 (0.0119) Loss: 0.7189 (0.8686)
[2022/12/29 02:53] | TRAIN(079): [750/879] Batch: 0.1594 (0.1625) Data: 0.0102 (0.0118) Loss: 0.4471 (0.8653)
[2022/12/29 02:53] | TRAIN(079): [800/879] Batch: 0.1592 (0.1624) Data: 0.0095 (0.0116) Loss: 0.9485 (0.8691)
[2022/12/29 02:54] | TRAIN(079): [850/879] Batch: 0.1582 (0.1623) Data: 0.0112 (0.0115) Loss: 1.0668 (0.8676)
[2022/12/29 02:54] | ------------------------------------------------------------
[2022/12/29 02:54] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:54] | ------------------------------------------------------------
[2022/12/29 02:54] |    TRAIN(79)     0:02:22     0:00:10     0:02:12      0.8677
[2022/12/29 02:54] | ------------------------------------------------------------
[2022/12/29 02:54] | VALID(079): [ 50/220] Batch: 0.0513 (0.0775) Data: 0.0369 (0.0553) Loss: 0.7835 (0.8465)
[2022/12/29 02:54] | VALID(079): [100/220] Batch: 0.0536 (0.0657) Data: 0.0367 (0.0451) Loss: 1.1072 (0.8684)
[2022/12/29 02:54] | VALID(079): [150/220] Batch: 0.0551 (0.0618) Data: 0.0349 (0.0416) Loss: 0.7597 (0.8606)
[2022/12/29 02:54] | VALID(079): [200/220] Batch: 0.0608 (0.0599) Data: 0.0227 (0.0391) Loss: 0.5206 (0.8681)
[2022/12/29 02:54] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:54] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:54] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:54] |    VALID(79)      0.8679      0.7347      0.4999      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:54] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:54] | ####################################################################################################
[2022/12/29 02:54] | TRAIN(080): [ 50/879] Batch: 0.1604 (0.1964) Data: 0.0100 (0.0430) Loss: 0.5548 (0.8630)
[2022/12/29 02:54] | TRAIN(080): [100/879] Batch: 0.1597 (0.1788) Data: 0.0099 (0.0265) Loss: 0.8896 (0.8843)
[2022/12/29 02:54] | TRAIN(080): [150/879] Batch: 0.1586 (0.1729) Data: 0.0099 (0.0210) Loss: 0.9201 (0.8840)
[2022/12/29 02:54] | TRAIN(080): [200/879] Batch: 0.1580 (0.1709) Data: 0.0106 (0.0184) Loss: 0.9532 (0.8834)
[2022/12/29 02:55] | TRAIN(080): [250/879] Batch: 0.1770 (0.1701) Data: 0.0114 (0.0167) Loss: 0.6801 (0.8710)
[2022/12/29 02:55] | TRAIN(080): [300/879] Batch: 0.1605 (0.1713) Data: 0.0098 (0.0158) Loss: 0.8479 (0.8671)
[2022/12/29 02:55] | TRAIN(080): [350/879] Batch: 0.1614 (0.1697) Data: 0.0099 (0.0149) Loss: 1.1860 (0.8635)
[2022/12/29 02:55] | TRAIN(080): [400/879] Batch: 0.1609 (0.1686) Data: 0.0097 (0.0143) Loss: 0.8179 (0.8665)
[2022/12/29 02:55] | TRAIN(080): [450/879] Batch: 0.1617 (0.1677) Data: 0.0090 (0.0138) Loss: 1.2369 (0.8662)
[2022/12/29 02:55] | TRAIN(080): [500/879] Batch: 0.1613 (0.1669) Data: 0.0086 (0.0133) Loss: 0.9204 (0.8681)
[2022/12/29 02:55] | TRAIN(080): [550/879] Batch: 0.1619 (0.1663) Data: 0.0095 (0.0130) Loss: 0.9438 (0.8717)
[2022/12/29 02:55] | TRAIN(080): [600/879] Batch: 0.1617 (0.1658) Data: 0.0091 (0.0127) Loss: 0.5712 (0.8704)
[2022/12/29 02:56] | TRAIN(080): [650/879] Batch: 0.1617 (0.1653) Data: 0.0089 (0.0124) Loss: 1.0040 (0.8693)
[2022/12/29 02:56] | TRAIN(080): [700/879] Batch: 0.1606 (0.1649) Data: 0.0090 (0.0122) Loss: 0.7744 (0.8694)
[2022/12/29 02:56] | TRAIN(080): [750/879] Batch: 0.1596 (0.1646) Data: 0.0087 (0.0119) Loss: 0.8881 (0.8715)
[2022/12/29 02:56] | TRAIN(080): [800/879] Batch: 0.1586 (0.1643) Data: 0.0091 (0.0118) Loss: 0.6633 (0.8701)
[2022/12/29 02:56] | TRAIN(080): [850/879] Batch: 0.1591 (0.1640) Data: 0.0088 (0.0116) Loss: 0.7490 (0.8686)
[2022/12/29 02:56] | ------------------------------------------------------------
[2022/12/29 02:56] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:56] | ------------------------------------------------------------
[2022/12/29 02:56] |    TRAIN(80)     0:02:23     0:00:10     0:02:13      0.8675
[2022/12/29 02:56] | ------------------------------------------------------------
[2022/12/29 02:56] | VALID(080): [ 50/220] Batch: 0.0541 (0.0754) Data: 0.0379 (0.0557) Loss: 0.7829 (0.8461)
[2022/12/29 02:56] | VALID(080): [100/220] Batch: 0.0532 (0.0648) Data: 0.0287 (0.0422) Loss: 1.1119 (0.8684)
[2022/12/29 02:56] | VALID(080): [150/220] Batch: 0.0533 (0.0614) Data: 0.0259 (0.0377) Loss: 0.7577 (0.8605)
[2022/12/29 02:56] | VALID(080): [200/220] Batch: 0.0512 (0.0596) Data: 0.0293 (0.0352) Loss: 0.5145 (0.8681)
[2022/12/29 02:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:56] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:56] |    VALID(80)      0.8679      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:56] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:56] | ####################################################################################################
[2022/12/29 02:57] | TRAIN(081): [ 50/879] Batch: 0.1574 (0.1960) Data: 0.0102 (0.0442) Loss: 1.0383 (0.8852)
[2022/12/29 02:57] | TRAIN(081): [100/879] Batch: 0.1829 (0.1798) Data: 0.0111 (0.0273) Loss: 0.7758 (0.8668)
[2022/12/29 02:57] | TRAIN(081): [150/879] Batch: 0.1865 (0.1744) Data: 0.0116 (0.0216) Loss: 0.8337 (0.8728)
[2022/12/29 02:57] | TRAIN(081): [200/879] Batch: 0.1634 (0.1719) Data: 0.0098 (0.0187) Loss: 0.9888 (0.8792)
[2022/12/29 02:57] | TRAIN(081): [250/879] Batch: 0.1616 (0.1695) Data: 0.0100 (0.0168) Loss: 1.1774 (0.8698)
[2022/12/29 02:57] | TRAIN(081): [300/879] Batch: 0.1585 (0.1679) Data: 0.0092 (0.0156) Loss: 1.0606 (0.8702)
[2022/12/29 02:57] | TRAIN(081): [350/879] Batch: 0.1630 (0.1668) Data: 0.0097 (0.0148) Loss: 0.9731 (0.8668)
[2022/12/29 02:58] | TRAIN(081): [400/879] Batch: 0.1588 (0.1663) Data: 0.0097 (0.0142) Loss: 0.6301 (0.8678)
[2022/12/29 02:58] | TRAIN(081): [450/879] Batch: 0.1587 (0.1656) Data: 0.0095 (0.0137) Loss: 0.8618 (0.8677)
[2022/12/29 02:58] | TRAIN(081): [500/879] Batch: 0.1600 (0.1651) Data: 0.0096 (0.0133) Loss: 0.4094 (0.8680)
[2022/12/29 02:58] | TRAIN(081): [550/879] Batch: 0.1616 (0.1646) Data: 0.0101 (0.0129) Loss: 0.8432 (0.8698)
[2022/12/29 02:58] | TRAIN(081): [600/879] Batch: 0.1612 (0.1643) Data: 0.0094 (0.0127) Loss: 0.7844 (0.8717)
[2022/12/29 02:58] | TRAIN(081): [650/879] Batch: 0.1690 (0.1640) Data: 0.0088 (0.0124) Loss: 0.8161 (0.8712)
[2022/12/29 02:58] | TRAIN(081): [700/879] Batch: 0.1592 (0.1640) Data: 0.0097 (0.0123) Loss: 0.8464 (0.8689)
[2022/12/29 02:58] | TRAIN(081): [750/879] Batch: 0.1591 (0.1637) Data: 0.0101 (0.0121) Loss: 0.9880 (0.8690)
[2022/12/29 02:59] | TRAIN(081): [800/879] Batch: 0.1592 (0.1640) Data: 0.0096 (0.0120) Loss: 0.9203 (0.8678)
[2022/12/29 02:59] | TRAIN(081): [850/879] Batch: 0.1614 (0.1641) Data: 0.0099 (0.0119) Loss: 0.9664 (0.8676)
[2022/12/29 02:59] | ------------------------------------------------------------
[2022/12/29 02:59] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 02:59] | ------------------------------------------------------------
[2022/12/29 02:59] |    TRAIN(81)     0:02:24     0:00:10     0:02:13      0.8676
[2022/12/29 02:59] | ------------------------------------------------------------
[2022/12/29 02:59] | VALID(081): [ 50/220] Batch: 0.0558 (0.0770) Data: 0.0339 (0.0562) Loss: 0.7830 (0.8463)
[2022/12/29 02:59] | VALID(081): [100/220] Batch: 0.0526 (0.0654) Data: 0.0370 (0.0455) Loss: 1.1101 (0.8684)
[2022/12/29 02:59] | VALID(081): [150/220] Batch: 0.0522 (0.0615) Data: 0.0361 (0.0419) Loss: 0.7584 (0.8606)
[2022/12/29 02:59] | VALID(081): [200/220] Batch: 0.0519 (0.0596) Data: 0.0303 (0.0398) Loss: 0.5174 (0.8681)
[2022/12/29 02:59] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:59] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 02:59] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:59] |    VALID(81)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 02:59] | ------------------------------------------------------------------------------------------------
[2022/12/29 02:59] | ####################################################################################################
[2022/12/29 02:59] | TRAIN(082): [ 50/879] Batch: 0.1637 (0.1977) Data: 0.0115 (0.0433) Loss: 0.8147 (0.8360)
[2022/12/29 02:59] | TRAIN(082): [100/879] Batch: 0.1612 (0.1802) Data: 0.0113 (0.0269) Loss: 0.8557 (0.8395)
[2022/12/29 02:59] | TRAIN(082): [150/879] Batch: 0.1678 (0.1741) Data: 0.0093 (0.0212) Loss: 0.9980 (0.8331)
[2022/12/29 03:00] | TRAIN(082): [200/879] Batch: 0.1619 (0.1706) Data: 0.0104 (0.0183) Loss: 1.1041 (0.8514)
[2022/12/29 03:00] | TRAIN(082): [250/879] Batch: 0.1615 (0.1684) Data: 0.0096 (0.0165) Loss: 0.8494 (0.8535)
[2022/12/29 03:00] | TRAIN(082): [300/879] Batch: 0.1627 (0.1670) Data: 0.0088 (0.0153) Loss: 0.7543 (0.8497)
[2022/12/29 03:00] | TRAIN(082): [350/879] Batch: 0.1625 (0.1660) Data: 0.0094 (0.0144) Loss: 0.6678 (0.8517)
[2022/12/29 03:00] | TRAIN(082): [400/879] Batch: 0.1612 (0.1653) Data: 0.0094 (0.0138) Loss: 0.7055 (0.8550)
[2022/12/29 03:00] | TRAIN(082): [450/879] Batch: 0.1573 (0.1647) Data: 0.0094 (0.0134) Loss: 0.9770 (0.8601)
[2022/12/29 03:00] | TRAIN(082): [500/879] Batch: 0.1592 (0.1645) Data: 0.0100 (0.0130) Loss: 0.8596 (0.8611)
[2022/12/29 03:01] | TRAIN(082): [550/879] Batch: 0.1586 (0.1643) Data: 0.0088 (0.0128) Loss: 0.8717 (0.8639)
[2022/12/29 03:01] | TRAIN(082): [600/879] Batch: 0.1591 (0.1640) Data: 0.0099 (0.0125) Loss: 0.7041 (0.8607)
[2022/12/29 03:01] | TRAIN(082): [650/879] Batch: 0.1598 (0.1637) Data: 0.0099 (0.0123) Loss: 0.8455 (0.8609)
[2022/12/29 03:01] | TRAIN(082): [700/879] Batch: 0.1600 (0.1635) Data: 0.0093 (0.0121) Loss: 0.9346 (0.8627)
[2022/12/29 03:01] | TRAIN(082): [750/879] Batch: 0.1603 (0.1633) Data: 0.0098 (0.0119) Loss: 0.5806 (0.8621)
[2022/12/29 03:01] | TRAIN(082): [800/879] Batch: 0.1576 (0.1631) Data: 0.0162 (0.0118) Loss: 0.9970 (0.8660)
[2022/12/29 03:01] | TRAIN(082): [850/879] Batch: 0.1583 (0.1629) Data: 0.0092 (0.0116) Loss: 1.0194 (0.8668)
[2022/12/29 03:01] | ------------------------------------------------------------
[2022/12/29 03:01] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:01] | ------------------------------------------------------------
[2022/12/29 03:01] |    TRAIN(82)     0:02:23     0:00:10     0:02:12      0.8675
[2022/12/29 03:01] | ------------------------------------------------------------
[2022/12/29 03:02] | VALID(082): [ 50/220] Batch: 0.0550 (0.0766) Data: 0.0348 (0.0541) Loss: 0.7839 (0.8468)
[2022/12/29 03:02] | VALID(082): [100/220] Batch: 0.0532 (0.0650) Data: 0.0334 (0.0442) Loss: 1.1038 (0.8685)
[2022/12/29 03:02] | VALID(082): [150/220] Batch: 0.0514 (0.0613) Data: 0.0292 (0.0393) Loss: 0.7613 (0.8609)
[2022/12/29 03:02] | VALID(082): [200/220] Batch: 0.0535 (0.0595) Data: 0.0253 (0.0363) Loss: 0.5267 (0.8682)
[2022/12/29 03:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:02] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:02] |    VALID(82)      0.8680      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:02] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:02] | ####################################################################################################
[2022/12/29 03:02] | TRAIN(083): [ 50/879] Batch: 0.1598 (0.1991) Data: 0.0099 (0.0462) Loss: 0.8734 (0.8385)
[2022/12/29 03:02] | TRAIN(083): [100/879] Batch: 0.1597 (0.1805) Data: 0.0095 (0.0282) Loss: 0.9227 (0.8522)
[2022/12/29 03:02] | TRAIN(083): [150/879] Batch: 0.1594 (0.1737) Data: 0.0096 (0.0221) Loss: 1.0486 (0.8587)
[2022/12/29 03:02] | TRAIN(083): [200/879] Batch: 0.1589 (0.1706) Data: 0.0093 (0.0190) Loss: 0.9756 (0.8601)
[2022/12/29 03:02] | TRAIN(083): [250/879] Batch: 0.1592 (0.1685) Data: 0.0095 (0.0172) Loss: 0.8068 (0.8669)
[2022/12/29 03:03] | TRAIN(083): [300/879] Batch: 0.1584 (0.1682) Data: 0.0100 (0.0160) Loss: 1.0800 (0.8724)
[2022/12/29 03:03] | TRAIN(083): [350/879] Batch: 0.1579 (0.1673) Data: 0.0092 (0.0151) Loss: 0.9122 (0.8744)
[2022/12/29 03:03] | TRAIN(083): [400/879] Batch: 0.1640 (0.1665) Data: 0.0098 (0.0144) Loss: 0.7669 (0.8687)
[2022/12/29 03:03] | TRAIN(083): [450/879] Batch: 0.1579 (0.1659) Data: 0.0100 (0.0139) Loss: 1.1445 (0.8700)
[2022/12/29 03:03] | TRAIN(083): [500/879] Batch: 0.1569 (0.1653) Data: 0.0091 (0.0134) Loss: 0.7276 (0.8667)
[2022/12/29 03:03] | TRAIN(083): [550/879] Batch: 0.1634 (0.1649) Data: 0.0088 (0.0131) Loss: 0.7749 (0.8610)
[2022/12/29 03:03] | TRAIN(083): [600/879] Batch: 0.1601 (0.1645) Data: 0.0075 (0.0128) Loss: 1.2190 (0.8647)
[2022/12/29 03:03] | TRAIN(083): [650/879] Batch: 0.1585 (0.1642) Data: 0.0095 (0.0126) Loss: 1.0895 (0.8629)
[2022/12/29 03:04] | TRAIN(083): [700/879] Batch: 0.1603 (0.1639) Data: 0.0100 (0.0124) Loss: 0.8010 (0.8623)
[2022/12/29 03:04] | TRAIN(083): [750/879] Batch: 0.1862 (0.1637) Data: 0.0120 (0.0122) Loss: 1.1613 (0.8669)
[2022/12/29 03:04] | TRAIN(083): [800/879] Batch: 0.1598 (0.1635) Data: 0.0097 (0.0120) Loss: 0.7042 (0.8647)
[2022/12/29 03:04] | TRAIN(083): [850/879] Batch: 0.1603 (0.1633) Data: 0.0093 (0.0119) Loss: 1.2158 (0.8663)
[2022/12/29 03:04] | ------------------------------------------------------------
[2022/12/29 03:04] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:04] | ------------------------------------------------------------
[2022/12/29 03:04] |    TRAIN(83)     0:02:23     0:00:10     0:02:12      0.8676
[2022/12/29 03:04] | ------------------------------------------------------------
[2022/12/29 03:04] | VALID(083): [ 50/220] Batch: 0.0517 (0.0767) Data: 0.0376 (0.0555) Loss: 0.7845 (0.8467)
[2022/12/29 03:04] | VALID(083): [100/220] Batch: 0.0548 (0.0652) Data: 0.0341 (0.0450) Loss: 1.1063 (0.8685)
[2022/12/29 03:04] | VALID(083): [150/220] Batch: 0.0534 (0.0614) Data: 0.0330 (0.0404) Loss: 0.7602 (0.8607)
[2022/12/29 03:04] | VALID(083): [200/220] Batch: 0.0552 (0.0595) Data: 0.0352 (0.0390) Loss: 0.5228 (0.8681)
[2022/12/29 03:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:04] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:04] |    VALID(83)      0.8680      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:04] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:04] | ####################################################################################################
[2022/12/29 03:04] | TRAIN(084): [ 50/879] Batch: 0.1596 (0.1964) Data: 0.0098 (0.0437) Loss: 0.8161 (0.9021)
[2022/12/29 03:05] | TRAIN(084): [100/879] Batch: 0.1593 (0.1796) Data: 0.0097 (0.0269) Loss: 0.7697 (0.8899)
[2022/12/29 03:05] | TRAIN(084): [150/879] Batch: 0.1612 (0.1732) Data: 0.0092 (0.0211) Loss: 0.6823 (0.8762)
[2022/12/29 03:05] | TRAIN(084): [200/879] Batch: 0.1618 (0.1699) Data: 0.0092 (0.0182) Loss: 1.0060 (0.8769)
[2022/12/29 03:05] | TRAIN(084): [250/879] Batch: 0.1593 (0.1679) Data: 0.0091 (0.0164) Loss: 1.1689 (0.8742)
[2022/12/29 03:05] | TRAIN(084): [300/879] Batch: 0.1589 (0.1666) Data: 0.0094 (0.0152) Loss: 0.9593 (0.8735)
[2022/12/29 03:05] | TRAIN(084): [350/879] Batch: 0.1586 (0.1657) Data: 0.0098 (0.0144) Loss: 1.1658 (0.8697)
[2022/12/29 03:05] | TRAIN(084): [400/879] Batch: 0.1573 (0.1650) Data: 0.0098 (0.0138) Loss: 1.0589 (0.8689)
[2022/12/29 03:06] | TRAIN(084): [450/879] Batch: 0.1583 (0.1644) Data: 0.0090 (0.0133) Loss: 0.5899 (0.8720)
[2022/12/29 03:06] | TRAIN(084): [500/879] Batch: 0.1583 (0.1640) Data: 0.0088 (0.0129) Loss: 0.6798 (0.8723)
[2022/12/29 03:06] | TRAIN(084): [550/879] Batch: 0.1595 (0.1636) Data: 0.0094 (0.0126) Loss: 1.1835 (0.8692)
[2022/12/29 03:06] | TRAIN(084): [600/879] Batch: 0.1584 (0.1633) Data: 0.0098 (0.0123) Loss: 0.6678 (0.8719)
[2022/12/29 03:06] | TRAIN(084): [650/879] Batch: 0.1584 (0.1631) Data: 0.0093 (0.0121) Loss: 0.8343 (0.8698)
[2022/12/29 03:06] | TRAIN(084): [700/879] Batch: 0.1583 (0.1628) Data: 0.0097 (0.0119) Loss: 0.8337 (0.8697)
[2022/12/29 03:06] | TRAIN(084): [750/879] Batch: 0.1588 (0.1627) Data: 0.0098 (0.0117) Loss: 1.1416 (0.8681)
[2022/12/29 03:06] | TRAIN(084): [800/879] Batch: 0.1581 (0.1626) Data: 0.0093 (0.0116) Loss: 0.6163 (0.8672)
[2022/12/29 03:07] | TRAIN(084): [850/879] Batch: 0.1578 (0.1624) Data: 0.0096 (0.0115) Loss: 0.8043 (0.8660)
[2022/12/29 03:07] | ------------------------------------------------------------
[2022/12/29 03:07] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:07] | ------------------------------------------------------------
[2022/12/29 03:07] |    TRAIN(84)     0:02:22     0:00:10     0:02:12      0.8676
[2022/12/29 03:07] | ------------------------------------------------------------
[2022/12/29 03:07] | VALID(084): [ 50/220] Batch: 0.0542 (0.0806) Data: 0.0280 (0.0570) Loss: 0.7829 (0.8463)
[2022/12/29 03:07] | VALID(084): [100/220] Batch: 0.0546 (0.0675) Data: 0.0257 (0.0424) Loss: 1.1088 (0.8685)
[2022/12/29 03:07] | VALID(084): [150/220] Batch: 0.0567 (0.0632) Data: 0.0301 (0.0374) Loss: 0.7587 (0.8606)
[2022/12/29 03:07] | VALID(084): [200/220] Batch: 0.0632 (0.0609) Data: 0.0255 (0.0354) Loss: 0.5175 (0.8681)
[2022/12/29 03:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:07] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:07] |    VALID(84)      0.8679      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:07] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:07] | ####################################################################################################
[2022/12/29 03:07] | TRAIN(085): [ 50/879] Batch: 0.1589 (0.1980) Data: 0.0096 (0.0431) Loss: 1.0254 (0.9101)
[2022/12/29 03:07] | TRAIN(085): [100/879] Batch: 0.1611 (0.1798) Data: 0.0091 (0.0265) Loss: 1.0188 (0.8879)
[2022/12/29 03:07] | TRAIN(085): [150/879] Batch: 0.1596 (0.1733) Data: 0.0093 (0.0208) Loss: 0.7923 (0.8835)
[2022/12/29 03:07] | TRAIN(085): [200/879] Batch: 0.1787 (0.1712) Data: 0.0113 (0.0182) Loss: 0.6100 (0.8670)
[2022/12/29 03:08] | TRAIN(085): [250/879] Batch: 0.1587 (0.1691) Data: 0.0087 (0.0165) Loss: 1.2209 (0.8648)
[2022/12/29 03:08] | TRAIN(085): [300/879] Batch: 0.1585 (0.1676) Data: 0.0097 (0.0153) Loss: 0.8329 (0.8612)
[2022/12/29 03:08] | TRAIN(085): [350/879] Batch: 0.1583 (0.1665) Data: 0.0096 (0.0145) Loss: 0.6764 (0.8597)
[2022/12/29 03:08] | TRAIN(085): [400/879] Batch: 0.1591 (0.1657) Data: 0.0098 (0.0138) Loss: 0.9903 (0.8572)
[2022/12/29 03:08] | TRAIN(085): [450/879] Batch: 0.1605 (0.1651) Data: 0.0093 (0.0133) Loss: 0.5878 (0.8537)
[2022/12/29 03:08] | TRAIN(085): [500/879] Batch: 0.1791 (0.1646) Data: 0.0113 (0.0130) Loss: 0.8764 (0.8594)
[2022/12/29 03:08] | TRAIN(085): [550/879] Batch: 0.1591 (0.1642) Data: 0.0091 (0.0126) Loss: 0.7664 (0.8572)
[2022/12/29 03:09] | TRAIN(085): [600/879] Batch: 0.1628 (0.1638) Data: 0.0100 (0.0124) Loss: 1.0729 (0.8609)
[2022/12/29 03:09] | TRAIN(085): [650/879] Batch: 0.1604 (0.1635) Data: 0.0088 (0.0121) Loss: 0.8171 (0.8628)
[2022/12/29 03:09] | TRAIN(085): [700/879] Batch: 0.1593 (0.1633) Data: 0.0106 (0.0119) Loss: 0.6541 (0.8627)
[2022/12/29 03:09] | TRAIN(085): [750/879] Batch: 0.1594 (0.1632) Data: 0.0095 (0.0118) Loss: 0.8340 (0.8653)
[2022/12/29 03:09] | TRAIN(085): [800/879] Batch: 0.1602 (0.1632) Data: 0.0098 (0.0117) Loss: 0.7829 (0.8675)
[2022/12/29 03:09] | TRAIN(085): [850/879] Batch: 0.1604 (0.1630) Data: 0.0093 (0.0115) Loss: 0.5836 (0.8686)
[2022/12/29 03:09] | ------------------------------------------------------------
[2022/12/29 03:09] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:09] | ------------------------------------------------------------
[2022/12/29 03:09] |    TRAIN(85)     0:02:23     0:00:10     0:02:13      0.8675
[2022/12/29 03:09] | ------------------------------------------------------------
[2022/12/29 03:09] | VALID(085): [ 50/220] Batch: 0.0532 (0.0770) Data: 0.0328 (0.0550) Loss: 0.7839 (0.8466)
[2022/12/29 03:09] | VALID(085): [100/220] Batch: 0.0549 (0.0654) Data: 0.0365 (0.0445) Loss: 1.1059 (0.8685)
[2022/12/29 03:09] | VALID(085): [150/220] Batch: 0.0522 (0.0616) Data: 0.0382 (0.0413) Loss: 0.7601 (0.8607)
[2022/12/29 03:09] | VALID(085): [200/220] Batch: 0.0539 (0.0598) Data: 0.0337 (0.0397) Loss: 0.5222 (0.8681)
[2022/12/29 03:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:09] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:09] |    VALID(85)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:09] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:09] | ####################################################################################################
[2022/12/29 03:10] | TRAIN(086): [ 50/879] Batch: 0.1739 (0.1977) Data: 0.0093 (0.0439) Loss: 1.1146 (0.8660)
[2022/12/29 03:10] | TRAIN(086): [100/879] Batch: 0.1603 (0.1791) Data: 0.0100 (0.0271) Loss: 0.9417 (0.8808)
[2022/12/29 03:10] | TRAIN(086): [150/879] Batch: 0.1617 (0.1728) Data: 0.0105 (0.0214) Loss: 0.8833 (0.8861)
[2022/12/29 03:10] | TRAIN(086): [200/879] Batch: 0.1585 (0.1696) Data: 0.0098 (0.0185) Loss: 0.6223 (0.8786)
[2022/12/29 03:10] | TRAIN(086): [250/879] Batch: 0.1783 (0.1680) Data: 0.0113 (0.0168) Loss: 0.9244 (0.8788)
[2022/12/29 03:10] | TRAIN(086): [300/879] Batch: 0.1609 (0.1667) Data: 0.0103 (0.0156) Loss: 1.0498 (0.8769)
[2022/12/29 03:10] | TRAIN(086): [350/879] Batch: 0.1585 (0.1659) Data: 0.0095 (0.0148) Loss: 0.9461 (0.8716)
[2022/12/29 03:11] | TRAIN(086): [400/879] Batch: 0.1616 (0.1654) Data: 0.0092 (0.0142) Loss: 0.9525 (0.8721)
[2022/12/29 03:11] | TRAIN(086): [450/879] Batch: 0.1600 (0.1649) Data: 0.0098 (0.0137) Loss: 0.7160 (0.8703)
[2022/12/29 03:11] | TRAIN(086): [500/879] Batch: 0.1602 (0.1645) Data: 0.0095 (0.0133) Loss: 1.1134 (0.8708)
[2022/12/29 03:11] | TRAIN(086): [550/879] Batch: 0.1629 (0.1642) Data: 0.0092 (0.0130) Loss: 1.1534 (0.8681)
[2022/12/29 03:11] | TRAIN(086): [600/879] Batch: 0.1609 (0.1640) Data: 0.0092 (0.0127) Loss: 0.7585 (0.8663)
[2022/12/29 03:11] | TRAIN(086): [650/879] Batch: 0.1582 (0.1639) Data: 0.0097 (0.0125) Loss: 0.8565 (0.8674)
[2022/12/29 03:11] | TRAIN(086): [700/879] Batch: 0.1593 (0.1636) Data: 0.0098 (0.0122) Loss: 0.7919 (0.8668)
[2022/12/29 03:12] | TRAIN(086): [750/879] Batch: 0.1592 (0.1633) Data: 0.0099 (0.0121) Loss: 0.5716 (0.8697)
[2022/12/29 03:12] | TRAIN(086): [800/879] Batch: 0.1588 (0.1631) Data: 0.0094 (0.0119) Loss: 0.9399 (0.8693)
[2022/12/29 03:12] | TRAIN(086): [850/879] Batch: 0.1631 (0.1630) Data: 0.0087 (0.0118) Loss: 1.0552 (0.8683)
[2022/12/29 03:12] | ------------------------------------------------------------
[2022/12/29 03:12] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:12] | ------------------------------------------------------------
[2022/12/29 03:12] |    TRAIN(86)     0:02:23     0:00:10     0:02:12      0.8676
[2022/12/29 03:12] | ------------------------------------------------------------
[2022/12/29 03:12] | VALID(086): [ 50/220] Batch: 0.0572 (0.0767) Data: 0.0305 (0.0548) Loss: 0.7835 (0.8463)
[2022/12/29 03:12] | VALID(086): [100/220] Batch: 0.0546 (0.0652) Data: 0.0344 (0.0447) Loss: 1.1082 (0.8683)
[2022/12/29 03:12] | VALID(086): [150/220] Batch: 0.0512 (0.0614) Data: 0.0359 (0.0413) Loss: 0.7590 (0.8605)
[2022/12/29 03:12] | VALID(086): [200/220] Batch: 0.0524 (0.0596) Data: 0.0303 (0.0388) Loss: 0.5185 (0.8680)
[2022/12/29 03:12] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:12] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:12] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:12] |    VALID(86)      0.8678      0.7347      0.4996      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:12] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:12] | ####################################################################################################
[2022/12/29 03:12] | TRAIN(087): [ 50/879] Batch: 0.1591 (0.1938) Data: 0.0099 (0.0418) Loss: 0.9534 (0.8906)
[2022/12/29 03:12] | TRAIN(087): [100/879] Batch: 0.1597 (0.1777) Data: 0.0101 (0.0259) Loss: 1.0446 (0.8839)
[2022/12/29 03:13] | TRAIN(087): [150/879] Batch: 0.1617 (0.1722) Data: 0.0100 (0.0206) Loss: 0.9624 (0.8919)
[2022/12/29 03:13] | TRAIN(087): [200/879] Batch: 0.1588 (0.1694) Data: 0.0094 (0.0179) Loss: 0.4106 (0.8853)
[2022/12/29 03:13] | TRAIN(087): [250/879] Batch: 0.1584 (0.1675) Data: 0.0102 (0.0162) Loss: 0.7834 (0.8780)
[2022/12/29 03:13] | TRAIN(087): [300/879] Batch: 0.1744 (0.1666) Data: 0.0115 (0.0152) Loss: 0.8334 (0.8812)
[2022/12/29 03:13] | TRAIN(087): [350/879] Batch: 0.1606 (0.1662) Data: 0.0099 (0.0144) Loss: 0.8736 (0.8755)
[2022/12/29 03:13] | TRAIN(087): [400/879] Batch: 0.1597 (0.1654) Data: 0.0101 (0.0138) Loss: 0.7909 (0.8746)
[2022/12/29 03:13] | TRAIN(087): [450/879] Batch: 0.1589 (0.1650) Data: 0.0096 (0.0134) Loss: 0.8864 (0.8695)
[2022/12/29 03:13] | TRAIN(087): [500/879] Batch: 0.1605 (0.1646) Data: 0.0087 (0.0130) Loss: 0.9529 (0.8704)
[2022/12/29 03:14] | TRAIN(087): [550/879] Batch: 0.1619 (0.1642) Data: 0.0096 (0.0127) Loss: 0.8406 (0.8668)
[2022/12/29 03:14] | TRAIN(087): [600/879] Batch: 0.1624 (0.1639) Data: 0.0099 (0.0125) Loss: 0.8335 (0.8666)
[2022/12/29 03:14] | TRAIN(087): [650/879] Batch: 0.1609 (0.1636) Data: 0.0092 (0.0122) Loss: 0.7392 (0.8652)
[2022/12/29 03:14] | TRAIN(087): [700/879] Batch: 0.1603 (0.1633) Data: 0.0094 (0.0121) Loss: 0.6538 (0.8698)
[2022/12/29 03:14] | TRAIN(087): [750/879] Batch: 0.1618 (0.1631) Data: 0.0099 (0.0119) Loss: 0.5403 (0.8663)
[2022/12/29 03:14] | TRAIN(087): [800/879] Batch: 0.1612 (0.1629) Data: 0.0089 (0.0117) Loss: 0.7020 (0.8654)
[2022/12/29 03:14] | TRAIN(087): [850/879] Batch: 0.1636 (0.1627) Data: 0.0091 (0.0116) Loss: 1.0388 (0.8675)
[2022/12/29 03:14] | ------------------------------------------------------------
[2022/12/29 03:14] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:14] | ------------------------------------------------------------
[2022/12/29 03:14] |    TRAIN(87)     0:02:22     0:00:10     0:02:12      0.8675
[2022/12/29 03:14] | ------------------------------------------------------------
[2022/12/29 03:15] | VALID(087): [ 50/220] Batch: 0.0520 (0.0764) Data: 0.0363 (0.0537) Loss: 0.7833 (0.8464)
[2022/12/29 03:15] | VALID(087): [100/220] Batch: 0.0514 (0.0650) Data: 0.0370 (0.0441) Loss: 1.1083 (0.8685)
[2022/12/29 03:15] | VALID(087): [150/220] Batch: 0.0528 (0.0612) Data: 0.0341 (0.0409) Loss: 0.7590 (0.8606)
[2022/12/29 03:15] | VALID(087): [200/220] Batch: 0.0550 (0.0594) Data: 0.0346 (0.0393) Loss: 0.5184 (0.8681)
[2022/12/29 03:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:15] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:15] |    VALID(87)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:15] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:15] | ####################################################################################################
[2022/12/29 03:15] | TRAIN(088): [ 50/879] Batch: 0.1603 (0.1970) Data: 0.0094 (0.0415) Loss: 0.8377 (0.8800)
[2022/12/29 03:15] | TRAIN(088): [100/879] Batch: 0.1598 (0.1795) Data: 0.0098 (0.0259) Loss: 1.0573 (0.8738)
[2022/12/29 03:15] | TRAIN(088): [150/879] Batch: 0.1588 (0.1730) Data: 0.0104 (0.0206) Loss: 0.9569 (0.8769)
[2022/12/29 03:15] | TRAIN(088): [200/879] Batch: 0.1627 (0.1697) Data: 0.0091 (0.0178) Loss: 0.6852 (0.8655)
[2022/12/29 03:15] | TRAIN(088): [250/879] Batch: 0.1586 (0.1685) Data: 0.0094 (0.0163) Loss: 1.1366 (0.8700)
[2022/12/29 03:16] | TRAIN(088): [300/879] Batch: 0.1608 (0.1675) Data: 0.0098 (0.0152) Loss: 0.8259 (0.8680)
[2022/12/29 03:16] | TRAIN(088): [350/879] Batch: 0.1578 (0.1665) Data: 0.0095 (0.0144) Loss: 0.5567 (0.8693)
[2022/12/29 03:16] | TRAIN(088): [400/879] Batch: 0.1580 (0.1657) Data: 0.0095 (0.0138) Loss: 1.1032 (0.8672)
[2022/12/29 03:16] | TRAIN(088): [450/879] Batch: 0.1636 (0.1651) Data: 0.0092 (0.0133) Loss: 0.8581 (0.8709)
[2022/12/29 03:16] | TRAIN(088): [500/879] Batch: 0.1616 (0.1647) Data: 0.0162 (0.0130) Loss: 0.6680 (0.8711)
[2022/12/29 03:16] | TRAIN(088): [550/879] Batch: 0.1615 (0.1644) Data: 0.0094 (0.0127) Loss: 0.8095 (0.8719)
[2022/12/29 03:16] | TRAIN(088): [600/879] Batch: 0.1587 (0.1641) Data: 0.0096 (0.0124) Loss: 0.8286 (0.8739)
[2022/12/29 03:16] | TRAIN(088): [650/879] Batch: 0.1588 (0.1638) Data: 0.0093 (0.0122) Loss: 0.7587 (0.8729)
[2022/12/29 03:17] | TRAIN(088): [700/879] Batch: 0.1615 (0.1636) Data: 0.0090 (0.0120) Loss: 0.5558 (0.8700)
[2022/12/29 03:17] | TRAIN(088): [750/879] Batch: 0.1589 (0.1634) Data: 0.0101 (0.0118) Loss: 0.9700 (0.8689)
[2022/12/29 03:17] | TRAIN(088): [800/879] Batch: 0.1583 (0.1632) Data: 0.0091 (0.0117) Loss: 0.7851 (0.8663)
[2022/12/29 03:17] | TRAIN(088): [850/879] Batch: 0.1621 (0.1630) Data: 0.0093 (0.0116) Loss: 0.8894 (0.8674)
[2022/12/29 03:17] | ------------------------------------------------------------
[2022/12/29 03:17] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:17] | ------------------------------------------------------------
[2022/12/29 03:17] |    TRAIN(88)     0:02:23     0:00:10     0:02:13      0.8675
[2022/12/29 03:17] | ------------------------------------------------------------
[2022/12/29 03:17] | VALID(088): [ 50/220] Batch: 0.0520 (0.0765) Data: 0.0349 (0.0522) Loss: 0.7833 (0.8463)
[2022/12/29 03:17] | VALID(088): [100/220] Batch: 0.0539 (0.0651) Data: 0.0338 (0.0435) Loss: 1.1080 (0.8683)
[2022/12/29 03:17] | VALID(088): [150/220] Batch: 0.0548 (0.0613) Data: 0.0347 (0.0405) Loss: 0.7590 (0.8605)
[2022/12/29 03:17] | VALID(088): [200/220] Batch: 0.0526 (0.0593) Data: 0.0356 (0.0389) Loss: 0.5188 (0.8680)
[2022/12/29 03:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:17] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:17] |    VALID(88)      0.8678      0.7347      0.4996      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:17] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:17] | ####################################################################################################
[2022/12/29 03:17] | TRAIN(089): [ 50/879] Batch: 0.1630 (0.1966) Data: 0.0092 (0.0424) Loss: 0.8395 (0.8527)
[2022/12/29 03:18] | TRAIN(089): [100/879] Batch: 0.1629 (0.1805) Data: 0.0097 (0.0265) Loss: 0.7852 (0.8551)
[2022/12/29 03:18] | TRAIN(089): [150/879] Batch: 0.1589 (0.1765) Data: 0.0096 (0.0213) Loss: 0.7734 (0.8492)
[2022/12/29 03:18] | TRAIN(089): [200/879] Batch: 0.1586 (0.1733) Data: 0.0096 (0.0186) Loss: 0.6674 (0.8558)
[2022/12/29 03:18] | TRAIN(089): [250/879] Batch: 0.1579 (0.1707) Data: 0.0091 (0.0168) Loss: 1.0210 (0.8556)
[2022/12/29 03:18] | TRAIN(089): [300/879] Batch: 0.1587 (0.1689) Data: 0.0094 (0.0156) Loss: 0.6258 (0.8551)
[2022/12/29 03:18] | TRAIN(089): [350/879] Batch: 0.1579 (0.1676) Data: 0.0099 (0.0147) Loss: 0.9301 (0.8587)
[2022/12/29 03:18] | TRAIN(089): [400/879] Batch: 0.1587 (0.1667) Data: 0.0093 (0.0141) Loss: 0.7734 (0.8644)
[2022/12/29 03:19] | TRAIN(089): [450/879] Batch: 0.1626 (0.1659) Data: 0.0100 (0.0136) Loss: 0.8148 (0.8662)
[2022/12/29 03:19] | TRAIN(089): [500/879] Batch: 0.1582 (0.1653) Data: 0.0098 (0.0132) Loss: 0.7853 (0.8651)
[2022/12/29 03:19] | TRAIN(089): [550/879] Batch: 0.1576 (0.1649) Data: 0.0095 (0.0129) Loss: 0.8283 (0.8655)
[2022/12/29 03:19] | TRAIN(089): [600/879] Batch: 0.1576 (0.1646) Data: 0.0095 (0.0126) Loss: 0.6843 (0.8621)
[2022/12/29 03:19] | TRAIN(089): [650/879] Batch: 0.1589 (0.1642) Data: 0.0096 (0.0123) Loss: 0.6772 (0.8623)
[2022/12/29 03:19] | TRAIN(089): [700/879] Batch: 0.1599 (0.1639) Data: 0.0087 (0.0121) Loss: 0.8717 (0.8634)
[2022/12/29 03:19] | TRAIN(089): [750/879] Batch: 0.1614 (0.1636) Data: 0.0092 (0.0119) Loss: 0.9082 (0.8649)
[2022/12/29 03:19] | TRAIN(089): [800/879] Batch: 0.1592 (0.1634) Data: 0.0099 (0.0118) Loss: 0.5812 (0.8681)
[2022/12/29 03:20] | TRAIN(089): [850/879] Batch: 0.1611 (0.1633) Data: 0.0097 (0.0117) Loss: 0.8585 (0.8681)
[2022/12/29 03:20] | ------------------------------------------------------------
[2022/12/29 03:20] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:20] | ------------------------------------------------------------
[2022/12/29 03:20] |    TRAIN(89)     0:02:23     0:00:10     0:02:13      0.8675
[2022/12/29 03:20] | ------------------------------------------------------------
[2022/12/29 03:20] | VALID(089): [ 50/220] Batch: 0.0531 (0.0760) Data: 0.0308 (0.0535) Loss: 0.7838 (0.8464)
[2022/12/29 03:20] | VALID(089): [100/220] Batch: 0.0557 (0.0651) Data: 0.0271 (0.0411) Loss: 1.1068 (0.8684)
[2022/12/29 03:20] | VALID(089): [150/220] Batch: 0.0554 (0.0614) Data: 0.0296 (0.0365) Loss: 0.7597 (0.8606)
[2022/12/29 03:20] | VALID(089): [200/220] Batch: 0.0546 (0.0597) Data: 0.0254 (0.0343) Loss: 0.5204 (0.8680)
[2022/12/29 03:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:20] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:20] |    VALID(89)      0.8679      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:20] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:20] | ####################################################################################################
[2022/12/29 03:20] | TRAIN(090): [ 50/879] Batch: 0.1681 (0.1986) Data: 0.0096 (0.0442) Loss: 0.9319 (0.8516)
[2022/12/29 03:20] | TRAIN(090): [100/879] Batch: 0.1619 (0.1797) Data: 0.0095 (0.0272) Loss: 1.0392 (0.8573)
[2022/12/29 03:20] | TRAIN(090): [150/879] Batch: 0.1620 (0.1732) Data: 0.0095 (0.0214) Loss: 0.9998 (0.8595)
[2022/12/29 03:20] | TRAIN(090): [200/879] Batch: 0.1590 (0.1699) Data: 0.0094 (0.0185) Loss: 1.0167 (0.8661)
[2022/12/29 03:21] | TRAIN(090): [250/879] Batch: 0.1775 (0.1689) Data: 0.0109 (0.0168) Loss: 1.0083 (0.8623)
[2022/12/29 03:21] | TRAIN(090): [300/879] Batch: 0.1611 (0.1674) Data: 0.0100 (0.0155) Loss: 0.6346 (0.8578)
[2022/12/29 03:21] | TRAIN(090): [350/879] Batch: 0.1585 (0.1663) Data: 0.0091 (0.0147) Loss: 0.8811 (0.8631)
[2022/12/29 03:21] | TRAIN(090): [400/879] Batch: 0.1607 (0.1655) Data: 0.0087 (0.0141) Loss: 0.8856 (0.8638)
[2022/12/29 03:21] | TRAIN(090): [450/879] Batch: 0.1586 (0.1650) Data: 0.0095 (0.0136) Loss: 1.1593 (0.8701)
[2022/12/29 03:21] | TRAIN(090): [500/879] Batch: 0.1610 (0.1646) Data: 0.0106 (0.0132) Loss: 0.6859 (0.8686)
[2022/12/29 03:21] | TRAIN(090): [550/879] Batch: 0.1580 (0.1642) Data: 0.0086 (0.0128) Loss: 0.8764 (0.8720)
[2022/12/29 03:22] | TRAIN(090): [600/879] Batch: 0.1582 (0.1639) Data: 0.0098 (0.0126) Loss: 0.6792 (0.8721)
[2022/12/29 03:22] | TRAIN(090): [650/879] Batch: 0.1596 (0.1636) Data: 0.0098 (0.0123) Loss: 0.8154 (0.8720)
[2022/12/29 03:22] | TRAIN(090): [700/879] Batch: 0.1576 (0.1633) Data: 0.0091 (0.0121) Loss: 1.2560 (0.8673)
[2022/12/29 03:22] | TRAIN(090): [750/879] Batch: 0.1704 (0.1631) Data: 0.0089 (0.0120) Loss: 0.8159 (0.8672)
[2022/12/29 03:22] | TRAIN(090): [800/879] Batch: 0.1626 (0.1630) Data: 0.0099 (0.0119) Loss: 0.8016 (0.8674)
[2022/12/29 03:22] | TRAIN(090): [850/879] Batch: 0.1599 (0.1629) Data: 0.0102 (0.0117) Loss: 0.7994 (0.8679)
[2022/12/29 03:22] | ------------------------------------------------------------
[2022/12/29 03:22] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:22] | ------------------------------------------------------------
[2022/12/29 03:22] |    TRAIN(90)     0:02:23     0:00:10     0:02:12      0.8675
[2022/12/29 03:22] | ------------------------------------------------------------
[2022/12/29 03:22] | VALID(090): [ 50/220] Batch: 0.0553 (0.0773) Data: 0.0345 (0.0568) Loss: 0.7835 (0.8464)
[2022/12/29 03:22] | VALID(090): [100/220] Batch: 0.0558 (0.0655) Data: 0.0359 (0.0457) Loss: 1.1076 (0.8684)
[2022/12/29 03:22] | VALID(090): [150/220] Batch: 0.0550 (0.0615) Data: 0.0347 (0.0419) Loss: 0.7593 (0.8606)
[2022/12/29 03:22] | VALID(090): [200/220] Batch: 0.0570 (0.0595) Data: 0.0373 (0.0402) Loss: 0.5192 (0.8681)
[2022/12/29 03:23] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:23] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:23] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:23] |    VALID(90)      0.8679      0.7347      0.4957      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:23] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:23] | ####################################################################################################
[2022/12/29 03:23] | TRAIN(091): [ 50/879] Batch: 0.1580 (0.1951) Data: 0.0090 (0.0422) Loss: 0.8147 (0.8916)
[2022/12/29 03:23] | TRAIN(091): [100/879] Batch: 0.1594 (0.1777) Data: 0.0103 (0.0259) Loss: 0.9389 (0.8696)
[2022/12/29 03:23] | TRAIN(091): [150/879] Batch: 0.1593 (0.1718) Data: 0.0087 (0.0203) Loss: 1.0783 (0.8714)
[2022/12/29 03:23] | TRAIN(091): [200/879] Batch: 0.1654 (0.1689) Data: 0.0100 (0.0176) Loss: 0.6530 (0.8639)
[2022/12/29 03:23] | TRAIN(091): [250/879] Batch: 0.1609 (0.1671) Data: 0.0093 (0.0160) Loss: 1.1121 (0.8667)
[2022/12/29 03:23] | TRAIN(091): [300/879] Batch: 0.1631 (0.1659) Data: 0.0103 (0.0150) Loss: 0.8145 (0.8687)
[2022/12/29 03:23] | TRAIN(091): [350/879] Batch: 0.1606 (0.1651) Data: 0.0092 (0.0142) Loss: 0.9100 (0.8745)
[2022/12/29 03:24] | TRAIN(091): [400/879] Batch: 0.1593 (0.1645) Data: 0.0095 (0.0137) Loss: 1.1112 (0.8685)
[2022/12/29 03:24] | TRAIN(091): [450/879] Batch: 0.1593 (0.1640) Data: 0.0098 (0.0133) Loss: 0.8571 (0.8672)
[2022/12/29 03:24] | TRAIN(091): [500/879] Batch: 0.1590 (0.1636) Data: 0.0094 (0.0129) Loss: 0.6793 (0.8659)
[2022/12/29 03:24] | TRAIN(091): [550/879] Batch: 0.1605 (0.1634) Data: 0.0095 (0.0126) Loss: 0.5862 (0.8660)
[2022/12/29 03:24] | TRAIN(091): [600/879] Batch: 0.1598 (0.1632) Data: 0.0097 (0.0124) Loss: 0.9150 (0.8663)
[2022/12/29 03:24] | TRAIN(091): [650/879] Batch: 0.1586 (0.1630) Data: 0.0097 (0.0122) Loss: 1.0842 (0.8669)
[2022/12/29 03:24] | TRAIN(091): [700/879] Batch: 0.1587 (0.1629) Data: 0.0091 (0.0120) Loss: 0.5861 (0.8654)
[2022/12/29 03:25] | TRAIN(091): [750/879] Batch: 0.1625 (0.1627) Data: 0.0092 (0.0118) Loss: 0.7300 (0.8663)
[2022/12/29 03:25] | TRAIN(091): [800/879] Batch: 0.1635 (0.1626) Data: 0.0097 (0.0117) Loss: 0.8858 (0.8653)
[2022/12/29 03:25] | TRAIN(091): [850/879] Batch: 0.1586 (0.1624) Data: 0.0099 (0.0116) Loss: 0.9635 (0.8674)
[2022/12/29 03:25] | ------------------------------------------------------------
[2022/12/29 03:25] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:25] | ------------------------------------------------------------
[2022/12/29 03:25] |    TRAIN(91)     0:02:22     0:00:10     0:02:12      0.8675
[2022/12/29 03:25] | ------------------------------------------------------------
[2022/12/29 03:25] | VALID(091): [ 50/220] Batch: 0.0545 (0.0776) Data: 0.0345 (0.0551) Loss: 0.7835 (0.8464)
[2022/12/29 03:25] | VALID(091): [100/220] Batch: 0.0522 (0.0655) Data: 0.0374 (0.0447) Loss: 1.1078 (0.8684)
[2022/12/29 03:25] | VALID(091): [150/220] Batch: 0.0531 (0.0616) Data: 0.0252 (0.0408) Loss: 0.7592 (0.8606)
[2022/12/29 03:25] | VALID(091): [200/220] Batch: 0.0525 (0.0599) Data: 0.0310 (0.0376) Loss: 0.5191 (0.8681)
[2022/12/29 03:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:25] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:25] |    VALID(91)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:25] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:25] | ####################################################################################################
[2022/12/29 03:25] | TRAIN(092): [ 50/879] Batch: 0.1593 (0.2001) Data: 0.0101 (0.0417) Loss: 0.6820 (0.8698)
[2022/12/29 03:25] | TRAIN(092): [100/879] Batch: 0.1588 (0.1806) Data: 0.0097 (0.0259) Loss: 1.2082 (0.8617)
[2022/12/29 03:26] | TRAIN(092): [150/879] Batch: 0.1604 (0.1741) Data: 0.0100 (0.0205) Loss: 1.0304 (0.8627)
[2022/12/29 03:26] | TRAIN(092): [200/879] Batch: 0.1588 (0.1707) Data: 0.0098 (0.0178) Loss: 0.6360 (0.8662)
[2022/12/29 03:26] | TRAIN(092): [250/879] Batch: 0.1588 (0.1685) Data: 0.0095 (0.0162) Loss: 0.8577 (0.8603)
[2022/12/29 03:26] | TRAIN(092): [300/879] Batch: 0.1586 (0.1671) Data: 0.0100 (0.0151) Loss: 0.8093 (0.8633)
[2022/12/29 03:26] | TRAIN(092): [350/879] Batch: 0.1595 (0.1661) Data: 0.0095 (0.0143) Loss: 0.9229 (0.8630)
[2022/12/29 03:26] | TRAIN(092): [400/879] Batch: 0.1574 (0.1655) Data: 0.0101 (0.0137) Loss: 0.6667 (0.8663)
[2022/12/29 03:26] | TRAIN(092): [450/879] Batch: 0.1586 (0.1652) Data: 0.0099 (0.0133) Loss: 0.8780 (0.8689)
[2022/12/29 03:26] | TRAIN(092): [500/879] Batch: 0.1677 (0.1648) Data: 0.0090 (0.0130) Loss: 1.1267 (0.8657)
[2022/12/29 03:27] | TRAIN(092): [550/879] Batch: 0.1619 (0.1644) Data: 0.0104 (0.0126) Loss: 1.0734 (0.8659)
[2022/12/29 03:27] | TRAIN(092): [600/879] Batch: 0.1585 (0.1641) Data: 0.0100 (0.0124) Loss: 0.9720 (0.8657)
[2022/12/29 03:27] | TRAIN(092): [650/879] Batch: 0.1590 (0.1637) Data: 0.0090 (0.0122) Loss: 0.7533 (0.8660)
[2022/12/29 03:27] | TRAIN(092): [700/879] Batch: 0.1587 (0.1634) Data: 0.0099 (0.0120) Loss: 0.9200 (0.8687)
[2022/12/29 03:27] | TRAIN(092): [750/879] Batch: 0.1588 (0.1632) Data: 0.0096 (0.0118) Loss: 0.7907 (0.8685)
[2022/12/29 03:27] | TRAIN(092): [800/879] Batch: 0.1624 (0.1630) Data: 0.0100 (0.0117) Loss: 0.7409 (0.8673)
[2022/12/29 03:27] | TRAIN(092): [850/879] Batch: 0.1609 (0.1629) Data: 0.0099 (0.0116) Loss: 0.6797 (0.8676)
[2022/12/29 03:28] | ------------------------------------------------------------
[2022/12/29 03:28] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:28] | ------------------------------------------------------------
[2022/12/29 03:28] |    TRAIN(92)     0:02:22     0:00:10     0:02:12      0.8675
[2022/12/29 03:28] | ------------------------------------------------------------
[2022/12/29 03:28] | VALID(092): [ 50/220] Batch: 0.0512 (0.0749) Data: 0.0363 (0.0542) Loss: 0.7834 (0.8463)
[2022/12/29 03:28] | VALID(092): [100/220] Batch: 0.0537 (0.0643) Data: 0.0369 (0.0444) Loss: 1.1075 (0.8683)
[2022/12/29 03:28] | VALID(092): [150/220] Batch: 0.0517 (0.0609) Data: 0.0309 (0.0386) Loss: 0.7592 (0.8605)
[2022/12/29 03:28] | VALID(092): [200/220] Batch: 0.0557 (0.0592) Data: 0.0268 (0.0358) Loss: 0.5192 (0.8679)
[2022/12/29 03:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:28] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:28] |    VALID(92)      0.8678      0.7347      0.5005      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:28] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:28] | ####################################################################################################
[2022/12/29 03:28] | TRAIN(093): [ 50/879] Batch: 0.1584 (0.1950) Data: 0.0101 (0.0426) Loss: 0.7239 (0.8271)
[2022/12/29 03:28] | TRAIN(093): [100/879] Batch: 0.1599 (0.1775) Data: 0.0106 (0.0263) Loss: 0.7422 (0.8620)
[2022/12/29 03:28] | TRAIN(093): [150/879] Batch: 0.1587 (0.1716) Data: 0.0094 (0.0208) Loss: 0.7305 (0.8525)
[2022/12/29 03:28] | TRAIN(093): [200/879] Batch: 0.1580 (0.1687) Data: 0.0095 (0.0180) Loss: 0.7494 (0.8663)
[2022/12/29 03:28] | TRAIN(093): [250/879] Batch: 0.1589 (0.1670) Data: 0.0099 (0.0163) Loss: 0.6792 (0.8670)
[2022/12/29 03:29] | TRAIN(093): [300/879] Batch: 0.1588 (0.1661) Data: 0.0096 (0.0152) Loss: 0.8156 (0.8734)
[2022/12/29 03:29] | TRAIN(093): [350/879] Batch: 0.1595 (0.1653) Data: 0.0099 (0.0144) Loss: 1.1765 (0.8804)
[2022/12/29 03:29] | TRAIN(093): [400/879] Batch: 0.1599 (0.1647) Data: 0.0094 (0.0138) Loss: 0.9030 (0.8792)
[2022/12/29 03:29] | TRAIN(093): [450/879] Batch: 0.1602 (0.1642) Data: 0.0095 (0.0133) Loss: 1.1507 (0.8858)
[2022/12/29 03:29] | TRAIN(093): [500/879] Batch: 0.1590 (0.1638) Data: 0.0096 (0.0129) Loss: 0.8084 (0.8831)
[2022/12/29 03:29] | TRAIN(093): [550/879] Batch: 0.1584 (0.1635) Data: 0.0094 (0.0126) Loss: 0.8887 (0.8810)
[2022/12/29 03:29] | TRAIN(093): [600/879] Batch: 0.1598 (0.1632) Data: 0.0093 (0.0123) Loss: 0.4832 (0.8784)
[2022/12/29 03:29] | TRAIN(093): [650/879] Batch: 0.1577 (0.1629) Data: 0.0098 (0.0121) Loss: 0.8491 (0.8757)
[2022/12/29 03:30] | TRAIN(093): [700/879] Batch: 0.1606 (0.1627) Data: 0.0094 (0.0119) Loss: 1.1202 (0.8720)
[2022/12/29 03:30] | TRAIN(093): [750/879] Batch: 0.1623 (0.1626) Data: 0.0097 (0.0118) Loss: 0.9018 (0.8715)
[2022/12/29 03:30] | TRAIN(093): [800/879] Batch: 0.1596 (0.1625) Data: 0.0096 (0.0116) Loss: 0.6034 (0.8684)
[2022/12/29 03:30] | TRAIN(093): [850/879] Batch: 0.1589 (0.1624) Data: 0.0094 (0.0115) Loss: 0.8750 (0.8688)
[2022/12/29 03:30] | ------------------------------------------------------------
[2022/12/29 03:30] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:30] | ------------------------------------------------------------
[2022/12/29 03:30] |    TRAIN(93)     0:02:22     0:00:10     0:02:12      0.8675
[2022/12/29 03:30] | ------------------------------------------------------------
[2022/12/29 03:30] | VALID(093): [ 50/220] Batch: 0.0538 (0.0767) Data: 0.0354 (0.0567) Loss: 0.7834 (0.8464)
[2022/12/29 03:30] | VALID(093): [100/220] Batch: 0.0546 (0.0651) Data: 0.0351 (0.0452) Loss: 1.1080 (0.8685)
[2022/12/29 03:30] | VALID(093): [150/220] Batch: 0.0550 (0.0613) Data: 0.0373 (0.0413) Loss: 0.7592 (0.8607)
[2022/12/29 03:30] | VALID(093): [200/220] Batch: 0.0546 (0.0594) Data: 0.0354 (0.0394) Loss: 0.5188 (0.8682)
[2022/12/29 03:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:30] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:30] |    VALID(93)      0.8680      0.7347      0.5005      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:30] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:30] | ####################################################################################################
[2022/12/29 03:30] | TRAIN(094): [ 50/879] Batch: 0.1586 (0.2001) Data: 0.0100 (0.0482) Loss: 0.5532 (0.8746)
[2022/12/29 03:31] | TRAIN(094): [100/879] Batch: 0.1595 (0.1815) Data: 0.0107 (0.0292) Loss: 0.8984 (0.8714)
[2022/12/29 03:31] | TRAIN(094): [150/879] Batch: 0.1759 (0.1788) Data: 0.0111 (0.0232) Loss: 0.9095 (0.8629)
[2022/12/29 03:31] | TRAIN(094): [200/879] Batch: 0.1587 (0.1755) Data: 0.0099 (0.0199) Loss: 0.8282 (0.8599)
[2022/12/29 03:31] | TRAIN(094): [250/879] Batch: 0.1587 (0.1729) Data: 0.0094 (0.0179) Loss: 0.7422 (0.8615)
[2022/12/29 03:31] | TRAIN(094): [300/879] Batch: 0.1589 (0.1710) Data: 0.0100 (0.0165) Loss: 0.8019 (0.8646)
[2022/12/29 03:31] | TRAIN(094): [350/879] Batch: 0.1600 (0.1694) Data: 0.0100 (0.0155) Loss: 1.0205 (0.8664)
[2022/12/29 03:31] | TRAIN(094): [400/879] Batch: 0.1585 (0.1683) Data: 0.0128 (0.0148) Loss: 1.0530 (0.8685)
[2022/12/29 03:32] | TRAIN(094): [450/879] Batch: 0.1601 (0.1674) Data: 0.0098 (0.0143) Loss: 0.8094 (0.8720)
[2022/12/29 03:32] | TRAIN(094): [500/879] Batch: 0.1740 (0.1668) Data: 0.0088 (0.0138) Loss: 0.8892 (0.8757)
[2022/12/29 03:32] | TRAIN(094): [550/879] Batch: 0.1572 (0.1662) Data: 0.0090 (0.0135) Loss: 0.8655 (0.8733)
[2022/12/29 03:32] | TRAIN(094): [600/879] Batch: 0.1587 (0.1657) Data: 0.0100 (0.0131) Loss: 0.3593 (0.8733)
[2022/12/29 03:32] | TRAIN(094): [650/879] Batch: 0.1584 (0.1653) Data: 0.0095 (0.0129) Loss: 0.7051 (0.8731)
[2022/12/29 03:32] | TRAIN(094): [700/879] Batch: 0.1829 (0.1650) Data: 0.0117 (0.0126) Loss: 1.0434 (0.8722)
[2022/12/29 03:32] | TRAIN(094): [750/879] Batch: 0.1625 (0.1647) Data: 0.0096 (0.0124) Loss: 0.8739 (0.8683)
[2022/12/29 03:33] | TRAIN(094): [800/879] Batch: 0.1640 (0.1644) Data: 0.0098 (0.0122) Loss: 0.7280 (0.8679)
[2022/12/29 03:33] | TRAIN(094): [850/879] Batch: 0.1647 (0.1641) Data: 0.0097 (0.0121) Loss: 0.9326 (0.8678)
[2022/12/29 03:33] | ------------------------------------------------------------
[2022/12/29 03:33] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:33] | ------------------------------------------------------------
[2022/12/29 03:33] |    TRAIN(94)     0:02:24     0:00:10     0:02:13      0.8675
[2022/12/29 03:33] | ------------------------------------------------------------
[2022/12/29 03:33] | VALID(094): [ 50/220] Batch: 0.0550 (0.0782) Data: 0.0336 (0.0551) Loss: 0.7833 (0.8463)
[2022/12/29 03:33] | VALID(094): [100/220] Batch: 0.0516 (0.0660) Data: 0.0372 (0.0448) Loss: 1.1078 (0.8684)
[2022/12/29 03:33] | VALID(094): [150/220] Batch: 0.0559 (0.0619) Data: 0.0356 (0.0413) Loss: 0.7591 (0.8605)
[2022/12/29 03:33] | VALID(094): [200/220] Batch: 0.0530 (0.0599) Data: 0.0275 (0.0384) Loss: 0.5190 (0.8680)
[2022/12/29 03:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:33] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:33] |    VALID(94)      0.8678      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:33] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:33] | ####################################################################################################
[2022/12/29 03:33] | TRAIN(095): [ 50/879] Batch: 0.1591 (0.1953) Data: 0.0100 (0.0436) Loss: 0.8961 (0.8945)
[2022/12/29 03:33] | TRAIN(095): [100/879] Batch: 0.1622 (0.1778) Data: 0.0092 (0.0268) Loss: 0.7519 (0.8831)
[2022/12/29 03:33] | TRAIN(095): [150/879] Batch: 0.1563 (0.1728) Data: 0.0099 (0.0213) Loss: 0.7158 (0.8827)
[2022/12/29 03:34] | TRAIN(095): [200/879] Batch: 0.1587 (0.1696) Data: 0.0096 (0.0184) Loss: 0.9325 (0.8757)
[2022/12/29 03:34] | TRAIN(095): [250/879] Batch: 0.1582 (0.1677) Data: 0.0098 (0.0167) Loss: 0.8314 (0.8693)
[2022/12/29 03:34] | TRAIN(095): [300/879] Batch: 0.1624 (0.1664) Data: 0.0097 (0.0155) Loss: 0.8465 (0.8692)
[2022/12/29 03:34] | TRAIN(095): [350/879] Batch: 0.1591 (0.1654) Data: 0.0094 (0.0147) Loss: 0.7904 (0.8761)
[2022/12/29 03:34] | TRAIN(095): [400/879] Batch: 0.1596 (0.1649) Data: 0.0096 (0.0140) Loss: 0.6430 (0.8797)
[2022/12/29 03:34] | TRAIN(095): [450/879] Batch: 0.1615 (0.1644) Data: 0.0096 (0.0135) Loss: 0.9231 (0.8755)
[2022/12/29 03:34] | TRAIN(095): [500/879] Batch: 0.1615 (0.1640) Data: 0.0102 (0.0132) Loss: 0.8588 (0.8741)
[2022/12/29 03:34] | TRAIN(095): [550/879] Batch: 0.1604 (0.1637) Data: 0.0092 (0.0128) Loss: 0.6302 (0.8735)
[2022/12/29 03:35] | TRAIN(095): [600/879] Batch: 0.1593 (0.1634) Data: 0.0095 (0.0126) Loss: 0.7028 (0.8753)
[2022/12/29 03:35] | TRAIN(095): [650/879] Batch: 0.1591 (0.1631) Data: 0.0097 (0.0123) Loss: 0.7347 (0.8715)
[2022/12/29 03:35] | TRAIN(095): [700/879] Batch: 0.1587 (0.1630) Data: 0.0100 (0.0122) Loss: 0.7031 (0.8718)
[2022/12/29 03:35] | TRAIN(095): [750/879] Batch: 0.1605 (0.1630) Data: 0.0086 (0.0120) Loss: 1.2143 (0.8702)
[2022/12/29 03:35] | TRAIN(095): [800/879] Batch: 0.1588 (0.1629) Data: 0.0095 (0.0119) Loss: 0.9316 (0.8674)
[2022/12/29 03:35] | TRAIN(095): [850/879] Batch: 0.1594 (0.1628) Data: 0.0093 (0.0117) Loss: 0.6295 (0.8670)
[2022/12/29 03:35] | ------------------------------------------------------------
[2022/12/29 03:35] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:35] | ------------------------------------------------------------
[2022/12/29 03:35] |    TRAIN(95)     0:02:22     0:00:10     0:02:12      0.8675
[2022/12/29 03:35] | ------------------------------------------------------------
[2022/12/29 03:35] | VALID(095): [ 50/220] Batch: 0.0536 (0.0757) Data: 0.0260 (0.0542) Loss: 0.7833 (0.8463)
[2022/12/29 03:35] | VALID(095): [100/220] Batch: 0.0516 (0.0651) Data: 0.0302 (0.0413) Loss: 1.1078 (0.8684)
[2022/12/29 03:35] | VALID(095): [150/220] Batch: 0.0538 (0.0616) Data: 0.0255 (0.0369) Loss: 0.7591 (0.8605)
[2022/12/29 03:36] | VALID(095): [200/220] Batch: 0.0544 (0.0598) Data: 0.0265 (0.0347) Loss: 0.5190 (0.8680)
[2022/12/29 03:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:36] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:36] |    VALID(95)      0.8678      0.7347      0.5008      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:36] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:36] | ####################################################################################################
[2022/12/29 03:36] | TRAIN(096): [ 50/879] Batch: 0.1590 (0.1989) Data: 0.0102 (0.0447) Loss: 0.9364 (0.8715)
[2022/12/29 03:36] | TRAIN(096): [100/879] Batch: 0.1596 (0.1798) Data: 0.0095 (0.0274) Loss: 0.9715 (0.8504)
[2022/12/29 03:36] | TRAIN(096): [150/879] Batch: 0.1697 (0.1739) Data: 0.0095 (0.0216) Loss: 0.7592 (0.8638)
[2022/12/29 03:36] | TRAIN(096): [200/879] Batch: 0.1581 (0.1710) Data: 0.0095 (0.0187) Loss: 0.8585 (0.8684)
[2022/12/29 03:36] | TRAIN(096): [250/879] Batch: 0.1571 (0.1690) Data: 0.0100 (0.0169) Loss: 0.7524 (0.8647)
[2022/12/29 03:36] | TRAIN(096): [300/879] Batch: 0.1582 (0.1677) Data: 0.0092 (0.0157) Loss: 0.7848 (0.8688)
[2022/12/29 03:37] | TRAIN(096): [350/879] Batch: 0.1612 (0.1666) Data: 0.0098 (0.0148) Loss: 0.6923 (0.8665)
[2022/12/29 03:37] | TRAIN(096): [400/879] Batch: 0.1615 (0.1658) Data: 0.0099 (0.0142) Loss: 1.3327 (0.8699)
[2022/12/29 03:37] | TRAIN(096): [450/879] Batch: 0.1599 (0.1651) Data: 0.0098 (0.0137) Loss: 0.6292 (0.8692)
[2022/12/29 03:37] | TRAIN(096): [500/879] Batch: 0.1816 (0.1651) Data: 0.0116 (0.0133) Loss: 1.0025 (0.8661)
[2022/12/29 03:37] | TRAIN(096): [550/879] Batch: 0.1663 (0.1647) Data: 0.0100 (0.0129) Loss: 1.0048 (0.8663)
[2022/12/29 03:37] | TRAIN(096): [600/879] Batch: 0.1627 (0.1643) Data: 0.0090 (0.0127) Loss: 0.7597 (0.8665)
[2022/12/29 03:37] | TRAIN(096): [650/879] Batch: 0.1685 (0.1640) Data: 0.0097 (0.0124) Loss: 0.9636 (0.8634)
[2022/12/29 03:37] | TRAIN(096): [700/879] Batch: 0.1585 (0.1637) Data: 0.0106 (0.0122) Loss: 0.8277 (0.8614)
[2022/12/29 03:38] | TRAIN(096): [750/879] Batch: 0.1614 (0.1635) Data: 0.0105 (0.0120) Loss: 1.0060 (0.8640)
[2022/12/29 03:38] | TRAIN(096): [800/879] Batch: 0.1595 (0.1633) Data: 0.0095 (0.0118) Loss: 0.9337 (0.8629)
[2022/12/29 03:38] | TRAIN(096): [850/879] Batch: 0.1608 (0.1631) Data: 0.0097 (0.0117) Loss: 0.8157 (0.8657)
[2022/12/29 03:38] | ------------------------------------------------------------
[2022/12/29 03:38] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:38] | ------------------------------------------------------------
[2022/12/29 03:38] |    TRAIN(96)     0:02:23     0:00:10     0:02:12      0.8674
[2022/12/29 03:38] | ------------------------------------------------------------
[2022/12/29 03:38] | VALID(096): [ 50/220] Batch: 0.0547 (0.0761) Data: 0.0372 (0.0543) Loss: 0.7835 (0.8464)
[2022/12/29 03:38] | VALID(096): [100/220] Batch: 0.0511 (0.0649) Data: 0.0380 (0.0444) Loss: 1.1079 (0.8685)
[2022/12/29 03:38] | VALID(096): [150/220] Batch: 0.0540 (0.0612) Data: 0.0342 (0.0407) Loss: 0.7592 (0.8606)
[2022/12/29 03:38] | VALID(096): [200/220] Batch: 0.0552 (0.0593) Data: 0.0348 (0.0392) Loss: 0.5189 (0.8681)
[2022/12/29 03:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:38] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:38] |    VALID(96)      0.8679      0.7347      0.5003      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:38] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:38] | ####################################################################################################
[2022/12/29 03:38] | TRAIN(097): [ 50/879] Batch: 0.1623 (0.1964) Data: 0.0087 (0.0430) Loss: 0.7270 (0.8606)
[2022/12/29 03:38] | TRAIN(097): [100/879] Batch: 0.1578 (0.1787) Data: 0.0102 (0.0265) Loss: 1.0066 (0.8795)
[2022/12/29 03:39] | TRAIN(097): [150/879] Batch: 0.1593 (0.1725) Data: 0.0087 (0.0209) Loss: 1.0158 (0.8835)
[2022/12/29 03:39] | TRAIN(097): [200/879] Batch: 0.1583 (0.1693) Data: 0.0087 (0.0180) Loss: 0.5061 (0.8754)
[2022/12/29 03:39] | TRAIN(097): [250/879] Batch: 0.1590 (0.1675) Data: 0.0100 (0.0163) Loss: 0.9962 (0.8668)
[2022/12/29 03:39] | TRAIN(097): [300/879] Batch: 0.1586 (0.1662) Data: 0.0090 (0.0152) Loss: 0.7596 (0.8655)
[2022/12/29 03:39] | TRAIN(097): [350/879] Batch: 0.1592 (0.1655) Data: 0.0096 (0.0144) Loss: 0.9875 (0.8652)
[2022/12/29 03:39] | TRAIN(097): [400/879] Batch: 0.1583 (0.1650) Data: 0.0087 (0.0138) Loss: 0.9519 (0.8658)
[2022/12/29 03:39] | TRAIN(097): [450/879] Batch: 0.1586 (0.1645) Data: 0.0091 (0.0134) Loss: 0.5558 (0.8686)
[2022/12/29 03:40] | TRAIN(097): [500/879] Batch: 0.1591 (0.1641) Data: 0.0101 (0.0130) Loss: 0.9134 (0.8719)
[2022/12/29 03:40] | TRAIN(097): [550/879] Batch: 0.1592 (0.1637) Data: 0.0099 (0.0127) Loss: 0.8586 (0.8689)
[2022/12/29 03:40] | TRAIN(097): [600/879] Batch: 0.1593 (0.1634) Data: 0.0088 (0.0124) Loss: 0.7920 (0.8688)
[2022/12/29 03:40] | TRAIN(097): [650/879] Batch: 0.1582 (0.1632) Data: 0.0095 (0.0122) Loss: 0.8163 (0.8670)
[2022/12/29 03:40] | TRAIN(097): [700/879] Batch: 0.1583 (0.1631) Data: 0.0099 (0.0120) Loss: 0.7899 (0.8680)
[2022/12/29 03:40] | TRAIN(097): [750/879] Batch: 0.1600 (0.1629) Data: 0.0097 (0.0119) Loss: 0.8413 (0.8693)
[2022/12/29 03:40] | TRAIN(097): [800/879] Batch: 0.1595 (0.1628) Data: 0.0091 (0.0117) Loss: 0.6436 (0.8680)
[2022/12/29 03:40] | TRAIN(097): [850/879] Batch: 0.1611 (0.1628) Data: 0.0095 (0.0116) Loss: 0.4812 (0.8671)
[2022/12/29 03:41] | ------------------------------------------------------------
[2022/12/29 03:41] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:41] | ------------------------------------------------------------
[2022/12/29 03:41] |    TRAIN(97)     0:02:22     0:00:10     0:02:12      0.8674
[2022/12/29 03:41] | ------------------------------------------------------------
[2022/12/29 03:41] | VALID(097): [ 50/220] Batch: 0.0520 (0.0758) Data: 0.0354 (0.0561) Loss: 0.7833 (0.8463)
[2022/12/29 03:41] | VALID(097): [100/220] Batch: 0.0549 (0.0649) Data: 0.0340 (0.0448) Loss: 1.1077 (0.8683)
[2022/12/29 03:41] | VALID(097): [150/220] Batch: 0.0546 (0.0611) Data: 0.0366 (0.0413) Loss: 0.7591 (0.8605)
[2022/12/29 03:41] | VALID(097): [200/220] Batch: 0.0555 (0.0593) Data: 0.0321 (0.0396) Loss: 0.5191 (0.8680)
[2022/12/29 03:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:41] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:41] |    VALID(97)      0.8678      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:41] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:41] | ####################################################################################################
[2022/12/29 03:41] | TRAIN(098): [ 50/879] Batch: 0.1590 (0.1940) Data: 0.0090 (0.0413) Loss: 1.0412 (0.8660)
[2022/12/29 03:41] | TRAIN(098): [100/879] Batch: 0.1569 (0.1778) Data: 0.0095 (0.0258) Loss: 1.0515 (0.8812)
[2022/12/29 03:41] | TRAIN(098): [150/879] Batch: 0.1593 (0.1726) Data: 0.0096 (0.0205) Loss: 0.6298 (0.8757)
[2022/12/29 03:41] | TRAIN(098): [200/879] Batch: 0.1593 (0.1695) Data: 0.0093 (0.0178) Loss: 0.9071 (0.8702)
[2022/12/29 03:41] | TRAIN(098): [250/879] Batch: 0.1588 (0.1676) Data: 0.0094 (0.0162) Loss: 0.9640 (0.8734)
[2022/12/29 03:42] | TRAIN(098): [300/879] Batch: 0.1582 (0.1663) Data: 0.0097 (0.0151) Loss: 0.8865 (0.8748)
[2022/12/29 03:42] | TRAIN(098): [350/879] Batch: 0.1575 (0.1654) Data: 0.0099 (0.0143) Loss: 0.9063 (0.8730)
[2022/12/29 03:42] | TRAIN(098): [400/879] Batch: 0.1583 (0.1647) Data: 0.0096 (0.0137) Loss: 0.8106 (0.8736)
[2022/12/29 03:42] | TRAIN(098): [450/879] Batch: 0.1637 (0.1643) Data: 0.0091 (0.0132) Loss: 0.7100 (0.8763)
[2022/12/29 03:42] | TRAIN(098): [500/879] Batch: 0.1628 (0.1638) Data: 0.0097 (0.0129) Loss: 0.8511 (0.8746)
[2022/12/29 03:42] | TRAIN(098): [550/879] Batch: 0.1594 (0.1635) Data: 0.0097 (0.0126) Loss: 0.8552 (0.8709)
[2022/12/29 03:42] | TRAIN(098): [600/879] Batch: 0.1626 (0.1632) Data: 0.0098 (0.0123) Loss: 0.6537 (0.8724)
[2022/12/29 03:43] | TRAIN(098): [650/879] Batch: 0.1617 (0.1629) Data: 0.0098 (0.0121) Loss: 0.8658 (0.8692)
[2022/12/29 03:43] | TRAIN(098): [700/879] Batch: 0.1621 (0.1627) Data: 0.0101 (0.0119) Loss: 0.5190 (0.8681)
[2022/12/29 03:43] | TRAIN(098): [750/879] Batch: 0.1605 (0.1626) Data: 0.0092 (0.0118) Loss: 0.7851 (0.8684)
[2022/12/29 03:43] | TRAIN(098): [800/879] Batch: 0.1584 (0.1624) Data: 0.0086 (0.0116) Loss: 0.8722 (0.8705)
[2022/12/29 03:43] | TRAIN(098): [850/879] Batch: 0.1603 (0.1623) Data: 0.0089 (0.0115) Loss: 1.0567 (0.8693)
[2022/12/29 03:43] | ------------------------------------------------------------
[2022/12/29 03:43] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:43] | ------------------------------------------------------------
[2022/12/29 03:43] |    TRAIN(98)     0:02:22     0:00:10     0:02:12      0.8675
[2022/12/29 03:43] | ------------------------------------------------------------
[2022/12/29 03:43] | VALID(098): [ 50/220] Batch: 0.0547 (0.0780) Data: 0.0342 (0.0573) Loss: 0.7835 (0.8464)
[2022/12/29 03:43] | VALID(098): [100/220] Batch: 0.0523 (0.0657) Data: 0.0330 (0.0459) Loss: 1.1078 (0.8685)
[2022/12/29 03:43] | VALID(098): [150/220] Batch: 0.0523 (0.0617) Data: 0.0379 (0.0421) Loss: 0.7593 (0.8607)
[2022/12/29 03:43] | VALID(098): [200/220] Batch: 0.0495 (0.0597) Data: 0.0330 (0.0394) Loss: 0.5193 (0.8681)
[2022/12/29 03:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:43] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:43] |    VALID(98)      0.8679      0.7347      0.5000      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:43] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:43] | ####################################################################################################
[2022/12/29 03:44] | TRAIN(099): [ 50/879] Batch: 0.1580 (0.2042) Data: 0.0087 (0.0469) Loss: 0.8657 (0.8688)
[2022/12/29 03:44] | TRAIN(099): [100/879] Batch: 0.1579 (0.1821) Data: 0.0096 (0.0284) Loss: 1.1538 (0.8827)
[2022/12/29 03:44] | TRAIN(099): [150/879] Batch: 0.1580 (0.1754) Data: 0.0093 (0.0222) Loss: 1.1119 (0.8711)
[2022/12/29 03:44] | TRAIN(099): [200/879] Batch: 0.1601 (0.1716) Data: 0.0091 (0.0191) Loss: 0.9143 (0.8693)
[2022/12/29 03:44] | TRAIN(099): [250/879] Batch: 0.1690 (0.1696) Data: 0.0103 (0.0172) Loss: 0.7768 (0.8621)
[2022/12/29 03:44] | TRAIN(099): [300/879] Batch: 0.1623 (0.1681) Data: 0.0095 (0.0159) Loss: 0.8076 (0.8553)
[2022/12/29 03:44] | TRAIN(099): [350/879] Batch: 0.1615 (0.1670) Data: 0.0097 (0.0150) Loss: 1.1557 (0.8585)
[2022/12/29 03:44] | TRAIN(099): [400/879] Batch: 0.1581 (0.1661) Data: 0.0097 (0.0143) Loss: 0.9551 (0.8565)
[2022/12/29 03:45] | TRAIN(099): [450/879] Batch: 0.1577 (0.1655) Data: 0.0095 (0.0137) Loss: 0.8267 (0.8579)
[2022/12/29 03:45] | TRAIN(099): [500/879] Batch: 0.1577 (0.1649) Data: 0.0097 (0.0133) Loss: 0.6101 (0.8602)
[2022/12/29 03:45] | TRAIN(099): [550/879] Batch: 0.1590 (0.1645) Data: 0.0099 (0.0129) Loss: 1.1808 (0.8617)
[2022/12/29 03:45] | TRAIN(099): [600/879] Batch: 0.1587 (0.1641) Data: 0.0097 (0.0127) Loss: 0.8753 (0.8609)
[2022/12/29 03:45] | TRAIN(099): [650/879] Batch: 0.1587 (0.1638) Data: 0.0100 (0.0124) Loss: 1.1257 (0.8639)
[2022/12/29 03:45] | TRAIN(099): [700/879] Batch: 0.1583 (0.1638) Data: 0.0089 (0.0122) Loss: 0.8721 (0.8656)
[2022/12/29 03:45] | TRAIN(099): [750/879] Batch: 0.1590 (0.1636) Data: 0.0095 (0.0121) Loss: 0.7485 (0.8651)
[2022/12/29 03:46] | TRAIN(099): [800/879] Batch: 0.1584 (0.1634) Data: 0.0097 (0.0119) Loss: 0.9072 (0.8650)
[2022/12/29 03:46] | TRAIN(099): [850/879] Batch: 0.1592 (0.1632) Data: 0.0088 (0.0118) Loss: 0.7353 (0.8684)
[2022/12/29 03:46] | ------------------------------------------------------------
[2022/12/29 03:46] |        Stage       Batch        Data       F+B+O        Loss
[2022/12/29 03:46] | ------------------------------------------------------------
[2022/12/29 03:46] |    TRAIN(99)     0:02:23     0:00:10     0:02:13      0.8675
[2022/12/29 03:46] | ------------------------------------------------------------
[2022/12/29 03:46] | VALID(099): [ 50/220] Batch: 0.0519 (0.0761) Data: 0.0282 (0.0517) Loss: 0.7835 (0.8464)
[2022/12/29 03:46] | VALID(099): [100/220] Batch: 0.0556 (0.0648) Data: 0.0274 (0.0396) Loss: 1.1078 (0.8685)
[2022/12/29 03:46] | VALID(099): [150/220] Batch: 0.0528 (0.0610) Data: 0.0267 (0.0355) Loss: 0.7593 (0.8607)
[2022/12/29 03:46] | VALID(099): [200/220] Batch: 0.0522 (0.0591) Data: 0.0354 (0.0345) Loss: 0.5193 (0.8681)
[2022/12/29 03:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:46] |        Stage        Loss    accuracy       auroc    f1_score   precision      recall specificity
[2022/12/29 03:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:46] |    VALID(99)      0.8679      0.7347      0.4997      0.7347      0.7347      0.7347      0.9337
[2022/12/29 03:46] | ------------------------------------------------------------------------------------------------
[2022/12/29 03:46] | ####################################################################################################
