[2023/01/15 23:30] | ---------------------------------------------------------------------------------
[2023/01/15 23:30] |                                    INFORMATION
[2023/01/15 23:30] | ---------------------------------------------------------------------------------
[2023/01/15 23:30] | Project Name              | MESEG
[2023/01/15 23:30] | Project Administrator     | jaejung
[2023/01/15 23:30] | Experiment Name           | btcv_v2_transunet_v26
[2023/01/15 23:30] | Experiment Start Time     | 2023-01-15 23:30:20
[2023/01/15 23:30] | Experiment Model Name     | transunet
[2023/01/15 23:30] | Experiment Log Directory  | log/btcv_v2_transunet_v26
[2023/01/15 23:30] | ---------------------------------------------------------------------------------
[2023/01/15 23:30] |                                 EXPERIMENT SETUP
[2023/01/15 23:30] | ---------------------------------------------------------------------------------
[2023/01/15 23:30] | train_size                | (224, 224)
[2023/01/15 23:30] | test_size                 | (224, 224)
[2023/01/15 23:30] | center_crop_ptr           | 0.875
[2023/01/15 23:30] | interpolation             | bicubic
[2023/01/15 23:30] | mean                      | (0.485, 0.456, 0.406)
[2023/01/15 23:30] | std                       | (0.229, 0.224, 0.225)
[2023/01/15 23:30] | hflip                     | 0.5
[2023/01/15 23:30] | auto_aug                  | False
[2023/01/15 23:30] | cutmix                    | None
[2023/01/15 23:30] | mixup                     | None
[2023/01/15 23:30] | remode                    | None
[2023/01/15 23:30] | model_name                | transunet
[2023/01/15 23:30] | lr                        | 0.001
[2023/01/15 23:30] | epoch                     | 150
[2023/01/15 23:30] | criterion                 | dicece
[2023/01/15 23:30] | optimizer                 | adamw
[2023/01/15 23:30] | weight_decay              | 1e-05
[2023/01/15 23:30] | scheduler                 | None
[2023/01/15 23:30] | warmup_epoch              | 5
[2023/01/15 23:30] | batch_size                | 1
[2023/01/15 23:30] | ---------------------------------------------------------------------------------
[2023/01/15 23:30] |                                   DATA & MODEL
[2023/01/15 23:30] | ---------------------------------------------------------------------------------
[2023/01/15 23:30] | Model Parameters(M)       | 105277081
[2023/01/15 23:30] | Number of Train Examples  | 2211
[2023/01/15 23:30] | Number of Valid Examples  | 12
[2023/01/15 23:30] | Number of Class           | 9
[2023/01/15 23:30] | ---------------------------------------------------------------------------------
[2023/01/15 23:30] | TRAIN(000): [  50/2211] Batch: 0.0997 (0.1794) Data: 0.0021 (0.0120) Loss: 0.5078 (0.7914)
[2023/01/15 23:30] | TRAIN(000): [ 100/2211] Batch: 0.1002 (0.1397) Data: 0.0020 (0.0070) Loss: 0.5046 (0.6755)
[2023/01/15 23:30] | TRAIN(000): [ 150/2211] Batch: 0.1043 (0.1279) Data: 0.0018 (0.0054) Loss: 0.6157 (0.6236)
[2023/01/15 23:30] | TRAIN(000): [ 200/2211] Batch: 0.0926 (0.1199) Data: 0.0017 (0.0045) Loss: 0.4863 (0.6035)
[2023/01/15 23:30] | TRAIN(000): [ 250/2211] Batch: 0.0899 (0.1152) Data: 0.0017 (0.0040) Loss: 0.4793 (0.5883)
[2023/01/15 23:31] | TRAIN(000): [ 300/2211] Batch: 0.0872 (0.1117) Data: 0.0018 (0.0036) Loss: 0.5825 (0.5761)
[2023/01/15 23:31] | TRAIN(000): [ 350/2211] Batch: 0.0906 (0.1103) Data: 0.0014 (0.0034) Loss: 0.4604 (0.5646)
[2023/01/15 23:31] | TRAIN(000): [ 400/2211] Batch: 0.0936 (0.1078) Data: 0.0019 (0.0032) Loss: 0.4531 (0.5561)
[2023/01/15 23:31] | TRAIN(000): [ 450/2211] Batch: 0.1122 (0.1076) Data: 0.0020 (0.0031) Loss: 0.4612 (0.5483)
[2023/01/15 23:31] | TRAIN(000): [ 500/2211] Batch: 0.1201 (0.1068) Data: 0.0025 (0.0030) Loss: 0.4523 (0.5444)
[2023/01/15 23:31] | TRAIN(000): [ 550/2211] Batch: 0.1107 (0.1062) Data: 0.0023 (0.0029) Loss: 0.4686 (0.5399)
[2023/01/15 23:31] | TRAIN(000): [ 600/2211] Batch: 0.1244 (0.1059) Data: 0.0026 (0.0028) Loss: 0.4647 (0.5361)
[2023/01/15 23:31] | TRAIN(000): [ 650/2211] Batch: 0.0979 (0.1061) Data: 0.0022 (0.0027) Loss: 0.4848 (0.5339)
[2023/01/15 23:31] | TRAIN(000): [ 700/2211] Batch: 0.1093 (0.1057) Data: 0.0018 (0.0027) Loss: 0.5486 (0.5309)
[2023/01/15 23:31] | TRAIN(000): [ 750/2211] Batch: 0.1056 (0.1052) Data: 0.0022 (0.0026) Loss: 0.5579 (0.5275)
[2023/01/15 23:31] | TRAIN(000): [ 800/2211] Batch: 0.0987 (0.1048) Data: 0.0018 (0.0026) Loss: 0.4684 (0.5252)
[2023/01/15 23:31] | TRAIN(000): [ 850/2211] Batch: 0.0912 (0.1043) Data: 0.0015 (0.0026) Loss: 0.4500 (0.5235)
[2023/01/15 23:32] | TRAIN(000): [ 900/2211] Batch: 0.0945 (0.1039) Data: 0.0021 (0.0025) Loss: 0.4325 (0.5221)
[2023/01/15 23:32] | TRAIN(000): [ 950/2211] Batch: 0.0998 (0.1036) Data: 0.0021 (0.0025) Loss: 0.4555 (0.5201)
[2023/01/15 23:32] | TRAIN(000): [1000/2211] Batch: 0.0927 (0.1035) Data: 0.0020 (0.0025) Loss: 0.4881 (0.5178)
[2023/01/15 23:32] | TRAIN(000): [1050/2211] Batch: 0.0969 (0.1034) Data: 0.0023 (0.0025) Loss: 0.4893 (0.5158)
[2023/01/15 23:32] | TRAIN(000): [1100/2211] Batch: 0.0986 (0.1033) Data: 0.0021 (0.0024) Loss: 0.4548 (0.5132)
[2023/01/15 23:32] | TRAIN(000): [1150/2211] Batch: 0.1155 (0.1035) Data: 0.0021 (0.0024) Loss: 0.4600 (0.5114)
[2023/01/15 23:32] | TRAIN(000): [1200/2211] Batch: 0.1275 (0.1041) Data: 0.0019 (0.0024) Loss: 0.4619 (0.5095)
[2023/01/15 23:32] | TRAIN(000): [1250/2211] Batch: 0.1002 (0.1045) Data: 0.0020 (0.0024) Loss: 0.4675 (0.5082)
[2023/01/15 23:32] | TRAIN(000): [1300/2211] Batch: 0.1003 (0.1044) Data: 0.0018 (0.0024) Loss: 0.4521 (0.5067)
[2023/01/15 23:32] | TRAIN(000): [1350/2211] Batch: 0.1029 (0.1044) Data: 0.0018 (0.0024) Loss: 0.4509 (0.5053)
[2023/01/15 23:32] | TRAIN(000): [1400/2211] Batch: 0.1267 (0.1043) Data: 0.0033 (0.0024) Loss: 0.4168 (0.5038)
[2023/01/15 23:33] | TRAIN(000): [1450/2211] Batch: 0.0958 (0.1044) Data: 0.0018 (0.0023) Loss: 0.4591 (0.5021)
[2023/01/15 23:33] | TRAIN(000): [1500/2211] Batch: 0.1257 (0.1046) Data: 0.0027 (0.0023) Loss: 0.4573 (0.5008)
[2023/01/15 23:33] | TRAIN(000): [1550/2211] Batch: 0.0968 (0.1046) Data: 0.0021 (0.0023) Loss: 0.5483 (0.4995)
[2023/01/15 23:33] | TRAIN(000): [1600/2211] Batch: 0.1108 (0.1044) Data: 0.0019 (0.0023) Loss: 0.4598 (0.4987)
[2023/01/15 23:33] | TRAIN(000): [1650/2211] Batch: 0.1062 (0.1043) Data: 0.0020 (0.0023) Loss: 0.4076 (0.4971)
[2023/01/15 23:33] | TRAIN(000): [1700/2211] Batch: 0.0922 (0.1040) Data: 0.0019 (0.0023) Loss: 0.4885 (0.4947)
[2023/01/15 23:33] | TRAIN(000): [1750/2211] Batch: 0.1202 (0.1044) Data: 0.0023 (0.0023) Loss: 0.4412 (0.4925)
[2023/01/15 23:33] | TRAIN(000): [1800/2211] Batch: 0.0938 (0.1046) Data: 0.0019 (0.0023) Loss: 0.4301 (0.4905)
[2023/01/15 23:33] | TRAIN(000): [1850/2211] Batch: 0.0920 (0.1046) Data: 0.0021 (0.0023) Loss: 0.4093 (0.4886)
[2023/01/15 23:33] | TRAIN(000): [1900/2211] Batch: 0.0933 (0.1044) Data: 0.0019 (0.0023) Loss: 0.4877 (0.4868)
[2023/01/15 23:33] | TRAIN(000): [1950/2211] Batch: 0.0929 (0.1042) Data: 0.0020 (0.0023) Loss: 0.3978 (0.4847)
[2023/01/15 23:33] | TRAIN(000): [2000/2211] Batch: 0.1016 (0.1040) Data: 0.0020 (0.0023) Loss: 0.4216 (0.4831)
[2023/01/15 23:34] | TRAIN(000): [2050/2211] Batch: 0.0936 (0.1039) Data: 0.0019 (0.0023) Loss: 0.4048 (0.4813)
[2023/01/15 23:34] | TRAIN(000): [2100/2211] Batch: 0.1055 (0.1041) Data: 0.0019 (0.0023) Loss: 0.3446 (0.4796)
[2023/01/15 23:34] | TRAIN(000): [2150/2211] Batch: 0.1002 (0.1039) Data: 0.0019 (0.0023) Loss: 0.4291 (0.4778)
[2023/01/15 23:34] | TRAIN(000): [2200/2211] Batch: 0.0991 (0.1038) Data: 0.0020 (0.0022) Loss: 0.4390 (0.4768)
[2023/01/15 23:34] | ------------------------------------------------------------
[2023/01/15 23:34] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/15 23:34] | ------------------------------------------------------------
[2023/01/15 23:34] |     TRAIN(0)     0:03:49     0:00:04     0:03:44      0.4765
[2023/01/15 23:34] | ------------------------------------------------------------
[2023/01/15 23:34] | **************************************************
[2023/01/15 23:34] | TRAIN(001): [  50/2211] Batch: 0.0936 (0.1289) Data: 0.0021 (0.0201) Loss: 0.4263 (0.4021)
[2023/01/15 23:34] | TRAIN(001): [ 100/2211] Batch: 0.0921 (0.1141) Data: 0.0019 (0.0112) Loss: 0.3997 (0.4117)
[2023/01/15 23:34] | TRAIN(001): [ 150/2211] Batch: 0.0960 (0.1081) Data: 0.0019 (0.0081) Loss: 0.4022 (0.4106)
[2023/01/15 23:34] | TRAIN(001): [ 200/2211] Batch: 0.1065 (0.1066) Data: 0.0027 (0.0066) Loss: 0.4539 (0.4129)
[2023/01/15 23:34] | TRAIN(001): [ 250/2211] Batch: 0.1124 (0.1049) Data: 0.0021 (0.0057) Loss: 0.4749 (0.4105)
[2023/01/15 23:34] | TRAIN(001): [ 300/2211] Batch: 0.0962 (0.1052) Data: 0.0021 (0.0051) Loss: 0.3872 (0.4084)
[2023/01/15 23:34] | TRAIN(001): [ 350/2211] Batch: 0.0971 (0.1035) Data: 0.0021 (0.0047) Loss: 0.3988 (0.4063)
[2023/01/15 23:35] | TRAIN(001): [ 400/2211] Batch: 0.0937 (0.1022) Data: 0.0021 (0.0043) Loss: 0.4474 (0.4054)
[2023/01/15 23:35] | TRAIN(001): [ 450/2211] Batch: 0.0961 (0.1015) Data: 0.0023 (0.0041) Loss: 0.4100 (0.4048)
[2023/01/15 23:35] | TRAIN(001): [ 500/2211] Batch: 0.1058 (0.1013) Data: 0.0022 (0.0039) Loss: 0.3954 (0.4045)
[2023/01/15 23:35] | TRAIN(001): [ 550/2211] Batch: 0.0981 (0.1012) Data: 0.0020 (0.0037) Loss: 0.3997 (0.4028)
[2023/01/15 23:35] | TRAIN(001): [ 600/2211] Batch: 0.1028 (0.1013) Data: 0.0023 (0.0036) Loss: 0.4022 (0.4033)
[2023/01/15 23:35] | TRAIN(001): [ 650/2211] Batch: 0.1023 (0.1011) Data: 0.0019 (0.0035) Loss: 0.3929 (0.4015)
[2023/01/15 23:35] | TRAIN(001): [ 700/2211] Batch: 0.0928 (0.1010) Data: 0.0020 (0.0034) Loss: 0.3978 (0.4014)
[2023/01/15 23:35] | TRAIN(001): [ 750/2211] Batch: 0.1301 (0.1018) Data: 0.0024 (0.0033) Loss: 0.3999 (0.4005)
[2023/01/15 23:35] | TRAIN(001): [ 800/2211] Batch: 0.1247 (0.1026) Data: 0.0026 (0.0032) Loss: 0.4099 (0.4008)
[2023/01/15 23:35] | TRAIN(001): [ 850/2211] Batch: 0.0937 (0.1032) Data: 0.0020 (0.0032) Loss: 0.3246 (0.4004)
[2023/01/15 23:35] | TRAIN(001): [ 900/2211] Batch: 0.1037 (0.1029) Data: 0.0023 (0.0031) Loss: 0.3985 (0.3995)
[2023/01/15 23:35] | TRAIN(001): [ 950/2211] Batch: 0.1136 (0.1028) Data: 0.0017 (0.0030) Loss: 0.3925 (0.3987)
[2023/01/15 23:36] | TRAIN(001): [1000/2211] Batch: 0.1036 (0.1026) Data: 0.0020 (0.0030) Loss: 0.4160 (0.3984)
[2023/01/15 23:36] | TRAIN(001): [1050/2211] Batch: 0.1261 (0.1025) Data: 0.0025 (0.0029) Loss: 0.2677 (0.3981)
[2023/01/15 23:36] | TRAIN(001): [1100/2211] Batch: 0.0943 (0.1027) Data: 0.0015 (0.0029) Loss: 0.3570 (0.3974)
[2023/01/15 23:36] | TRAIN(001): [1150/2211] Batch: 0.1017 (0.1024) Data: 0.0020 (0.0029) Loss: 0.4110 (0.3972)
[2023/01/15 23:36] | TRAIN(001): [1200/2211] Batch: 0.1009 (0.1023) Data: 0.0022 (0.0028) Loss: 0.3545 (0.3965)
[2023/01/15 23:36] | TRAIN(001): [1250/2211] Batch: 0.0909 (0.1023) Data: 0.0020 (0.0028) Loss: 0.3746 (0.3960)
[2023/01/15 23:36] | TRAIN(001): [1300/2211] Batch: 0.1082 (0.1023) Data: 0.0020 (0.0028) Loss: 0.3961 (0.3961)
[2023/01/15 23:36] | TRAIN(001): [1350/2211] Batch: 0.0901 (0.1021) Data: 0.0019 (0.0027) Loss: 0.3403 (0.3959)
[2023/01/15 23:36] | TRAIN(001): [1400/2211] Batch: 0.0949 (0.1019) Data: 0.0017 (0.0027) Loss: 0.3694 (0.3950)
[2023/01/15 23:36] | TRAIN(001): [1450/2211] Batch: 0.0939 (0.1018) Data: 0.0020 (0.0027) Loss: 0.3957 (0.3945)
[2023/01/15 23:36] | TRAIN(001): [1500/2211] Batch: 0.1007 (0.1016) Data: 0.0019 (0.0027) Loss: 0.4058 (0.3942)
[2023/01/15 23:36] | TRAIN(001): [1550/2211] Batch: 0.1117 (0.1017) Data: 0.0019 (0.0026) Loss: 0.4112 (0.3943)
[2023/01/15 23:37] | TRAIN(001): [1600/2211] Batch: 0.0952 (0.1017) Data: 0.0016 (0.0026) Loss: 0.3915 (0.3944)
[2023/01/15 23:37] | TRAIN(001): [1650/2211] Batch: 0.1054 (0.1018) Data: 0.0019 (0.0026) Loss: 0.3914 (0.3941)
[2023/01/15 23:37] | TRAIN(001): [1700/2211] Batch: 0.1013 (0.1019) Data: 0.0018 (0.0026) Loss: 0.3262 (0.3936)
[2023/01/15 23:37] | TRAIN(001): [1750/2211] Batch: 0.0923 (0.1020) Data: 0.0019 (0.0025) Loss: 0.2425 (0.3929)
[2023/01/15 23:37] | TRAIN(001): [1800/2211] Batch: 0.0931 (0.1018) Data: 0.0020 (0.0025) Loss: 0.5539 (0.3924)
[2023/01/15 23:37] | TRAIN(001): [1850/2211] Batch: 0.0935 (0.1020) Data: 0.0019 (0.0025) Loss: 0.3361 (0.3913)
[2023/01/15 23:37] | TRAIN(001): [1900/2211] Batch: 0.1171 (0.1020) Data: 0.0022 (0.0025) Loss: 0.2802 (0.3895)
[2023/01/15 23:37] | TRAIN(001): [1950/2211] Batch: 0.0968 (0.1021) Data: 0.0019 (0.0025) Loss: 0.3081 (0.3894)
[2023/01/15 23:37] | TRAIN(001): [2000/2211] Batch: 0.0921 (0.1023) Data: 0.0020 (0.0025) Loss: 0.2901 (0.3889)
[2023/01/15 23:37] | TRAIN(001): [2050/2211] Batch: 0.0916 (0.1022) Data: 0.0019 (0.0025) Loss: 0.2708 (0.3880)
[2023/01/15 23:37] | TRAIN(001): [2100/2211] Batch: 0.1187 (0.1022) Data: 0.0023 (0.0025) Loss: 0.4933 (0.3869)
[2023/01/15 23:37] | TRAIN(001): [2150/2211] Batch: 0.1261 (0.1023) Data: 0.0020 (0.0024) Loss: 0.2415 (0.3852)
[2023/01/15 23:38] | TRAIN(001): [2200/2211] Batch: 0.0994 (0.1026) Data: 0.0016 (0.0024) Loss: 0.2992 (0.3840)
[2023/01/15 23:38] | ------------------------------------------------------------
[2023/01/15 23:38] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/15 23:38] | ------------------------------------------------------------
[2023/01/15 23:38] |     TRAIN(1)     0:03:46     0:00:05     0:03:41      0.3837
[2023/01/15 23:38] | ------------------------------------------------------------
[2023/01/15 23:38] | **************************************************
[2023/01/15 23:38] | TRAIN(002): [  50/2211] Batch: 0.1201 (0.1316) Data: 0.0023 (0.0216) Loss: 0.4744 (0.3347)
[2023/01/15 23:38] | TRAIN(002): [ 100/2211] Batch: 0.1095 (0.1178) Data: 0.0020 (0.0119) Loss: 0.2851 (0.3217)
[2023/01/15 23:38] | TRAIN(002): [ 150/2211] Batch: 0.1217 (0.1130) Data: 0.0020 (0.0086) Loss: 0.2710 (0.3202)
[2023/01/15 23:38] | TRAIN(002): [ 200/2211] Batch: 0.0971 (0.1101) Data: 0.0019 (0.0069) Loss: 0.3155 (0.3221)
[2023/01/15 23:38] | TRAIN(002): [ 250/2211] Batch: 0.0989 (0.1104) Data: 0.0020 (0.0060) Loss: 0.2677 (0.3199)
[2023/01/15 23:38] | TRAIN(002): [ 300/2211] Batch: 0.1110 (0.1105) Data: 0.0019 (0.0053) Loss: 0.2799 (0.3198)
[2023/01/15 23:38] | TRAIN(002): [ 350/2211] Batch: 0.1109 (0.1109) Data: 0.0019 (0.0049) Loss: 0.2953 (0.3197)
[2023/01/15 23:38] | TRAIN(002): [ 400/2211] Batch: 0.1127 (0.1112) Data: 0.0019 (0.0045) Loss: 0.2969 (0.3217)
[2023/01/15 23:38] | TRAIN(002): [ 450/2211] Batch: 0.1136 (0.1114) Data: 0.0020 (0.0042) Loss: 0.2444 (0.3225)
[2023/01/15 23:39] | TRAIN(002): [ 500/2211] Batch: 0.1158 (0.1113) Data: 0.0020 (0.0040) Loss: 0.2579 (0.3223)
[2023/01/15 23:39] | TRAIN(002): [ 550/2211] Batch: 0.1195 (0.1115) Data: 0.0019 (0.0038) Loss: 0.2840 (0.3229)
[2023/01/15 23:39] | TRAIN(002): [ 600/2211] Batch: 0.1029 (0.1115) Data: 0.0019 (0.0037) Loss: 0.2747 (0.3204)
[2023/01/15 23:39] | TRAIN(002): [ 650/2211] Batch: 0.1131 (0.1114) Data: 0.0018 (0.0035) Loss: 0.2282 (0.3186)
[2023/01/15 23:39] | TRAIN(002): [ 700/2211] Batch: 0.1012 (0.1110) Data: 0.0020 (0.0034) Loss: 0.2780 (0.3176)
[2023/01/15 23:39] | TRAIN(002): [ 750/2211] Batch: 0.0991 (0.1103) Data: 0.0019 (0.0033) Loss: 0.5609 (0.3173)
[2023/01/15 23:39] | TRAIN(002): [ 800/2211] Batch: 0.1246 (0.1096) Data: 0.0016 (0.0032) Loss: 0.2215 (0.3172)
[2023/01/15 23:39] | TRAIN(002): [ 850/2211] Batch: 0.1199 (0.1092) Data: 0.0022 (0.0032) Loss: 0.2794 (0.3159)
[2023/01/15 23:39] | TRAIN(002): [ 900/2211] Batch: 0.0898 (0.1085) Data: 0.0020 (0.0031) Loss: 0.1688 (0.3127)
[2023/01/15 23:39] | TRAIN(002): [ 950/2211] Batch: 0.0937 (0.1081) Data: 0.0019 (0.0030) Loss: 0.4474 (0.3121)
[2023/01/15 23:39] | TRAIN(002): [1000/2211] Batch: 0.0963 (0.1075) Data: 0.0018 (0.0030) Loss: 0.1685 (0.3092)
[2023/01/15 23:39] | TRAIN(002): [1050/2211] Batch: 0.0923 (0.1077) Data: 0.0019 (0.0029) Loss: 0.1697 (0.3088)
[2023/01/15 23:40] | TRAIN(002): [1100/2211] Batch: 0.0913 (0.1072) Data: 0.0019 (0.0029) Loss: 0.2767 (0.3063)
[2023/01/15 23:40] | TRAIN(002): [1150/2211] Batch: 0.0932 (0.1067) Data: 0.0020 (0.0028) Loss: 0.4270 (0.3034)
[2023/01/15 23:40] | TRAIN(002): [1200/2211] Batch: 0.0935 (0.1063) Data: 0.0019 (0.0028) Loss: 0.1774 (0.3021)
[2023/01/15 23:40] | TRAIN(002): [1250/2211] Batch: 0.1000 (0.1061) Data: 0.0019 (0.0028) Loss: 0.3758 (0.3019)
[2023/01/15 23:40] | TRAIN(002): [1300/2211] Batch: 0.0928 (0.1058) Data: 0.0019 (0.0027) Loss: 0.4661 (0.2997)
[2023/01/15 23:40] | TRAIN(002): [1350/2211] Batch: 0.1017 (0.1054) Data: 0.0019 (0.0027) Loss: 0.1276 (0.2983)
[2023/01/15 23:40] | TRAIN(002): [1400/2211] Batch: 0.0922 (0.1050) Data: 0.0019 (0.0027) Loss: 0.1684 (0.2968)
[2023/01/15 23:40] | TRAIN(002): [1450/2211] Batch: 0.0985 (0.1049) Data: 0.0020 (0.0027) Loss: 0.1716 (0.2954)
[2023/01/15 23:40] | TRAIN(002): [1500/2211] Batch: 0.1034 (0.1047) Data: 0.0020 (0.0026) Loss: 0.1708 (0.2947)
[2023/01/15 23:40] | TRAIN(002): [1550/2211] Batch: 0.0989 (0.1044) Data: 0.0022 (0.0026) Loss: 0.2296 (0.2929)
[2023/01/15 23:40] | TRAIN(002): [1600/2211] Batch: 0.0877 (0.1041) Data: 0.0018 (0.0026) Loss: 0.1694 (0.2905)
[2023/01/15 23:40] | TRAIN(002): [1650/2211] Batch: 0.1003 (0.1040) Data: 0.0019 (0.0026) Loss: 0.1739 (0.2887)
[2023/01/15 23:41] | TRAIN(002): [1700/2211] Batch: 0.1006 (0.1038) Data: 0.0020 (0.0026) Loss: 0.4187 (0.2875)
[2023/01/15 23:41] | TRAIN(002): [1750/2211] Batch: 0.1008 (0.1037) Data: 0.0020 (0.0025) Loss: 0.2918 (0.2871)
[2023/01/15 23:41] | TRAIN(002): [1800/2211] Batch: 0.1363 (0.1037) Data: 0.0025 (0.0025) Loss: 0.1718 (0.2866)
[2023/01/15 23:41] | TRAIN(002): [1850/2211] Batch: 0.1154 (0.1040) Data: 0.0020 (0.0025) Loss: 0.4431 (0.2850)
[2023/01/15 23:41] | TRAIN(002): [1900/2211] Batch: 0.0918 (0.1039) Data: 0.0020 (0.0025) Loss: 0.0776 (0.2833)
[2023/01/15 23:41] | TRAIN(002): [1950/2211] Batch: 0.0942 (0.1037) Data: 0.0019 (0.0025) Loss: 0.1671 (0.2818)
[2023/01/15 23:41] | TRAIN(002): [2000/2211] Batch: 0.1191 (0.1037) Data: 0.0020 (0.0025) Loss: 0.1694 (0.2811)
[2023/01/15 23:41] | TRAIN(002): [2050/2211] Batch: 0.0949 (0.1040) Data: 0.0016 (0.0025) Loss: 0.1592 (0.2802)
[2023/01/15 23:41] | TRAIN(002): [2100/2211] Batch: 0.1130 (0.1040) Data: 0.0020 (0.0024) Loss: 0.1676 (0.2802)
[2023/01/15 23:41] | TRAIN(002): [2150/2211] Batch: 0.0894 (0.1037) Data: 0.0019 (0.0024) Loss: 0.1711 (0.2798)
[2023/01/15 23:41] | TRAIN(002): [2200/2211] Batch: 0.0938 (0.1037) Data: 0.0018 (0.0024) Loss: 0.3224 (0.2797)
[2023/01/15 23:41] | ------------------------------------------------------------
[2023/01/15 23:41] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/15 23:41] | ------------------------------------------------------------
[2023/01/15 23:41] |     TRAIN(2)     0:03:49     0:00:05     0:03:43      0.2795
[2023/01/15 23:41] | ------------------------------------------------------------
[2023/01/15 23:41] | **************************************************
[2023/01/15 23:42] | TRAIN(003): [  50/2211] Batch: 0.0899 (0.1264) Data: 0.0020 (0.0186) Loss: 0.1009 (0.2378)
[2023/01/15 23:42] | TRAIN(003): [ 100/2211] Batch: 0.1064 (0.1139) Data: 0.0023 (0.0104) Loss: 0.1678 (0.2510)
[2023/01/15 23:42] | TRAIN(003): [ 150/2211] Batch: 0.0919 (0.1098) Data: 0.0021 (0.0076) Loss: 0.1702 (0.2501)
[2023/01/15 23:42] | TRAIN(003): [ 200/2211] Batch: 0.1214 (0.1090) Data: 0.0025 (0.0062) Loss: 0.1258 (0.2491)
[2023/01/15 23:42] | TRAIN(003): [ 250/2211] Batch: 0.0989 (0.1084) Data: 0.0017 (0.0054) Loss: 0.1696 (0.2562)
[2023/01/15 23:42] | TRAIN(003): [ 300/2211] Batch: 0.1049 (0.1076) Data: 0.0020 (0.0048) Loss: 0.1674 (0.2546)
[2023/01/15 23:42] | TRAIN(003): [ 350/2211] Batch: 0.0921 (0.1081) Data: 0.0020 (0.0044) Loss: 0.1746 (0.2543)
[2023/01/15 23:42] | TRAIN(003): [ 400/2211] Batch: 0.0892 (0.1076) Data: 0.0019 (0.0041) Loss: 0.3808 (0.2552)
[2023/01/15 23:42] | TRAIN(003): [ 450/2211] Batch: 0.0942 (0.1075) Data: 0.0019 (0.0039) Loss: 0.1709 (0.2572)
[2023/01/15 23:42] | TRAIN(003): [ 500/2211] Batch: 0.1010 (0.1069) Data: 0.0019 (0.0037) Loss: 0.2521 (0.2533)
[2023/01/15 23:42] | TRAIN(003): [ 550/2211] Batch: 0.0936 (0.1061) Data: 0.0018 (0.0035) Loss: 0.1763 (0.2507)
[2023/01/15 23:42] | TRAIN(003): [ 600/2211] Batch: 0.0896 (0.1051) Data: 0.0019 (0.0034) Loss: 0.5292 (0.2495)
[2023/01/15 23:43] | TRAIN(003): [ 650/2211] Batch: 0.0901 (0.1040) Data: 0.0020 (0.0033) Loss: 0.4155 (0.2498)
[2023/01/15 23:43] | TRAIN(003): [ 700/2211] Batch: 0.1163 (0.1040) Data: 0.0019 (0.0032) Loss: 0.3431 (0.2471)
[2023/01/15 23:43] | TRAIN(003): [ 750/2211] Batch: 0.0931 (0.1046) Data: 0.0018 (0.0031) Loss: 0.1691 (0.2447)
[2023/01/15 23:43] | TRAIN(003): [ 800/2211] Batch: 0.1003 (0.1050) Data: 0.0019 (0.0030) Loss: 0.1249 (0.2452)
[2023/01/15 23:43] | TRAIN(003): [ 850/2211] Batch: 0.1047 (0.1053) Data: 0.0019 (0.0030) Loss: 0.4143 (0.2440)
[2023/01/15 23:43] | TRAIN(003): [ 900/2211] Batch: 0.1098 (0.1055) Data: 0.0019 (0.0029) Loss: 0.1669 (0.2424)
[2023/01/15 23:43] | TRAIN(003): [ 950/2211] Batch: 0.1014 (0.1057) Data: 0.0019 (0.0029) Loss: 0.1683 (0.2423)
[2023/01/15 23:43] | TRAIN(003): [1000/2211] Batch: 0.0963 (0.1056) Data: 0.0019 (0.0028) Loss: 0.5849 (0.2426)
[2023/01/15 23:43] | TRAIN(003): [1050/2211] Batch: 0.0934 (0.1056) Data: 0.0019 (0.0028) Loss: 0.1730 (0.2425)
[2023/01/15 23:43] | TRAIN(003): [1100/2211] Batch: 0.0924 (0.1053) Data: 0.0019 (0.0027) Loss: 0.1678 (0.2434)
[2023/01/15 23:43] | TRAIN(003): [1150/2211] Batch: 0.0917 (0.1050) Data: 0.0019 (0.0027) Loss: 0.1710 (0.2438)
[2023/01/15 23:44] | TRAIN(003): [1200/2211] Batch: 0.1096 (0.1055) Data: 0.0019 (0.0027) Loss: 0.1673 (0.2438)
[2023/01/15 23:44] | TRAIN(003): [1250/2211] Batch: 0.0974 (0.1053) Data: 0.0019 (0.0026) Loss: 0.1678 (0.2432)
[2023/01/15 23:44] | TRAIN(003): [1300/2211] Batch: 0.0905 (0.1050) Data: 0.0018 (0.0026) Loss: 0.1450 (0.2423)
[2023/01/15 23:44] | TRAIN(003): [1350/2211] Batch: 0.0917 (0.1046) Data: 0.0019 (0.0026) Loss: 0.1700 (0.2427)
[2023/01/15 23:44] | TRAIN(003): [1400/2211] Batch: 0.1064 (0.1048) Data: 0.0019 (0.0026) Loss: 0.1572 (0.2425)
[2023/01/15 23:44] | TRAIN(003): [1450/2211] Batch: 0.0873 (0.1048) Data: 0.0020 (0.0025) Loss: 0.1335 (0.2422)
[2023/01/15 23:44] | TRAIN(003): [1500/2211] Batch: 0.0898 (0.1044) Data: 0.0020 (0.0025) Loss: 0.1679 (0.2426)
[2023/01/15 23:44] | TRAIN(003): [1550/2211] Batch: 0.0971 (0.1041) Data: 0.0022 (0.0025) Loss: 0.2638 (0.2408)
[2023/01/15 23:44] | TRAIN(003): [1600/2211] Batch: 0.0956 (0.1038) Data: 0.0022 (0.0025) Loss: 0.1674 (0.2409)
[2023/01/15 23:44] | TRAIN(003): [1650/2211] Batch: 0.0940 (0.1035) Data: 0.0023 (0.0025) Loss: 0.1368 (0.2403)
[2023/01/15 23:44] | TRAIN(003): [1700/2211] Batch: 0.0960 (0.1032) Data: 0.0019 (0.0025) Loss: 0.1679 (0.2400)
[2023/01/15 23:44] | TRAIN(003): [1750/2211] Batch: 0.1008 (0.1033) Data: 0.0023 (0.0025) Loss: 0.2030 (0.2395)
[2023/01/15 23:45] | TRAIN(003): [1800/2211] Batch: 0.1249 (0.1034) Data: 0.0027 (0.0025) Loss: 0.4324 (0.2405)
[2023/01/15 23:45] | TRAIN(003): [1850/2211] Batch: 0.1071 (0.1034) Data: 0.0022 (0.0025) Loss: 0.1703 (0.2416)
[2023/01/15 23:45] | TRAIN(003): [1900/2211] Batch: 0.1078 (0.1034) Data: 0.0023 (0.0025) Loss: 0.1391 (0.2414)
[2023/01/15 23:45] | TRAIN(003): [1950/2211] Batch: 0.0983 (0.1034) Data: 0.0018 (0.0025) Loss: 0.3863 (0.2415)
[2023/01/15 23:45] | TRAIN(003): [2000/2211] Batch: 0.1011 (0.1033) Data: 0.0022 (0.0024) Loss: 0.1729 (0.2415)
[2023/01/15 23:45] | TRAIN(003): [2050/2211] Batch: 0.0928 (0.1033) Data: 0.0016 (0.0024) Loss: 0.3374 (0.2412)
[2023/01/15 23:45] | TRAIN(003): [2100/2211] Batch: 0.1206 (0.1035) Data: 0.0017 (0.0024) Loss: 0.2825 (0.2417)
[2023/01/15 23:45] | TRAIN(003): [2150/2211] Batch: 0.1023 (0.1036) Data: 0.0019 (0.0024) Loss: 0.1560 (0.2419)
[2023/01/15 23:45] | TRAIN(003): [2200/2211] Batch: 0.0925 (0.1035) Data: 0.0020 (0.0024) Loss: 0.1683 (0.2414)
[2023/01/15 23:45] | ------------------------------------------------------------
[2023/01/15 23:45] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/15 23:45] | ------------------------------------------------------------
[2023/01/15 23:45] |     TRAIN(3)     0:03:48     0:00:05     0:03:43      0.2412
[2023/01/15 23:45] | ------------------------------------------------------------
[2023/01/15 23:45] | **************************************************
[2023/01/15 23:45] | TRAIN(004): [  50/2211] Batch: 0.0992 (0.1318) Data: 0.0019 (0.0211) Loss: 0.1299 (0.2506)
[2023/01/15 23:45] | TRAIN(004): [ 100/2211] Batch: 0.0925 (0.1157) Data: 0.0020 (0.0117) Loss: 0.1675 (0.2304)
[2023/01/15 23:46] | TRAIN(004): [ 150/2211] Batch: 0.0977 (0.1098) Data: 0.0023 (0.0086) Loss: 0.1691 (0.2249)
[2023/01/15 23:46] | TRAIN(004): [ 200/2211] Batch: 0.1325 (0.1089) Data: 0.0026 (0.0070) Loss: 0.1672 (0.2257)
[2023/01/15 23:46] | TRAIN(004): [ 250/2211] Batch: 0.0973 (0.1079) Data: 0.0022 (0.0060) Loss: 0.3735 (0.2345)
[2023/01/15 23:46] | TRAIN(004): [ 300/2211] Batch: 0.1144 (0.1085) Data: 0.0021 (0.0054) Loss: 0.6651 (0.2363)
[2023/01/15 23:46] | TRAIN(004): [ 350/2211] Batch: 0.1225 (0.1096) Data: 0.0020 (0.0049) Loss: 0.1683 (0.2336)
[2023/01/15 23:46] | TRAIN(004): [ 400/2211] Batch: 0.1044 (0.1087) Data: 0.0019 (0.0046) Loss: 0.1201 (0.2311)
[2023/01/15 23:46] | TRAIN(004): [ 450/2211] Batch: 0.0925 (0.1074) Data: 0.0020 (0.0043) Loss: 0.4657 (0.2330)
[2023/01/15 23:46] | TRAIN(004): [ 500/2211] Batch: 0.1222 (0.1071) Data: 0.0017 (0.0040) Loss: 0.1173 (0.2313)
[2023/01/15 23:46] | TRAIN(004): [ 550/2211] Batch: 0.0985 (0.1062) Data: 0.0019 (0.0038) Loss: 0.2597 (0.2283)
[2023/01/15 23:46] | TRAIN(004): [ 600/2211] Batch: 0.1005 (0.1072) Data: 0.0019 (0.0037) Loss: 0.5266 (0.2272)
[2023/01/15 23:46] | TRAIN(004): [ 650/2211] Batch: 0.0924 (0.1074) Data: 0.0015 (0.0036) Loss: 0.2272 (0.2258)
[2023/01/15 23:46] | TRAIN(004): [ 700/2211] Batch: 0.1038 (0.1069) Data: 0.0020 (0.0034) Loss: 0.1683 (0.2264)
[2023/01/15 23:47] | TRAIN(004): [ 750/2211] Batch: 0.0990 (0.1064) Data: 0.0019 (0.0033) Loss: 0.1687 (0.2261)
[2023/01/15 23:47] | TRAIN(004): [ 800/2211] Batch: 0.0929 (0.1061) Data: 0.0021 (0.0032) Loss: 0.1677 (0.2259)
[2023/01/15 23:47] | TRAIN(004): [ 850/2211] Batch: 0.0917 (0.1055) Data: 0.0018 (0.0032) Loss: 0.4401 (0.2256)
[2023/01/15 23:47] | TRAIN(004): [ 900/2211] Batch: 0.0907 (0.1049) Data: 0.0018 (0.0031) Loss: 0.1677 (0.2258)
[2023/01/15 23:47] | TRAIN(004): [ 950/2211] Batch: 0.0982 (0.1044) Data: 0.0015 (0.0030) Loss: 0.4318 (0.2252)
[2023/01/15 23:47] | TRAIN(004): [1000/2211] Batch: 0.0879 (0.1040) Data: 0.0018 (0.0030) Loss: 0.0938 (0.2265)
[2023/01/15 23:47] | TRAIN(004): [1050/2211] Batch: 0.0863 (0.1036) Data: 0.0018 (0.0029) Loss: 0.3593 (0.2278)
[2023/01/15 23:47] | TRAIN(004): [1100/2211] Batch: 0.1055 (0.1035) Data: 0.0022 (0.0029) Loss: 0.4438 (0.2284)
[2023/01/15 23:47] | TRAIN(004): [1150/2211] Batch: 0.0937 (0.1034) Data: 0.0017 (0.0028) Loss: 0.4431 (0.2290)
[2023/01/15 23:47] | TRAIN(004): [1200/2211] Batch: 0.0941 (0.1033) Data: 0.0024 (0.0028) Loss: 0.4801 (0.2303)
[2023/01/15 23:47] | TRAIN(004): [1250/2211] Batch: 0.1010 (0.1038) Data: 0.0024 (0.0028) Loss: 0.4375 (0.2300)
[2023/01/15 23:47] | TRAIN(004): [1300/2211] Batch: 0.0988 (0.1038) Data: 0.0022 (0.0028) Loss: 0.2820 (0.2290)
[2023/01/15 23:48] | TRAIN(004): [1350/2211] Batch: 0.1011 (0.1037) Data: 0.0021 (0.0027) Loss: 0.1700 (0.2301)
[2023/01/15 23:48] | TRAIN(004): [1400/2211] Batch: 0.0993 (0.1037) Data: 0.0020 (0.0027) Loss: 0.1669 (0.2304)
[2023/01/15 23:48] | TRAIN(004): [1450/2211] Batch: 0.1085 (0.1037) Data: 0.0023 (0.0027) Loss: 0.1682 (0.2308)
[2023/01/15 23:48] | TRAIN(004): [1500/2211] Batch: 0.0978 (0.1035) Data: 0.0020 (0.0027) Loss: 0.1275 (0.2304)
[2023/01/15 23:48] | TRAIN(004): [1550/2211] Batch: 0.1104 (0.1034) Data: 0.0020 (0.0027) Loss: 0.1613 (0.2306)
[2023/01/15 23:48] | TRAIN(004): [1600/2211] Batch: 0.0984 (0.1033) Data: 0.0019 (0.0026) Loss: 0.1670 (0.2313)
[2023/01/15 23:48] | TRAIN(004): [1650/2211] Batch: 0.1008 (0.1032) Data: 0.0025 (0.0026) Loss: 0.1678 (0.2314)
[2023/01/15 23:48] | TRAIN(004): [1700/2211] Batch: 0.0928 (0.1031) Data: 0.0019 (0.0026) Loss: 0.1433 (0.2308)
[2023/01/15 23:48] | TRAIN(004): [1750/2211] Batch: 0.0994 (0.1032) Data: 0.0018 (0.0026) Loss: 0.1675 (0.2300)
[2023/01/15 23:48] | TRAIN(004): [1800/2211] Batch: 0.0924 (0.1030) Data: 0.0019 (0.0026) Loss: 0.1689 (0.2296)
[2023/01/15 23:48] | TRAIN(004): [1850/2211] Batch: 0.1016 (0.1028) Data: 0.0020 (0.0025) Loss: 0.2446 (0.2287)
[2023/01/15 23:48] | TRAIN(004): [1900/2211] Batch: 0.0940 (0.1027) Data: 0.0020 (0.0025) Loss: 0.1695 (0.2290)
[2023/01/15 23:49] | TRAIN(004): [1950/2211] Batch: 0.0891 (0.1026) Data: 0.0019 (0.0025) Loss: 0.1673 (0.2299)
[2023/01/15 23:49] | TRAIN(004): [2000/2211] Batch: 0.0995 (0.1024) Data: 0.0021 (0.0025) Loss: 0.1708 (0.2292)
[2023/01/15 23:49] | TRAIN(004): [2050/2211] Batch: 0.0994 (0.1023) Data: 0.0015 (0.0025) Loss: 0.0727 (0.2286)
[2023/01/15 23:49] | TRAIN(004): [2100/2211] Batch: 0.1209 (0.1022) Data: 0.0023 (0.0025) Loss: 0.3989 (0.2287)
[2023/01/15 23:49] | TRAIN(004): [2150/2211] Batch: 0.1005 (0.1021) Data: 0.0022 (0.0025) Loss: 0.3110 (0.2281)
[2023/01/15 23:49] | TRAIN(004): [2200/2211] Batch: 0.1206 (0.1023) Data: 0.0022 (0.0025) Loss: 0.3166 (0.2285)
[2023/01/15 23:49] | ------------------------------------------------------------
[2023/01/15 23:49] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/15 23:49] | ------------------------------------------------------------
[2023/01/15 23:49] |     TRAIN(4)     0:03:46     0:00:05     0:03:40      0.2284
[2023/01/15 23:49] | ------------------------------------------------------------
[2023/01/15 23:49] | **************************************************
[2023/01/15 23:49] | TRAIN(005): [  50/2211] Batch: 0.0989 (0.1165) Data: 0.0016 (0.0217) Loss: 0.3953 (0.2117)
[2023/01/15 23:49] | TRAIN(005): [ 100/2211] Batch: 0.0993 (0.1077) Data: 0.0021 (0.0119) Loss: 0.1000 (0.2051)
[2023/01/15 23:49] | TRAIN(005): [ 150/2211] Batch: 0.0913 (0.1038) Data: 0.0018 (0.0086) Loss: 0.1135 (0.1993)
[2023/01/15 23:49] | TRAIN(005): [ 200/2211] Batch: 0.1043 (0.1025) Data: 0.0019 (0.0070) Loss: 0.1468 (0.2044)
[2023/01/15 23:49] | TRAIN(005): [ 250/2211] Batch: 0.1021 (0.1022) Data: 0.0020 (0.0060) Loss: 0.0631 (0.1991)
[2023/01/15 23:50] | TRAIN(005): [ 300/2211] Batch: 0.1196 (0.1046) Data: 0.0020 (0.0053) Loss: 0.2378 (0.1919)
[2023/01/15 23:50] | TRAIN(005): [ 350/2211] Batch: 0.1147 (0.1068) Data: 0.0020 (0.0048) Loss: 0.1126 (0.1938)
[2023/01/15 23:50] | TRAIN(005): [ 400/2211] Batch: 0.1222 (0.1082) Data: 0.0020 (0.0045) Loss: 0.0675 (0.1972)
[2023/01/15 23:50] | TRAIN(005): [ 450/2211] Batch: 0.0867 (0.1087) Data: 0.0018 (0.0042) Loss: 0.0559 (0.1951)
[2023/01/15 23:50] | TRAIN(005): [ 500/2211] Batch: 0.0892 (0.1075) Data: 0.0020 (0.0040) Loss: 0.0680 (0.1941)
[2023/01/15 23:50] | TRAIN(005): [ 550/2211] Batch: 0.0934 (0.1067) Data: 0.0020 (0.0038) Loss: 0.3240 (0.1915)
[2023/01/15 23:50] | TRAIN(005): [ 600/2211] Batch: 0.1011 (0.1058) Data: 0.0018 (0.0037) Loss: 0.0557 (0.1888)
[2023/01/15 23:50] | TRAIN(005): [ 650/2211] Batch: 0.0990 (0.1051) Data: 0.0018 (0.0035) Loss: 0.2522 (0.1872)
[2023/01/15 23:50] | TRAIN(005): [ 700/2211] Batch: 0.1025 (0.1045) Data: 0.0019 (0.0034) Loss: 0.0560 (0.1868)
[2023/01/15 23:50] | TRAIN(005): [ 750/2211] Batch: 0.0968 (0.1039) Data: 0.0023 (0.0033) Loss: 0.1682 (0.1854)
[2023/01/15 23:50] | TRAIN(005): [ 800/2211] Batch: 0.1076 (0.1034) Data: 0.0020 (0.0032) Loss: 0.1946 (0.1842)
[2023/01/15 23:50] | TRAIN(005): [ 850/2211] Batch: 0.1311 (0.1034) Data: 0.0024 (0.0032) Loss: 0.0543 (0.1845)
[2023/01/15 23:51] | TRAIN(005): [ 900/2211] Batch: 0.0995 (0.1033) Data: 0.0020 (0.0031) Loss: 0.1493 (0.1845)
[2023/01/15 23:51] | TRAIN(005): [ 950/2211] Batch: 0.0892 (0.1031) Data: 0.0026 (0.0030) Loss: 0.2527 (0.1858)
[2023/01/15 23:51] | TRAIN(005): [1000/2211] Batch: 0.0931 (0.1039) Data: 0.0019 (0.0030) Loss: 0.2896 (0.1848)
[2023/01/15 23:51] | TRAIN(005): [1050/2211] Batch: 0.0920 (0.1036) Data: 0.0019 (0.0030) Loss: 0.0035 (0.1818)
[2023/01/15 23:51] | TRAIN(005): [1100/2211] Batch: 0.1227 (0.1035) Data: 0.0023 (0.0029) Loss: 0.0564 (0.1801)
[2023/01/15 23:51] | TRAIN(005): [1150/2211] Batch: 0.0982 (0.1035) Data: 0.0019 (0.0029) Loss: 0.0372 (0.1798)
[2023/01/15 23:51] | TRAIN(005): [1200/2211] Batch: 0.0916 (0.1032) Data: 0.0019 (0.0028) Loss: 0.0567 (0.1797)
[2023/01/15 23:51] | TRAIN(005): [1250/2211] Batch: 0.1235 (0.1030) Data: 0.0020 (0.0028) Loss: 0.0560 (0.1796)
[2023/01/15 23:51] | TRAIN(005): [1300/2211] Batch: 0.1017 (0.1029) Data: 0.0021 (0.0028) Loss: 0.1513 (0.1798)
[2023/01/15 23:51] | TRAIN(005): [1350/2211] Batch: 0.0942 (0.1027) Data: 0.0019 (0.0027) Loss: 0.3260 (0.1792)
[2023/01/15 23:51] | TRAIN(005): [1400/2211] Batch: 0.1014 (0.1025) Data: 0.0016 (0.0027) Loss: 0.0570 (0.1787)
[2023/01/15 23:51] | TRAIN(005): [1450/2211] Batch: 0.0930 (0.1023) Data: 0.0020 (0.0027) Loss: 0.0519 (0.1792)
[2023/01/15 23:52] | TRAIN(005): [1500/2211] Batch: 0.0953 (0.1022) Data: 0.0020 (0.0027) Loss: 0.0559 (0.1790)
[2023/01/15 23:52] | TRAIN(005): [1550/2211] Batch: 0.0935 (0.1022) Data: 0.0018 (0.0026) Loss: 0.0074 (0.1783)
[2023/01/15 23:52] | TRAIN(005): [1600/2211] Batch: 0.1093 (0.1022) Data: 0.0019 (0.0026) Loss: 0.0169 (0.1781)
[2023/01/15 23:52] | TRAIN(005): [1650/2211] Batch: 0.0953 (0.1022) Data: 0.0023 (0.0026) Loss: 0.1643 (0.1778)
[2023/01/15 23:52] | TRAIN(005): [1700/2211] Batch: 0.0936 (0.1024) Data: 0.0019 (0.0026) Loss: 0.0586 (0.1773)
[2023/01/15 23:52] | TRAIN(005): [1750/2211] Batch: 0.0946 (0.1025) Data: 0.0021 (0.0026) Loss: 0.1191 (0.1771)
[2023/01/15 23:52] | TRAIN(005): [1800/2211] Batch: 0.1048 (0.1026) Data: 0.0019 (0.0026) Loss: 0.3157 (0.1763)
[2023/01/15 23:52] | TRAIN(005): [1850/2211] Batch: 0.1012 (0.1027) Data: 0.0020 (0.0025) Loss: 0.6851 (0.1765)
[2023/01/15 23:52] | TRAIN(005): [1900/2211] Batch: 0.0920 (0.1026) Data: 0.0019 (0.0025) Loss: 0.5513 (0.1770)
[2023/01/15 23:52] | TRAIN(005): [1950/2211] Batch: 0.0976 (0.1024) Data: 0.0020 (0.0025) Loss: 0.0558 (0.1762)
[2023/01/15 23:52] | TRAIN(005): [2000/2211] Batch: 0.0986 (0.1021) Data: 0.0021 (0.0025) Loss: 0.5661 (0.1752)
[2023/01/15 23:53] | TRAIN(005): [2050/2211] Batch: 0.0940 (0.1021) Data: 0.0017 (0.0025) Loss: 0.1434 (0.1753)
[2023/01/15 23:53] | TRAIN(005): [2100/2211] Batch: 0.0945 (0.1021) Data: 0.0021 (0.0025) Loss: 0.3865 (0.1742)
[2023/01/15 23:53] | TRAIN(005): [2150/2211] Batch: 0.0926 (0.1020) Data: 0.0016 (0.0025) Loss: 0.0237 (0.1732)
[2023/01/15 23:53] | TRAIN(005): [2200/2211] Batch: 0.0952 (0.1019) Data: 0.0020 (0.0024) Loss: 0.0581 (0.1731)
[2023/01/15 23:53] | ------------------------------------------------------------
[2023/01/15 23:53] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/15 23:53] | ------------------------------------------------------------
[2023/01/15 23:53] |     TRAIN(5)     0:03:45     0:00:05     0:03:39      0.1737
[2023/01/15 23:53] | ------------------------------------------------------------
[2023/01/15 23:53] | **************************************************
[2023/01/15 23:53] | TRAIN(006): [  50/2211] Batch: 0.0968 (0.1188) Data: 0.0019 (0.0201) Loss: 0.5118 (0.1455)
[2023/01/15 23:53] | TRAIN(006): [ 100/2211] Batch: 0.0939 (0.1079) Data: 0.0020 (0.0111) Loss: 0.0564 (0.1341)
[2023/01/15 23:53] | TRAIN(006): [ 150/2211] Batch: 0.0992 (0.1052) Data: 0.0020 (0.0081) Loss: 0.0556 (0.1350)
[2023/01/15 23:53] | TRAIN(006): [ 200/2211] Batch: 0.1003 (0.1048) Data: 0.0020 (0.0066) Loss: 0.5912 (0.1428)
[2023/01/15 23:53] | TRAIN(006): [ 250/2211] Batch: 0.1199 (0.1037) Data: 0.0022 (0.0057) Loss: 0.0557 (0.1427)
[2023/01/15 23:53] | TRAIN(006): [ 300/2211] Batch: 0.0933 (0.1065) Data: 0.0020 (0.0051) Loss: 0.0574 (0.1467)
[2023/01/15 23:53] | TRAIN(006): [ 350/2211] Batch: 0.0928 (0.1069) Data: 0.0019 (0.0047) Loss: 0.3817 (0.1436)
[2023/01/15 23:53] | TRAIN(006): [ 400/2211] Batch: 0.0949 (0.1067) Data: 0.0019 (0.0044) Loss: 0.1076 (0.1444)
[2023/01/15 23:54] | TRAIN(006): [ 450/2211] Batch: 0.1373 (0.1062) Data: 0.0027 (0.0041) Loss: 0.1327 (0.1436)
[2023/01/15 23:54] | TRAIN(006): [ 500/2211] Batch: 0.0919 (0.1053) Data: 0.0018 (0.0039) Loss: 0.0562 (0.1438)
[2023/01/15 23:54] | TRAIN(006): [ 550/2211] Batch: 0.0936 (0.1046) Data: 0.0017 (0.0037) Loss: 0.0558 (0.1444)
[2023/01/15 23:54] | TRAIN(006): [ 600/2211] Batch: 0.0998 (0.1042) Data: 0.0014 (0.0036) Loss: 0.0835 (0.1472)
[2023/01/15 23:54] | TRAIN(006): [ 650/2211] Batch: 0.1104 (0.1038) Data: 0.0019 (0.0034) Loss: 0.0958 (0.1458)
[2023/01/15 23:54] | TRAIN(006): [ 700/2211] Batch: 0.0958 (0.1042) Data: 0.0020 (0.0034) Loss: 0.2430 (0.1443)
[2023/01/15 23:54] | TRAIN(006): [ 750/2211] Batch: 0.0932 (0.1040) Data: 0.0019 (0.0033) Loss: 0.2823 (0.1441)
[2023/01/15 23:54] | TRAIN(006): [ 800/2211] Batch: 0.1155 (0.1044) Data: 0.0021 (0.0032) Loss: 0.2022 (0.1440)
[2023/01/15 23:54] | TRAIN(006): [ 850/2211] Batch: 0.0993 (0.1042) Data: 0.0027 (0.0031) Loss: 0.0558 (0.1461)
[2023/01/15 23:54] | TRAIN(006): [ 900/2211] Batch: 0.0931 (0.1039) Data: 0.0019 (0.0031) Loss: 0.0700 (0.1471)
[2023/01/15 23:54] | TRAIN(006): [ 950/2211] Batch: 0.1336 (0.1038) Data: 0.0027 (0.0030) Loss: 0.0565 (0.1480)
[2023/01/15 23:55] | TRAIN(006): [1000/2211] Batch: 0.0934 (0.1040) Data: 0.0020 (0.0030) Loss: 0.0559 (0.1493)
[2023/01/15 23:55] | TRAIN(006): [1050/2211] Batch: 0.1038 (0.1040) Data: 0.0022 (0.0029) Loss: 0.3297 (0.1492)
[2023/01/15 23:55] | TRAIN(006): [1100/2211] Batch: 0.1137 (0.1042) Data: 0.0030 (0.0029) Loss: 0.1525 (0.1491)
[2023/01/15 23:55] | TRAIN(006): [1150/2211] Batch: 0.1030 (0.1045) Data: 0.0021 (0.0029) Loss: 0.1121 (0.1497)
[2023/01/15 23:55] | TRAIN(006): [1200/2211] Batch: 0.1024 (0.1048) Data: 0.0020 (0.0028) Loss: 0.1457 (0.1506)
[2023/01/15 23:55] | TRAIN(006): [1250/2211] Batch: 0.1171 (0.1050) Data: 0.0017 (0.0028) Loss: 0.2993 (0.1500)
[2023/01/15 23:55] | TRAIN(006): [1300/2211] Batch: 0.1011 (0.1051) Data: 0.0015 (0.0028) Loss: 0.0564 (0.1495)
[2023/01/15 23:55] | TRAIN(006): [1350/2211] Batch: 0.0916 (0.1047) Data: 0.0020 (0.0027) Loss: 0.1432 (0.1485)
[2023/01/15 23:55] | TRAIN(006): [1400/2211] Batch: 0.1059 (0.1045) Data: 0.0020 (0.0027) Loss: 0.0575 (0.1473)
[2023/01/15 23:55] | TRAIN(006): [1450/2211] Batch: 0.0933 (0.1042) Data: 0.0018 (0.0027) Loss: 0.1866 (0.1474)
[2023/01/15 23:55] | TRAIN(006): [1500/2211] Batch: 0.0967 (0.1041) Data: 0.0017 (0.0027) Loss: 0.0590 (0.1471)
[2023/01/15 23:55] | TRAIN(006): [1550/2211] Batch: 0.1103 (0.1041) Data: 0.0021 (0.0026) Loss: 0.0563 (0.1469)
[2023/01/15 23:56] | TRAIN(006): [1600/2211] Batch: 0.1188 (0.1044) Data: 0.0023 (0.0026) Loss: 0.0558 (0.1471)
[2023/01/15 23:56] | TRAIN(006): [1650/2211] Batch: 0.0996 (0.1041) Data: 0.0018 (0.0026) Loss: 0.2423 (0.1471)
[2023/01/15 23:56] | TRAIN(006): [1700/2211] Batch: 0.0980 (0.1039) Data: 0.0020 (0.0026) Loss: 0.0555 (0.1468)
[2023/01/15 23:56] | TRAIN(006): [1750/2211] Batch: 0.1551 (0.1040) Data: 0.0022 (0.0026) Loss: 0.3710 (0.1465)
[2023/01/15 23:56] | TRAIN(006): [1800/2211] Batch: 0.1120 (0.1044) Data: 0.0029 (0.0025) Loss: 0.3133 (0.1469)
[2023/01/15 23:56] | TRAIN(006): [1850/2211] Batch: 0.0910 (0.1044) Data: 0.0016 (0.0025) Loss: 0.0558 (0.1469)
[2023/01/15 23:56] | TRAIN(006): [1900/2211] Batch: 0.1039 (0.1043) Data: 0.0019 (0.0025) Loss: 0.0020 (0.1471)
[2023/01/15 23:56] | TRAIN(006): [1950/2211] Batch: 0.0931 (0.1041) Data: 0.0018 (0.0025) Loss: 0.2421 (0.1472)
[2023/01/15 23:56] | TRAIN(006): [2000/2211] Batch: 0.0978 (0.1040) Data: 0.0019 (0.0025) Loss: 0.0560 (0.1473)
[2023/01/15 23:56] | TRAIN(006): [2050/2211] Batch: 0.0993 (0.1040) Data: 0.0016 (0.0025) Loss: 0.1237 (0.1471)
[2023/01/15 23:56] | TRAIN(006): [2100/2211] Batch: 0.0985 (0.1040) Data: 0.0019 (0.0024) Loss: 0.1924 (0.1477)
[2023/01/15 23:56] | TRAIN(006): [2150/2211] Batch: 0.0986 (0.1037) Data: 0.0019 (0.0024) Loss: 0.4040 (0.1477)
[2023/01/15 23:57] | TRAIN(006): [2200/2211] Batch: 0.0903 (0.1035) Data: 0.0015 (0.0024) Loss: 0.0596 (0.1477)
[2023/01/15 23:57] | ------------------------------------------------------------
[2023/01/15 23:57] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/15 23:57] | ------------------------------------------------------------
[2023/01/15 23:57] |     TRAIN(6)     0:03:48     0:00:05     0:03:43      0.1477
[2023/01/15 23:57] | ------------------------------------------------------------
[2023/01/15 23:57] | **************************************************
[2023/01/15 23:57] | TRAIN(007): [  50/2211] Batch: 0.1016 (0.1190) Data: 0.0019 (0.0194) Loss: 0.0558 (0.1825)
[2023/01/15 23:57] | TRAIN(007): [ 100/2211] Batch: 0.0907 (0.1080) Data: 0.0019 (0.0108) Loss: 0.0563 (0.1628)
[2023/01/15 23:57] | TRAIN(007): [ 150/2211] Batch: 0.1009 (0.1065) Data: 0.0020 (0.0079) Loss: 0.2290 (0.1603)
[2023/01/15 23:57] | TRAIN(007): [ 200/2211] Batch: 0.0994 (0.1038) Data: 0.0022 (0.0064) Loss: 0.0561 (0.1619)
[2023/01/15 23:57] | TRAIN(007): [ 250/2211] Batch: 0.0923 (0.1031) Data: 0.0020 (0.0055) Loss: 0.0557 (0.1567)
[2023/01/15 23:57] | TRAIN(007): [ 300/2211] Batch: 0.0945 (0.1029) Data: 0.0021 (0.0049) Loss: 0.3480 (0.1557)
[2023/01/15 23:57] | TRAIN(007): [ 350/2211] Batch: 0.0990 (0.1036) Data: 0.0018 (0.0045) Loss: 0.0560 (0.1536)
[2023/01/15 23:57] | TRAIN(007): [ 400/2211] Batch: 0.0931 (0.1027) Data: 0.0019 (0.0042) Loss: 0.0567 (0.1543)
[2023/01/15 23:57] | TRAIN(007): [ 450/2211] Batch: 0.1177 (0.1031) Data: 0.0022 (0.0040) Loss: 0.0566 (0.1495)
[2023/01/15 23:57] | TRAIN(007): [ 500/2211] Batch: 0.1139 (0.1046) Data: 0.0018 (0.0038) Loss: 0.3487 (0.1478)
[2023/01/15 23:58] | TRAIN(007): [ 550/2211] Batch: 0.0928 (0.1053) Data: 0.0020 (0.0036) Loss: 0.2079 (0.1462)
[2023/01/15 23:58] | TRAIN(007): [ 600/2211] Batch: 0.1029 (0.1048) Data: 0.0019 (0.0035) Loss: 0.0963 (0.1465)
[2023/01/15 23:58] | TRAIN(007): [ 650/2211] Batch: 0.1057 (0.1045) Data: 0.0022 (0.0033) Loss: 0.0709 (0.1465)
[2023/01/15 23:58] | TRAIN(007): [ 700/2211] Batch: 0.1073 (0.1040) Data: 0.0018 (0.0032) Loss: 0.0560 (0.1470)
[2023/01/15 23:58] | TRAIN(007): [ 750/2211] Batch: 0.1154 (0.1044) Data: 0.0022 (0.0032) Loss: 0.0563 (0.1458)
[2023/01/15 23:58] | TRAIN(007): [ 800/2211] Batch: 0.0923 (0.1040) Data: 0.0020 (0.0031) Loss: 0.0433 (0.1439)
[2023/01/15 23:58] | TRAIN(007): [ 850/2211] Batch: 0.0914 (0.1036) Data: 0.0018 (0.0030) Loss: 0.1432 (0.1437)
[2023/01/15 23:58] | TRAIN(007): [ 900/2211] Batch: 0.1181 (0.1033) Data: 0.0020 (0.0030) Loss: 0.0558 (0.1421)
[2023/01/15 23:58] | TRAIN(007): [ 950/2211] Batch: 0.1298 (0.1040) Data: 0.0023 (0.0029) Loss: 0.0558 (0.1434)
[2023/01/15 23:58] | TRAIN(007): [1000/2211] Batch: 0.0944 (0.1045) Data: 0.0020 (0.0029) Loss: 0.1210 (0.1431)
[2023/01/15 23:58] | TRAIN(007): [1050/2211] Batch: 0.1002 (0.1044) Data: 0.0020 (0.0028) Loss: 0.0056 (0.1442)
[2023/01/15 23:58] | TRAIN(007): [1100/2211] Batch: 0.0969 (0.1040) Data: 0.0022 (0.0028) Loss: 0.3047 (0.1427)
[2023/01/15 23:59] | TRAIN(007): [1150/2211] Batch: 0.0927 (0.1038) Data: 0.0019 (0.0028) Loss: 0.0851 (0.1428)
[2023/01/15 23:59] | TRAIN(007): [1200/2211] Batch: 0.1012 (0.1037) Data: 0.0022 (0.0027) Loss: 0.0571 (0.1435)
[2023/01/15 23:59] | TRAIN(007): [1250/2211] Batch: 0.0928 (0.1035) Data: 0.0018 (0.0027) Loss: 0.2072 (0.1439)
[2023/01/15 23:59] | TRAIN(007): [1300/2211] Batch: 0.0920 (0.1033) Data: 0.0018 (0.0027) Loss: 0.4865 (0.1434)
[2023/01/15 23:59] | TRAIN(007): [1350/2211] Batch: 0.0910 (0.1030) Data: 0.0019 (0.0026) Loss: 0.0557 (0.1437)
[2023/01/15 23:59] | TRAIN(007): [1400/2211] Batch: 0.0977 (0.1028) Data: 0.0020 (0.0026) Loss: 0.3015 (0.1431)
[2023/01/15 23:59] | TRAIN(007): [1450/2211] Batch: 0.0923 (0.1028) Data: 0.0015 (0.0026) Loss: 0.1365 (0.1437)
[2023/01/15 23:59] | TRAIN(007): [1500/2211] Batch: 0.1037 (0.1026) Data: 0.0020 (0.0026) Loss: 0.3992 (0.1445)
[2023/01/15 23:59] | TRAIN(007): [1550/2211] Batch: 0.1216 (0.1025) Data: 0.0019 (0.0025) Loss: 0.0579 (0.1455)
[2023/01/15 23:59] | TRAIN(007): [1600/2211] Batch: 0.1168 (0.1025) Data: 0.0018 (0.0025) Loss: 0.1640 (0.1452)
[2023/01/15 23:59] | TRAIN(007): [1650/2211] Batch: 0.1066 (0.1024) Data: 0.0019 (0.0025) Loss: 0.3466 (0.1462)
[2023/01/15 23:59] | TRAIN(007): [1700/2211] Batch: 0.0910 (0.1024) Data: 0.0019 (0.0025) Loss: 0.0561 (0.1454)
[2023/01/16 00:00] | TRAIN(007): [1750/2211] Batch: 0.0935 (0.1023) Data: 0.0020 (0.0025) Loss: 0.3592 (0.1452)
[2023/01/16 00:00] | TRAIN(007): [1800/2211] Batch: 0.0922 (0.1021) Data: 0.0019 (0.0025) Loss: 0.1789 (0.1447)
[2023/01/16 00:00] | TRAIN(007): [1850/2211] Batch: 0.0981 (0.1020) Data: 0.0019 (0.0024) Loss: 0.0561 (0.1449)
[2023/01/16 00:00] | TRAIN(007): [1900/2211] Batch: 0.0937 (0.1021) Data: 0.0018 (0.0024) Loss: 0.0557 (0.1445)
[2023/01/16 00:00] | TRAIN(007): [1950/2211] Batch: 0.1199 (0.1022) Data: 0.0021 (0.0024) Loss: 0.0239 (0.1447)
[2023/01/16 00:00] | TRAIN(007): [2000/2211] Batch: 0.0926 (0.1022) Data: 0.0021 (0.0024) Loss: 0.0603 (0.1445)
[2023/01/16 00:00] | TRAIN(007): [2050/2211] Batch: 0.0938 (0.1021) Data: 0.0020 (0.0024) Loss: 0.0033 (0.1447)
[2023/01/16 00:00] | TRAIN(007): [2100/2211] Batch: 0.0960 (0.1021) Data: 0.0018 (0.0024) Loss: 0.2585 (0.1442)
[2023/01/16 00:00] | TRAIN(007): [2150/2211] Batch: 0.0934 (0.1020) Data: 0.0019 (0.0024) Loss: 0.1131 (0.1445)
[2023/01/16 00:00] | TRAIN(007): [2200/2211] Batch: 0.1041 (0.1020) Data: 0.0016 (0.0024) Loss: 0.0557 (0.1443)
[2023/01/16 00:00] | ------------------------------------------------------------
[2023/01/16 00:00] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:00] | ------------------------------------------------------------
[2023/01/16 00:00] |     TRAIN(7)     0:03:45     0:00:05     0:03:40      0.1444
[2023/01/16 00:00] | ------------------------------------------------------------
[2023/01/16 00:00] | **************************************************
[2023/01/16 00:00] | TRAIN(008): [  50/2211] Batch: 0.0991 (0.1240) Data: 0.0021 (0.0213) Loss: 0.0832 (0.1170)
[2023/01/16 00:01] | TRAIN(008): [ 100/2211] Batch: 0.1052 (0.1164) Data: 0.0020 (0.0118) Loss: 0.3125 (0.1401)
[2023/01/16 00:01] | TRAIN(008): [ 150/2211] Batch: 0.0989 (0.1103) Data: 0.0020 (0.0085) Loss: 0.1810 (0.1394)
[2023/01/16 00:01] | TRAIN(008): [ 200/2211] Batch: 0.0934 (0.1092) Data: 0.0019 (0.0069) Loss: 0.4060 (0.1443)
[2023/01/16 00:01] | TRAIN(008): [ 250/2211] Batch: 0.0965 (0.1069) Data: 0.0023 (0.0059) Loss: 0.0566 (0.1349)
[2023/01/16 00:01] | TRAIN(008): [ 300/2211] Batch: 0.0917 (0.1056) Data: 0.0019 (0.0053) Loss: 0.0563 (0.1343)
[2023/01/16 00:01] | TRAIN(008): [ 350/2211] Batch: 0.1003 (0.1045) Data: 0.0021 (0.0048) Loss: 0.2070 (0.1384)
[2023/01/16 00:01] | TRAIN(008): [ 400/2211] Batch: 0.1168 (0.1040) Data: 0.0024 (0.0045) Loss: 0.0575 (0.1364)
[2023/01/16 00:01] | TRAIN(008): [ 450/2211] Batch: 0.1278 (0.1067) Data: 0.0026 (0.0043) Loss: 0.0567 (0.1332)
[2023/01/16 00:01] | TRAIN(008): [ 500/2211] Batch: 0.1005 (0.1069) Data: 0.0020 (0.0040) Loss: 0.2933 (0.1337)
[2023/01/16 00:01] | TRAIN(008): [ 550/2211] Batch: 0.0976 (0.1062) Data: 0.0020 (0.0039) Loss: 0.0556 (0.1332)
[2023/01/16 00:01] | TRAIN(008): [ 600/2211] Batch: 0.0937 (0.1056) Data: 0.0020 (0.0037) Loss: 0.3311 (0.1338)
[2023/01/16 00:01] | TRAIN(008): [ 650/2211] Batch: 0.1187 (0.1054) Data: 0.0015 (0.0036) Loss: 0.0758 (0.1342)
[2023/01/16 00:02] | TRAIN(008): [ 700/2211] Batch: 0.1042 (0.1050) Data: 0.0017 (0.0035) Loss: 0.1463 (0.1348)
[2023/01/16 00:02] | TRAIN(008): [ 750/2211] Batch: 0.1211 (0.1049) Data: 0.0024 (0.0034) Loss: 0.1949 (0.1344)
[2023/01/16 00:02] | TRAIN(008): [ 800/2211] Batch: 0.0952 (0.1049) Data: 0.0020 (0.0033) Loss: 0.0570 (0.1336)
[2023/01/16 00:02] | TRAIN(008): [ 850/2211] Batch: 0.0947 (0.1045) Data: 0.0019 (0.0032) Loss: 0.2490 (0.1338)
[2023/01/16 00:02] | TRAIN(008): [ 900/2211] Batch: 0.0930 (0.1046) Data: 0.0020 (0.0032) Loss: 0.1782 (0.1358)
[2023/01/16 00:02] | TRAIN(008): [ 950/2211] Batch: 0.1229 (0.1043) Data: 0.0025 (0.0031) Loss: 0.0557 (0.1357)
[2023/01/16 00:02] | TRAIN(008): [1000/2211] Batch: 0.0925 (0.1042) Data: 0.0021 (0.0030) Loss: 0.3394 (0.1359)
[2023/01/16 00:02] | TRAIN(008): [1050/2211] Batch: 0.0979 (0.1039) Data: 0.0020 (0.0030) Loss: 0.3298 (0.1349)
[2023/01/16 00:02] | TRAIN(008): [1100/2211] Batch: 0.0921 (0.1036) Data: 0.0019 (0.0029) Loss: 0.0856 (0.1337)
[2023/01/16 00:02] | TRAIN(008): [1150/2211] Batch: 0.1212 (0.1035) Data: 0.0019 (0.0029) Loss: 0.1232 (0.1339)
[2023/01/16 00:02] | TRAIN(008): [1200/2211] Batch: 0.1012 (0.1038) Data: 0.0020 (0.0029) Loss: 0.3493 (0.1338)
[2023/01/16 00:03] | TRAIN(008): [1250/2211] Batch: 0.0940 (0.1038) Data: 0.0018 (0.0028) Loss: 0.3457 (0.1367)
[2023/01/16 00:03] | TRAIN(008): [1300/2211] Batch: 0.1003 (0.1037) Data: 0.0019 (0.0028) Loss: 0.0693 (0.1384)
[2023/01/16 00:03] | TRAIN(008): [1350/2211] Batch: 0.0927 (0.1036) Data: 0.0020 (0.0028) Loss: 0.0559 (0.1391)
[2023/01/16 00:03] | TRAIN(008): [1400/2211] Batch: 0.0942 (0.1034) Data: 0.0019 (0.0027) Loss: 0.0558 (0.1394)
[2023/01/16 00:03] | TRAIN(008): [1450/2211] Batch: 0.0942 (0.1031) Data: 0.0023 (0.0027) Loss: 0.3136 (0.1392)
[2023/01/16 00:03] | TRAIN(008): [1500/2211] Batch: 0.0942 (0.1031) Data: 0.0019 (0.0027) Loss: 0.0706 (0.1386)
[2023/01/16 00:03] | TRAIN(008): [1550/2211] Batch: 0.1212 (0.1031) Data: 0.0024 (0.0027) Loss: 0.0557 (0.1371)
[2023/01/16 00:03] | TRAIN(008): [1600/2211] Batch: 0.0930 (0.1030) Data: 0.0019 (0.0026) Loss: 0.2941 (0.1365)
[2023/01/16 00:03] | TRAIN(008): [1650/2211] Batch: 0.1105 (0.1033) Data: 0.0019 (0.0026) Loss: 0.0538 (0.1368)
[2023/01/16 00:03] | TRAIN(008): [1700/2211] Batch: 0.0951 (0.1033) Data: 0.0020 (0.0026) Loss: 0.0581 (0.1363)
[2023/01/16 00:03] | TRAIN(008): [1750/2211] Batch: 0.0905 (0.1032) Data: 0.0020 (0.0026) Loss: 0.1128 (0.1356)
[2023/01/16 00:03] | TRAIN(008): [1800/2211] Batch: 0.0971 (0.1031) Data: 0.0020 (0.0026) Loss: 0.0556 (0.1358)
[2023/01/16 00:04] | TRAIN(008): [1850/2211] Batch: 0.1004 (0.1030) Data: 0.0019 (0.0025) Loss: 0.4203 (0.1355)
[2023/01/16 00:04] | TRAIN(008): [1900/2211] Batch: 0.1053 (0.1028) Data: 0.0021 (0.0025) Loss: 0.0558 (0.1358)
[2023/01/16 00:04] | TRAIN(008): [1950/2211] Batch: 0.1229 (0.1028) Data: 0.0025 (0.0025) Loss: 0.1394 (0.1363)
[2023/01/16 00:04] | TRAIN(008): [2000/2211] Batch: 0.1030 (0.1027) Data: 0.0020 (0.0025) Loss: 0.0557 (0.1369)
[2023/01/16 00:04] | TRAIN(008): [2050/2211] Batch: 0.1047 (0.1026) Data: 0.0020 (0.0025) Loss: 0.0557 (0.1374)
[2023/01/16 00:04] | TRAIN(008): [2100/2211] Batch: 0.0952 (0.1025) Data: 0.0020 (0.0025) Loss: 0.0566 (0.1383)
[2023/01/16 00:04] | TRAIN(008): [2150/2211] Batch: 0.1045 (0.1025) Data: 0.0021 (0.0025) Loss: 0.0561 (0.1378)
[2023/01/16 00:04] | TRAIN(008): [2200/2211] Batch: 0.0994 (0.1024) Data: 0.0018 (0.0025) Loss: 0.0335 (0.1374)
[2023/01/16 00:04] | ------------------------------------------------------------
[2023/01/16 00:04] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:04] | ------------------------------------------------------------
[2023/01/16 00:04] |     TRAIN(8)     0:03:46     0:00:05     0:03:40      0.1370
[2023/01/16 00:04] | ------------------------------------------------------------
[2023/01/16 00:04] | **************************************************
[2023/01/16 00:04] | TRAIN(009): [  50/2211] Batch: 0.0876 (0.1219) Data: 0.0016 (0.0211) Loss: 0.0557 (0.1241)
[2023/01/16 00:04] | TRAIN(009): [ 100/2211] Batch: 0.0936 (0.1096) Data: 0.0019 (0.0116) Loss: 0.1976 (0.1296)
[2023/01/16 00:04] | TRAIN(009): [ 150/2211] Batch: 0.0928 (0.1091) Data: 0.0020 (0.0084) Loss: 0.0023 (0.1258)
[2023/01/16 00:04] | TRAIN(009): [ 200/2211] Batch: 0.0996 (0.1073) Data: 0.0020 (0.0068) Loss: 0.1248 (0.1275)
[2023/01/16 00:05] | TRAIN(009): [ 250/2211] Batch: 0.0994 (0.1056) Data: 0.0019 (0.0059) Loss: 0.1936 (0.1317)
[2023/01/16 00:05] | TRAIN(009): [ 300/2211] Batch: 0.1599 (0.1066) Data: 0.0024 (0.0052) Loss: 0.3000 (0.1371)
[2023/01/16 00:05] | TRAIN(009): [ 350/2211] Batch: 0.1193 (0.1096) Data: 0.0023 (0.0048) Loss: 0.0574 (0.1345)
[2023/01/16 00:05] | TRAIN(009): [ 400/2211] Batch: 0.0956 (0.1095) Data: 0.0022 (0.0045) Loss: 0.0565 (0.1354)
[2023/01/16 00:05] | TRAIN(009): [ 450/2211] Batch: 0.1002 (0.1084) Data: 0.0018 (0.0042) Loss: 0.6356 (0.1356)
[2023/01/16 00:05] | TRAIN(009): [ 500/2211] Batch: 0.0963 (0.1074) Data: 0.0019 (0.0040) Loss: 0.5195 (0.1359)
[2023/01/16 00:05] | TRAIN(009): [ 550/2211] Batch: 0.1020 (0.1069) Data: 0.0020 (0.0038) Loss: 0.1283 (0.1342)
[2023/01/16 00:05] | TRAIN(009): [ 600/2211] Batch: 0.0900 (0.1056) Data: 0.0019 (0.0037) Loss: 0.2117 (0.1327)
[2023/01/16 00:05] | TRAIN(009): [ 650/2211] Batch: 0.1423 (0.1047) Data: 0.0024 (0.0035) Loss: 0.1646 (0.1344)
[2023/01/16 00:05] | TRAIN(009): [ 700/2211] Batch: 0.0981 (0.1062) Data: 0.0019 (0.0034) Loss: 0.3645 (0.1375)
[2023/01/16 00:05] | TRAIN(009): [ 750/2211] Batch: 0.0979 (0.1060) Data: 0.0020 (0.0034) Loss: 0.0910 (0.1353)
[2023/01/16 00:06] | TRAIN(009): [ 800/2211] Batch: 0.0907 (0.1056) Data: 0.0019 (0.0033) Loss: 0.1166 (0.1355)
[2023/01/16 00:06] | TRAIN(009): [ 850/2211] Batch: 0.0989 (0.1054) Data: 0.0019 (0.0032) Loss: 0.0000 (0.1362)
[2023/01/16 00:06] | TRAIN(009): [ 900/2211] Batch: 0.1102 (0.1056) Data: 0.0022 (0.0031) Loss: 0.2199 (0.1345)
[2023/01/16 00:06] | TRAIN(009): [ 950/2211] Batch: 0.1181 (0.1059) Data: 0.0020 (0.0031) Loss: 0.1147 (0.1342)
[2023/01/16 00:06] | TRAIN(009): [1000/2211] Batch: 0.1048 (0.1060) Data: 0.0025 (0.0030) Loss: 0.1235 (0.1321)
[2023/01/16 00:06] | TRAIN(009): [1050/2211] Batch: 0.1145 (0.1061) Data: 0.0019 (0.0030) Loss: 0.0000 (0.1291)
[2023/01/16 00:06] | TRAIN(009): [1100/2211] Batch: 0.0923 (0.1057) Data: 0.0020 (0.0029) Loss: 0.2919 (0.1288)
[2023/01/16 00:06] | TRAIN(009): [1150/2211] Batch: 0.0989 (0.1053) Data: 0.0019 (0.0029) Loss: 0.1688 (0.1286)
[2023/01/16 00:06] | TRAIN(009): [1200/2211] Batch: 0.0922 (0.1050) Data: 0.0019 (0.0028) Loss: 0.2946 (0.1292)
[2023/01/16 00:06] | TRAIN(009): [1250/2211] Batch: 0.0935 (0.1048) Data: 0.0016 (0.0028) Loss: 0.0851 (0.1296)
[2023/01/16 00:06] | TRAIN(009): [1300/2211] Batch: 0.1014 (0.1046) Data: 0.0020 (0.0028) Loss: 0.2453 (0.1310)
[2023/01/16 00:06] | TRAIN(009): [1350/2211] Batch: 0.1025 (0.1043) Data: 0.0019 (0.0027) Loss: 0.0555 (0.1298)
[2023/01/16 00:07] | TRAIN(009): [1400/2211] Batch: 0.0902 (0.1041) Data: 0.0018 (0.0027) Loss: 0.1745 (0.1289)
[2023/01/16 00:07] | TRAIN(009): [1450/2211] Batch: 0.0875 (0.1037) Data: 0.0020 (0.0027) Loss: 0.1246 (0.1284)
[2023/01/16 00:07] | TRAIN(009): [1500/2211] Batch: 0.0913 (0.1037) Data: 0.0020 (0.0027) Loss: 0.3667 (0.1273)
[2023/01/16 00:07] | TRAIN(009): [1550/2211] Batch: 0.0924 (0.1034) Data: 0.0021 (0.0026) Loss: 0.1251 (0.1272)
[2023/01/16 00:07] | TRAIN(009): [1600/2211] Batch: 0.0984 (0.1038) Data: 0.0023 (0.0026) Loss: 0.0226 (0.1266)
[2023/01/16 00:07] | TRAIN(009): [1650/2211] Batch: 0.0972 (0.1042) Data: 0.0020 (0.0026) Loss: 0.0844 (0.1263)
[2023/01/16 00:07] | TRAIN(009): [1700/2211] Batch: 0.1027 (0.1041) Data: 0.0021 (0.0026) Loss: 0.3275 (0.1261)
[2023/01/16 00:07] | TRAIN(009): [1750/2211] Batch: 0.1023 (0.1041) Data: 0.0016 (0.0026) Loss: 0.0000 (0.1246)
[2023/01/16 00:07] | TRAIN(009): [1800/2211] Batch: 0.0960 (0.1040) Data: 0.0019 (0.0026) Loss: 0.0005 (0.1238)
[2023/01/16 00:07] | TRAIN(009): [1850/2211] Batch: 0.0989 (0.1038) Data: 0.0022 (0.0026) Loss: 0.0801 (0.1240)
[2023/01/16 00:07] | TRAIN(009): [1900/2211] Batch: 0.0964 (0.1037) Data: 0.0024 (0.0025) Loss: 0.2466 (0.1241)
[2023/01/16 00:07] | TRAIN(009): [1950/2211] Batch: 0.0973 (0.1036) Data: 0.0021 (0.0025) Loss: 0.0001 (0.1237)
[2023/01/16 00:08] | TRAIN(009): [2000/2211] Batch: 0.0949 (0.1035) Data: 0.0019 (0.0025) Loss: 0.0804 (0.1228)
[2023/01/16 00:08] | TRAIN(009): [2050/2211] Batch: 0.0948 (0.1036) Data: 0.0020 (0.0025) Loss: 0.0000 (0.1218)
[2023/01/16 00:08] | TRAIN(009): [2100/2211] Batch: 0.0999 (0.1035) Data: 0.0019 (0.0025) Loss: 0.0000 (0.1221)
[2023/01/16 00:08] | TRAIN(009): [2150/2211] Batch: 0.0978 (0.1034) Data: 0.0019 (0.0025) Loss: 0.0003 (0.1220)
[2023/01/16 00:08] | TRAIN(009): [2200/2211] Batch: 0.0933 (0.1033) Data: 0.0020 (0.0025) Loss: 0.0000 (0.1220)
[2023/01/16 00:08] | ------------------------------------------------------------
[2023/01/16 00:08] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:08] | ------------------------------------------------------------
[2023/01/16 00:08] |     TRAIN(9)     0:03:48     0:00:05     0:03:42      0.1217
[2023/01/16 00:08] | ------------------------------------------------------------
[2023/01/16 00:08] | **************************************************
[2023/01/16 00:08] | TRAIN(010): [  50/2211] Batch: 0.0953 (0.1249) Data: 0.0021 (0.0215) Loss: 0.4969 (0.1065)
[2023/01/16 00:08] | TRAIN(010): [ 100/2211] Batch: 0.0927 (0.1140) Data: 0.0021 (0.0119) Loss: 0.0080 (0.1111)
[2023/01/16 00:08] | TRAIN(010): [ 150/2211] Batch: 0.1004 (0.1101) Data: 0.0017 (0.0086) Loss: 0.1126 (0.1124)
[2023/01/16 00:08] | TRAIN(010): [ 200/2211] Batch: 0.1019 (0.1085) Data: 0.0018 (0.0070) Loss: 0.0917 (0.1100)
[2023/01/16 00:08] | TRAIN(010): [ 250/2211] Batch: 0.1046 (0.1065) Data: 0.0022 (0.0060) Loss: 0.0000 (0.1047)
[2023/01/16 00:08] | TRAIN(010): [ 300/2211] Batch: 0.0924 (0.1057) Data: 0.0021 (0.0054) Loss: 0.2953 (0.1085)
[2023/01/16 00:09] | TRAIN(010): [ 350/2211] Batch: 0.0968 (0.1060) Data: 0.0020 (0.0049) Loss: 0.2449 (0.1043)
[2023/01/16 00:09] | TRAIN(010): [ 400/2211] Batch: 0.0979 (0.1050) Data: 0.0018 (0.0045) Loss: 0.0000 (0.1068)
[2023/01/16 00:09] | TRAIN(010): [ 450/2211] Batch: 0.0872 (0.1038) Data: 0.0017 (0.0042) Loss: 0.1656 (0.1082)
[2023/01/16 00:09] | TRAIN(010): [ 500/2211] Batch: 0.0982 (0.1029) Data: 0.0018 (0.0040) Loss: 0.0266 (0.1097)
[2023/01/16 00:09] | TRAIN(010): [ 550/2211] Batch: 0.1002 (0.1029) Data: 0.0019 (0.0038) Loss: 0.0000 (0.1098)
[2023/01/16 00:09] | TRAIN(010): [ 600/2211] Batch: 0.0933 (0.1025) Data: 0.0020 (0.0037) Loss: 0.0000 (0.1084)
[2023/01/16 00:09] | TRAIN(010): [ 650/2211] Batch: 0.0921 (0.1022) Data: 0.0019 (0.0035) Loss: 0.2783 (0.1105)
[2023/01/16 00:09] | TRAIN(010): [ 700/2211] Batch: 0.0911 (0.1019) Data: 0.0019 (0.0034) Loss: 0.0001 (0.1099)
[2023/01/16 00:09] | TRAIN(010): [ 750/2211] Batch: 0.0920 (0.1016) Data: 0.0017 (0.0033) Loss: 0.0395 (0.1085)
[2023/01/16 00:09] | TRAIN(010): [ 800/2211] Batch: 0.0968 (0.1014) Data: 0.0017 (0.0032) Loss: 0.0025 (0.1094)
[2023/01/16 00:09] | TRAIN(010): [ 850/2211] Batch: 0.0872 (0.1011) Data: 0.0018 (0.0032) Loss: 0.0000 (0.1074)
[2023/01/16 00:09] | TRAIN(010): [ 900/2211] Batch: 0.0960 (0.1013) Data: 0.0019 (0.0031) Loss: 0.1386 (0.1065)
[2023/01/16 00:10] | TRAIN(010): [ 950/2211] Batch: 0.1029 (0.1011) Data: 0.0020 (0.0030) Loss: 0.1288 (0.1071)
[2023/01/16 00:10] | TRAIN(010): [1000/2211] Batch: 0.1263 (0.1015) Data: 0.0021 (0.0030) Loss: 0.4271 (0.1083)
[2023/01/16 00:10] | TRAIN(010): [1050/2211] Batch: 0.0916 (0.1014) Data: 0.0016 (0.0029) Loss: 0.3377 (0.1087)
[2023/01/16 00:10] | TRAIN(010): [1100/2211] Batch: 0.0904 (0.1013) Data: 0.0018 (0.0029) Loss: 0.4722 (0.1103)
[2023/01/16 00:10] | TRAIN(010): [1150/2211] Batch: 0.1012 (0.1012) Data: 0.0021 (0.0028) Loss: 0.7106 (0.1115)
[2023/01/16 00:10] | TRAIN(010): [1200/2211] Batch: 0.0962 (0.1010) Data: 0.0019 (0.0028) Loss: 0.0118 (0.1120)
[2023/01/16 00:10] | TRAIN(010): [1250/2211] Batch: 0.1072 (0.1008) Data: 0.0024 (0.0028) Loss: 0.0000 (0.1115)
[2023/01/16 00:10] | TRAIN(010): [1300/2211] Batch: 0.0996 (0.1008) Data: 0.0018 (0.0027) Loss: 0.0000 (0.1138)
[2023/01/16 00:10] | TRAIN(010): [1350/2211] Batch: 0.0977 (0.1008) Data: 0.0021 (0.0027) Loss: 0.0000 (0.1145)
[2023/01/16 00:10] | TRAIN(010): [1400/2211] Batch: 0.1224 (0.1006) Data: 0.0023 (0.0027) Loss: 0.0000 (0.1156)
[2023/01/16 00:10] | TRAIN(010): [1450/2211] Batch: 0.0962 (0.1008) Data: 0.0020 (0.0026) Loss: 0.0985 (0.1148)
[2023/01/16 00:10] | TRAIN(010): [1500/2211] Batch: 0.0964 (0.1009) Data: 0.0019 (0.0026) Loss: 0.0001 (0.1150)
[2023/01/16 00:11] | TRAIN(010): [1550/2211] Batch: 0.0925 (0.1013) Data: 0.0015 (0.0026) Loss: 0.0000 (0.1145)
[2023/01/16 00:11] | TRAIN(010): [1600/2211] Batch: 0.1021 (0.1015) Data: 0.0020 (0.0026) Loss: 0.3978 (0.1145)
[2023/01/16 00:11] | TRAIN(010): [1650/2211] Batch: 0.1020 (0.1014) Data: 0.0019 (0.0026) Loss: 0.2360 (0.1144)
[2023/01/16 00:11] | TRAIN(010): [1700/2211] Batch: 0.0968 (0.1013) Data: 0.0019 (0.0026) Loss: 0.2073 (0.1151)
[2023/01/16 00:11] | TRAIN(010): [1750/2211] Batch: 0.1051 (0.1012) Data: 0.0020 (0.0025) Loss: 0.1581 (0.1148)
[2023/01/16 00:11] | TRAIN(010): [1800/2211] Batch: 0.0973 (0.1011) Data: 0.0020 (0.0025) Loss: 0.0000 (0.1139)
[2023/01/16 00:11] | TRAIN(010): [1850/2211] Batch: 0.1237 (0.1011) Data: 0.0023 (0.0025) Loss: 0.0000 (0.1128)
[2023/01/16 00:11] | TRAIN(010): [1900/2211] Batch: 0.0890 (0.1016) Data: 0.0019 (0.0025) Loss: 0.0000 (0.1116)
[2023/01/16 00:11] | TRAIN(010): [1950/2211] Batch: 0.1088 (0.1017) Data: 0.0016 (0.0025) Loss: 0.1343 (0.1113)
[2023/01/16 00:11] | TRAIN(010): [2000/2211] Batch: 0.0913 (0.1015) Data: 0.0019 (0.0025) Loss: 0.1135 (0.1113)
[2023/01/16 00:11] | TRAIN(010): [2050/2211] Batch: 0.0926 (0.1019) Data: 0.0018 (0.0025) Loss: 0.2938 (0.1116)
[2023/01/16 00:11] | TRAIN(010): [2100/2211] Batch: 0.1008 (0.1018) Data: 0.0017 (0.0024) Loss: 0.2075 (0.1108)
[2023/01/16 00:12] | TRAIN(010): [2150/2211] Batch: 0.0915 (0.1016) Data: 0.0019 (0.0024) Loss: 0.3670 (0.1107)
[2023/01/16 00:12] | TRAIN(010): [2200/2211] Batch: 0.0920 (0.1015) Data: 0.0018 (0.0024) Loss: 0.0176 (0.1109)
[2023/01/16 00:12] | ------------------------------------------------------------
[2023/01/16 00:12] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:12] | ------------------------------------------------------------
[2023/01/16 00:12] |    TRAIN(10)     0:03:44     0:00:05     0:03:39      0.1111
[2023/01/16 00:12] | ------------------------------------------------------------
[2023/01/16 00:12] | **************************************************
[2023/01/16 00:12] | TRAIN(011): [  50/2211] Batch: 0.0922 (0.1216) Data: 0.0019 (0.0194) Loss: 0.0000 (0.0888)
[2023/01/16 00:12] | TRAIN(011): [ 100/2211] Batch: 0.0972 (0.1095) Data: 0.0021 (0.0107) Loss: 0.1397 (0.1063)
[2023/01/16 00:12] | TRAIN(011): [ 150/2211] Batch: 0.0910 (0.1059) Data: 0.0017 (0.0078) Loss: 0.2427 (0.1152)
[2023/01/16 00:12] | TRAIN(011): [ 200/2211] Batch: 0.1218 (0.1050) Data: 0.0021 (0.0063) Loss: 0.0676 (0.1094)
[2023/01/16 00:12] | TRAIN(011): [ 250/2211] Batch: 0.0987 (0.1051) Data: 0.0018 (0.0055) Loss: 0.2146 (0.1122)
[2023/01/16 00:12] | TRAIN(011): [ 300/2211] Batch: 0.1268 (0.1080) Data: 0.0022 (0.0049) Loss: 0.0000 (0.1067)
[2023/01/16 00:12] | TRAIN(011): [ 350/2211] Batch: 0.1186 (0.1086) Data: 0.0020 (0.0045) Loss: 0.0000 (0.1045)
[2023/01/16 00:12] | TRAIN(011): [ 400/2211] Batch: 0.0921 (0.1083) Data: 0.0016 (0.0042) Loss: 0.0000 (0.1041)
[2023/01/16 00:12] | TRAIN(011): [ 450/2211] Batch: 0.1042 (0.1075) Data: 0.0017 (0.0039) Loss: 0.2628 (0.1069)
[2023/01/16 00:13] | TRAIN(011): [ 500/2211] Batch: 0.0975 (0.1065) Data: 0.0018 (0.0037) Loss: 0.0000 (0.1059)
[2023/01/16 00:13] | TRAIN(011): [ 550/2211] Batch: 0.0913 (0.1054) Data: 0.0019 (0.0035) Loss: 0.0000 (0.1033)
[2023/01/16 00:13] | TRAIN(011): [ 600/2211] Batch: 0.0923 (0.1045) Data: 0.0017 (0.0034) Loss: 0.1098 (0.1030)
[2023/01/16 00:13] | TRAIN(011): [ 650/2211] Batch: 0.1197 (0.1046) Data: 0.0018 (0.0033) Loss: 0.0556 (0.1031)
[2023/01/16 00:13] | TRAIN(011): [ 700/2211] Batch: 0.0934 (0.1047) Data: 0.0019 (0.0032) Loss: 0.2057 (0.1035)
[2023/01/16 00:13] | TRAIN(011): [ 750/2211] Batch: 0.0992 (0.1057) Data: 0.0020 (0.0031) Loss: 0.0000 (0.1038)
[2023/01/16 00:13] | TRAIN(011): [ 800/2211] Batch: 0.0940 (0.1054) Data: 0.0018 (0.0030) Loss: 0.0007 (0.1035)
[2023/01/16 00:13] | TRAIN(011): [ 850/2211] Batch: 0.1005 (0.1048) Data: 0.0020 (0.0029) Loss: 0.0095 (0.1044)
[2023/01/16 00:13] | TRAIN(011): [ 900/2211] Batch: 0.0913 (0.1043) Data: 0.0018 (0.0029) Loss: 0.0000 (0.1064)
[2023/01/16 00:13] | TRAIN(011): [ 950/2211] Batch: 0.1001 (0.1039) Data: 0.0019 (0.0028) Loss: 0.0482 (0.1063)
[2023/01/16 00:13] | TRAIN(011): [1000/2211] Batch: 0.0879 (0.1035) Data: 0.0018 (0.0028) Loss: 0.3028 (0.1084)
[2023/01/16 00:13] | TRAIN(011): [1050/2211] Batch: 0.1207 (0.1029) Data: 0.0021 (0.0027) Loss: 0.4204 (0.1074)
[2023/01/16 00:14] | TRAIN(011): [1100/2211] Batch: 0.0998 (0.1027) Data: 0.0018 (0.0027) Loss: 0.0000 (0.1051)
[2023/01/16 00:14] | TRAIN(011): [1150/2211] Batch: 0.1026 (0.1026) Data: 0.0018 (0.0027) Loss: 0.1824 (0.1054)
[2023/01/16 00:14] | TRAIN(011): [1200/2211] Batch: 0.0921 (0.1023) Data: 0.0017 (0.0026) Loss: 0.0000 (0.1071)
[2023/01/16 00:14] | TRAIN(011): [1250/2211] Batch: 0.0919 (0.1021) Data: 0.0018 (0.0026) Loss: 0.0001 (0.1075)
[2023/01/16 00:14] | TRAIN(011): [1300/2211] Batch: 0.0954 (0.1020) Data: 0.0018 (0.0026) Loss: 0.2211 (0.1087)
[2023/01/16 00:14] | TRAIN(011): [1350/2211] Batch: 0.0941 (0.1021) Data: 0.0020 (0.0025) Loss: 0.1584 (0.1089)
[2023/01/16 00:14] | TRAIN(011): [1400/2211] Batch: 0.1264 (0.1022) Data: 0.0018 (0.0025) Loss: 0.0442 (0.1095)
[2023/01/16 00:14] | TRAIN(011): [1450/2211] Batch: 0.0940 (0.1021) Data: 0.0018 (0.0025) Loss: 0.0000 (0.1091)
[2023/01/16 00:14] | TRAIN(011): [1500/2211] Batch: 0.1041 (0.1020) Data: 0.0019 (0.0025) Loss: 0.0000 (0.1088)
[2023/01/16 00:14] | TRAIN(011): [1550/2211] Batch: 0.0996 (0.1020) Data: 0.0019 (0.0025) Loss: 0.0001 (0.1087)
[2023/01/16 00:14] | TRAIN(011): [1600/2211] Batch: 0.0998 (0.1022) Data: 0.0018 (0.0024) Loss: 0.0768 (0.1078)
[2023/01/16 00:14] | TRAIN(011): [1650/2211] Batch: 0.1038 (0.1021) Data: 0.0018 (0.0024) Loss: 0.0000 (0.1071)
[2023/01/16 00:15] | TRAIN(011): [1700/2211] Batch: 0.1030 (0.1021) Data: 0.0019 (0.0024) Loss: 0.0000 (0.1080)
[2023/01/16 00:15] | TRAIN(011): [1750/2211] Batch: 0.0995 (0.1020) Data: 0.0018 (0.0024) Loss: 0.0021 (0.1082)
[2023/01/16 00:15] | TRAIN(011): [1800/2211] Batch: 0.0965 (0.1019) Data: 0.0019 (0.0024) Loss: 0.2430 (0.1076)
[2023/01/16 00:15] | TRAIN(011): [1850/2211] Batch: 0.0983 (0.1020) Data: 0.0024 (0.0024) Loss: 0.0000 (0.1071)
[2023/01/16 00:15] | TRAIN(011): [1900/2211] Batch: 0.0978 (0.1019) Data: 0.0020 (0.0024) Loss: 0.0001 (0.1077)
[2023/01/16 00:15] | TRAIN(011): [1950/2211] Batch: 0.0999 (0.1019) Data: 0.0019 (0.0024) Loss: 0.2021 (0.1073)
[2023/01/16 00:15] | TRAIN(011): [2000/2211] Batch: 0.1022 (0.1018) Data: 0.0018 (0.0023) Loss: 0.0518 (0.1067)
[2023/01/16 00:15] | TRAIN(011): [2050/2211] Batch: 0.0883 (0.1018) Data: 0.0019 (0.0023) Loss: 0.1353 (0.1066)
[2023/01/16 00:15] | TRAIN(011): [2100/2211] Batch: 0.0922 (0.1017) Data: 0.0018 (0.0023) Loss: 0.4992 (0.1062)
[2023/01/16 00:15] | TRAIN(011): [2150/2211] Batch: 0.1217 (0.1017) Data: 0.0022 (0.0023) Loss: 0.1525 (0.1066)
[2023/01/16 00:15] | TRAIN(011): [2200/2211] Batch: 0.0898 (0.1017) Data: 0.0018 (0.0023) Loss: 0.2205 (0.1066)
[2023/01/16 00:15] | ------------------------------------------------------------
[2023/01/16 00:15] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:15] | ------------------------------------------------------------
[2023/01/16 00:15] |    TRAIN(11)     0:03:44     0:00:05     0:03:39      0.1064
[2023/01/16 00:15] | ------------------------------------------------------------
[2023/01/16 00:15] | **************************************************
[2023/01/16 00:16] | TRAIN(012): [  50/2211] Batch: 0.0932 (0.1284) Data: 0.0021 (0.0199) Loss: 0.1153 (0.1323)
[2023/01/16 00:16] | TRAIN(012): [ 100/2211] Batch: 0.0917 (0.1135) Data: 0.0018 (0.0110) Loss: 0.0000 (0.1239)
[2023/01/16 00:16] | TRAIN(012): [ 150/2211] Batch: 0.1001 (0.1075) Data: 0.0019 (0.0080) Loss: 0.1267 (0.1184)
[2023/01/16 00:16] | TRAIN(012): [ 200/2211] Batch: 0.1087 (0.1047) Data: 0.0020 (0.0065) Loss: 0.0732 (0.1103)
[2023/01/16 00:16] | TRAIN(012): [ 250/2211] Batch: 0.0967 (0.1034) Data: 0.0019 (0.0056) Loss: 0.0759 (0.1075)
[2023/01/16 00:16] | TRAIN(012): [ 300/2211] Batch: 0.0927 (0.1020) Data: 0.0019 (0.0050) Loss: 0.2609 (0.1020)
[2023/01/16 00:16] | TRAIN(012): [ 350/2211] Batch: 0.0997 (0.1010) Data: 0.0019 (0.0045) Loss: 0.3198 (0.0982)
[2023/01/16 00:16] | TRAIN(012): [ 400/2211] Batch: 0.1347 (0.1022) Data: 0.0024 (0.0042) Loss: 0.0002 (0.1004)
[2023/01/16 00:16] | TRAIN(012): [ 450/2211] Batch: 0.1048 (0.1030) Data: 0.0020 (0.0040) Loss: 0.0000 (0.1019)
[2023/01/16 00:16] | TRAIN(012): [ 500/2211] Batch: 0.0993 (0.1025) Data: 0.0018 (0.0038) Loss: 0.0000 (0.1016)
[2023/01/16 00:16] | TRAIN(012): [ 550/2211] Batch: 0.0989 (0.1024) Data: 0.0018 (0.0036) Loss: 0.0002 (0.1029)
[2023/01/16 00:16] | TRAIN(012): [ 600/2211] Batch: 0.0988 (0.1018) Data: 0.0019 (0.0035) Loss: 0.1323 (0.1009)
[2023/01/16 00:17] | TRAIN(012): [ 650/2211] Batch: 0.0914 (0.1020) Data: 0.0019 (0.0034) Loss: 0.0060 (0.1004)
[2023/01/16 00:17] | TRAIN(012): [ 700/2211] Batch: 0.0934 (0.1016) Data: 0.0020 (0.0033) Loss: 0.0890 (0.1006)
[2023/01/16 00:17] | TRAIN(012): [ 750/2211] Batch: 0.0984 (0.1014) Data: 0.0024 (0.0032) Loss: 0.0021 (0.1006)
[2023/01/16 00:17] | TRAIN(012): [ 800/2211] Batch: 0.0941 (0.1012) Data: 0.0022 (0.0031) Loss: 0.0000 (0.1007)
[2023/01/16 00:17] | TRAIN(012): [ 850/2211] Batch: 0.0990 (0.1010) Data: 0.0022 (0.0030) Loss: 0.6315 (0.1010)
[2023/01/16 00:17] | TRAIN(012): [ 900/2211] Batch: 0.0890 (0.1008) Data: 0.0015 (0.0030) Loss: 0.1641 (0.1018)
[2023/01/16 00:17] | TRAIN(012): [ 950/2211] Batch: 0.0939 (0.1007) Data: 0.0019 (0.0029) Loss: 0.0026 (0.1027)
[2023/01/16 00:17] | TRAIN(012): [1000/2211] Batch: 0.0939 (0.1008) Data: 0.0020 (0.0029) Loss: 0.1288 (0.1024)
[2023/01/16 00:17] | TRAIN(012): [1050/2211] Batch: 0.1006 (0.1009) Data: 0.0016 (0.0028) Loss: 0.2456 (0.1010)
[2023/01/16 00:17] | TRAIN(012): [1100/2211] Batch: 0.0990 (0.1007) Data: 0.0022 (0.0028) Loss: 0.1810 (0.1013)
[2023/01/16 00:17] | TRAIN(012): [1150/2211] Batch: 0.0964 (0.1005) Data: 0.0021 (0.0028) Loss: 0.0952 (0.1016)
[2023/01/16 00:17] | TRAIN(012): [1200/2211] Batch: 0.1013 (0.1004) Data: 0.0019 (0.0027) Loss: 0.0036 (0.1008)
[2023/01/16 00:18] | TRAIN(012): [1250/2211] Batch: 0.0927 (0.1004) Data: 0.0021 (0.0027) Loss: 0.3575 (0.1033)
[2023/01/16 00:18] | TRAIN(012): [1300/2211] Batch: 0.1028 (0.1007) Data: 0.0019 (0.0027) Loss: 0.0001 (0.1038)
[2023/01/16 00:18] | TRAIN(012): [1350/2211] Batch: 0.1103 (0.1008) Data: 0.0019 (0.0027) Loss: 0.0000 (0.1036)
[2023/01/16 00:18] | TRAIN(012): [1400/2211] Batch: 0.0906 (0.1007) Data: 0.0019 (0.0026) Loss: 0.0000 (0.1029)
[2023/01/16 00:18] | TRAIN(012): [1450/2211] Batch: 0.0991 (0.1006) Data: 0.0018 (0.0026) Loss: 0.2250 (0.1034)
[2023/01/16 00:18] | TRAIN(012): [1500/2211] Batch: 0.0944 (0.1008) Data: 0.0021 (0.0026) Loss: 0.0763 (0.1039)
[2023/01/16 00:18] | TRAIN(012): [1550/2211] Batch: 0.0964 (0.1008) Data: 0.0017 (0.0026) Loss: 0.0000 (0.1038)
[2023/01/16 00:18] | TRAIN(012): [1600/2211] Batch: 0.0993 (0.1007) Data: 0.0019 (0.0026) Loss: 0.0000 (0.1032)
[2023/01/16 00:18] | TRAIN(012): [1650/2211] Batch: 0.1063 (0.1006) Data: 0.0019 (0.0025) Loss: 0.1465 (0.1036)
[2023/01/16 00:18] | TRAIN(012): [1700/2211] Batch: 0.0993 (0.1005) Data: 0.0019 (0.0025) Loss: 0.0000 (0.1032)
[2023/01/16 00:18] | TRAIN(012): [1750/2211] Batch: 0.1574 (0.1009) Data: 0.0027 (0.0025) Loss: 0.0000 (0.1028)
[2023/01/16 00:18] | TRAIN(012): [1800/2211] Batch: 0.0899 (0.1011) Data: 0.0019 (0.0025) Loss: 0.0001 (0.1031)
[2023/01/16 00:19] | TRAIN(012): [1850/2211] Batch: 0.1226 (0.1011) Data: 0.0024 (0.0025) Loss: 0.2339 (0.1027)
[2023/01/16 00:19] | TRAIN(012): [1900/2211] Batch: 0.1212 (0.1011) Data: 0.0024 (0.0025) Loss: 0.1353 (0.1032)
[2023/01/16 00:19] | TRAIN(012): [1950/2211] Batch: 0.1019 (0.1013) Data: 0.0018 (0.0025) Loss: 0.0000 (0.1033)
[2023/01/16 00:19] | TRAIN(012): [2000/2211] Batch: 0.1019 (0.1013) Data: 0.0018 (0.0025) Loss: 0.2509 (0.1029)
[2023/01/16 00:19] | TRAIN(012): [2050/2211] Batch: 0.1039 (0.1014) Data: 0.0020 (0.0024) Loss: 0.0785 (0.1031)
[2023/01/16 00:19] | TRAIN(012): [2100/2211] Batch: 0.1009 (0.1014) Data: 0.0021 (0.0024) Loss: 0.3503 (0.1030)
[2023/01/16 00:19] | TRAIN(012): [2150/2211] Batch: 0.1230 (0.1018) Data: 0.0026 (0.0024) Loss: 0.2764 (0.1030)
[2023/01/16 00:19] | TRAIN(012): [2200/2211] Batch: 0.1030 (0.1019) Data: 0.0021 (0.0024) Loss: 0.0000 (0.1027)
[2023/01/16 00:19] | ------------------------------------------------------------
[2023/01/16 00:19] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:19] | ------------------------------------------------------------
[2023/01/16 00:19] |    TRAIN(12)     0:03:45     0:00:05     0:03:39      0.1026
[2023/01/16 00:19] | ------------------------------------------------------------
[2023/01/16 00:19] | **************************************************
[2023/01/16 00:19] | TRAIN(013): [  50/2211] Batch: 0.0936 (0.1340) Data: 0.0016 (0.0199) Loss: 0.2052 (0.0882)
[2023/01/16 00:19] | TRAIN(013): [ 100/2211] Batch: 0.1082 (0.1155) Data: 0.0018 (0.0110) Loss: 0.0000 (0.0912)
[2023/01/16 00:19] | TRAIN(013): [ 150/2211] Batch: 0.1039 (0.1102) Data: 0.0019 (0.0080) Loss: 0.0000 (0.0942)
[2023/01/16 00:20] | TRAIN(013): [ 200/2211] Batch: 0.1240 (0.1086) Data: 0.0023 (0.0065) Loss: 0.1196 (0.0952)
[2023/01/16 00:20] | TRAIN(013): [ 250/2211] Batch: 0.0941 (0.1067) Data: 0.0021 (0.0056) Loss: 0.3569 (0.0959)
[2023/01/16 00:20] | TRAIN(013): [ 300/2211] Batch: 0.0993 (0.1067) Data: 0.0016 (0.0050) Loss: 0.0867 (0.0971)
[2023/01/16 00:20] | TRAIN(013): [ 350/2211] Batch: 0.1042 (0.1060) Data: 0.0021 (0.0046) Loss: 0.3416 (0.0955)
[2023/01/16 00:20] | TRAIN(013): [ 400/2211] Batch: 0.1048 (0.1055) Data: 0.0019 (0.0043) Loss: 0.0000 (0.0945)
[2023/01/16 00:20] | TRAIN(013): [ 450/2211] Batch: 0.1006 (0.1048) Data: 0.0022 (0.0040) Loss: 0.1910 (0.0926)
[2023/01/16 00:20] | TRAIN(013): [ 500/2211] Batch: 0.0953 (0.1044) Data: 0.0020 (0.0038) Loss: 0.2897 (0.0933)
[2023/01/16 00:20] | TRAIN(013): [ 550/2211] Batch: 0.0981 (0.1038) Data: 0.0020 (0.0037) Loss: 0.1489 (0.0938)
[2023/01/16 00:20] | TRAIN(013): [ 600/2211] Batch: 0.0926 (0.1033) Data: 0.0020 (0.0035) Loss: 0.0668 (0.0967)
[2023/01/16 00:20] | TRAIN(013): [ 650/2211] Batch: 0.0959 (0.1028) Data: 0.0020 (0.0034) Loss: 0.0000 (0.0945)
[2023/01/16 00:20] | TRAIN(013): [ 700/2211] Batch: 0.0930 (0.1031) Data: 0.0023 (0.0033) Loss: 0.0000 (0.0923)
[2023/01/16 00:20] | TRAIN(013): [ 750/2211] Batch: 0.0937 (0.1029) Data: 0.0020 (0.0032) Loss: 0.0501 (0.0931)
[2023/01/16 00:21] | TRAIN(013): [ 800/2211] Batch: 0.0979 (0.1027) Data: 0.0020 (0.0032) Loss: 0.1591 (0.0931)
[2023/01/16 00:21] | TRAIN(013): [ 850/2211] Batch: 0.1015 (0.1025) Data: 0.0022 (0.0031) Loss: 0.1072 (0.0925)
[2023/01/16 00:21] | TRAIN(013): [ 900/2211] Batch: 0.0931 (0.1023) Data: 0.0021 (0.0030) Loss: 0.0000 (0.0920)
[2023/01/16 00:21] | TRAIN(013): [ 950/2211] Batch: 0.1121 (0.1023) Data: 0.0022 (0.0030) Loss: 0.0000 (0.0922)
[2023/01/16 00:21] | TRAIN(013): [1000/2211] Batch: 0.0951 (0.1025) Data: 0.0018 (0.0030) Loss: 0.1549 (0.0937)
[2023/01/16 00:21] | TRAIN(013): [1050/2211] Batch: 0.1039 (0.1024) Data: 0.0016 (0.0029) Loss: 0.3292 (0.0936)
[2023/01/16 00:21] | TRAIN(013): [1100/2211] Batch: 0.0939 (0.1028) Data: 0.0020 (0.0029) Loss: 0.0000 (0.0944)
[2023/01/16 00:21] | TRAIN(013): [1150/2211] Batch: 0.0940 (0.1026) Data: 0.0020 (0.0028) Loss: 0.3027 (0.0945)
[2023/01/16 00:21] | TRAIN(013): [1200/2211] Batch: 0.1033 (0.1026) Data: 0.0020 (0.0028) Loss: 0.0000 (0.0937)
[2023/01/16 00:21] | TRAIN(013): [1250/2211] Batch: 0.1057 (0.1028) Data: 0.0020 (0.0028) Loss: 0.2055 (0.0951)
[2023/01/16 00:21] | TRAIN(013): [1300/2211] Batch: 0.1156 (0.1030) Data: 0.0022 (0.0027) Loss: 0.0888 (0.0953)
[2023/01/16 00:22] | TRAIN(013): [1350/2211] Batch: 0.1030 (0.1035) Data: 0.0021 (0.0027) Loss: 0.0000 (0.0952)
[2023/01/16 00:22] | TRAIN(013): [1400/2211] Batch: 0.1001 (0.1039) Data: 0.0020 (0.0027) Loss: 0.0001 (0.0957)
[2023/01/16 00:22] | TRAIN(013): [1450/2211] Batch: 0.0926 (0.1037) Data: 0.0021 (0.0027) Loss: 0.0419 (0.0953)
[2023/01/16 00:22] | TRAIN(013): [1500/2211] Batch: 0.1231 (0.1038) Data: 0.0022 (0.0027) Loss: 0.0001 (0.0953)
[2023/01/16 00:22] | TRAIN(013): [1550/2211] Batch: 0.0934 (0.1038) Data: 0.0019 (0.0026) Loss: 0.2753 (0.0950)
[2023/01/16 00:22] | TRAIN(013): [1600/2211] Batch: 0.1185 (0.1038) Data: 0.0019 (0.0026) Loss: 0.2259 (0.0949)
[2023/01/16 00:22] | TRAIN(013): [1650/2211] Batch: 0.0997 (0.1039) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0953)
[2023/01/16 00:22] | TRAIN(013): [1700/2211] Batch: 0.1091 (0.1037) Data: 0.0026 (0.0026) Loss: 0.0000 (0.0950)
[2023/01/16 00:22] | TRAIN(013): [1750/2211] Batch: 0.0989 (0.1037) Data: 0.0023 (0.0026) Loss: 0.1026 (0.0965)
[2023/01/16 00:22] | TRAIN(013): [1800/2211] Batch: 0.1163 (0.1038) Data: 0.0024 (0.0026) Loss: 0.1633 (0.0974)
[2023/01/16 00:22] | TRAIN(013): [1850/2211] Batch: 0.1092 (0.1039) Data: 0.0023 (0.0026) Loss: 0.0716 (0.0977)
[2023/01/16 00:22] | TRAIN(013): [1900/2211] Batch: 0.1040 (0.1040) Data: 0.0022 (0.0026) Loss: 0.0729 (0.0977)
[2023/01/16 00:23] | TRAIN(013): [1950/2211] Batch: 0.1286 (0.1040) Data: 0.0026 (0.0025) Loss: 0.0001 (0.0975)
[2023/01/16 00:23] | TRAIN(013): [2000/2211] Batch: 0.0975 (0.1042) Data: 0.0020 (0.0025) Loss: 0.0336 (0.0981)
[2023/01/16 00:23] | TRAIN(013): [2050/2211] Batch: 0.1352 (0.1045) Data: 0.0028 (0.0025) Loss: 0.3181 (0.0980)
[2023/01/16 00:23] | TRAIN(013): [2100/2211] Batch: 0.1059 (0.1046) Data: 0.0022 (0.0025) Loss: 0.0002 (0.0985)
[2023/01/16 00:23] | TRAIN(013): [2150/2211] Batch: 0.1170 (0.1049) Data: 0.0025 (0.0025) Loss: 0.0002 (0.0981)
[2023/01/16 00:23] | TRAIN(013): [2200/2211] Batch: 0.1005 (0.1052) Data: 0.0024 (0.0025) Loss: 0.0001 (0.0981)
[2023/01/16 00:23] | ------------------------------------------------------------
[2023/01/16 00:23] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:23] | ------------------------------------------------------------
[2023/01/16 00:23] |    TRAIN(13)     0:03:52     0:00:05     0:03:46      0.0980
[2023/01/16 00:23] | ------------------------------------------------------------
[2023/01/16 00:23] | **************************************************
[2023/01/16 00:23] | TRAIN(014): [  50/2211] Batch: 0.1008 (0.1299) Data: 0.0020 (0.0202) Loss: 0.1765 (0.1240)
[2023/01/16 00:23] | TRAIN(014): [ 100/2211] Batch: 0.1045 (0.1195) Data: 0.0025 (0.0114) Loss: 0.1394 (0.1290)
[2023/01/16 00:23] | TRAIN(014): [ 150/2211] Batch: 0.1313 (0.1191) Data: 0.0035 (0.0086) Loss: 0.1223 (0.1098)
[2023/01/16 00:23] | TRAIN(014): [ 200/2211] Batch: 0.1001 (0.1190) Data: 0.0023 (0.0071) Loss: 0.1415 (0.1043)
[2023/01/16 00:24] | TRAIN(014): [ 250/2211] Batch: 0.0911 (0.1158) Data: 0.0020 (0.0061) Loss: 0.0001 (0.0962)
[2023/01/16 00:24] | TRAIN(014): [ 300/2211] Batch: 0.0943 (0.1125) Data: 0.0022 (0.0054) Loss: 0.0000 (0.0968)
[2023/01/16 00:24] | TRAIN(014): [ 350/2211] Batch: 0.0918 (0.1109) Data: 0.0019 (0.0050) Loss: 0.2157 (0.0976)
[2023/01/16 00:24] | TRAIN(014): [ 400/2211] Batch: 0.0997 (0.1097) Data: 0.0022 (0.0046) Loss: 0.1896 (0.1019)
[2023/01/16 00:24] | TRAIN(014): [ 450/2211] Batch: 0.1025 (0.1087) Data: 0.0020 (0.0043) Loss: 0.2576 (0.1004)
[2023/01/16 00:24] | TRAIN(014): [ 500/2211] Batch: 0.1106 (0.1079) Data: 0.0020 (0.0041) Loss: 0.2254 (0.0990)
[2023/01/16 00:24] | TRAIN(014): [ 550/2211] Batch: 0.0908 (0.1076) Data: 0.0020 (0.0039) Loss: 0.3395 (0.0984)
[2023/01/16 00:24] | TRAIN(014): [ 600/2211] Batch: 0.0959 (0.1070) Data: 0.0021 (0.0038) Loss: 0.0000 (0.0992)
[2023/01/16 00:24] | TRAIN(014): [ 650/2211] Batch: 0.1174 (0.1066) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0999)
[2023/01/16 00:24] | TRAIN(014): [ 700/2211] Batch: 0.0995 (0.1064) Data: 0.0021 (0.0035) Loss: 0.0031 (0.0986)
[2023/01/16 00:24] | TRAIN(014): [ 750/2211] Batch: 0.0936 (0.1059) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0969)
[2023/01/16 00:24] | TRAIN(014): [ 800/2211] Batch: 0.0927 (0.1057) Data: 0.0035 (0.0033) Loss: 0.0037 (0.0975)
[2023/01/16 00:25] | TRAIN(014): [ 850/2211] Batch: 0.0976 (0.1057) Data: 0.0022 (0.0033) Loss: 0.0000 (0.0994)
[2023/01/16 00:25] | TRAIN(014): [ 900/2211] Batch: 0.1077 (0.1054) Data: 0.0019 (0.0032) Loss: 0.2416 (0.1004)
[2023/01/16 00:25] | TRAIN(014): [ 950/2211] Batch: 0.1135 (0.1054) Data: 0.0021 (0.0032) Loss: 0.0007 (0.1005)
[2023/01/16 00:25] | TRAIN(014): [1000/2211] Batch: 0.1054 (0.1055) Data: 0.0022 (0.0031) Loss: 0.0000 (0.1005)
[2023/01/16 00:25] | TRAIN(014): [1050/2211] Batch: 0.1224 (0.1061) Data: 0.0024 (0.0031) Loss: 0.0548 (0.0998)
[2023/01/16 00:25] | TRAIN(014): [1100/2211] Batch: 0.1027 (0.1062) Data: 0.0023 (0.0030) Loss: 0.0495 (0.1000)
[2023/01/16 00:25] | TRAIN(014): [1150/2211] Batch: 0.0990 (0.1063) Data: 0.0023 (0.0030) Loss: 0.0035 (0.0997)
[2023/01/16 00:25] | TRAIN(014): [1200/2211] Batch: 0.1101 (0.1066) Data: 0.0024 (0.0030) Loss: 0.0000 (0.0986)
[2023/01/16 00:25] | TRAIN(014): [1250/2211] Batch: 0.1250 (0.1065) Data: 0.0028 (0.0030) Loss: 0.0000 (0.0979)
[2023/01/16 00:25] | TRAIN(014): [1300/2211] Batch: 0.1248 (0.1067) Data: 0.0020 (0.0029) Loss: 0.0887 (0.0979)
[2023/01/16 00:25] | TRAIN(014): [1350/2211] Batch: 0.1040 (0.1070) Data: 0.0025 (0.0029) Loss: 0.2235 (0.0981)
[2023/01/16 00:26] | TRAIN(014): [1400/2211] Batch: 0.0994 (0.1068) Data: 0.0018 (0.0029) Loss: 0.2588 (0.0984)
[2023/01/16 00:26] | TRAIN(014): [1450/2211] Batch: 0.1151 (0.1068) Data: 0.0020 (0.0029) Loss: 0.5026 (0.0982)
[2023/01/16 00:26] | TRAIN(014): [1500/2211] Batch: 0.1266 (0.1068) Data: 0.0028 (0.0029) Loss: 0.0000 (0.0977)
[2023/01/16 00:26] | TRAIN(014): [1550/2211] Batch: 0.0978 (0.1068) Data: 0.0020 (0.0028) Loss: 0.1900 (0.0988)
[2023/01/16 00:26] | TRAIN(014): [1600/2211] Batch: 0.0968 (0.1068) Data: 0.0022 (0.0028) Loss: 0.0001 (0.0978)
[2023/01/16 00:26] | TRAIN(014): [1650/2211] Batch: 0.1327 (0.1071) Data: 0.0024 (0.0028) Loss: 0.0503 (0.0972)
[2023/01/16 00:26] | TRAIN(014): [1700/2211] Batch: 0.1064 (0.1070) Data: 0.0021 (0.0028) Loss: 0.0755 (0.0967)
[2023/01/16 00:26] | TRAIN(014): [1750/2211] Batch: 0.0994 (0.1069) Data: 0.0023 (0.0028) Loss: 0.0834 (0.0973)
[2023/01/16 00:26] | TRAIN(014): [1800/2211] Batch: 0.1058 (0.1068) Data: 0.0023 (0.0028) Loss: 0.0000 (0.0976)
[2023/01/16 00:26] | TRAIN(014): [1850/2211] Batch: 0.0991 (0.1068) Data: 0.0023 (0.0028) Loss: 0.0912 (0.0971)
[2023/01/16 00:26] | TRAIN(014): [1900/2211] Batch: 0.0988 (0.1068) Data: 0.0022 (0.0027) Loss: 0.0001 (0.0964)
[2023/01/16 00:27] | TRAIN(014): [1950/2211] Batch: 0.1001 (0.1067) Data: 0.0023 (0.0027) Loss: 0.2614 (0.0968)
[2023/01/16 00:27] | TRAIN(014): [2000/2211] Batch: 0.1105 (0.1066) Data: 0.0025 (0.0027) Loss: 0.3838 (0.0968)
[2023/01/16 00:27] | TRAIN(014): [2050/2211] Batch: 0.1224 (0.1068) Data: 0.0022 (0.0027) Loss: 0.3685 (0.0967)
[2023/01/16 00:27] | TRAIN(014): [2100/2211] Batch: 0.1013 (0.1068) Data: 0.0025 (0.0027) Loss: 0.0680 (0.0969)
[2023/01/16 00:27] | TRAIN(014): [2150/2211] Batch: 0.0917 (0.1066) Data: 0.0020 (0.0027) Loss: 0.3217 (0.0969)
[2023/01/16 00:27] | TRAIN(014): [2200/2211] Batch: 0.1266 (0.1065) Data: 0.0029 (0.0027) Loss: 0.0001 (0.0966)
[2023/01/16 00:27] | ------------------------------------------------------------
[2023/01/16 00:27] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:27] | ------------------------------------------------------------
[2023/01/16 00:27] |    TRAIN(14)     0:03:55     0:00:05     0:03:49      0.0967
[2023/01/16 00:27] | ------------------------------------------------------------
[2023/01/16 00:27] | **************************************************
[2023/01/16 00:27] | TRAIN(015): [  50/2211] Batch: 0.1220 (0.1272) Data: 0.0027 (0.0201) Loss: 0.0402 (0.0907)
[2023/01/16 00:27] | TRAIN(015): [ 100/2211] Batch: 0.0940 (0.1209) Data: 0.0034 (0.0113) Loss: 0.0000 (0.0851)
[2023/01/16 00:27] | TRAIN(015): [ 150/2211] Batch: 0.1192 (0.1178) Data: 0.0041 (0.0083) Loss: 0.0955 (0.0854)
[2023/01/16 00:27] | TRAIN(015): [ 200/2211] Batch: 0.1101 (0.1147) Data: 0.0022 (0.0068) Loss: 0.0000 (0.0893)
[2023/01/16 00:27] | TRAIN(015): [ 250/2211] Batch: 0.1189 (0.1118) Data: 0.0022 (0.0059) Loss: 0.0000 (0.0878)
[2023/01/16 00:28] | TRAIN(015): [ 300/2211] Batch: 0.0953 (0.1112) Data: 0.0018 (0.0052) Loss: 0.0000 (0.0917)
[2023/01/16 00:28] | TRAIN(015): [ 350/2211] Batch: 0.0946 (0.1101) Data: 0.0020 (0.0048) Loss: 0.1199 (0.0968)
[2023/01/16 00:28] | TRAIN(015): [ 400/2211] Batch: 0.0944 (0.1084) Data: 0.0022 (0.0044) Loss: 0.0003 (0.0987)
[2023/01/16 00:28] | TRAIN(015): [ 450/2211] Batch: 0.0943 (0.1077) Data: 0.0020 (0.0042) Loss: 0.3767 (0.0979)
[2023/01/16 00:28] | TRAIN(015): [ 500/2211] Batch: 0.1030 (0.1074) Data: 0.0019 (0.0040) Loss: 0.1428 (0.0978)
[2023/01/16 00:28] | TRAIN(015): [ 550/2211] Batch: 0.1026 (0.1065) Data: 0.0018 (0.0038) Loss: 0.0001 (0.0997)
[2023/01/16 00:28] | TRAIN(015): [ 600/2211] Batch: 0.1213 (0.1063) Data: 0.0025 (0.0037) Loss: 0.0002 (0.1006)
[2023/01/16 00:28] | TRAIN(015): [ 650/2211] Batch: 0.0937 (0.1057) Data: 0.0023 (0.0035) Loss: 0.0424 (0.0992)
[2023/01/16 00:28] | TRAIN(015): [ 700/2211] Batch: 0.1015 (0.1055) Data: 0.0017 (0.0035) Loss: 0.0789 (0.1018)
[2023/01/16 00:28] | TRAIN(015): [ 750/2211] Batch: 0.1069 (0.1056) Data: 0.0020 (0.0034) Loss: 0.0651 (0.0998)
[2023/01/16 00:28] | TRAIN(015): [ 800/2211] Batch: 0.1034 (0.1057) Data: 0.0021 (0.0033) Loss: 0.0659 (0.0985)
[2023/01/16 00:28] | TRAIN(015): [ 850/2211] Batch: 0.0975 (0.1057) Data: 0.0024 (0.0033) Loss: 0.1136 (0.0975)
[2023/01/16 00:29] | TRAIN(015): [ 900/2211] Batch: 0.1002 (0.1058) Data: 0.0023 (0.0032) Loss: 0.1956 (0.0978)
[2023/01/16 00:29] | TRAIN(015): [ 950/2211] Batch: 0.1112 (0.1058) Data: 0.0022 (0.0032) Loss: 0.0000 (0.0974)
[2023/01/16 00:29] | TRAIN(015): [1000/2211] Batch: 0.1010 (0.1059) Data: 0.0021 (0.0031) Loss: 0.1386 (0.0963)
[2023/01/16 00:29] | TRAIN(015): [1050/2211] Batch: 0.0969 (0.1059) Data: 0.0022 (0.0031) Loss: 0.0000 (0.0960)
[2023/01/16 00:29] | TRAIN(015): [1100/2211] Batch: 0.1001 (0.1062) Data: 0.0025 (0.0030) Loss: 0.2036 (0.0962)
[2023/01/16 00:29] | TRAIN(015): [1150/2211] Batch: 0.1339 (0.1061) Data: 0.0020 (0.0030) Loss: 0.0231 (0.0963)
[2023/01/16 00:29] | TRAIN(015): [1200/2211] Batch: 0.1238 (0.1070) Data: 0.0021 (0.0030) Loss: 0.2124 (0.0968)
[2023/01/16 00:29] | TRAIN(015): [1250/2211] Batch: 0.0957 (0.1070) Data: 0.0016 (0.0030) Loss: 0.0000 (0.0964)
[2023/01/16 00:29] | TRAIN(015): [1300/2211] Batch: 0.1259 (0.1071) Data: 0.0030 (0.0029) Loss: 0.0000 (0.0955)
[2023/01/16 00:29] | TRAIN(015): [1350/2211] Batch: 0.0971 (0.1072) Data: 0.0019 (0.0029) Loss: 0.1466 (0.0947)
[2023/01/16 00:29] | TRAIN(015): [1400/2211] Batch: 0.1562 (0.1072) Data: 0.0028 (0.0029) Loss: 0.0584 (0.0944)
[2023/01/16 00:30] | TRAIN(015): [1450/2211] Batch: 0.1081 (0.1074) Data: 0.0024 (0.0029) Loss: 0.0906 (0.0933)
[2023/01/16 00:30] | TRAIN(015): [1500/2211] Batch: 0.1208 (0.1079) Data: 0.0024 (0.0029) Loss: 0.0000 (0.0932)
[2023/01/16 00:30] | TRAIN(015): [1550/2211] Batch: 0.0998 (0.1079) Data: 0.0020 (0.0028) Loss: 0.0001 (0.0931)
[2023/01/16 00:30] | TRAIN(015): [1600/2211] Batch: 0.1083 (0.1080) Data: 0.0025 (0.0028) Loss: 0.2726 (0.0941)
[2023/01/16 00:30] | TRAIN(015): [1650/2211] Batch: 0.1195 (0.1083) Data: 0.0024 (0.0028) Loss: 0.1750 (0.0948)
[2023/01/16 00:30] | TRAIN(015): [1700/2211] Batch: 0.0993 (0.1085) Data: 0.0018 (0.0028) Loss: 0.2105 (0.0950)
[2023/01/16 00:30] | TRAIN(015): [1750/2211] Batch: 0.1037 (0.1084) Data: 0.0024 (0.0028) Loss: 0.0000 (0.0947)
[2023/01/16 00:30] | TRAIN(015): [1800/2211] Batch: 0.1285 (0.1082) Data: 0.0025 (0.0027) Loss: 0.0535 (0.0940)
[2023/01/16 00:30] | TRAIN(015): [1850/2211] Batch: 0.1253 (0.1083) Data: 0.0023 (0.0027) Loss: 0.1920 (0.0936)
[2023/01/16 00:30] | TRAIN(015): [1900/2211] Batch: 0.0991 (0.1083) Data: 0.0017 (0.0027) Loss: 0.0126 (0.0934)
[2023/01/16 00:30] | TRAIN(015): [1950/2211] Batch: 0.0926 (0.1081) Data: 0.0032 (0.0027) Loss: 0.0025 (0.0932)
[2023/01/16 00:31] | TRAIN(015): [2000/2211] Batch: 0.0899 (0.1079) Data: 0.0021 (0.0027) Loss: 0.0000 (0.0923)
[2023/01/16 00:31] | TRAIN(015): [2050/2211] Batch: 0.0991 (0.1078) Data: 0.0020 (0.0027) Loss: 0.2767 (0.0928)
[2023/01/16 00:31] | TRAIN(015): [2100/2211] Batch: 0.0933 (0.1076) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0926)
[2023/01/16 00:31] | TRAIN(015): [2150/2211] Batch: 0.1052 (0.1074) Data: 0.0022 (0.0026) Loss: 0.0072 (0.0930)
[2023/01/16 00:31] | TRAIN(015): [2200/2211] Batch: 0.0897 (0.1072) Data: 0.0020 (0.0026) Loss: 0.0281 (0.0929)
[2023/01/16 00:31] | ------------------------------------------------------------
[2023/01/16 00:31] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:31] | ------------------------------------------------------------
[2023/01/16 00:31] |    TRAIN(15)     0:03:56     0:00:05     0:03:51      0.0931
[2023/01/16 00:31] | ------------------------------------------------------------
[2023/01/16 00:31] | **************************************************
[2023/01/16 00:31] | TRAIN(016): [  50/2211] Batch: 0.1132 (0.1261) Data: 0.0021 (0.0202) Loss: 0.0435 (0.0564)
[2023/01/16 00:31] | TRAIN(016): [ 100/2211] Batch: 0.1000 (0.1122) Data: 0.0019 (0.0112) Loss: 0.0500 (0.0702)
[2023/01/16 00:31] | TRAIN(016): [ 150/2211] Batch: 0.1220 (0.1155) Data: 0.0018 (0.0082) Loss: 0.0835 (0.0723)
[2023/01/16 00:31] | TRAIN(016): [ 200/2211] Batch: 0.0927 (0.1128) Data: 0.0020 (0.0067) Loss: 0.1531 (0.0747)
[2023/01/16 00:31] | TRAIN(016): [ 250/2211] Batch: 0.1005 (0.1105) Data: 0.0021 (0.0058) Loss: 0.1515 (0.0761)
[2023/01/16 00:31] | TRAIN(016): [ 300/2211] Batch: 0.1011 (0.1099) Data: 0.0022 (0.0052) Loss: 0.0380 (0.0784)
[2023/01/16 00:32] | TRAIN(016): [ 350/2211] Batch: 0.1227 (0.1109) Data: 0.0021 (0.0048) Loss: 0.0486 (0.0821)
[2023/01/16 00:32] | TRAIN(016): [ 400/2211] Batch: 0.0950 (0.1109) Data: 0.0020 (0.0044) Loss: 0.2187 (0.0880)
[2023/01/16 00:32] | TRAIN(016): [ 450/2211] Batch: 0.1100 (0.1124) Data: 0.0027 (0.0042) Loss: 0.0659 (0.0888)
[2023/01/16 00:32] | TRAIN(016): [ 500/2211] Batch: 0.1074 (0.1127) Data: 0.0020 (0.0041) Loss: 0.2441 (0.0925)
[2023/01/16 00:32] | TRAIN(016): [ 550/2211] Batch: 0.1160 (0.1136) Data: 0.0017 (0.0039) Loss: 0.1984 (0.0933)
[2023/01/16 00:32] | TRAIN(016): [ 600/2211] Batch: 0.1542 (0.1146) Data: 0.0024 (0.0038) Loss: 0.2159 (0.0933)
[2023/01/16 00:32] | TRAIN(016): [ 650/2211] Batch: 0.1199 (0.1152) Data: 0.0024 (0.0037) Loss: 0.0001 (0.0924)
[2023/01/16 00:32] | TRAIN(016): [ 700/2211] Batch: 0.1051 (0.1148) Data: 0.0022 (0.0036) Loss: 0.0655 (0.0925)
[2023/01/16 00:32] | TRAIN(016): [ 750/2211] Batch: 0.1256 (0.1136) Data: 0.0025 (0.0035) Loss: 0.0735 (0.0907)
[2023/01/16 00:32] | TRAIN(016): [ 800/2211] Batch: 0.1034 (0.1141) Data: 0.0023 (0.0034) Loss: 0.0000 (0.0914)
[2023/01/16 00:33] | TRAIN(016): [ 850/2211] Batch: 0.1008 (0.1143) Data: 0.0024 (0.0034) Loss: 0.0002 (0.0925)
[2023/01/16 00:33] | TRAIN(016): [ 900/2211] Batch: 0.1017 (0.1138) Data: 0.0024 (0.0033) Loss: 0.0577 (0.0925)
[2023/01/16 00:33] | TRAIN(016): [ 950/2211] Batch: 0.1370 (0.1134) Data: 0.0024 (0.0032) Loss: 0.0001 (0.0933)
[2023/01/16 00:33] | TRAIN(016): [1000/2211] Batch: 0.1009 (0.1132) Data: 0.0023 (0.0032) Loss: 0.0640 (0.0933)
[2023/01/16 00:33] | TRAIN(016): [1050/2211] Batch: 0.1181 (0.1130) Data: 0.0019 (0.0031) Loss: 0.1231 (0.0932)
[2023/01/16 00:33] | TRAIN(016): [1100/2211] Batch: 0.0983 (0.1133) Data: 0.0023 (0.0031) Loss: 0.0000 (0.0930)
[2023/01/16 00:33] | TRAIN(016): [1150/2211] Batch: 0.1092 (0.1130) Data: 0.0024 (0.0031) Loss: 0.1067 (0.0921)
[2023/01/16 00:33] | TRAIN(016): [1200/2211] Batch: 0.0981 (0.1130) Data: 0.0022 (0.0030) Loss: 0.0000 (0.0919)
[2023/01/16 00:33] | TRAIN(016): [1250/2211] Batch: 0.0971 (0.1127) Data: 0.0024 (0.0030) Loss: 0.0712 (0.0931)
[2023/01/16 00:33] | TRAIN(016): [1300/2211] Batch: 0.0962 (0.1126) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0930)
[2023/01/16 00:33] | TRAIN(016): [1350/2211] Batch: 0.0906 (0.1120) Data: 0.0016 (0.0029) Loss: 0.0017 (0.0939)
[2023/01/16 00:34] | TRAIN(016): [1400/2211] Batch: 0.0943 (0.1113) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0940)
[2023/01/16 00:34] | TRAIN(016): [1450/2211] Batch: 0.0929 (0.1112) Data: 0.0020 (0.0029) Loss: 0.0166 (0.0937)
[2023/01/16 00:34] | TRAIN(016): [1500/2211] Batch: 0.0946 (0.1108) Data: 0.0021 (0.0029) Loss: 0.2086 (0.0935)
[2023/01/16 00:34] | TRAIN(016): [1550/2211] Batch: 0.1148 (0.1106) Data: 0.0016 (0.0028) Loss: 0.1549 (0.0933)
[2023/01/16 00:34] | TRAIN(016): [1600/2211] Batch: 0.0994 (0.1104) Data: 0.0020 (0.0028) Loss: 0.2100 (0.0931)
[2023/01/16 00:34] | TRAIN(016): [1650/2211] Batch: 0.0938 (0.1100) Data: 0.0021 (0.0028) Loss: 0.0655 (0.0930)
[2023/01/16 00:34] | TRAIN(016): [1700/2211] Batch: 0.0996 (0.1099) Data: 0.0029 (0.0028) Loss: 0.0000 (0.0927)
[2023/01/16 00:34] | TRAIN(016): [1750/2211] Batch: 0.1087 (0.1096) Data: 0.0022 (0.0027) Loss: 0.0833 (0.0919)
[2023/01/16 00:34] | TRAIN(016): [1800/2211] Batch: 0.1022 (0.1093) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0915)
[2023/01/16 00:34] | TRAIN(016): [1850/2211] Batch: 0.1115 (0.1092) Data: 0.0042 (0.0027) Loss: 0.0000 (0.0918)
[2023/01/16 00:34] | TRAIN(016): [1900/2211] Batch: 0.1076 (0.1092) Data: 0.0015 (0.0027) Loss: 0.0196 (0.0914)
[2023/01/16 00:34] | TRAIN(016): [1950/2211] Batch: 0.1123 (0.1093) Data: 0.0020 (0.0027) Loss: 0.1181 (0.0913)
[2023/01/16 00:35] | TRAIN(016): [2000/2211] Batch: 0.1002 (0.1092) Data: 0.0019 (0.0027) Loss: 0.1231 (0.0917)
[2023/01/16 00:35] | TRAIN(016): [2050/2211] Batch: 0.0973 (0.1090) Data: 0.0015 (0.0027) Loss: 0.0036 (0.0916)
[2023/01/16 00:35] | TRAIN(016): [2100/2211] Batch: 0.1445 (0.1089) Data: 0.0022 (0.0026) Loss: 0.0696 (0.0915)
[2023/01/16 00:35] | TRAIN(016): [2150/2211] Batch: 0.1213 (0.1090) Data: 0.0026 (0.0026) Loss: 0.0000 (0.0913)
[2023/01/16 00:35] | TRAIN(016): [2200/2211] Batch: 0.1152 (0.1091) Data: 0.0027 (0.0026) Loss: 0.1281 (0.0912)
[2023/01/16 00:35] | ------------------------------------------------------------
[2023/01/16 00:35] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:35] | ------------------------------------------------------------
[2023/01/16 00:35] |    TRAIN(16)     0:04:01     0:00:05     0:03:55      0.0910
[2023/01/16 00:35] | ------------------------------------------------------------
[2023/01/16 00:35] | **************************************************
[2023/01/16 00:35] | TRAIN(017): [  50/2211] Batch: 0.1076 (0.1324) Data: 0.0024 (0.0218) Loss: 0.0946 (0.0542)
[2023/01/16 00:35] | TRAIN(017): [ 100/2211] Batch: 0.1193 (0.1231) Data: 0.0032 (0.0122) Loss: 0.0000 (0.0800)
[2023/01/16 00:35] | TRAIN(017): [ 150/2211] Batch: 0.1001 (0.1199) Data: 0.0021 (0.0089) Loss: 0.0000 (0.0802)
[2023/01/16 00:35] | TRAIN(017): [ 200/2211] Batch: 0.1140 (0.1151) Data: 0.0025 (0.0073) Loss: 0.0186 (0.0843)
[2023/01/16 00:35] | TRAIN(017): [ 250/2211] Batch: 0.1245 (0.1147) Data: 0.0025 (0.0063) Loss: 0.0000 (0.0884)
[2023/01/16 00:36] | TRAIN(017): [ 300/2211] Batch: 0.1651 (0.1147) Data: 0.0030 (0.0057) Loss: 0.0001 (0.0858)
[2023/01/16 00:36] | TRAIN(017): [ 350/2211] Batch: 0.1018 (0.1139) Data: 0.0024 (0.0052) Loss: 0.0691 (0.0840)
[2023/01/16 00:36] | TRAIN(017): [ 400/2211] Batch: 0.1029 (0.1132) Data: 0.0025 (0.0049) Loss: 0.1145 (0.0841)
[2023/01/16 00:36] | TRAIN(017): [ 450/2211] Batch: 0.1211 (0.1131) Data: 0.0022 (0.0046) Loss: 0.0000 (0.0833)
[2023/01/16 00:36] | TRAIN(017): [ 500/2211] Batch: 0.1480 (0.1132) Data: 0.0024 (0.0044) Loss: 0.0000 (0.0826)
[2023/01/16 00:36] | TRAIN(017): [ 550/2211] Batch: 0.1261 (0.1140) Data: 0.0024 (0.0042) Loss: 0.0354 (0.0841)
[2023/01/16 00:36] | TRAIN(017): [ 600/2211] Batch: 0.1189 (0.1147) Data: 0.0024 (0.0041) Loss: 0.1509 (0.0839)
[2023/01/16 00:36] | TRAIN(017): [ 650/2211] Batch: 0.1402 (0.1142) Data: 0.0024 (0.0039) Loss: 0.1166 (0.0850)
[2023/01/16 00:36] | TRAIN(017): [ 700/2211] Batch: 0.1768 (0.1162) Data: 0.0040 (0.0038) Loss: 0.4475 (0.0847)
[2023/01/16 00:36] | TRAIN(017): [ 750/2211] Batch: 0.1138 (0.1164) Data: 0.0024 (0.0038) Loss: 0.0601 (0.0881)
[2023/01/16 00:37] | TRAIN(017): [ 800/2211] Batch: 0.1551 (0.1165) Data: 0.0027 (0.0037) Loss: 0.0000 (0.0875)
[2023/01/16 00:37] | TRAIN(017): [ 850/2211] Batch: 0.1013 (0.1165) Data: 0.0025 (0.0036) Loss: 0.3249 (0.0890)
[2023/01/16 00:37] | TRAIN(017): [ 900/2211] Batch: 0.1052 (0.1158) Data: 0.0041 (0.0035) Loss: 0.1225 (0.0879)
[2023/01/16 00:37] | TRAIN(017): [ 950/2211] Batch: 0.1015 (0.1154) Data: 0.0022 (0.0035) Loss: 0.0040 (0.0887)
[2023/01/16 00:37] | TRAIN(017): [1000/2211] Batch: 0.0944 (0.1147) Data: 0.0021 (0.0034) Loss: 0.0000 (0.0883)
[2023/01/16 00:37] | TRAIN(017): [1050/2211] Batch: 0.1207 (0.1151) Data: 0.0017 (0.0033) Loss: 0.0302 (0.0887)
[2023/01/16 00:37] | TRAIN(017): [1100/2211] Batch: 0.1198 (0.1154) Data: 0.0022 (0.0033) Loss: 0.2078 (0.0898)
[2023/01/16 00:37] | TRAIN(017): [1150/2211] Batch: 0.0960 (0.1154) Data: 0.0022 (0.0033) Loss: 0.0000 (0.0900)
[2023/01/16 00:37] | TRAIN(017): [1200/2211] Batch: 0.1221 (0.1148) Data: 0.0023 (0.0032) Loss: 0.1967 (0.0910)
[2023/01/16 00:37] | TRAIN(017): [1250/2211] Batch: 0.1206 (0.1143) Data: 0.0025 (0.0032) Loss: 0.2346 (0.0907)
[2023/01/16 00:37] | TRAIN(017): [1300/2211] Batch: 0.0933 (0.1138) Data: 0.0020 (0.0031) Loss: 0.0603 (0.0908)
[2023/01/16 00:38] | TRAIN(017): [1350/2211] Batch: 0.1267 (0.1134) Data: 0.0020 (0.0031) Loss: 0.0844 (0.0912)
[2023/01/16 00:38] | TRAIN(017): [1400/2211] Batch: 0.1168 (0.1136) Data: 0.0022 (0.0031) Loss: 0.2897 (0.0909)
[2023/01/16 00:38] | TRAIN(017): [1450/2211] Batch: 0.0886 (0.1137) Data: 0.0020 (0.0030) Loss: 0.0000 (0.0921)
[2023/01/16 00:38] | TRAIN(017): [1500/2211] Batch: 0.0941 (0.1132) Data: 0.0020 (0.0030) Loss: 0.0004 (0.0909)
[2023/01/16 00:38] | TRAIN(017): [1550/2211] Batch: 0.0937 (0.1132) Data: 0.0020 (0.0030) Loss: 0.2216 (0.0906)
[2023/01/16 00:38] | TRAIN(017): [1600/2211] Batch: 0.0900 (0.1127) Data: 0.0020 (0.0029) Loss: 0.0869 (0.0901)
[2023/01/16 00:38] | TRAIN(017): [1650/2211] Batch: 0.1002 (0.1122) Data: 0.0026 (0.0029) Loss: 0.0000 (0.0899)
[2023/01/16 00:38] | TRAIN(017): [1700/2211] Batch: 0.1395 (0.1124) Data: 0.0029 (0.0029) Loss: 0.1508 (0.0892)
[2023/01/16 00:38] | TRAIN(017): [1750/2211] Batch: 0.1018 (0.1127) Data: 0.0024 (0.0029) Loss: 0.0644 (0.0883)
[2023/01/16 00:38] | TRAIN(017): [1800/2211] Batch: 0.1168 (0.1127) Data: 0.0021 (0.0029) Loss: 0.2574 (0.0880)
[2023/01/16 00:38] | TRAIN(017): [1850/2211] Batch: 0.1037 (0.1125) Data: 0.0025 (0.0029) Loss: 0.1527 (0.0881)
[2023/01/16 00:39] | TRAIN(017): [1900/2211] Batch: 0.1001 (0.1124) Data: 0.0023 (0.0029) Loss: 0.0688 (0.0878)
[2023/01/16 00:39] | TRAIN(017): [1950/2211] Batch: 0.1079 (0.1125) Data: 0.0024 (0.0028) Loss: 0.1362 (0.0879)
[2023/01/16 00:39] | TRAIN(017): [2000/2211] Batch: 0.1424 (0.1128) Data: 0.0024 (0.0028) Loss: 0.0002 (0.0870)
[2023/01/16 00:39] | TRAIN(017): [2050/2211] Batch: 0.1137 (0.1125) Data: 0.0021 (0.0028) Loss: 0.2056 (0.0878)
[2023/01/16 00:39] | TRAIN(017): [2100/2211] Batch: 0.1006 (0.1124) Data: 0.0023 (0.0028) Loss: 0.2401 (0.0879)
[2023/01/16 00:39] | TRAIN(017): [2150/2211] Batch: 0.1123 (0.1122) Data: 0.0023 (0.0028) Loss: 0.0000 (0.0876)
[2023/01/16 00:39] | TRAIN(017): [2200/2211] Batch: 0.1047 (0.1121) Data: 0.0023 (0.0028) Loss: 0.5117 (0.0881)
[2023/01/16 00:39] | ------------------------------------------------------------
[2023/01/16 00:39] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:39] | ------------------------------------------------------------
[2023/01/16 00:39] |    TRAIN(17)     0:04:07     0:00:06     0:04:01      0.0884
[2023/01/16 00:39] | ------------------------------------------------------------
[2023/01/16 00:39] | **************************************************
[2023/01/16 00:39] | TRAIN(018): [  50/2211] Batch: 0.0932 (0.1275) Data: 0.0020 (0.0204) Loss: 0.0959 (0.0786)
[2023/01/16 00:39] | TRAIN(018): [ 100/2211] Batch: 0.1119 (0.1169) Data: 0.0025 (0.0114) Loss: 0.2658 (0.0724)
[2023/01/16 00:39] | TRAIN(018): [ 150/2211] Batch: 0.1262 (0.1169) Data: 0.0023 (0.0085) Loss: 0.2138 (0.0921)
[2023/01/16 00:39] | TRAIN(018): [ 200/2211] Batch: 0.1126 (0.1157) Data: 0.0022 (0.0070) Loss: 0.1151 (0.0892)
[2023/01/16 00:40] | TRAIN(018): [ 250/2211] Batch: 0.1343 (0.1157) Data: 0.0030 (0.0061) Loss: 0.0001 (0.0913)
[2023/01/16 00:40] | TRAIN(018): [ 300/2211] Batch: 0.1035 (0.1149) Data: 0.0026 (0.0055) Loss: 0.0001 (0.0921)
[2023/01/16 00:40] | TRAIN(018): [ 350/2211] Batch: 0.1091 (0.1141) Data: 0.0024 (0.0051) Loss: 0.1143 (0.0902)
[2023/01/16 00:40] | TRAIN(018): [ 400/2211] Batch: 0.1379 (0.1147) Data: 0.0026 (0.0048) Loss: 0.1392 (0.0860)
[2023/01/16 00:40] | TRAIN(018): [ 450/2211] Batch: 0.1467 (0.1155) Data: 0.0027 (0.0045) Loss: 0.0000 (0.0843)
[2023/01/16 00:40] | TRAIN(018): [ 500/2211] Batch: 0.1350 (0.1169) Data: 0.0025 (0.0044) Loss: 0.0557 (0.0843)
[2023/01/16 00:40] | TRAIN(018): [ 550/2211] Batch: 0.0922 (0.1170) Data: 0.0020 (0.0042) Loss: 0.0153 (0.0858)
[2023/01/16 00:40] | TRAIN(018): [ 600/2211] Batch: 0.1440 (0.1160) Data: 0.0026 (0.0040) Loss: 0.1040 (0.0858)
[2023/01/16 00:40] | TRAIN(018): [ 650/2211] Batch: 0.1365 (0.1158) Data: 0.0026 (0.0039) Loss: 0.0031 (0.0863)
[2023/01/16 00:40] | TRAIN(018): [ 700/2211] Batch: 0.0917 (0.1160) Data: 0.0020 (0.0038) Loss: 0.0002 (0.0833)
[2023/01/16 00:41] | TRAIN(018): [ 750/2211] Batch: 0.0955 (0.1157) Data: 0.0020 (0.0037) Loss: 0.0058 (0.0823)
[2023/01/16 00:41] | TRAIN(018): [ 800/2211] Batch: 0.0935 (0.1148) Data: 0.0021 (0.0036) Loss: 0.2073 (0.0827)
[2023/01/16 00:41] | TRAIN(018): [ 850/2211] Batch: 0.0950 (0.1147) Data: 0.0021 (0.0035) Loss: 0.1438 (0.0840)
[2023/01/16 00:41] | TRAIN(018): [ 900/2211] Batch: 0.1254 (0.1152) Data: 0.0041 (0.0035) Loss: 0.0287 (0.0838)
[2023/01/16 00:41] | TRAIN(018): [ 950/2211] Batch: 0.1010 (0.1152) Data: 0.0020 (0.0034) Loss: 0.0000 (0.0830)
[2023/01/16 00:41] | TRAIN(018): [1000/2211] Batch: 0.0956 (0.1144) Data: 0.0021 (0.0034) Loss: 0.1309 (0.0829)
[2023/01/16 00:41] | TRAIN(018): [1050/2211] Batch: 0.0895 (0.1143) Data: 0.0006 (0.0033) Loss: 0.0000 (0.0837)
[2023/01/16 00:41] | TRAIN(018): [1100/2211] Batch: 0.0964 (0.1144) Data: 0.0021 (0.0033) Loss: 0.0047 (0.0836)
[2023/01/16 00:41] | TRAIN(018): [1150/2211] Batch: 0.1035 (0.1139) Data: 0.0022 (0.0032) Loss: 0.4393 (0.0844)
[2023/01/16 00:41] | TRAIN(018): [1200/2211] Batch: 0.0941 (0.1141) Data: 0.0020 (0.0032) Loss: 0.0000 (0.0835)
[2023/01/16 00:41] | TRAIN(018): [1250/2211] Batch: 0.1022 (0.1138) Data: 0.0019 (0.0032) Loss: 0.0096 (0.0842)
[2023/01/16 00:42] | TRAIN(018): [1300/2211] Batch: 0.1073 (0.1142) Data: 0.0023 (0.0031) Loss: 0.0038 (0.0836)
[2023/01/16 00:42] | TRAIN(018): [1350/2211] Batch: 0.1050 (0.1142) Data: 0.0020 (0.0031) Loss: 0.0899 (0.0840)
[2023/01/16 00:42] | TRAIN(018): [1400/2211] Batch: 0.1157 (0.1144) Data: 0.0019 (0.0031) Loss: 0.1628 (0.0854)
[2023/01/16 00:42] | TRAIN(018): [1450/2211] Batch: 0.1019 (0.1142) Data: 0.0019 (0.0030) Loss: 0.4593 (0.0853)
[2023/01/16 00:42] | TRAIN(018): [1500/2211] Batch: 0.1080 (0.1138) Data: 0.0021 (0.0030) Loss: 0.0000 (0.0853)
[2023/01/16 00:42] | TRAIN(018): [1550/2211] Batch: 0.0988 (0.1135) Data: 0.0022 (0.0030) Loss: 0.0000 (0.0849)
[2023/01/16 00:42] | TRAIN(018): [1600/2211] Batch: 0.0942 (0.1131) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0844)
[2023/01/16 00:42] | TRAIN(018): [1650/2211] Batch: 0.1194 (0.1129) Data: 0.0023 (0.0029) Loss: 0.2676 (0.0844)
[2023/01/16 00:42] | TRAIN(018): [1700/2211] Batch: 0.0994 (0.1128) Data: 0.0019 (0.0029) Loss: 0.0719 (0.0841)
[2023/01/16 00:42] | TRAIN(018): [1750/2211] Batch: 0.1443 (0.1129) Data: 0.0024 (0.0029) Loss: 0.0162 (0.0845)
[2023/01/16 00:42] | TRAIN(018): [1800/2211] Batch: 0.1004 (0.1128) Data: 0.0021 (0.0028) Loss: 0.0665 (0.0849)
[2023/01/16 00:43] | TRAIN(018): [1850/2211] Batch: 0.0994 (0.1125) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0854)
[2023/01/16 00:43] | TRAIN(018): [1900/2211] Batch: 0.1091 (0.1127) Data: 0.0022 (0.0028) Loss: 0.0000 (0.0847)
[2023/01/16 00:43] | TRAIN(018): [1950/2211] Batch: 0.1095 (0.1124) Data: 0.0020 (0.0028) Loss: 0.0506 (0.0846)
[2023/01/16 00:43] | TRAIN(018): [2000/2211] Batch: 0.1008 (0.1125) Data: 0.0020 (0.0028) Loss: 0.0079 (0.0843)
[2023/01/16 00:43] | TRAIN(018): [2050/2211] Batch: 0.1123 (0.1123) Data: 0.0021 (0.0028) Loss: 0.1552 (0.0849)
[2023/01/16 00:43] | TRAIN(018): [2100/2211] Batch: 0.1234 (0.1121) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0851)
[2023/01/16 00:43] | TRAIN(018): [2150/2211] Batch: 0.1223 (0.1122) Data: 0.0020 (0.0027) Loss: 0.2730 (0.0854)
[2023/01/16 00:43] | TRAIN(018): [2200/2211] Batch: 0.0966 (0.1121) Data: 0.0015 (0.0027) Loss: 0.2293 (0.0853)
[2023/01/16 00:43] | ------------------------------------------------------------
[2023/01/16 00:43] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:43] | ------------------------------------------------------------
[2023/01/16 00:43] |    TRAIN(18)     0:04:07     0:00:05     0:04:01      0.0857
[2023/01/16 00:43] | ------------------------------------------------------------
[2023/01/16 00:43] | **************************************************
[2023/01/16 00:43] | TRAIN(019): [  50/2211] Batch: 0.1378 (0.1613) Data: 0.0022 (0.0256) Loss: 0.0001 (0.0616)
[2023/01/16 00:43] | TRAIN(019): [ 100/2211] Batch: 0.1551 (0.1398) Data: 0.0025 (0.0139) Loss: 0.0893 (0.0682)
[2023/01/16 00:44] | TRAIN(019): [ 150/2211] Batch: 0.1230 (0.1329) Data: 0.0023 (0.0100) Loss: 0.0000 (0.0768)
[2023/01/16 00:44] | TRAIN(019): [ 200/2211] Batch: 0.0970 (0.1252) Data: 0.0021 (0.0080) Loss: 0.0001 (0.0786)
[2023/01/16 00:44] | TRAIN(019): [ 250/2211] Batch: 0.0984 (0.1209) Data: 0.0025 (0.0068) Loss: 0.0662 (0.0843)
[2023/01/16 00:44] | TRAIN(019): [ 300/2211] Batch: 0.1516 (0.1198) Data: 0.0024 (0.0060) Loss: 0.1423 (0.0921)
[2023/01/16 00:44] | TRAIN(019): [ 350/2211] Batch: 0.0998 (0.1212) Data: 0.0021 (0.0055) Loss: 0.1588 (0.0913)
[2023/01/16 00:44] | TRAIN(019): [ 400/2211] Batch: 0.1005 (0.1197) Data: 0.0019 (0.0051) Loss: 0.1936 (0.0947)
[2023/01/16 00:44] | TRAIN(019): [ 450/2211] Batch: 0.1126 (0.1186) Data: 0.0019 (0.0047) Loss: 0.0000 (0.0947)
[2023/01/16 00:44] | TRAIN(019): [ 500/2211] Batch: 0.0890 (0.1168) Data: 0.0021 (0.0045) Loss: 0.3815 (0.0954)
[2023/01/16 00:44] | TRAIN(019): [ 550/2211] Batch: 0.0937 (0.1173) Data: 0.0018 (0.0043) Loss: 0.0370 (0.0928)
[2023/01/16 00:44] | TRAIN(019): [ 600/2211] Batch: 0.1046 (0.1167) Data: 0.0020 (0.0041) Loss: 0.1467 (0.0943)
[2023/01/16 00:44] | TRAIN(019): [ 650/2211] Batch: 0.1240 (0.1158) Data: 0.0023 (0.0039) Loss: 0.3245 (0.0939)
[2023/01/16 00:45] | TRAIN(019): [ 700/2211] Batch: 0.1154 (0.1156) Data: 0.0019 (0.0038) Loss: 0.0414 (0.0920)
[2023/01/16 00:45] | TRAIN(019): [ 750/2211] Batch: 0.0982 (0.1156) Data: 0.0020 (0.0037) Loss: 0.1006 (0.0918)
[2023/01/16 00:45] | TRAIN(019): [ 800/2211] Batch: 0.1022 (0.1158) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0919)
[2023/01/16 00:45] | TRAIN(019): [ 850/2211] Batch: 0.1067 (0.1153) Data: 0.0020 (0.0035) Loss: 0.0000 (0.0896)
[2023/01/16 00:45] | TRAIN(019): [ 900/2211] Batch: 0.1038 (0.1149) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0890)
[2023/01/16 00:45] | TRAIN(019): [ 950/2211] Batch: 0.1319 (0.1146) Data: 0.0020 (0.0033) Loss: 0.0000 (0.0885)
[2023/01/16 00:45] | TRAIN(019): [1000/2211] Batch: 0.0962 (0.1149) Data: 0.0020 (0.0032) Loss: 0.0520 (0.0883)
[2023/01/16 00:45] | TRAIN(019): [1050/2211] Batch: 0.1000 (0.1143) Data: 0.0016 (0.0032) Loss: 0.0675 (0.0872)
[2023/01/16 00:45] | TRAIN(019): [1100/2211] Batch: 0.0974 (0.1140) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0872)
[2023/01/16 00:45] | TRAIN(019): [1150/2211] Batch: 0.1213 (0.1139) Data: 0.0019 (0.0031) Loss: 0.1333 (0.0868)
[2023/01/16 00:45] | TRAIN(019): [1200/2211] Batch: 0.1015 (0.1135) Data: 0.0019 (0.0030) Loss: 0.2729 (0.0878)
[2023/01/16 00:46] | TRAIN(019): [1250/2211] Batch: 0.1033 (0.1131) Data: 0.0019 (0.0030) Loss: 0.2010 (0.0872)
[2023/01/16 00:46] | TRAIN(019): [1300/2211] Batch: 0.0968 (0.1127) Data: 0.0015 (0.0029) Loss: 0.0000 (0.0869)
[2023/01/16 00:46] | TRAIN(019): [1350/2211] Batch: 0.0979 (0.1124) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0869)
[2023/01/16 00:46] | TRAIN(019): [1400/2211] Batch: 0.1008 (0.1120) Data: 0.0019 (0.0028) Loss: 0.0002 (0.0865)
[2023/01/16 00:46] | TRAIN(019): [1450/2211] Batch: 0.1002 (0.1117) Data: 0.0020 (0.0028) Loss: 0.0001 (0.0860)
[2023/01/16 00:46] | TRAIN(019): [1500/2211] Batch: 0.0980 (0.1115) Data: 0.0019 (0.0028) Loss: 0.1058 (0.0853)
[2023/01/16 00:46] | TRAIN(019): [1550/2211] Batch: 0.1053 (0.1112) Data: 0.0018 (0.0028) Loss: 0.2324 (0.0847)
[2023/01/16 00:46] | TRAIN(019): [1600/2211] Batch: 0.1072 (0.1111) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0858)
[2023/01/16 00:46] | TRAIN(019): [1650/2211] Batch: 0.1010 (0.1109) Data: 0.0019 (0.0027) Loss: 0.1696 (0.0860)
[2023/01/16 00:46] | TRAIN(019): [1700/2211] Batch: 0.1039 (0.1108) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0868)
[2023/01/16 00:46] | TRAIN(019): [1750/2211] Batch: 0.1054 (0.1105) Data: 0.0023 (0.0027) Loss: 0.2302 (0.0867)
[2023/01/16 00:47] | TRAIN(019): [1800/2211] Batch: 0.1116 (0.1104) Data: 0.0019 (0.0026) Loss: 0.1691 (0.0867)
[2023/01/16 00:47] | TRAIN(019): [1850/2211] Batch: 0.1040 (0.1103) Data: 0.0014 (0.0026) Loss: 0.0001 (0.0870)
[2023/01/16 00:47] | TRAIN(019): [1900/2211] Batch: 0.0983 (0.1101) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0872)
[2023/01/16 00:47] | TRAIN(019): [1950/2211] Batch: 0.0971 (0.1099) Data: 0.0018 (0.0026) Loss: 0.1275 (0.0870)
[2023/01/16 00:47] | TRAIN(019): [2000/2211] Batch: 0.1006 (0.1097) Data: 0.0018 (0.0026) Loss: 0.0965 (0.0869)
[2023/01/16 00:47] | TRAIN(019): [2050/2211] Batch: 0.0971 (0.1095) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0868)
[2023/01/16 00:47] | TRAIN(019): [2100/2211] Batch: 0.1096 (0.1094) Data: 0.0021 (0.0025) Loss: 0.0000 (0.0867)
[2023/01/16 00:47] | TRAIN(019): [2150/2211] Batch: 0.0968 (0.1092) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0865)
[2023/01/16 00:47] | TRAIN(019): [2200/2211] Batch: 0.0970 (0.1090) Data: 0.0020 (0.0025) Loss: 0.0031 (0.0864)
[2023/01/16 00:47] | ------------------------------------------------------------
[2023/01/16 00:47] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:47] | ------------------------------------------------------------
[2023/01/16 00:47] |    TRAIN(19)     0:04:00     0:00:05     0:03:55      0.0866
[2023/01/16 00:47] | ------------------------------------------------------------
[2023/01/16 00:47] | **************************************************
[2023/01/16 00:47] | TRAIN(020): [  50/2211] Batch: 0.1296 (0.1477) Data: 0.0022 (0.0251) Loss: 0.1667 (0.0948)
[2023/01/16 00:47] | TRAIN(020): [ 100/2211] Batch: 0.0958 (0.1254) Data: 0.0018 (0.0136) Loss: 0.0000 (0.0922)
[2023/01/16 00:48] | TRAIN(020): [ 150/2211] Batch: 0.0954 (0.1170) Data: 0.0019 (0.0098) Loss: 0.0000 (0.0782)
[2023/01/16 00:48] | TRAIN(020): [ 200/2211] Batch: 0.0978 (0.1142) Data: 0.0019 (0.0078) Loss: 0.0000 (0.0737)
[2023/01/16 00:48] | TRAIN(020): [ 250/2211] Batch: 0.0996 (0.1114) Data: 0.0018 (0.0066) Loss: 0.0000 (0.0791)
[2023/01/16 00:48] | TRAIN(020): [ 300/2211] Batch: 0.0986 (0.1094) Data: 0.0019 (0.0058) Loss: 0.2517 (0.0801)
[2023/01/16 00:48] | TRAIN(020): [ 350/2211] Batch: 0.1085 (0.1101) Data: 0.0019 (0.0053) Loss: 0.1895 (0.0761)
[2023/01/16 00:48] | TRAIN(020): [ 400/2211] Batch: 0.1077 (0.1096) Data: 0.0019 (0.0049) Loss: 0.1000 (0.0789)
[2023/01/16 00:48] | TRAIN(020): [ 450/2211] Batch: 0.0964 (0.1086) Data: 0.0018 (0.0046) Loss: 0.3032 (0.0760)
[2023/01/16 00:48] | TRAIN(020): [ 500/2211] Batch: 0.0982 (0.1086) Data: 0.0020 (0.0043) Loss: 0.0000 (0.0751)
[2023/01/16 00:48] | TRAIN(020): [ 550/2211] Batch: 0.0977 (0.1080) Data: 0.0020 (0.0041) Loss: 0.1453 (0.0767)
[2023/01/16 00:48] | TRAIN(020): [ 600/2211] Batch: 0.0985 (0.1076) Data: 0.0019 (0.0039) Loss: 0.1549 (0.0758)
[2023/01/16 00:48] | TRAIN(020): [ 650/2211] Batch: 0.1115 (0.1073) Data: 0.0017 (0.0037) Loss: 0.1021 (0.0776)
[2023/01/16 00:48] | TRAIN(020): [ 700/2211] Batch: 0.1282 (0.1078) Data: 0.0021 (0.0036) Loss: 0.0000 (0.0781)
[2023/01/16 00:49] | TRAIN(020): [ 750/2211] Batch: 0.0993 (0.1076) Data: 0.0019 (0.0035) Loss: 0.2107 (0.0775)
[2023/01/16 00:49] | TRAIN(020): [ 800/2211] Batch: 0.0957 (0.1074) Data: 0.0018 (0.0034) Loss: 0.1218 (0.0778)
[2023/01/16 00:49] | TRAIN(020): [ 850/2211] Batch: 0.1025 (0.1071) Data: 0.0021 (0.0033) Loss: 0.0000 (0.0777)
[2023/01/16 00:49] | TRAIN(020): [ 900/2211] Batch: 0.1016 (0.1068) Data: 0.0018 (0.0032) Loss: 0.1062 (0.0771)
[2023/01/16 00:49] | TRAIN(020): [ 950/2211] Batch: 0.0941 (0.1069) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0763)
[2023/01/16 00:49] | TRAIN(020): [1000/2211] Batch: 0.1264 (0.1073) Data: 0.0026 (0.0031) Loss: 0.1446 (0.0771)
[2023/01/16 00:49] | TRAIN(020): [1050/2211] Batch: 0.1260 (0.1077) Data: 0.0023 (0.0031) Loss: 0.0000 (0.0760)
[2023/01/16 00:49] | TRAIN(020): [1100/2211] Batch: 0.1072 (0.1079) Data: 0.0020 (0.0030) Loss: 0.1350 (0.0767)
[2023/01/16 00:49] | TRAIN(020): [1150/2211] Batch: 0.1001 (0.1079) Data: 0.0019 (0.0030) Loss: 0.0481 (0.0778)
[2023/01/16 00:49] | TRAIN(020): [1200/2211] Batch: 0.1060 (0.1076) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0790)
[2023/01/16 00:49] | TRAIN(020): [1250/2211] Batch: 0.1010 (0.1074) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0795)
[2023/01/16 00:50] | TRAIN(020): [1300/2211] Batch: 0.1108 (0.1075) Data: 0.0021 (0.0029) Loss: 0.1226 (0.0795)
[2023/01/16 00:50] | TRAIN(020): [1350/2211] Batch: 0.0981 (0.1075) Data: 0.0018 (0.0028) Loss: 0.0711 (0.0795)
[2023/01/16 00:50] | TRAIN(020): [1400/2211] Batch: 0.1089 (0.1077) Data: 0.0021 (0.0028) Loss: 0.0026 (0.0790)
[2023/01/16 00:50] | TRAIN(020): [1450/2211] Batch: 0.1069 (0.1076) Data: 0.0019 (0.0028) Loss: 0.0283 (0.0787)
[2023/01/16 00:50] | TRAIN(020): [1500/2211] Batch: 0.0956 (0.1075) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0785)
[2023/01/16 00:50] | TRAIN(020): [1550/2211] Batch: 0.1031 (0.1073) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0783)
[2023/01/16 00:50] | TRAIN(020): [1600/2211] Batch: 0.1026 (0.1073) Data: 0.0018 (0.0027) Loss: 0.0908 (0.0787)
[2023/01/16 00:50] | TRAIN(020): [1650/2211] Batch: 0.1249 (0.1073) Data: 0.0022 (0.0027) Loss: 0.0001 (0.0788)
[2023/01/16 00:50] | TRAIN(020): [1700/2211] Batch: 0.0961 (0.1072) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0790)
[2023/01/16 00:50] | TRAIN(020): [1750/2211] Batch: 0.1031 (0.1071) Data: 0.0020 (0.0026) Loss: 0.0300 (0.0787)
[2023/01/16 00:50] | TRAIN(020): [1800/2211] Batch: 0.1104 (0.1071) Data: 0.0018 (0.0026) Loss: 0.2717 (0.0788)
[2023/01/16 00:51] | TRAIN(020): [1850/2211] Batch: 0.1063 (0.1070) Data: 0.0018 (0.0026) Loss: 0.0794 (0.0789)
[2023/01/16 00:51] | TRAIN(020): [1900/2211] Batch: 0.0973 (0.1070) Data: 0.0021 (0.0026) Loss: 0.1060 (0.0792)
[2023/01/16 00:51] | TRAIN(020): [1950/2211] Batch: 0.0958 (0.1068) Data: 0.0021 (0.0026) Loss: 0.1261 (0.0796)
[2023/01/16 00:51] | TRAIN(020): [2000/2211] Batch: 0.1391 (0.1067) Data: 0.0018 (0.0025) Loss: 0.1728 (0.0796)
[2023/01/16 00:51] | TRAIN(020): [2050/2211] Batch: 0.1042 (0.1069) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0800)
[2023/01/16 00:51] | TRAIN(020): [2100/2211] Batch: 0.0960 (0.1068) Data: 0.0020 (0.0025) Loss: 0.1247 (0.0797)
[2023/01/16 00:51] | TRAIN(020): [2150/2211] Batch: 0.1059 (0.1067) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0796)
[2023/01/16 00:51] | TRAIN(020): [2200/2211] Batch: 0.0895 (0.1066) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0796)
[2023/01/16 00:51] | ------------------------------------------------------------
[2023/01/16 00:51] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:51] | ------------------------------------------------------------
[2023/01/16 00:51] |    TRAIN(20)     0:03:55     0:00:05     0:03:50      0.0796
[2023/01/16 00:51] | ------------------------------------------------------------
[2023/01/16 00:51] | **************************************************
[2023/01/16 00:51] | TRAIN(021): [  50/2211] Batch: 0.0980 (0.1323) Data: 0.0019 (0.0256) Loss: 0.0000 (0.0574)
[2023/01/16 00:51] | TRAIN(021): [ 100/2211] Batch: 0.1061 (0.1260) Data: 0.0021 (0.0140) Loss: 0.1252 (0.0566)
[2023/01/16 00:51] | TRAIN(021): [ 150/2211] Batch: 0.0983 (0.1196) Data: 0.0020 (0.0100) Loss: 0.0035 (0.0567)
[2023/01/16 00:52] | TRAIN(021): [ 200/2211] Batch: 0.1180 (0.1167) Data: 0.0020 (0.0080) Loss: 0.0000 (0.0618)
[2023/01/16 00:52] | TRAIN(021): [ 250/2211] Batch: 0.1481 (0.1177) Data: 0.0019 (0.0068) Loss: 0.0785 (0.0674)
[2023/01/16 00:52] | TRAIN(021): [ 300/2211] Batch: 0.1142 (0.1176) Data: 0.0021 (0.0061) Loss: 0.0000 (0.0660)
[2023/01/16 00:52] | TRAIN(021): [ 350/2211] Batch: 0.1038 (0.1163) Data: 0.0021 (0.0055) Loss: 0.0000 (0.0715)
[2023/01/16 00:52] | TRAIN(021): [ 400/2211] Batch: 0.1051 (0.1155) Data: 0.0019 (0.0051) Loss: 0.0449 (0.0738)
[2023/01/16 00:52] | TRAIN(021): [ 450/2211] Batch: 0.1104 (0.1159) Data: 0.0020 (0.0047) Loss: 0.0504 (0.0738)
[2023/01/16 00:52] | TRAIN(021): [ 500/2211] Batch: 0.1067 (0.1146) Data: 0.0019 (0.0044) Loss: 0.1832 (0.0752)
[2023/01/16 00:52] | TRAIN(021): [ 550/2211] Batch: 0.1243 (0.1140) Data: 0.0021 (0.0042) Loss: 0.3172 (0.0775)
[2023/01/16 00:52] | TRAIN(021): [ 600/2211] Batch: 0.1042 (0.1134) Data: 0.0019 (0.0040) Loss: 0.0337 (0.0749)
[2023/01/16 00:52] | TRAIN(021): [ 650/2211] Batch: 0.1048 (0.1128) Data: 0.0021 (0.0039) Loss: 0.0308 (0.0760)
[2023/01/16 00:52] | TRAIN(021): [ 700/2211] Batch: 0.1141 (0.1123) Data: 0.0021 (0.0037) Loss: 0.1814 (0.0766)
[2023/01/16 00:53] | TRAIN(021): [ 750/2211] Batch: 0.1069 (0.1119) Data: 0.0018 (0.0036) Loss: 0.0216 (0.0788)
[2023/01/16 00:53] | TRAIN(021): [ 800/2211] Batch: 0.1075 (0.1117) Data: 0.0019 (0.0035) Loss: 0.0000 (0.0779)
[2023/01/16 00:53] | TRAIN(021): [ 850/2211] Batch: 0.1141 (0.1115) Data: 0.0020 (0.0034) Loss: 0.1194 (0.0775)
[2023/01/16 00:53] | TRAIN(021): [ 900/2211] Batch: 0.1243 (0.1114) Data: 0.0019 (0.0034) Loss: 0.1652 (0.0771)
[2023/01/16 00:53] | TRAIN(021): [ 950/2211] Batch: 0.1037 (0.1115) Data: 0.0019 (0.0033) Loss: 0.0316 (0.0776)
[2023/01/16 00:53] | TRAIN(021): [1000/2211] Batch: 0.0998 (0.1112) Data: 0.0016 (0.0032) Loss: 0.0000 (0.0759)
[2023/01/16 00:53] | TRAIN(021): [1050/2211] Batch: 0.1102 (0.1112) Data: 0.0018 (0.0032) Loss: 0.0001 (0.0752)
[2023/01/16 00:53] | TRAIN(021): [1100/2211] Batch: 0.0994 (0.1108) Data: 0.0018 (0.0031) Loss: 0.1335 (0.0758)
[2023/01/16 00:53] | TRAIN(021): [1150/2211] Batch: 0.1282 (0.1105) Data: 0.0022 (0.0030) Loss: 0.1518 (0.0754)
[2023/01/16 00:53] | TRAIN(021): [1200/2211] Batch: 0.1021 (0.1103) Data: 0.0019 (0.0030) Loss: 0.0837 (0.0765)
[2023/01/16 00:53] | TRAIN(021): [1250/2211] Batch: 0.1034 (0.1102) Data: 0.0018 (0.0030) Loss: 0.0999 (0.0775)
[2023/01/16 00:54] | TRAIN(021): [1300/2211] Batch: 0.1015 (0.1099) Data: 0.0022 (0.0029) Loss: 0.0000 (0.0776)
[2023/01/16 00:54] | TRAIN(021): [1350/2211] Batch: 0.0987 (0.1096) Data: 0.0022 (0.0029) Loss: 0.0000 (0.0776)
[2023/01/16 00:54] | TRAIN(021): [1400/2211] Batch: 0.1080 (0.1097) Data: 0.0022 (0.0029) Loss: 0.0000 (0.0774)
[2023/01/16 00:54] | TRAIN(021): [1450/2211] Batch: 0.1206 (0.1096) Data: 0.0020 (0.0028) Loss: 0.0000 (0.0774)
[2023/01/16 00:54] | TRAIN(021): [1500/2211] Batch: 0.1162 (0.1100) Data: 0.0021 (0.0028) Loss: 0.0000 (0.0772)
[2023/01/16 00:54] | TRAIN(021): [1550/2211] Batch: 0.1098 (0.1100) Data: 0.0021 (0.0028) Loss: 0.1413 (0.0774)
[2023/01/16 00:54] | TRAIN(021): [1600/2211] Batch: 0.0993 (0.1099) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0768)
[2023/01/16 00:54] | TRAIN(021): [1650/2211] Batch: 0.0977 (0.1099) Data: 0.0019 (0.0027) Loss: 0.5267 (0.0764)
[2023/01/16 00:54] | TRAIN(021): [1700/2211] Batch: 0.0975 (0.1096) Data: 0.0015 (0.0027) Loss: 0.0000 (0.0764)
[2023/01/16 00:54] | TRAIN(021): [1750/2211] Batch: 0.0997 (0.1095) Data: 0.0017 (0.0027) Loss: 0.0419 (0.0770)
[2023/01/16 00:54] | TRAIN(021): [1800/2211] Batch: 0.0982 (0.1094) Data: 0.0019 (0.0027) Loss: 0.1431 (0.0773)
[2023/01/16 00:55] | TRAIN(021): [1850/2211] Batch: 0.0976 (0.1093) Data: 0.0020 (0.0027) Loss: 0.0272 (0.0776)
[2023/01/16 00:55] | TRAIN(021): [1900/2211] Batch: 0.1210 (0.1095) Data: 0.0019 (0.0026) Loss: 0.0040 (0.0771)
[2023/01/16 00:55] | TRAIN(021): [1950/2211] Batch: 0.1198 (0.1093) Data: 0.0019 (0.0026) Loss: 0.1153 (0.0770)
[2023/01/16 00:55] | TRAIN(021): [2000/2211] Batch: 0.1185 (0.1093) Data: 0.0020 (0.0026) Loss: 0.3298 (0.0768)
[2023/01/16 00:55] | TRAIN(021): [2050/2211] Batch: 0.1033 (0.1092) Data: 0.0019 (0.0026) Loss: 0.0761 (0.0770)
[2023/01/16 00:55] | TRAIN(021): [2100/2211] Batch: 0.1295 (0.1093) Data: 0.0023 (0.0026) Loss: 0.0174 (0.0769)
[2023/01/16 00:55] | TRAIN(021): [2150/2211] Batch: 0.1359 (0.1095) Data: 0.0023 (0.0026) Loss: 0.1727 (0.0770)
[2023/01/16 00:55] | TRAIN(021): [2200/2211] Batch: 0.1030 (0.1098) Data: 0.0018 (0.0026) Loss: 0.0003 (0.0776)
[2023/01/16 00:55] | ------------------------------------------------------------
[2023/01/16 00:55] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:55] | ------------------------------------------------------------
[2023/01/16 00:55] |    TRAIN(21)     0:04:02     0:00:05     0:03:57      0.0776
[2023/01/16 00:55] | ------------------------------------------------------------
[2023/01/16 00:55] | **************************************************
[2023/01/16 00:55] | TRAIN(022): [  50/2211] Batch: 0.1062 (0.1341) Data: 0.0020 (0.0271) Loss: 0.1525 (0.0761)
[2023/01/16 00:55] | TRAIN(022): [ 100/2211] Batch: 0.1019 (0.1232) Data: 0.0020 (0.0147) Loss: 0.0000 (0.0714)
[2023/01/16 00:56] | TRAIN(022): [ 150/2211] Batch: 0.1274 (0.1191) Data: 0.0022 (0.0105) Loss: 0.0000 (0.0759)
[2023/01/16 00:56] | TRAIN(022): [ 200/2211] Batch: 0.0956 (0.1165) Data: 0.0022 (0.0084) Loss: 0.0001 (0.0817)
[2023/01/16 00:56] | TRAIN(022): [ 250/2211] Batch: 0.0977 (0.1135) Data: 0.0018 (0.0071) Loss: 0.1833 (0.0843)
[2023/01/16 00:56] | TRAIN(022): [ 300/2211] Batch: 0.1074 (0.1131) Data: 0.0020 (0.0063) Loss: 0.0000 (0.0827)
[2023/01/16 00:56] | TRAIN(022): [ 350/2211] Batch: 0.1180 (0.1124) Data: 0.0019 (0.0057) Loss: 0.0000 (0.0824)
[2023/01/16 00:56] | TRAIN(022): [ 400/2211] Batch: 0.1513 (0.1115) Data: 0.0023 (0.0052) Loss: 0.0745 (0.0817)
[2023/01/16 00:56] | TRAIN(022): [ 450/2211] Batch: 0.1065 (0.1109) Data: 0.0019 (0.0048) Loss: 0.0000 (0.0803)
[2023/01/16 00:56] | TRAIN(022): [ 500/2211] Batch: 0.0981 (0.1101) Data: 0.0016 (0.0045) Loss: 0.0025 (0.0818)
[2023/01/16 00:56] | TRAIN(022): [ 550/2211] Batch: 0.1023 (0.1095) Data: 0.0019 (0.0043) Loss: 0.0000 (0.0802)
[2023/01/16 00:56] | TRAIN(022): [ 600/2211] Batch: 0.1097 (0.1091) Data: 0.0020 (0.0041) Loss: 0.0000 (0.0821)
[2023/01/16 00:56] | TRAIN(022): [ 650/2211] Batch: 0.1053 (0.1090) Data: 0.0020 (0.0039) Loss: 0.3505 (0.0827)
[2023/01/16 00:56] | TRAIN(022): [ 700/2211] Batch: 0.1072 (0.1085) Data: 0.0019 (0.0038) Loss: 0.0000 (0.0835)
[2023/01/16 00:57] | TRAIN(022): [ 750/2211] Batch: 0.1003 (0.1085) Data: 0.0020 (0.0037) Loss: 0.0000 (0.0813)
[2023/01/16 00:57] | TRAIN(022): [ 800/2211] Batch: 0.1077 (0.1093) Data: 0.0022 (0.0036) Loss: 0.1690 (0.0794)
[2023/01/16 00:57] | TRAIN(022): [ 850/2211] Batch: 0.0994 (0.1089) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0780)
[2023/01/16 00:57] | TRAIN(022): [ 900/2211] Batch: 0.0954 (0.1087) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0771)
[2023/01/16 00:57] | TRAIN(022): [ 950/2211] Batch: 0.1267 (0.1089) Data: 0.0025 (0.0033) Loss: 0.0058 (0.0779)
[2023/01/16 00:57] | TRAIN(022): [1000/2211] Batch: 0.1020 (0.1087) Data: 0.0020 (0.0033) Loss: 0.0436 (0.0780)
[2023/01/16 00:57] | TRAIN(022): [1050/2211] Batch: 0.0978 (0.1082) Data: 0.0020 (0.0032) Loss: 0.0000 (0.0765)
[2023/01/16 00:57] | TRAIN(022): [1100/2211] Batch: 0.0955 (0.1079) Data: 0.0021 (0.0031) Loss: 0.0000 (0.0774)
[2023/01/16 00:57] | TRAIN(022): [1150/2211] Batch: 0.1006 (0.1075) Data: 0.0020 (0.0031) Loss: 0.0986 (0.0776)
[2023/01/16 00:57] | TRAIN(022): [1200/2211] Batch: 0.1022 (0.1076) Data: 0.0018 (0.0030) Loss: 0.0577 (0.0773)
[2023/01/16 00:57] | TRAIN(022): [1250/2211] Batch: 0.0956 (0.1074) Data: 0.0019 (0.0030) Loss: 0.0001 (0.0769)
[2023/01/16 00:58] | TRAIN(022): [1300/2211] Batch: 0.0979 (0.1072) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0768)
[2023/01/16 00:58] | TRAIN(022): [1350/2211] Batch: 0.1389 (0.1078) Data: 0.0023 (0.0029) Loss: 0.0002 (0.0765)
[2023/01/16 00:58] | TRAIN(022): [1400/2211] Batch: 0.1026 (0.1081) Data: 0.0017 (0.0029) Loss: 0.1668 (0.0770)
[2023/01/16 00:58] | TRAIN(022): [1450/2211] Batch: 0.1047 (0.1079) Data: 0.0019 (0.0029) Loss: 0.0001 (0.0765)
[2023/01/16 00:58] | TRAIN(022): [1500/2211] Batch: 0.1254 (0.1077) Data: 0.0023 (0.0028) Loss: 0.0000 (0.0768)
[2023/01/16 00:58] | TRAIN(022): [1550/2211] Batch: 0.0959 (0.1079) Data: 0.0019 (0.0028) Loss: 0.1427 (0.0765)
[2023/01/16 00:58] | TRAIN(022): [1600/2211] Batch: 0.0968 (0.1077) Data: 0.0017 (0.0028) Loss: 0.0776 (0.0757)
[2023/01/16 00:58] | TRAIN(022): [1650/2211] Batch: 0.1290 (0.1076) Data: 0.0018 (0.0028) Loss: 0.0436 (0.0749)
[2023/01/16 00:58] | TRAIN(022): [1700/2211] Batch: 0.0988 (0.1075) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0741)
[2023/01/16 00:58] | TRAIN(022): [1750/2211] Batch: 0.1037 (0.1078) Data: 0.0021 (0.0027) Loss: 0.1443 (0.0747)
[2023/01/16 00:58] | TRAIN(022): [1800/2211] Batch: 0.1020 (0.1079) Data: 0.0015 (0.0027) Loss: 0.0000 (0.0749)
[2023/01/16 00:59] | TRAIN(022): [1850/2211] Batch: 0.1080 (0.1079) Data: 0.0022 (0.0027) Loss: 0.0000 (0.0743)
[2023/01/16 00:59] | TRAIN(022): [1900/2211] Batch: 0.1017 (0.1079) Data: 0.0020 (0.0027) Loss: 0.0003 (0.0742)
[2023/01/16 00:59] | TRAIN(022): [1950/2211] Batch: 0.1040 (0.1081) Data: 0.0020 (0.0027) Loss: 0.0838 (0.0741)
[2023/01/16 00:59] | TRAIN(022): [2000/2211] Batch: 0.0980 (0.1082) Data: 0.0022 (0.0026) Loss: 0.0891 (0.0747)
[2023/01/16 00:59] | TRAIN(022): [2050/2211] Batch: 0.1056 (0.1081) Data: 0.0020 (0.0026) Loss: 0.0223 (0.0742)
[2023/01/16 00:59] | TRAIN(022): [2100/2211] Batch: 0.1036 (0.1080) Data: 0.0019 (0.0026) Loss: 0.2893 (0.0749)
[2023/01/16 00:59] | TRAIN(022): [2150/2211] Batch: 0.1198 (0.1079) Data: 0.0020 (0.0026) Loss: 0.1977 (0.0750)
[2023/01/16 00:59] | TRAIN(022): [2200/2211] Batch: 0.1018 (0.1078) Data: 0.0016 (0.0026) Loss: 0.0000 (0.0751)
[2023/01/16 00:59] | ------------------------------------------------------------
[2023/01/16 00:59] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 00:59] | ------------------------------------------------------------
[2023/01/16 00:59] |    TRAIN(22)     0:03:58     0:00:05     0:03:52      0.0751
[2023/01/16 00:59] | ------------------------------------------------------------
[2023/01/16 00:59] | **************************************************
[2023/01/16 00:59] | TRAIN(023): [  50/2211] Batch: 0.1217 (0.1404) Data: 0.0019 (0.0266) Loss: 0.0166 (0.0616)
[2023/01/16 00:59] | TRAIN(023): [ 100/2211] Batch: 0.1104 (0.1252) Data: 0.0022 (0.0144) Loss: 0.0450 (0.0741)
[2023/01/16 00:59] | TRAIN(023): [ 150/2211] Batch: 0.1076 (0.1190) Data: 0.0023 (0.0103) Loss: 0.0001 (0.0815)
[2023/01/16 01:00] | TRAIN(023): [ 200/2211] Batch: 0.1080 (0.1169) Data: 0.0021 (0.0082) Loss: 0.1517 (0.0769)
[2023/01/16 01:00] | TRAIN(023): [ 250/2211] Batch: 0.1043 (0.1165) Data: 0.0022 (0.0070) Loss: 0.0542 (0.0768)
[2023/01/16 01:00] | TRAIN(023): [ 300/2211] Batch: 0.1061 (0.1154) Data: 0.0023 (0.0062) Loss: 0.0079 (0.0764)
[2023/01/16 01:00] | TRAIN(023): [ 350/2211] Batch: 0.1073 (0.1138) Data: 0.0022 (0.0056) Loss: 0.1053 (0.0733)
[2023/01/16 01:00] | TRAIN(023): [ 400/2211] Batch: 0.0974 (0.1134) Data: 0.0022 (0.0052) Loss: 0.0422 (0.0730)
[2023/01/16 01:00] | TRAIN(023): [ 450/2211] Batch: 0.1027 (0.1124) Data: 0.0019 (0.0048) Loss: 0.0089 (0.0700)
[2023/01/16 01:00] | TRAIN(023): [ 500/2211] Batch: 0.0969 (0.1115) Data: 0.0026 (0.0045) Loss: 0.1772 (0.0701)
[2023/01/16 01:00] | TRAIN(023): [ 550/2211] Batch: 0.1019 (0.1107) Data: 0.0019 (0.0043) Loss: 0.0000 (0.0711)
[2023/01/16 01:00] | TRAIN(023): [ 600/2211] Batch: 0.1085 (0.1102) Data: 0.0019 (0.0041) Loss: 0.0000 (0.0720)
[2023/01/16 01:00] | TRAIN(023): [ 650/2211] Batch: 0.1154 (0.1096) Data: 0.0019 (0.0039) Loss: 0.2835 (0.0717)
[2023/01/16 01:00] | TRAIN(023): [ 700/2211] Batch: 0.1259 (0.1094) Data: 0.0018 (0.0038) Loss: 0.1672 (0.0722)
[2023/01/16 01:01] | TRAIN(023): [ 750/2211] Batch: 0.1095 (0.1091) Data: 0.0022 (0.0037) Loss: 0.3030 (0.0750)
[2023/01/16 01:01] | TRAIN(023): [ 800/2211] Batch: 0.0963 (0.1085) Data: 0.0020 (0.0036) Loss: 0.0091 (0.0742)
[2023/01/16 01:01] | TRAIN(023): [ 850/2211] Batch: 0.0921 (0.1085) Data: 0.0019 (0.0035) Loss: 0.0000 (0.0748)
[2023/01/16 01:01] | TRAIN(023): [ 900/2211] Batch: 0.1151 (0.1088) Data: 0.0021 (0.0034) Loss: 0.1150 (0.0745)
[2023/01/16 01:01] | TRAIN(023): [ 950/2211] Batch: 0.0963 (0.1083) Data: 0.0019 (0.0033) Loss: 0.0000 (0.0766)
[2023/01/16 01:01] | TRAIN(023): [1000/2211] Batch: 0.0945 (0.1080) Data: 0.0019 (0.0032) Loss: 0.2667 (0.0762)
[2023/01/16 01:01] | TRAIN(023): [1050/2211] Batch: 0.0999 (0.1077) Data: 0.0020 (0.0032) Loss: 0.2327 (0.0764)
[2023/01/16 01:01] | TRAIN(023): [1100/2211] Batch: 0.1028 (0.1077) Data: 0.0020 (0.0031) Loss: 0.0561 (0.0751)
[2023/01/16 01:01] | TRAIN(023): [1150/2211] Batch: 0.1189 (0.1078) Data: 0.0021 (0.0031) Loss: 0.0209 (0.0749)
[2023/01/16 01:01] | TRAIN(023): [1200/2211] Batch: 0.0992 (0.1077) Data: 0.0020 (0.0030) Loss: 0.0000 (0.0754)
[2023/01/16 01:01] | TRAIN(023): [1250/2211] Batch: 0.0944 (0.1075) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0756)
[2023/01/16 01:02] | TRAIN(023): [1300/2211] Batch: 0.1220 (0.1076) Data: 0.0020 (0.0029) Loss: 0.0000 (0.0755)
[2023/01/16 01:02] | TRAIN(023): [1350/2211] Batch: 0.1029 (0.1077) Data: 0.0020 (0.0029) Loss: 0.0001 (0.0751)
[2023/01/16 01:02] | TRAIN(023): [1400/2211] Batch: 0.1151 (0.1078) Data: 0.0021 (0.0029) Loss: 0.0093 (0.0751)
[2023/01/16 01:02] | TRAIN(023): [1450/2211] Batch: 0.1020 (0.1077) Data: 0.0021 (0.0029) Loss: 0.0000 (0.0748)
[2023/01/16 01:02] | TRAIN(023): [1500/2211] Batch: 0.1022 (0.1075) Data: 0.0018 (0.0028) Loss: 0.0720 (0.0746)
[2023/01/16 01:02] | TRAIN(023): [1550/2211] Batch: 0.1066 (0.1078) Data: 0.0021 (0.0028) Loss: 0.0000 (0.0745)
[2023/01/16 01:02] | TRAIN(023): [1600/2211] Batch: 0.1206 (0.1079) Data: 0.0022 (0.0028) Loss: 0.1097 (0.0741)
[2023/01/16 01:02] | TRAIN(023): [1650/2211] Batch: 0.1071 (0.1079) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0745)
[2023/01/16 01:02] | TRAIN(023): [1700/2211] Batch: 0.1106 (0.1080) Data: 0.0015 (0.0027) Loss: 0.0000 (0.0752)
[2023/01/16 01:02] | TRAIN(023): [1750/2211] Batch: 0.1213 (0.1080) Data: 0.0022 (0.0027) Loss: 0.0181 (0.0756)
[2023/01/16 01:02] | TRAIN(023): [1800/2211] Batch: 0.1007 (0.1085) Data: 0.0016 (0.0027) Loss: 0.0564 (0.0754)
[2023/01/16 01:03] | TRAIN(023): [1850/2211] Batch: 0.1009 (0.1088) Data: 0.0019 (0.0027) Loss: 0.1876 (0.0757)
[2023/01/16 01:03] | TRAIN(023): [1900/2211] Batch: 0.1201 (0.1086) Data: 0.0016 (0.0027) Loss: 0.0508 (0.0755)
[2023/01/16 01:03] | TRAIN(023): [1950/2211] Batch: 0.1196 (0.1085) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0749)
[2023/01/16 01:03] | TRAIN(023): [2000/2211] Batch: 0.1311 (0.1084) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0753)
[2023/01/16 01:03] | TRAIN(023): [2050/2211] Batch: 0.1179 (0.1085) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0757)
[2023/01/16 01:03] | TRAIN(023): [2100/2211] Batch: 0.1022 (0.1085) Data: 0.0021 (0.0026) Loss: 0.0000 (0.0759)
[2023/01/16 01:03] | TRAIN(023): [2150/2211] Batch: 0.1019 (0.1085) Data: 0.0019 (0.0026) Loss: 0.0248 (0.0757)
[2023/01/16 01:03] | TRAIN(023): [2200/2211] Batch: 0.0906 (0.1085) Data: 0.0021 (0.0026) Loss: 0.1942 (0.0760)
[2023/01/16 01:03] | ------------------------------------------------------------
[2023/01/16 01:03] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:03] | ------------------------------------------------------------
[2023/01/16 01:03] |    TRAIN(23)     0:03:59     0:00:05     0:03:54      0.0763
[2023/01/16 01:03] | ------------------------------------------------------------
[2023/01/16 01:03] | **************************************************
[2023/01/16 01:03] | TRAIN(024): [  50/2211] Batch: 0.0969 (0.1402) Data: 0.0019 (0.0270) Loss: 0.0000 (0.0857)
[2023/01/16 01:03] | TRAIN(024): [ 100/2211] Batch: 0.0985 (0.1287) Data: 0.0018 (0.0147) Loss: 0.0655 (0.0973)
[2023/01/16 01:03] | TRAIN(024): [ 150/2211] Batch: 0.1007 (0.1214) Data: 0.0024 (0.0105) Loss: 0.0611 (0.0927)
[2023/01/16 01:04] | TRAIN(024): [ 200/2211] Batch: 0.0950 (0.1151) Data: 0.0023 (0.0084) Loss: 0.1342 (0.0891)
[2023/01/16 01:04] | TRAIN(024): [ 250/2211] Batch: 0.0918 (0.1133) Data: 0.0020 (0.0071) Loss: 0.0000 (0.0841)
[2023/01/16 01:04] | TRAIN(024): [ 300/2211] Batch: 0.0932 (0.1111) Data: 0.0020 (0.0063) Loss: 0.0238 (0.0799)
[2023/01/16 01:04] | TRAIN(024): [ 350/2211] Batch: 0.0928 (0.1098) Data: 0.0018 (0.0057) Loss: 0.0001 (0.0785)
[2023/01/16 01:04] | TRAIN(024): [ 400/2211] Batch: 0.0906 (0.1083) Data: 0.0022 (0.0052) Loss: 0.0343 (0.0751)
[2023/01/16 01:04] | TRAIN(024): [ 450/2211] Batch: 0.1137 (0.1075) Data: 0.0018 (0.0048) Loss: 0.0147 (0.0752)
[2023/01/16 01:04] | TRAIN(024): [ 500/2211] Batch: 0.0889 (0.1067) Data: 0.0016 (0.0045) Loss: 0.1925 (0.0760)
[2023/01/16 01:04] | TRAIN(024): [ 550/2211] Batch: 0.0998 (0.1060) Data: 0.0019 (0.0043) Loss: 0.0000 (0.0755)
[2023/01/16 01:04] | TRAIN(024): [ 600/2211] Batch: 0.0911 (0.1052) Data: 0.0018 (0.0041) Loss: 0.0918 (0.0759)
[2023/01/16 01:04] | TRAIN(024): [ 650/2211] Batch: 0.1182 (0.1054) Data: 0.0024 (0.0039) Loss: 0.2462 (0.0750)
[2023/01/16 01:04] | TRAIN(024): [ 700/2211] Batch: 0.1019 (0.1052) Data: 0.0021 (0.0038) Loss: 0.0139 (0.0725)
[2023/01/16 01:05] | TRAIN(024): [ 750/2211] Batch: 0.0943 (0.1052) Data: 0.0020 (0.0037) Loss: 0.0000 (0.0720)
[2023/01/16 01:05] | TRAIN(024): [ 800/2211] Batch: 0.0966 (0.1048) Data: 0.0020 (0.0036) Loss: 0.2812 (0.0728)
[2023/01/16 01:05] | TRAIN(024): [ 850/2211] Batch: 0.0961 (0.1046) Data: 0.0019 (0.0035) Loss: 0.0000 (0.0729)
[2023/01/16 01:05] | TRAIN(024): [ 900/2211] Batch: 0.0956 (0.1042) Data: 0.0017 (0.0034) Loss: 0.1056 (0.0729)
[2023/01/16 01:05] | TRAIN(024): [ 950/2211] Batch: 0.1102 (0.1044) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0720)
[2023/01/16 01:05] | TRAIN(024): [1000/2211] Batch: 0.1060 (0.1047) Data: 0.0021 (0.0032) Loss: 0.1195 (0.0726)
[2023/01/16 01:05] | TRAIN(024): [1050/2211] Batch: 0.1035 (0.1046) Data: 0.0022 (0.0032) Loss: 0.0001 (0.0728)
[2023/01/16 01:05] | TRAIN(024): [1100/2211] Batch: 0.0980 (0.1044) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0727)
[2023/01/16 01:05] | TRAIN(024): [1150/2211] Batch: 0.1273 (0.1042) Data: 0.0023 (0.0031) Loss: 0.0018 (0.0732)
[2023/01/16 01:05] | TRAIN(024): [1200/2211] Batch: 0.0905 (0.1044) Data: 0.0019 (0.0030) Loss: 0.1389 (0.0732)
[2023/01/16 01:05] | TRAIN(024): [1250/2211] Batch: 0.1046 (0.1042) Data: 0.0020 (0.0030) Loss: 0.2678 (0.0729)
[2023/01/16 01:05] | TRAIN(024): [1300/2211] Batch: 0.0969 (0.1042) Data: 0.0020 (0.0030) Loss: 0.1315 (0.0730)
[2023/01/16 01:06] | TRAIN(024): [1350/2211] Batch: 0.0977 (0.1040) Data: 0.0021 (0.0029) Loss: 0.0329 (0.0735)
[2023/01/16 01:06] | TRAIN(024): [1400/2211] Batch: 0.0994 (0.1039) Data: 0.0020 (0.0029) Loss: 0.0208 (0.0739)
[2023/01/16 01:06] | TRAIN(024): [1450/2211] Batch: 0.0949 (0.1040) Data: 0.0020 (0.0028) Loss: 0.4770 (0.0746)
[2023/01/16 01:06] | TRAIN(024): [1500/2211] Batch: 0.0965 (0.1039) Data: 0.0019 (0.0028) Loss: 0.0017 (0.0753)
[2023/01/16 01:06] | TRAIN(024): [1550/2211] Batch: 0.1053 (0.1038) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0760)
[2023/01/16 01:06] | TRAIN(024): [1600/2211] Batch: 0.1202 (0.1037) Data: 0.0021 (0.0028) Loss: 0.3389 (0.0755)
[2023/01/16 01:06] | TRAIN(024): [1650/2211] Batch: 0.0940 (0.1036) Data: 0.0019 (0.0027) Loss: 0.0977 (0.0752)
[2023/01/16 01:06] | TRAIN(024): [1700/2211] Batch: 0.1249 (0.1037) Data: 0.0023 (0.0027) Loss: 0.0000 (0.0749)
[2023/01/16 01:06] | TRAIN(024): [1750/2211] Batch: 0.1085 (0.1040) Data: 0.0019 (0.0027) Loss: 0.0404 (0.0752)
[2023/01/16 01:06] | TRAIN(024): [1800/2211] Batch: 0.1119 (0.1039) Data: 0.0020 (0.0027) Loss: 0.0035 (0.0749)
[2023/01/16 01:06] | TRAIN(024): [1850/2211] Batch: 0.0948 (0.1038) Data: 0.0020 (0.0027) Loss: 0.0612 (0.0754)
[2023/01/16 01:06] | TRAIN(024): [1900/2211] Batch: 0.1127 (0.1037) Data: 0.0020 (0.0026) Loss: 0.0449 (0.0751)
[2023/01/16 01:07] | TRAIN(024): [1950/2211] Batch: 0.0983 (0.1036) Data: 0.0016 (0.0026) Loss: 0.2012 (0.0753)
[2023/01/16 01:07] | TRAIN(024): [2000/2211] Batch: 0.0965 (0.1035) Data: 0.0020 (0.0026) Loss: 0.0814 (0.0758)
[2023/01/16 01:07] | TRAIN(024): [2050/2211] Batch: 0.0996 (0.1034) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0748)
[2023/01/16 01:07] | TRAIN(024): [2100/2211] Batch: 0.0921 (0.1034) Data: 0.0019 (0.0026) Loss: 0.4162 (0.0746)
[2023/01/16 01:07] | TRAIN(024): [2150/2211] Batch: 0.1240 (0.1035) Data: 0.0020 (0.0025) Loss: 0.0775 (0.0751)
[2023/01/16 01:07] | TRAIN(024): [2200/2211] Batch: 0.0887 (0.1035) Data: 0.0016 (0.0025) Loss: 0.0017 (0.0746)
[2023/01/16 01:07] | ------------------------------------------------------------
[2023/01/16 01:07] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:07] | ------------------------------------------------------------
[2023/01/16 01:07] |    TRAIN(24)     0:03:48     0:00:05     0:03:43      0.0744
[2023/01/16 01:07] | ------------------------------------------------------------
[2023/01/16 01:07] | **************************************************
[2023/01/16 01:07] | TRAIN(025): [  50/2211] Batch: 0.0940 (0.1255) Data: 0.0022 (0.0254) Loss: 0.1426 (0.0909)
[2023/01/16 01:07] | TRAIN(025): [ 100/2211] Batch: 0.0922 (0.1109) Data: 0.0019 (0.0138) Loss: 0.0000 (0.0871)
[2023/01/16 01:07] | TRAIN(025): [ 150/2211] Batch: 0.0908 (0.1059) Data: 0.0018 (0.0099) Loss: 0.0561 (0.0825)
[2023/01/16 01:07] | TRAIN(025): [ 200/2211] Batch: 0.0957 (0.1035) Data: 0.0020 (0.0079) Loss: 0.0201 (0.0813)
[2023/01/16 01:07] | TRAIN(025): [ 250/2211] Batch: 0.0994 (0.1030) Data: 0.0018 (0.0067) Loss: 0.0558 (0.0791)
[2023/01/16 01:08] | TRAIN(025): [ 300/2211] Batch: 0.0942 (0.1018) Data: 0.0021 (0.0059) Loss: 0.1898 (0.0773)
[2023/01/16 01:08] | TRAIN(025): [ 350/2211] Batch: 0.0855 (0.1008) Data: 0.0020 (0.0054) Loss: 0.0000 (0.0739)
[2023/01/16 01:08] | TRAIN(025): [ 400/2211] Batch: 0.1150 (0.1015) Data: 0.0019 (0.0050) Loss: 0.0000 (0.0717)
[2023/01/16 01:08] | TRAIN(025): [ 450/2211] Batch: 0.0948 (0.1014) Data: 0.0020 (0.0046) Loss: 0.0217 (0.0693)
[2023/01/16 01:08] | TRAIN(025): [ 500/2211] Batch: 0.1016 (0.1009) Data: 0.0019 (0.0044) Loss: 0.0859 (0.0725)
[2023/01/16 01:08] | TRAIN(025): [ 550/2211] Batch: 0.0931 (0.1003) Data: 0.0019 (0.0041) Loss: 0.0045 (0.0742)
[2023/01/16 01:08] | TRAIN(025): [ 600/2211] Batch: 0.1203 (0.1005) Data: 0.0023 (0.0040) Loss: 0.0000 (0.0741)
[2023/01/16 01:08] | TRAIN(025): [ 650/2211] Batch: 0.1025 (0.1002) Data: 0.0020 (0.0038) Loss: 0.0046 (0.0732)
[2023/01/16 01:08] | TRAIN(025): [ 700/2211] Batch: 0.0900 (0.0998) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0701)
[2023/01/16 01:08] | TRAIN(025): [ 750/2211] Batch: 0.0994 (0.0997) Data: 0.0019 (0.0035) Loss: 0.1542 (0.0700)
[2023/01/16 01:08] | TRAIN(025): [ 800/2211] Batch: 0.0928 (0.1003) Data: 0.0019 (0.0035) Loss: 0.0630 (0.0711)
[2023/01/16 01:08] | TRAIN(025): [ 850/2211] Batch: 0.0917 (0.1002) Data: 0.0019 (0.0034) Loss: 0.2261 (0.0715)
[2023/01/16 01:09] | TRAIN(025): [ 900/2211] Batch: 0.0929 (0.1001) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0716)
[2023/01/16 01:09] | TRAIN(025): [ 950/2211] Batch: 0.0900 (0.1000) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0719)
[2023/01/16 01:09] | TRAIN(025): [1000/2211] Batch: 0.0914 (0.1000) Data: 0.0019 (0.0032) Loss: 0.2795 (0.0710)
[2023/01/16 01:09] | TRAIN(025): [1050/2211] Batch: 0.1273 (0.1000) Data: 0.0022 (0.0031) Loss: 0.0000 (0.0707)
[2023/01/16 01:09] | TRAIN(025): [1100/2211] Batch: 0.0949 (0.1002) Data: 0.0018 (0.0030) Loss: 0.0328 (0.0704)
[2023/01/16 01:09] | TRAIN(025): [1150/2211] Batch: 0.1217 (0.1004) Data: 0.0023 (0.0030) Loss: 0.0370 (0.0709)
[2023/01/16 01:09] | TRAIN(025): [1200/2211] Batch: 0.1065 (0.1004) Data: 0.0020 (0.0029) Loss: 0.2090 (0.0711)
[2023/01/16 01:09] | TRAIN(025): [1250/2211] Batch: 0.0989 (0.1004) Data: 0.0019 (0.0029) Loss: 0.0001 (0.0709)
[2023/01/16 01:09] | TRAIN(025): [1300/2211] Batch: 0.0956 (0.1005) Data: 0.0015 (0.0029) Loss: 0.2010 (0.0707)
[2023/01/16 01:09] | TRAIN(025): [1350/2211] Batch: 0.1203 (0.1005) Data: 0.0023 (0.0028) Loss: 0.0360 (0.0707)
[2023/01/16 01:09] | TRAIN(025): [1400/2211] Batch: 0.1183 (0.1007) Data: 0.0017 (0.0028) Loss: 0.0784 (0.0710)
[2023/01/16 01:09] | TRAIN(025): [1450/2211] Batch: 0.0977 (0.1006) Data: 0.0018 (0.0028) Loss: 0.1070 (0.0708)
[2023/01/16 01:10] | TRAIN(025): [1500/2211] Batch: 0.0934 (0.1009) Data: 0.0018 (0.0027) Loss: 0.1204 (0.0712)
[2023/01/16 01:10] | TRAIN(025): [1550/2211] Batch: 0.1232 (0.1009) Data: 0.0022 (0.0027) Loss: 0.0000 (0.0713)
[2023/01/16 01:10] | TRAIN(025): [1600/2211] Batch: 0.0999 (0.1009) Data: 0.0019 (0.0027) Loss: 0.1291 (0.0712)
[2023/01/16 01:10] | TRAIN(025): [1650/2211] Batch: 0.1542 (0.1010) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0707)
[2023/01/16 01:10] | TRAIN(025): [1700/2211] Batch: 0.0932 (0.1011) Data: 0.0018 (0.0026) Loss: 0.0549 (0.0705)
[2023/01/16 01:10] | TRAIN(025): [1750/2211] Batch: 0.1082 (0.1011) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0704)
[2023/01/16 01:10] | TRAIN(025): [1800/2211] Batch: 0.0946 (0.1010) Data: 0.0019 (0.0026) Loss: 0.0028 (0.0703)
[2023/01/16 01:10] | TRAIN(025): [1850/2211] Batch: 0.1013 (0.1009) Data: 0.0021 (0.0026) Loss: 0.0100 (0.0702)
[2023/01/16 01:10] | TRAIN(025): [1900/2211] Batch: 0.1269 (0.1010) Data: 0.0022 (0.0026) Loss: 0.3197 (0.0697)
[2023/01/16 01:10] | TRAIN(025): [1950/2211] Batch: 0.1052 (0.1009) Data: 0.0016 (0.0025) Loss: 0.2891 (0.0702)
[2023/01/16 01:10] | TRAIN(025): [2000/2211] Batch: 0.0934 (0.1008) Data: 0.0018 (0.0025) Loss: 0.0428 (0.0705)
[2023/01/16 01:10] | TRAIN(025): [2050/2211] Batch: 0.1012 (0.1006) Data: 0.0023 (0.0025) Loss: 0.0000 (0.0702)
[2023/01/16 01:11] | TRAIN(025): [2100/2211] Batch: 0.0922 (0.1005) Data: 0.0019 (0.0025) Loss: 0.0517 (0.0703)
[2023/01/16 01:11] | TRAIN(025): [2150/2211] Batch: 0.0958 (0.1004) Data: 0.0017 (0.0025) Loss: 0.2654 (0.0707)
[2023/01/16 01:11] | TRAIN(025): [2200/2211] Batch: 0.0921 (0.1003) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0703)
[2023/01/16 01:11] | ------------------------------------------------------------
[2023/01/16 01:11] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:11] | ------------------------------------------------------------
[2023/01/16 01:11] |    TRAIN(25)     0:03:41     0:00:05     0:03:36      0.0703
[2023/01/16 01:11] | ------------------------------------------------------------
[2023/01/16 01:11] | **************************************************
[2023/01/16 01:11] | TRAIN(026): [  50/2211] Batch: 0.1360 (0.1276) Data: 0.0022 (0.0265) Loss: 0.1315 (0.0724)
[2023/01/16 01:11] | TRAIN(026): [ 100/2211] Batch: 0.0999 (0.1170) Data: 0.0020 (0.0144) Loss: 0.0676 (0.0780)
[2023/01/16 01:11] | TRAIN(026): [ 150/2211] Batch: 0.0988 (0.1101) Data: 0.0024 (0.0103) Loss: 0.0000 (0.0778)
[2023/01/16 01:11] | TRAIN(026): [ 200/2211] Batch: 0.0936 (0.1068) Data: 0.0017 (0.0082) Loss: 0.0044 (0.0724)
[2023/01/16 01:11] | TRAIN(026): [ 250/2211] Batch: 0.1020 (0.1047) Data: 0.0019 (0.0069) Loss: 0.0000 (0.0719)
[2023/01/16 01:11] | TRAIN(026): [ 300/2211] Batch: 0.1067 (0.1036) Data: 0.0021 (0.0061) Loss: 0.0000 (0.0710)
[2023/01/16 01:11] | TRAIN(026): [ 350/2211] Batch: 0.1043 (0.1028) Data: 0.0018 (0.0055) Loss: 0.0000 (0.0715)
[2023/01/16 01:11] | TRAIN(026): [ 400/2211] Batch: 0.0955 (0.1026) Data: 0.0019 (0.0051) Loss: 0.0163 (0.0754)
[2023/01/16 01:11] | TRAIN(026): [ 450/2211] Batch: 0.0861 (0.1015) Data: 0.0018 (0.0047) Loss: 0.1764 (0.0749)
[2023/01/16 01:12] | TRAIN(026): [ 500/2211] Batch: 0.0908 (0.1018) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0723)
[2023/01/16 01:12] | TRAIN(026): [ 550/2211] Batch: 0.0909 (0.1023) Data: 0.0019 (0.0042) Loss: 0.0123 (0.0722)
[2023/01/16 01:12] | TRAIN(026): [ 600/2211] Batch: 0.1038 (0.1018) Data: 0.0018 (0.0040) Loss: 0.1921 (0.0712)
[2023/01/16 01:12] | TRAIN(026): [ 650/2211] Batch: 0.1208 (0.1017) Data: 0.0023 (0.0038) Loss: 0.0017 (0.0713)
[2023/01/16 01:12] | TRAIN(026): [ 700/2211] Batch: 0.0946 (0.1018) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0707)
[2023/01/16 01:12] | TRAIN(026): [ 750/2211] Batch: 0.1109 (0.1018) Data: 0.0019 (0.0036) Loss: 0.0611 (0.0696)
[2023/01/16 01:12] | TRAIN(026): [ 800/2211] Batch: 0.1004 (0.1016) Data: 0.0019 (0.0035) Loss: 0.1830 (0.0682)
[2023/01/16 01:12] | TRAIN(026): [ 850/2211] Batch: 0.1341 (0.1019) Data: 0.0019 (0.0034) Loss: 0.1513 (0.0686)
[2023/01/16 01:12] | TRAIN(026): [ 900/2211] Batch: 0.0988 (0.1022) Data: 0.0015 (0.0033) Loss: 0.0839 (0.0694)
[2023/01/16 01:12] | TRAIN(026): [ 950/2211] Batch: 0.0922 (0.1026) Data: 0.0018 (0.0032) Loss: 0.1358 (0.0700)
[2023/01/16 01:12] | TRAIN(026): [1000/2211] Batch: 0.0957 (0.1024) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0689)
[2023/01/16 01:12] | TRAIN(026): [1050/2211] Batch: 0.0885 (0.1020) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0685)
[2023/01/16 01:13] | TRAIN(026): [1100/2211] Batch: 0.0899 (0.1015) Data: 0.0016 (0.0030) Loss: 0.1446 (0.0698)
[2023/01/16 01:13] | TRAIN(026): [1150/2211] Batch: 0.0884 (0.1010) Data: 0.0018 (0.0030) Loss: 0.2526 (0.0701)
[2023/01/16 01:13] | TRAIN(026): [1200/2211] Batch: 0.0886 (0.1005) Data: 0.0019 (0.0029) Loss: 0.0789 (0.0702)
[2023/01/16 01:13] | TRAIN(026): [1250/2211] Batch: 0.0924 (0.1001) Data: 0.0015 (0.0029) Loss: 0.1192 (0.0703)
[2023/01/16 01:13] | TRAIN(026): [1300/2211] Batch: 0.0894 (0.0997) Data: 0.0019 (0.0029) Loss: 0.1769 (0.0696)
[2023/01/16 01:13] | TRAIN(026): [1350/2211] Batch: 0.1387 (0.1001) Data: 0.0021 (0.0028) Loss: 0.0000 (0.0695)
[2023/01/16 01:13] | TRAIN(026): [1400/2211] Batch: 0.1010 (0.1003) Data: 0.0016 (0.0028) Loss: 0.0000 (0.0696)
[2023/01/16 01:13] | TRAIN(026): [1450/2211] Batch: 0.0878 (0.1002) Data: 0.0019 (0.0027) Loss: 0.0029 (0.0695)
[2023/01/16 01:13] | TRAIN(026): [1500/2211] Batch: 0.0960 (0.1002) Data: 0.0019 (0.0027) Loss: 0.1889 (0.0697)
[2023/01/16 01:13] | TRAIN(026): [1550/2211] Batch: 0.1005 (0.1003) Data: 0.0015 (0.0027) Loss: 0.1013 (0.0698)
[2023/01/16 01:13] | TRAIN(026): [1600/2211] Batch: 0.1051 (0.1003) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0694)
[2023/01/16 01:13] | TRAIN(026): [1650/2211] Batch: 0.0911 (0.1002) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0692)
[2023/01/16 01:14] | TRAIN(026): [1700/2211] Batch: 0.0920 (0.1000) Data: 0.0018 (0.0026) Loss: 0.1985 (0.0694)
[2023/01/16 01:14] | TRAIN(026): [1750/2211] Batch: 0.1013 (0.0999) Data: 0.0024 (0.0026) Loss: 0.0001 (0.0692)
[2023/01/16 01:14] | TRAIN(026): [1800/2211] Batch: 0.0875 (0.0998) Data: 0.0018 (0.0026) Loss: 0.1535 (0.0696)
[2023/01/16 01:14] | TRAIN(026): [1850/2211] Batch: 0.0949 (0.0999) Data: 0.0020 (0.0025) Loss: 0.0000 (0.0695)
[2023/01/16 01:14] | TRAIN(026): [1900/2211] Batch: 0.0907 (0.0998) Data: 0.0018 (0.0025) Loss: 0.0938 (0.0696)
[2023/01/16 01:14] | TRAIN(026): [1950/2211] Batch: 0.1055 (0.0998) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0700)
[2023/01/16 01:14] | TRAIN(026): [2000/2211] Batch: 0.0985 (0.0999) Data: 0.0018 (0.0025) Loss: 0.1408 (0.0700)
[2023/01/16 01:14] | TRAIN(026): [2050/2211] Batch: 0.0887 (0.1001) Data: 0.0019 (0.0025) Loss: 0.0038 (0.0704)
[2023/01/16 01:14] | TRAIN(026): [2100/2211] Batch: 0.0945 (0.1001) Data: 0.0019 (0.0025) Loss: 0.1510 (0.0701)
[2023/01/16 01:14] | TRAIN(026): [2150/2211] Batch: 0.0935 (0.1000) Data: 0.0020 (0.0025) Loss: 0.2767 (0.0697)
[2023/01/16 01:14] | TRAIN(026): [2200/2211] Batch: 0.0899 (0.1000) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0695)
[2023/01/16 01:14] | ------------------------------------------------------------
[2023/01/16 01:14] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:14] | ------------------------------------------------------------
[2023/01/16 01:14] |    TRAIN(26)     0:03:41     0:00:05     0:03:35      0.0697
[2023/01/16 01:14] | ------------------------------------------------------------
[2023/01/16 01:14] | **************************************************
[2023/01/16 01:14] | TRAIN(027): [  50/2211] Batch: 0.1015 (0.1260) Data: 0.0019 (0.0256) Loss: 0.0524 (0.0906)
[2023/01/16 01:15] | TRAIN(027): [ 100/2211] Batch: 0.0964 (0.1102) Data: 0.0018 (0.0139) Loss: 0.0000 (0.0736)
[2023/01/16 01:15] | TRAIN(027): [ 150/2211] Batch: 0.0950 (0.1058) Data: 0.0020 (0.0099) Loss: 0.1339 (0.0752)
[2023/01/16 01:15] | TRAIN(027): [ 200/2211] Batch: 0.1012 (0.1045) Data: 0.0019 (0.0079) Loss: 0.0396 (0.0662)
[2023/01/16 01:15] | TRAIN(027): [ 250/2211] Batch: 0.0980 (0.1039) Data: 0.0017 (0.0067) Loss: 0.0001 (0.0659)
[2023/01/16 01:15] | TRAIN(027): [ 300/2211] Batch: 0.0898 (0.1028) Data: 0.0018 (0.0059) Loss: 0.0001 (0.0681)
[2023/01/16 01:15] | TRAIN(027): [ 350/2211] Batch: 0.0982 (0.1021) Data: 0.0015 (0.0054) Loss: 0.0404 (0.0672)
[2023/01/16 01:15] | TRAIN(027): [ 400/2211] Batch: 0.0989 (0.1013) Data: 0.0017 (0.0049) Loss: 0.2042 (0.0674)
[2023/01/16 01:15] | TRAIN(027): [ 450/2211] Batch: 0.0923 (0.1007) Data: 0.0018 (0.0046) Loss: 0.0000 (0.0645)
[2023/01/16 01:15] | TRAIN(027): [ 500/2211] Batch: 0.0933 (0.1003) Data: 0.0018 (0.0043) Loss: 0.0000 (0.0664)
[2023/01/16 01:15] | TRAIN(027): [ 550/2211] Batch: 0.1191 (0.1006) Data: 0.0021 (0.0041) Loss: 0.0331 (0.0669)
[2023/01/16 01:15] | TRAIN(027): [ 600/2211] Batch: 0.0993 (0.1004) Data: 0.0015 (0.0039) Loss: 0.0000 (0.0671)
[2023/01/16 01:15] | TRAIN(027): [ 650/2211] Batch: 0.0990 (0.1003) Data: 0.0014 (0.0037) Loss: 0.0308 (0.0676)
[2023/01/16 01:16] | TRAIN(027): [ 700/2211] Batch: 0.1009 (0.0999) Data: 0.0015 (0.0036) Loss: 0.2595 (0.0678)
[2023/01/16 01:16] | TRAIN(027): [ 750/2211] Batch: 0.0984 (0.1000) Data: 0.0018 (0.0035) Loss: 0.0099 (0.0675)
[2023/01/16 01:16] | TRAIN(027): [ 800/2211] Batch: 0.0995 (0.0998) Data: 0.0023 (0.0034) Loss: 0.0710 (0.0679)
[2023/01/16 01:16] | TRAIN(027): [ 850/2211] Batch: 0.0881 (0.0997) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0667)
[2023/01/16 01:16] | TRAIN(027): [ 900/2211] Batch: 0.0974 (0.0998) Data: 0.0019 (0.0032) Loss: 0.0002 (0.0677)
[2023/01/16 01:16] | TRAIN(027): [ 950/2211] Batch: 0.0950 (0.0998) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0683)
[2023/01/16 01:16] | TRAIN(027): [1000/2211] Batch: 0.0887 (0.0997) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0674)
[2023/01/16 01:16] | TRAIN(027): [1050/2211] Batch: 0.0890 (0.0997) Data: 0.0014 (0.0030) Loss: 0.2208 (0.0678)
[2023/01/16 01:16] | TRAIN(027): [1100/2211] Batch: 0.0911 (0.0997) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0669)
[2023/01/16 01:16] | TRAIN(027): [1150/2211] Batch: 0.0974 (0.0996) Data: 0.0021 (0.0029) Loss: 0.0000 (0.0669)
[2023/01/16 01:16] | TRAIN(027): [1200/2211] Batch: 0.1272 (0.0995) Data: 0.0024 (0.0028) Loss: 0.0000 (0.0674)
[2023/01/16 01:16] | TRAIN(027): [1250/2211] Batch: 0.0952 (0.0995) Data: 0.0018 (0.0028) Loss: 0.0732 (0.0684)
[2023/01/16 01:17] | TRAIN(027): [1300/2211] Batch: 0.1048 (0.0995) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0680)
[2023/01/16 01:17] | TRAIN(027): [1350/2211] Batch: 0.1046 (0.0996) Data: 0.0015 (0.0027) Loss: 0.0285 (0.0678)
[2023/01/16 01:17] | TRAIN(027): [1400/2211] Batch: 0.0931 (0.0997) Data: 0.0018 (0.0027) Loss: 0.0561 (0.0681)
[2023/01/16 01:17] | TRAIN(027): [1450/2211] Batch: 0.1320 (0.0997) Data: 0.0021 (0.0027) Loss: 0.0973 (0.0682)
[2023/01/16 01:17] | TRAIN(027): [1500/2211] Batch: 0.1104 (0.0997) Data: 0.0020 (0.0026) Loss: 0.0241 (0.0683)
[2023/01/16 01:17] | TRAIN(027): [1550/2211] Batch: 0.1006 (0.0996) Data: 0.0019 (0.0026) Loss: 0.0819 (0.0675)
[2023/01/16 01:17] | TRAIN(027): [1600/2211] Batch: 0.0899 (0.0994) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0672)
[2023/01/16 01:17] | TRAIN(027): [1650/2211] Batch: 0.0934 (0.0993) Data: 0.0021 (0.0026) Loss: 0.0000 (0.0669)
[2023/01/16 01:17] | TRAIN(027): [1700/2211] Batch: 0.1246 (0.0993) Data: 0.0019 (0.0026) Loss: 0.0206 (0.0672)
[2023/01/16 01:17] | TRAIN(027): [1750/2211] Batch: 0.1073 (0.0993) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0670)
[2023/01/16 01:17] | TRAIN(027): [1800/2211] Batch: 0.1032 (0.0993) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0664)
[2023/01/16 01:17] | TRAIN(027): [1850/2211] Batch: 0.0994 (0.0993) Data: 0.0022 (0.0025) Loss: 0.1292 (0.0668)
[2023/01/16 01:18] | TRAIN(027): [1900/2211] Batch: 0.0922 (0.0992) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0674)
[2023/01/16 01:18] | TRAIN(027): [1950/2211] Batch: 0.0929 (0.0991) Data: 0.0019 (0.0025) Loss: 0.0002 (0.0672)
[2023/01/16 01:18] | TRAIN(027): [2000/2211] Batch: 0.1001 (0.0991) Data: 0.0020 (0.0025) Loss: 0.0000 (0.0673)
[2023/01/16 01:18] | TRAIN(027): [2050/2211] Batch: 0.0929 (0.0990) Data: 0.0018 (0.0025) Loss: 0.2898 (0.0673)
[2023/01/16 01:18] | TRAIN(027): [2100/2211] Batch: 0.0983 (0.0991) Data: 0.0020 (0.0024) Loss: 0.1830 (0.0672)
[2023/01/16 01:18] | TRAIN(027): [2150/2211] Batch: 0.1300 (0.0994) Data: 0.0024 (0.0024) Loss: 0.0816 (0.0672)
[2023/01/16 01:18] | TRAIN(027): [2200/2211] Batch: 0.0896 (0.0995) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0670)
[2023/01/16 01:18] | ------------------------------------------------------------
[2023/01/16 01:18] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:18] | ------------------------------------------------------------
[2023/01/16 01:18] |    TRAIN(27)     0:03:39     0:00:05     0:03:34      0.0670
[2023/01/16 01:18] | ------------------------------------------------------------
[2023/01/16 01:18] | **************************************************
[2023/01/16 01:18] | TRAIN(028): [  50/2211] Batch: 0.0864 (0.1168) Data: 0.0019 (0.0259) Loss: 0.0000 (0.0482)
[2023/01/16 01:18] | TRAIN(028): [ 100/2211] Batch: 0.0997 (0.1063) Data: 0.0019 (0.0140) Loss: 0.1498 (0.0596)
[2023/01/16 01:18] | TRAIN(028): [ 150/2211] Batch: 0.0936 (0.1036) Data: 0.0020 (0.0100) Loss: 0.0023 (0.0626)
[2023/01/16 01:18] | TRAIN(028): [ 200/2211] Batch: 0.0979 (0.1041) Data: 0.0019 (0.0079) Loss: 0.0563 (0.0698)
[2023/01/16 01:18] | TRAIN(028): [ 250/2211] Batch: 0.1352 (0.1040) Data: 0.0020 (0.0067) Loss: 0.0000 (0.0683)
[2023/01/16 01:19] | TRAIN(028): [ 300/2211] Batch: 0.1188 (0.1042) Data: 0.0019 (0.0059) Loss: 0.1647 (0.0712)
[2023/01/16 01:19] | TRAIN(028): [ 350/2211] Batch: 0.0927 (0.1046) Data: 0.0019 (0.0053) Loss: 0.0000 (0.0699)
[2023/01/16 01:19] | TRAIN(028): [ 400/2211] Batch: 0.0867 (0.1044) Data: 0.0015 (0.0049) Loss: 0.1844 (0.0760)
[2023/01/16 01:19] | TRAIN(028): [ 450/2211] Batch: 0.1126 (0.1043) Data: 0.0016 (0.0046) Loss: 0.0797 (0.0734)
[2023/01/16 01:19] | TRAIN(028): [ 500/2211] Batch: 0.0940 (0.1048) Data: 0.0018 (0.0043) Loss: 0.0309 (0.0760)
[2023/01/16 01:19] | TRAIN(028): [ 550/2211] Batch: 0.0989 (0.1046) Data: 0.0018 (0.0041) Loss: 0.1264 (0.0768)
[2023/01/16 01:19] | TRAIN(028): [ 600/2211] Batch: 0.1029 (0.1040) Data: 0.0023 (0.0039) Loss: 0.1287 (0.0763)
[2023/01/16 01:19] | TRAIN(028): [ 650/2211] Batch: 0.0935 (0.1038) Data: 0.0019 (0.0038) Loss: 0.0966 (0.0746)
[2023/01/16 01:19] | TRAIN(028): [ 700/2211] Batch: 0.0954 (0.1038) Data: 0.0019 (0.0037) Loss: 0.0557 (0.0731)
[2023/01/16 01:19] | TRAIN(028): [ 750/2211] Batch: 0.0937 (0.1035) Data: 0.0020 (0.0035) Loss: 0.0000 (0.0725)
[2023/01/16 01:19] | TRAIN(028): [ 800/2211] Batch: 0.1026 (0.1036) Data: 0.0019 (0.0035) Loss: 0.0000 (0.0718)
[2023/01/16 01:20] | TRAIN(028): [ 850/2211] Batch: 0.0960 (0.1032) Data: 0.0023 (0.0034) Loss: 0.1916 (0.0710)
[2023/01/16 01:20] | TRAIN(028): [ 900/2211] Batch: 0.0986 (0.1029) Data: 0.0022 (0.0033) Loss: 0.0000 (0.0703)
[2023/01/16 01:20] | TRAIN(028): [ 950/2211] Batch: 0.0890 (0.1024) Data: 0.0022 (0.0032) Loss: 0.0700 (0.0688)
[2023/01/16 01:20] | TRAIN(028): [1000/2211] Batch: 0.0895 (0.1019) Data: 0.0019 (0.0032) Loss: 0.0120 (0.0684)
[2023/01/16 01:20] | TRAIN(028): [1050/2211] Batch: 0.0907 (0.1014) Data: 0.0020 (0.0031) Loss: 0.1171 (0.0687)
[2023/01/16 01:20] | TRAIN(028): [1100/2211] Batch: 0.0913 (0.1010) Data: 0.0021 (0.0031) Loss: 0.0000 (0.0684)
[2023/01/16 01:20] | TRAIN(028): [1150/2211] Batch: 0.0915 (0.1006) Data: 0.0019 (0.0030) Loss: 0.0974 (0.0681)
[2023/01/16 01:20] | TRAIN(028): [1200/2211] Batch: 0.0995 (0.1005) Data: 0.0024 (0.0030) Loss: 0.0000 (0.0665)
[2023/01/16 01:20] | TRAIN(028): [1250/2211] Batch: 0.1053 (0.1011) Data: 0.0021 (0.0029) Loss: 0.0000 (0.0667)
[2023/01/16 01:20] | TRAIN(028): [1300/2211] Batch: 0.0991 (0.1010) Data: 0.0020 (0.0029) Loss: 0.0000 (0.0672)
[2023/01/16 01:20] | TRAIN(028): [1350/2211] Batch: 0.0893 (0.1007) Data: 0.0018 (0.0029) Loss: 0.0447 (0.0672)
[2023/01/16 01:20] | TRAIN(028): [1400/2211] Batch: 0.0979 (0.1006) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0669)
[2023/01/16 01:20] | TRAIN(028): [1450/2211] Batch: 0.1004 (0.1004) Data: 0.0019 (0.0028) Loss: 0.1064 (0.0665)
[2023/01/16 01:21] | TRAIN(028): [1500/2211] Batch: 0.0958 (0.1003) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0665)
[2023/01/16 01:21] | TRAIN(028): [1550/2211] Batch: 0.0888 (0.1000) Data: 0.0020 (0.0027) Loss: 0.1375 (0.0664)
[2023/01/16 01:21] | TRAIN(028): [1600/2211] Batch: 0.1030 (0.0998) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0664)
[2023/01/16 01:21] | TRAIN(028): [1650/2211] Batch: 0.0941 (0.0998) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0663)
[2023/01/16 01:21] | TRAIN(028): [1700/2211] Batch: 0.0921 (0.0997) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0663)
[2023/01/16 01:21] | TRAIN(028): [1750/2211] Batch: 0.1210 (0.0997) Data: 0.0023 (0.0026) Loss: 0.0130 (0.0663)
[2023/01/16 01:21] | TRAIN(028): [1800/2211] Batch: 0.0974 (0.0997) Data: 0.0018 (0.0026) Loss: 0.2708 (0.0668)
[2023/01/16 01:21] | TRAIN(028): [1850/2211] Batch: 0.1017 (0.0996) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0668)
[2023/01/16 01:21] | TRAIN(028): [1900/2211] Batch: 0.1003 (0.0996) Data: 0.0020 (0.0026) Loss: 0.1052 (0.0667)
[2023/01/16 01:21] | TRAIN(028): [1950/2211] Batch: 0.1044 (0.0995) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0672)
[2023/01/16 01:21] | TRAIN(028): [2000/2211] Batch: 0.1016 (0.0994) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0672)
[2023/01/16 01:21] | TRAIN(028): [2050/2211] Batch: 0.0844 (0.0993) Data: 0.0017 (0.0025) Loss: 0.0033 (0.0667)
[2023/01/16 01:22] | TRAIN(028): [2100/2211] Batch: 0.0898 (0.0992) Data: 0.0019 (0.0025) Loss: 0.0001 (0.0666)
[2023/01/16 01:22] | TRAIN(028): [2150/2211] Batch: 0.1013 (0.0992) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0659)
[2023/01/16 01:22] | TRAIN(028): [2200/2211] Batch: 0.0861 (0.0992) Data: 0.0013 (0.0025) Loss: 0.0000 (0.0659)
[2023/01/16 01:22] | ------------------------------------------------------------
[2023/01/16 01:22] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:22] | ------------------------------------------------------------
[2023/01/16 01:22] |    TRAIN(28)     0:03:39     0:00:05     0:03:33      0.0658
[2023/01/16 01:22] | ------------------------------------------------------------
[2023/01/16 01:22] | **************************************************
[2023/01/16 01:22] | TRAIN(029): [  50/2211] Batch: 0.0906 (0.1361) Data: 0.0020 (0.0267) Loss: 0.0662 (0.0435)
[2023/01/16 01:22] | TRAIN(029): [ 100/2211] Batch: 0.0969 (0.1187) Data: 0.0018 (0.0145) Loss: 0.0000 (0.0571)
[2023/01/16 01:22] | TRAIN(029): [ 150/2211] Batch: 0.1405 (0.1119) Data: 0.0025 (0.0103) Loss: 0.0000 (0.0614)
[2023/01/16 01:22] | TRAIN(029): [ 200/2211] Batch: 0.1096 (0.1115) Data: 0.0021 (0.0082) Loss: 0.0028 (0.0665)
[2023/01/16 01:22] | TRAIN(029): [ 250/2211] Batch: 0.1442 (0.1121) Data: 0.0024 (0.0070) Loss: 0.1046 (0.0636)
[2023/01/16 01:22] | TRAIN(029): [ 300/2211] Batch: 0.1379 (0.1132) Data: 0.0023 (0.0062) Loss: 0.1578 (0.0659)
[2023/01/16 01:22] | TRAIN(029): [ 350/2211] Batch: 0.1240 (0.1142) Data: 0.0019 (0.0056) Loss: 0.0386 (0.0682)
[2023/01/16 01:22] | TRAIN(029): [ 400/2211] Batch: 0.0897 (0.1153) Data: 0.0018 (0.0052) Loss: 0.0000 (0.0655)
[2023/01/16 01:23] | TRAIN(029): [ 450/2211] Batch: 0.1203 (0.1141) Data: 0.0014 (0.0048) Loss: 0.0220 (0.0655)
[2023/01/16 01:23] | TRAIN(029): [ 500/2211] Batch: 0.0887 (0.1124) Data: 0.0020 (0.0045) Loss: 0.0000 (0.0636)
[2023/01/16 01:23] | TRAIN(029): [ 550/2211] Batch: 0.0978 (0.1112) Data: 0.0019 (0.0043) Loss: 0.0823 (0.0661)
[2023/01/16 01:23] | TRAIN(029): [ 600/2211] Batch: 0.0883 (0.1097) Data: 0.0017 (0.0041) Loss: 0.0000 (0.0670)
[2023/01/16 01:23] | TRAIN(029): [ 650/2211] Batch: 0.1082 (0.1084) Data: 0.0018 (0.0039) Loss: 0.0047 (0.0659)
[2023/01/16 01:23] | TRAIN(029): [ 700/2211] Batch: 0.0886 (0.1074) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0653)
[2023/01/16 01:23] | TRAIN(029): [ 750/2211] Batch: 0.0884 (0.1065) Data: 0.0018 (0.0036) Loss: 0.0872 (0.0656)
[2023/01/16 01:23] | TRAIN(029): [ 800/2211] Batch: 0.0895 (0.1056) Data: 0.0018 (0.0035) Loss: 0.1740 (0.0658)
[2023/01/16 01:23] | TRAIN(029): [ 850/2211] Batch: 0.1192 (0.1050) Data: 0.0021 (0.0034) Loss: 0.0465 (0.0657)
[2023/01/16 01:23] | TRAIN(029): [ 900/2211] Batch: 0.1171 (0.1051) Data: 0.0022 (0.0033) Loss: 0.0001 (0.0650)
[2023/01/16 01:23] | TRAIN(029): [ 950/2211] Batch: 0.0907 (0.1050) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0637)
[2023/01/16 01:23] | TRAIN(029): [1000/2211] Batch: 0.0917 (0.1049) Data: 0.0017 (0.0032) Loss: 0.0761 (0.0647)
[2023/01/16 01:24] | TRAIN(029): [1050/2211] Batch: 0.0988 (0.1045) Data: 0.0018 (0.0031) Loss: 0.2289 (0.0657)
[2023/01/16 01:24] | TRAIN(029): [1100/2211] Batch: 0.0913 (0.1041) Data: 0.0019 (0.0031) Loss: 0.0083 (0.0661)
[2023/01/16 01:24] | TRAIN(029): [1150/2211] Batch: 0.1046 (0.1037) Data: 0.0013 (0.0030) Loss: 0.0021 (0.0657)
[2023/01/16 01:24] | TRAIN(029): [1200/2211] Batch: 0.0905 (0.1035) Data: 0.0018 (0.0030) Loss: 0.0835 (0.0665)
[2023/01/16 01:24] | TRAIN(029): [1250/2211] Batch: 0.0998 (0.1032) Data: 0.0018 (0.0029) Loss: 0.0022 (0.0658)
[2023/01/16 01:24] | TRAIN(029): [1300/2211] Batch: 0.0918 (0.1028) Data: 0.0018 (0.0029) Loss: 0.0061 (0.0664)
[2023/01/16 01:24] | TRAIN(029): [1350/2211] Batch: 0.0986 (0.1024) Data: 0.0018 (0.0028) Loss: 0.0636 (0.0667)
[2023/01/16 01:24] | TRAIN(029): [1400/2211] Batch: 0.0999 (0.1021) Data: 0.0018 (0.0028) Loss: 0.0062 (0.0669)
[2023/01/16 01:24] | TRAIN(029): [1450/2211] Batch: 0.0883 (0.1020) Data: 0.0018 (0.0028) Loss: 0.2168 (0.0668)
[2023/01/16 01:24] | TRAIN(029): [1500/2211] Batch: 0.0890 (0.1016) Data: 0.0018 (0.0028) Loss: 0.0986 (0.0666)
[2023/01/16 01:24] | TRAIN(029): [1550/2211] Batch: 0.0964 (0.1013) Data: 0.0018 (0.0027) Loss: 0.0001 (0.0666)
[2023/01/16 01:24] | TRAIN(029): [1600/2211] Batch: 0.0982 (0.1013) Data: 0.0018 (0.0027) Loss: 0.0010 (0.0664)
[2023/01/16 01:24] | TRAIN(029): [1650/2211] Batch: 0.0899 (0.1012) Data: 0.0017 (0.0027) Loss: 0.0094 (0.0669)
[2023/01/16 01:25] | TRAIN(029): [1700/2211] Batch: 0.0946 (0.1009) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0670)
[2023/01/16 01:25] | TRAIN(029): [1750/2211] Batch: 0.0892 (0.1007) Data: 0.0018 (0.0026) Loss: 0.0810 (0.0665)
[2023/01/16 01:25] | TRAIN(029): [1800/2211] Batch: 0.0888 (0.1005) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0664)
[2023/01/16 01:25] | TRAIN(029): [1850/2211] Batch: 0.0892 (0.1003) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0659)
[2023/01/16 01:25] | TRAIN(029): [1900/2211] Batch: 0.0888 (0.1003) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0665)
[2023/01/16 01:25] | TRAIN(029): [1950/2211] Batch: 0.0890 (0.1000) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0668)
[2023/01/16 01:25] | TRAIN(029): [2000/2211] Batch: 0.0882 (0.0998) Data: 0.0018 (0.0025) Loss: 0.0041 (0.0664)
[2023/01/16 01:25] | TRAIN(029): [2050/2211] Batch: 0.0961 (0.0997) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0661)
[2023/01/16 01:25] | TRAIN(029): [2100/2211] Batch: 0.0996 (0.0999) Data: 0.0020 (0.0025) Loss: 0.0282 (0.0665)
[2023/01/16 01:25] | TRAIN(029): [2150/2211] Batch: 0.1275 (0.0999) Data: 0.0032 (0.0025) Loss: 0.3166 (0.0666)
[2023/01/16 01:25] | TRAIN(029): [2200/2211] Batch: 0.0921 (0.1000) Data: 0.0015 (0.0025) Loss: 0.0000 (0.0665)
[2023/01/16 01:25] | ------------------------------------------------------------
[2023/01/16 01:25] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:25] | ------------------------------------------------------------
[2023/01/16 01:25] |    TRAIN(29)     0:03:41     0:00:05     0:03:35      0.0664
[2023/01/16 01:25] | ------------------------------------------------------------
[2023/01/16 01:25] | **************************************************
[2023/01/16 01:26] | TRAIN(030): [  50/2211] Batch: 0.0958 (0.1247) Data: 0.0018 (0.0249) Loss: 0.0592 (0.0711)
[2023/01/16 01:26] | TRAIN(030): [ 100/2211] Batch: 0.0869 (0.1080) Data: 0.0018 (0.0135) Loss: 0.0000 (0.0617)
[2023/01/16 01:26] | TRAIN(030): [ 150/2211] Batch: 0.0960 (0.1033) Data: 0.0018 (0.0096) Loss: 0.0000 (0.0636)
[2023/01/16 01:26] | TRAIN(030): [ 200/2211] Batch: 0.0901 (0.1008) Data: 0.0017 (0.0077) Loss: 0.1701 (0.0607)
[2023/01/16 01:26] | TRAIN(030): [ 250/2211] Batch: 0.0937 (0.0991) Data: 0.0018 (0.0065) Loss: 0.0000 (0.0610)
[2023/01/16 01:26] | TRAIN(030): [ 300/2211] Batch: 0.0877 (0.0980) Data: 0.0018 (0.0057) Loss: 0.0000 (0.0621)
[2023/01/16 01:26] | TRAIN(030): [ 350/2211] Batch: 0.0933 (0.0970) Data: 0.0017 (0.0051) Loss: 0.0000 (0.0619)
[2023/01/16 01:26] | TRAIN(030): [ 400/2211] Batch: 0.0874 (0.0965) Data: 0.0017 (0.0047) Loss: 0.0000 (0.0608)
[2023/01/16 01:26] | TRAIN(030): [ 450/2211] Batch: 0.0876 (0.0958) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0612)
[2023/01/16 01:26] | TRAIN(030): [ 500/2211] Batch: 0.0936 (0.0954) Data: 0.0017 (0.0041) Loss: 0.3307 (0.0614)
[2023/01/16 01:26] | TRAIN(030): [ 550/2211] Batch: 0.0942 (0.0953) Data: 0.0016 (0.0039) Loss: 0.0029 (0.0616)
[2023/01/16 01:26] | TRAIN(030): [ 600/2211] Batch: 0.0980 (0.0948) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0604)
[2023/01/16 01:26] | TRAIN(030): [ 650/2211] Batch: 0.1005 (0.0946) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0619)
[2023/01/16 01:27] | TRAIN(030): [ 700/2211] Batch: 0.0881 (0.0945) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0619)
[2023/01/16 01:27] | TRAIN(030): [ 750/2211] Batch: 0.0901 (0.0943) Data: 0.0019 (0.0034) Loss: 0.1856 (0.0621)
[2023/01/16 01:27] | TRAIN(030): [ 800/2211] Batch: 0.0992 (0.0945) Data: 0.0017 (0.0033) Loss: 0.1175 (0.0633)
[2023/01/16 01:27] | TRAIN(030): [ 850/2211] Batch: 0.0950 (0.0942) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0621)
[2023/01/16 01:27] | TRAIN(030): [ 900/2211] Batch: 0.0872 (0.0940) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0614)
[2023/01/16 01:27] | TRAIN(030): [ 950/2211] Batch: 0.0887 (0.0938) Data: 0.0019 (0.0030) Loss: 0.2122 (0.0613)
[2023/01/16 01:27] | TRAIN(030): [1000/2211] Batch: 0.0908 (0.0938) Data: 0.0018 (0.0030) Loss: 0.1153 (0.0605)
[2023/01/16 01:27] | TRAIN(030): [1050/2211] Batch: 0.0880 (0.0936) Data: 0.0018 (0.0029) Loss: 0.1592 (0.0621)
[2023/01/16 01:27] | TRAIN(030): [1100/2211] Batch: 0.1193 (0.0941) Data: 0.0022 (0.0029) Loss: 0.0287 (0.0620)
[2023/01/16 01:27] | TRAIN(030): [1150/2211] Batch: 0.0886 (0.0942) Data: 0.0017 (0.0028) Loss: 0.1160 (0.0621)
[2023/01/16 01:27] | TRAIN(030): [1200/2211] Batch: 0.0964 (0.0943) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0625)
[2023/01/16 01:27] | TRAIN(030): [1250/2211] Batch: 0.0956 (0.0943) Data: 0.0018 (0.0027) Loss: 0.0032 (0.0618)
[2023/01/16 01:27] | TRAIN(030): [1300/2211] Batch: 0.0925 (0.0944) Data: 0.0017 (0.0027) Loss: 0.0079 (0.0614)
[2023/01/16 01:28] | TRAIN(030): [1350/2211] Batch: 0.0894 (0.0944) Data: 0.0018 (0.0027) Loss: 0.0094 (0.0618)
[2023/01/16 01:28] | TRAIN(030): [1400/2211] Batch: 0.0894 (0.0943) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0615)
[2023/01/16 01:28] | TRAIN(030): [1450/2211] Batch: 0.1188 (0.0943) Data: 0.0023 (0.0026) Loss: 0.1749 (0.0615)
[2023/01/16 01:28] | TRAIN(030): [1500/2211] Batch: 0.0935 (0.0944) Data: 0.0017 (0.0026) Loss: 0.0991 (0.0617)
[2023/01/16 01:28] | TRAIN(030): [1550/2211] Batch: 0.0969 (0.0947) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0616)
[2023/01/16 01:28] | TRAIN(030): [1600/2211] Batch: 0.1078 (0.0947) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0626)
[2023/01/16 01:28] | TRAIN(030): [1650/2211] Batch: 0.0983 (0.0947) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0626)
[2023/01/16 01:28] | TRAIN(030): [1700/2211] Batch: 0.0925 (0.0947) Data: 0.0018 (0.0025) Loss: 0.0821 (0.0619)
[2023/01/16 01:28] | TRAIN(030): [1750/2211] Batch: 0.0919 (0.0944) Data: 0.0018 (0.0025) Loss: 0.0879 (0.0615)
[2023/01/16 01:28] | TRAIN(030): [1800/2211] Batch: 0.0882 (0.0942) Data: 0.0020 (0.0025) Loss: 0.0000 (0.0613)
[2023/01/16 01:28] | TRAIN(030): [1850/2211] Batch: 0.0996 (0.0943) Data: 0.0018 (0.0024) Loss: 0.1473 (0.0619)
[2023/01/16 01:28] | TRAIN(030): [1900/2211] Batch: 0.0955 (0.0944) Data: 0.0018 (0.0024) Loss: 0.1137 (0.0622)
[2023/01/16 01:28] | TRAIN(030): [1950/2211] Batch: 0.0936 (0.0944) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0628)
[2023/01/16 01:29] | TRAIN(030): [2000/2211] Batch: 0.1064 (0.0945) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0630)
[2023/01/16 01:29] | TRAIN(030): [2050/2211] Batch: 0.0994 (0.0947) Data: 0.0019 (0.0024) Loss: 0.2375 (0.0632)
[2023/01/16 01:29] | TRAIN(030): [2100/2211] Batch: 0.1156 (0.0952) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0630)
[2023/01/16 01:29] | TRAIN(030): [2150/2211] Batch: 0.0884 (0.0952) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0625)
[2023/01/16 01:29] | TRAIN(030): [2200/2211] Batch: 0.1084 (0.0953) Data: 0.0016 (0.0023) Loss: 0.2213 (0.0623)
[2023/01/16 01:29] | ------------------------------------------------------------
[2023/01/16 01:29] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:29] | ------------------------------------------------------------
[2023/01/16 01:29] |    TRAIN(30)     0:03:30     0:00:05     0:03:25      0.0621
[2023/01/16 01:29] | ------------------------------------------------------------
[2023/01/16 01:29] | **************************************************
[2023/01/16 01:29] | TRAIN(031): [  50/2211] Batch: 0.0904 (0.1247) Data: 0.0018 (0.0244) Loss: 0.1035 (0.0931)
[2023/01/16 01:29] | TRAIN(031): [ 100/2211] Batch: 0.1240 (0.1172) Data: 0.0022 (0.0133) Loss: 0.3626 (0.0773)
[2023/01/16 01:29] | TRAIN(031): [ 150/2211] Batch: 0.0986 (0.1183) Data: 0.0017 (0.0097) Loss: 0.0000 (0.0837)
[2023/01/16 01:29] | TRAIN(031): [ 200/2211] Batch: 0.0959 (0.1118) Data: 0.0018 (0.0077) Loss: 0.0000 (0.0734)
[2023/01/16 01:29] | TRAIN(031): [ 250/2211] Batch: 0.1171 (0.1083) Data: 0.0021 (0.0065) Loss: 0.0035 (0.0769)
[2023/01/16 01:29] | TRAIN(031): [ 300/2211] Batch: 0.0875 (0.1067) Data: 0.0018 (0.0058) Loss: 0.0864 (0.0730)
[2023/01/16 01:30] | TRAIN(031): [ 350/2211] Batch: 0.0942 (0.1047) Data: 0.0017 (0.0052) Loss: 0.0000 (0.0741)
[2023/01/16 01:30] | TRAIN(031): [ 400/2211] Batch: 0.0914 (0.1034) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0695)
[2023/01/16 01:30] | TRAIN(031): [ 450/2211] Batch: 0.0890 (0.1024) Data: 0.0016 (0.0044) Loss: 0.1642 (0.0702)
[2023/01/16 01:30] | TRAIN(031): [ 500/2211] Batch: 0.0956 (0.1013) Data: 0.0019 (0.0041) Loss: 0.1529 (0.0704)
[2023/01/16 01:30] | TRAIN(031): [ 550/2211] Batch: 0.0938 (0.1005) Data: 0.0018 (0.0039) Loss: 0.1273 (0.0691)
[2023/01/16 01:30] | TRAIN(031): [ 600/2211] Batch: 0.0920 (0.1001) Data: 0.0019 (0.0037) Loss: 0.1410 (0.0676)
[2023/01/16 01:30] | TRAIN(031): [ 650/2211] Batch: 0.0881 (0.0996) Data: 0.0014 (0.0036) Loss: 0.0378 (0.0666)
[2023/01/16 01:30] | TRAIN(031): [ 700/2211] Batch: 0.1247 (0.0991) Data: 0.0022 (0.0034) Loss: 0.0022 (0.0651)
[2023/01/16 01:30] | TRAIN(031): [ 750/2211] Batch: 0.1278 (0.0993) Data: 0.0021 (0.0033) Loss: 0.0438 (0.0657)
[2023/01/16 01:30] | TRAIN(031): [ 800/2211] Batch: 0.0951 (0.0989) Data: 0.0018 (0.0032) Loss: 0.0065 (0.0646)
[2023/01/16 01:30] | TRAIN(031): [ 850/2211] Batch: 0.1418 (0.0987) Data: 0.0022 (0.0032) Loss: 0.0476 (0.0641)
[2023/01/16 01:30] | TRAIN(031): [ 900/2211] Batch: 0.0877 (0.0987) Data: 0.0017 (0.0031) Loss: 0.0016 (0.0643)
[2023/01/16 01:30] | TRAIN(031): [ 950/2211] Batch: 0.0977 (0.0985) Data: 0.0016 (0.0030) Loss: 0.0499 (0.0638)
[2023/01/16 01:31] | TRAIN(031): [1000/2211] Batch: 0.0911 (0.0983) Data: 0.0019 (0.0029) Loss: 0.2155 (0.0640)
[2023/01/16 01:31] | TRAIN(031): [1050/2211] Batch: 0.0917 (0.0980) Data: 0.0017 (0.0029) Loss: 0.0944 (0.0635)
[2023/01/16 01:31] | TRAIN(031): [1100/2211] Batch: 0.0893 (0.0977) Data: 0.0020 (0.0028) Loss: 0.1187 (0.0633)
[2023/01/16 01:31] | TRAIN(031): [1150/2211] Batch: 0.1045 (0.0974) Data: 0.0015 (0.0028) Loss: 0.0000 (0.0630)
[2023/01/16 01:31] | TRAIN(031): [1200/2211] Batch: 0.0997 (0.0973) Data: 0.0020 (0.0028) Loss: 0.0000 (0.0627)
[2023/01/16 01:31] | TRAIN(031): [1250/2211] Batch: 0.0886 (0.0970) Data: 0.0018 (0.0027) Loss: 0.0103 (0.0613)
[2023/01/16 01:31] | TRAIN(031): [1300/2211] Batch: 0.0995 (0.0968) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0606)
[2023/01/16 01:31] | TRAIN(031): [1350/2211] Batch: 0.0911 (0.0967) Data: 0.0017 (0.0027) Loss: 0.0807 (0.0599)
[2023/01/16 01:31] | TRAIN(031): [1400/2211] Batch: 0.0985 (0.0965) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0597)
[2023/01/16 01:31] | TRAIN(031): [1450/2211] Batch: 0.0906 (0.0964) Data: 0.0019 (0.0026) Loss: 0.0043 (0.0593)
[2023/01/16 01:31] | TRAIN(031): [1500/2211] Batch: 0.0848 (0.0963) Data: 0.0018 (0.0026) Loss: 0.0301 (0.0590)
[2023/01/16 01:31] | TRAIN(031): [1550/2211] Batch: 0.0999 (0.0961) Data: 0.0017 (0.0025) Loss: 0.1124 (0.0607)
[2023/01/16 01:31] | TRAIN(031): [1600/2211] Batch: 0.1180 (0.0962) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0625)
[2023/01/16 01:32] | TRAIN(031): [1650/2211] Batch: 0.1011 (0.0961) Data: 0.0018 (0.0025) Loss: 0.1656 (0.0636)
[2023/01/16 01:32] | TRAIN(031): [1700/2211] Batch: 0.1181 (0.0966) Data: 0.0021 (0.0025) Loss: 0.0000 (0.0637)
[2023/01/16 01:32] | TRAIN(031): [1750/2211] Batch: 0.0900 (0.0966) Data: 0.0017 (0.0025) Loss: 0.0907 (0.0637)
[2023/01/16 01:32] | TRAIN(031): [1800/2211] Batch: 0.0908 (0.0965) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0637)
[2023/01/16 01:32] | TRAIN(031): [1850/2211] Batch: 0.0873 (0.0964) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0636)
[2023/01/16 01:32] | TRAIN(031): [1900/2211] Batch: 0.0970 (0.0963) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0633)
[2023/01/16 01:32] | TRAIN(031): [1950/2211] Batch: 0.0917 (0.0962) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0630)
[2023/01/16 01:32] | TRAIN(031): [2000/2211] Batch: 0.0887 (0.0961) Data: 0.0017 (0.0024) Loss: 0.0736 (0.0629)
[2023/01/16 01:32] | TRAIN(031): [2050/2211] Batch: 0.0880 (0.0960) Data: 0.0017 (0.0024) Loss: 0.0039 (0.0626)
[2023/01/16 01:32] | TRAIN(031): [2100/2211] Batch: 0.0832 (0.0959) Data: 0.0018 (0.0024) Loss: 0.0562 (0.0626)
[2023/01/16 01:32] | TRAIN(031): [2150/2211] Batch: 0.1260 (0.0961) Data: 0.0021 (0.0023) Loss: 0.0588 (0.0627)
[2023/01/16 01:32] | TRAIN(031): [2200/2211] Batch: 0.0869 (0.0962) Data: 0.0016 (0.0023) Loss: 0.1047 (0.0625)
[2023/01/16 01:32] | ------------------------------------------------------------
[2023/01/16 01:32] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:32] | ------------------------------------------------------------
[2023/01/16 01:32] |    TRAIN(31)     0:03:32     0:00:05     0:03:27      0.0625
[2023/01/16 01:32] | ------------------------------------------------------------
[2023/01/16 01:32] | **************************************************
[2023/01/16 01:33] | TRAIN(032): [  50/2211] Batch: 0.0890 (0.1231) Data: 0.0018 (0.0243) Loss: 0.1110 (0.0383)
[2023/01/16 01:33] | TRAIN(032): [ 100/2211] Batch: 0.0956 (0.1075) Data: 0.0018 (0.0131) Loss: 0.0226 (0.0563)
[2023/01/16 01:33] | TRAIN(032): [ 150/2211] Batch: 0.0912 (0.1019) Data: 0.0016 (0.0094) Loss: 0.0072 (0.0525)
[2023/01/16 01:33] | TRAIN(032): [ 200/2211] Batch: 0.1234 (0.1045) Data: 0.0018 (0.0076) Loss: 0.0000 (0.0525)
[2023/01/16 01:33] | TRAIN(032): [ 250/2211] Batch: 0.1268 (0.1067) Data: 0.0023 (0.0065) Loss: 0.0000 (0.0517)
[2023/01/16 01:33] | TRAIN(032): [ 300/2211] Batch: 0.0992 (0.1092) Data: 0.0022 (0.0058) Loss: 0.0000 (0.0504)
[2023/01/16 01:33] | TRAIN(032): [ 350/2211] Batch: 0.0911 (0.1093) Data: 0.0018 (0.0052) Loss: 0.0000 (0.0511)
[2023/01/16 01:33] | TRAIN(032): [ 400/2211] Batch: 0.0877 (0.1074) Data: 0.0018 (0.0048) Loss: 0.0409 (0.0533)
[2023/01/16 01:33] | TRAIN(032): [ 450/2211] Batch: 0.0880 (0.1056) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0540)
[2023/01/16 01:33] | TRAIN(032): [ 500/2211] Batch: 0.0925 (0.1048) Data: 0.0018 (0.0042) Loss: 0.0292 (0.0542)
[2023/01/16 01:33] | TRAIN(032): [ 550/2211] Batch: 0.0966 (0.1035) Data: 0.0017 (0.0040) Loss: 0.0000 (0.0549)
[2023/01/16 01:33] | TRAIN(032): [ 600/2211] Batch: 0.0872 (0.1027) Data: 0.0018 (0.0038) Loss: 0.1354 (0.0549)
[2023/01/16 01:34] | TRAIN(032): [ 650/2211] Batch: 0.0874 (0.1023) Data: 0.0018 (0.0037) Loss: 0.2103 (0.0553)
[2023/01/16 01:34] | TRAIN(032): [ 700/2211] Batch: 0.0978 (0.1017) Data: 0.0019 (0.0035) Loss: 0.0000 (0.0561)
[2023/01/16 01:34] | TRAIN(032): [ 750/2211] Batch: 0.0959 (0.1014) Data: 0.0018 (0.0034) Loss: 0.1914 (0.0562)
[2023/01/16 01:34] | TRAIN(032): [ 800/2211] Batch: 0.1011 (0.1009) Data: 0.0017 (0.0033) Loss: 0.2112 (0.0567)
[2023/01/16 01:34] | TRAIN(032): [ 850/2211] Batch: 0.0903 (0.1005) Data: 0.0018 (0.0032) Loss: 0.3076 (0.0580)
[2023/01/16 01:34] | TRAIN(032): [ 900/2211] Batch: 0.0898 (0.1000) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0587)
[2023/01/16 01:34] | TRAIN(032): [ 950/2211] Batch: 0.0946 (0.0996) Data: 0.0017 (0.0031) Loss: 0.0923 (0.0589)
[2023/01/16 01:34] | TRAIN(032): [1000/2211] Batch: 0.0891 (0.0995) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0587)
[2023/01/16 01:34] | TRAIN(032): [1050/2211] Batch: 0.0875 (0.0998) Data: 0.0017 (0.0030) Loss: 0.2492 (0.0592)
[2023/01/16 01:34] | TRAIN(032): [1100/2211] Batch: 0.0970 (0.0994) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0600)
[2023/01/16 01:34] | TRAIN(032): [1150/2211] Batch: 0.0988 (0.0997) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0604)
[2023/01/16 01:34] | TRAIN(032): [1200/2211] Batch: 0.0909 (0.0995) Data: 0.0018 (0.0028) Loss: 0.0639 (0.0605)
[2023/01/16 01:35] | TRAIN(032): [1250/2211] Batch: 0.0947 (0.0992) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0615)
[2023/01/16 01:35] | TRAIN(032): [1300/2211] Batch: 0.0969 (0.0991) Data: 0.0018 (0.0028) Loss: 0.2085 (0.0614)
[2023/01/16 01:35] | TRAIN(032): [1350/2211] Batch: 0.0889 (0.0992) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0609)
[2023/01/16 01:35] | TRAIN(032): [1400/2211] Batch: 0.1001 (0.0989) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0609)
[2023/01/16 01:35] | TRAIN(032): [1450/2211] Batch: 0.0937 (0.0987) Data: 0.0019 (0.0027) Loss: 0.0166 (0.0604)
[2023/01/16 01:35] | TRAIN(032): [1500/2211] Batch: 0.0900 (0.0989) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0604)
[2023/01/16 01:35] | TRAIN(032): [1550/2211] Batch: 0.0900 (0.0988) Data: 0.0018 (0.0026) Loss: 0.0729 (0.0602)
[2023/01/16 01:35] | TRAIN(032): [1600/2211] Batch: 0.0890 (0.0986) Data: 0.0017 (0.0026) Loss: 0.1356 (0.0597)
[2023/01/16 01:35] | TRAIN(032): [1650/2211] Batch: 0.0915 (0.0985) Data: 0.0018 (0.0026) Loss: 0.0027 (0.0601)
[2023/01/16 01:35] | TRAIN(032): [1700/2211] Batch: 0.1211 (0.0983) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0599)
[2023/01/16 01:35] | TRAIN(032): [1750/2211] Batch: 0.1179 (0.0989) Data: 0.0021 (0.0025) Loss: 0.0000 (0.0597)
[2023/01/16 01:35] | TRAIN(032): [1800/2211] Batch: 0.0882 (0.0990) Data: 0.0018 (0.0025) Loss: 0.0339 (0.0598)
[2023/01/16 01:36] | TRAIN(032): [1850/2211] Batch: 0.0882 (0.0987) Data: 0.0018 (0.0025) Loss: 0.0613 (0.0594)
[2023/01/16 01:36] | TRAIN(032): [1900/2211] Batch: 0.1232 (0.0988) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0588)
[2023/01/16 01:36] | TRAIN(032): [1950/2211] Batch: 0.1004 (0.0987) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0588)
[2023/01/16 01:36] | TRAIN(032): [2000/2211] Batch: 0.0879 (0.0990) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0584)
[2023/01/16 01:36] | TRAIN(032): [2050/2211] Batch: 0.0846 (0.0989) Data: 0.0018 (0.0024) Loss: 0.0688 (0.0596)
[2023/01/16 01:36] | TRAIN(032): [2100/2211] Batch: 0.0939 (0.0991) Data: 0.0018 (0.0024) Loss: 0.0347 (0.0599)
[2023/01/16 01:36] | TRAIN(032): [2150/2211] Batch: 0.0889 (0.0990) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0600)
[2023/01/16 01:36] | TRAIN(032): [2200/2211] Batch: 0.0864 (0.0988) Data: 0.0016 (0.0024) Loss: 0.0134 (0.0598)
[2023/01/16 01:36] | ------------------------------------------------------------
[2023/01/16 01:36] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:36] | ------------------------------------------------------------
[2023/01/16 01:36] |    TRAIN(32)     0:03:38     0:00:05     0:03:33      0.0598
[2023/01/16 01:36] | ------------------------------------------------------------
[2023/01/16 01:36] | **************************************************
[2023/01/16 01:36] | TRAIN(033): [  50/2211] Batch: 0.1249 (0.1519) Data: 0.0022 (0.0264) Loss: 0.0672 (0.0624)
[2023/01/16 01:36] | TRAIN(033): [ 100/2211] Batch: 0.1270 (0.1328) Data: 0.0022 (0.0144) Loss: 0.0000 (0.0660)
[2023/01/16 01:36] | TRAIN(033): [ 150/2211] Batch: 0.0906 (0.1243) Data: 0.0017 (0.0103) Loss: 0.0946 (0.0729)
[2023/01/16 01:36] | TRAIN(033): [ 200/2211] Batch: 0.0909 (0.1168) Data: 0.0017 (0.0082) Loss: 0.0000 (0.0721)
[2023/01/16 01:37] | TRAIN(033): [ 250/2211] Batch: 0.1045 (0.1130) Data: 0.0019 (0.0069) Loss: 0.2284 (0.0714)
[2023/01/16 01:37] | TRAIN(033): [ 300/2211] Batch: 0.0944 (0.1094) Data: 0.0019 (0.0060) Loss: 0.0917 (0.0690)
[2023/01/16 01:37] | TRAIN(033): [ 350/2211] Batch: 0.0955 (0.1075) Data: 0.0017 (0.0054) Loss: 0.0000 (0.0663)
[2023/01/16 01:37] | TRAIN(033): [ 400/2211] Batch: 0.0913 (0.1059) Data: 0.0018 (0.0050) Loss: 0.0000 (0.0667)
[2023/01/16 01:37] | TRAIN(033): [ 450/2211] Batch: 0.1004 (0.1049) Data: 0.0019 (0.0046) Loss: 0.0298 (0.0661)
[2023/01/16 01:37] | TRAIN(033): [ 500/2211] Batch: 0.1183 (0.1065) Data: 0.0022 (0.0044) Loss: 0.1483 (0.0647)
[2023/01/16 01:37] | TRAIN(033): [ 550/2211] Batch: 0.0914 (0.1073) Data: 0.0017 (0.0042) Loss: 0.0797 (0.0624)
[2023/01/16 01:37] | TRAIN(033): [ 600/2211] Batch: 0.1175 (0.1080) Data: 0.0022 (0.0040) Loss: 0.0933 (0.0609)
[2023/01/16 01:37] | TRAIN(033): [ 650/2211] Batch: 0.0878 (0.1070) Data: 0.0017 (0.0038) Loss: 0.0854 (0.0618)
[2023/01/16 01:37] | TRAIN(033): [ 700/2211] Batch: 0.0896 (0.1059) Data: 0.0019 (0.0037) Loss: 0.0000 (0.0626)
[2023/01/16 01:37] | TRAIN(033): [ 750/2211] Batch: 0.0836 (0.1063) Data: 0.0017 (0.0036) Loss: 0.1113 (0.0631)
[2023/01/16 01:38] | TRAIN(033): [ 800/2211] Batch: 0.0959 (0.1058) Data: 0.0018 (0.0035) Loss: 0.0993 (0.0625)
[2023/01/16 01:38] | TRAIN(033): [ 850/2211] Batch: 0.0862 (0.1050) Data: 0.0016 (0.0034) Loss: 0.0000 (0.0627)
[2023/01/16 01:38] | TRAIN(033): [ 900/2211] Batch: 0.0844 (0.1043) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0632)
[2023/01/16 01:38] | TRAIN(033): [ 950/2211] Batch: 0.0900 (0.1040) Data: 0.0018 (0.0032) Loss: 0.2425 (0.0624)
[2023/01/16 01:38] | TRAIN(033): [1000/2211] Batch: 0.1040 (0.1034) Data: 0.0018 (0.0032) Loss: 0.0415 (0.0622)
[2023/01/16 01:38] | TRAIN(033): [1050/2211] Batch: 0.0899 (0.1029) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0619)
[2023/01/16 01:38] | TRAIN(033): [1100/2211] Batch: 0.0896 (0.1024) Data: 0.0018 (0.0030) Loss: 0.0173 (0.0623)
[2023/01/16 01:38] | TRAIN(033): [1150/2211] Batch: 0.1167 (0.1021) Data: 0.0021 (0.0030) Loss: 0.1351 (0.0626)
[2023/01/16 01:38] | TRAIN(033): [1200/2211] Batch: 0.0921 (0.1019) Data: 0.0018 (0.0029) Loss: 0.0798 (0.0623)
[2023/01/16 01:38] | TRAIN(033): [1250/2211] Batch: 0.0910 (0.1015) Data: 0.0018 (0.0029) Loss: 0.1037 (0.0624)
[2023/01/16 01:38] | TRAIN(033): [1300/2211] Batch: 0.0930 (0.1013) Data: 0.0020 (0.0028) Loss: 0.0000 (0.0618)
[2023/01/16 01:38] | TRAIN(033): [1350/2211] Batch: 0.0968 (0.1010) Data: 0.0017 (0.0028) Loss: 0.1401 (0.0614)
[2023/01/16 01:38] | TRAIN(033): [1400/2211] Batch: 0.1053 (0.1007) Data: 0.0019 (0.0028) Loss: 0.0951 (0.0606)
[2023/01/16 01:39] | TRAIN(033): [1450/2211] Batch: 0.0962 (0.1006) Data: 0.0016 (0.0027) Loss: 0.2137 (0.0604)
[2023/01/16 01:39] | TRAIN(033): [1500/2211] Batch: 0.0884 (0.1003) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0602)
[2023/01/16 01:39] | TRAIN(033): [1550/2211] Batch: 0.1181 (0.1003) Data: 0.0022 (0.0027) Loss: 0.0000 (0.0607)
[2023/01/16 01:39] | TRAIN(033): [1600/2211] Batch: 0.0994 (0.1005) Data: 0.0019 (0.0027) Loss: 0.0266 (0.0603)
[2023/01/16 01:39] | TRAIN(033): [1650/2211] Batch: 0.0977 (0.1004) Data: 0.0018 (0.0026) Loss: 0.2280 (0.0605)
[2023/01/16 01:39] | TRAIN(033): [1700/2211] Batch: 0.0895 (0.1002) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0601)
[2023/01/16 01:39] | TRAIN(033): [1750/2211] Batch: 0.0934 (0.0999) Data: 0.0018 (0.0026) Loss: 0.0718 (0.0605)
[2023/01/16 01:39] | TRAIN(033): [1800/2211] Batch: 0.0933 (0.1000) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0610)
[2023/01/16 01:39] | TRAIN(033): [1850/2211] Batch: 0.0932 (0.0998) Data: 0.0017 (0.0025) Loss: 0.2286 (0.0615)
[2023/01/16 01:39] | TRAIN(033): [1900/2211] Batch: 0.1024 (0.0998) Data: 0.0018 (0.0025) Loss: 0.0544 (0.0616)
[2023/01/16 01:39] | TRAIN(033): [1950/2211] Batch: 0.1009 (0.0997) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0620)
[2023/01/16 01:39] | TRAIN(033): [2000/2211] Batch: 0.0952 (0.0996) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0621)
[2023/01/16 01:40] | TRAIN(033): [2050/2211] Batch: 0.0957 (0.0995) Data: 0.0018 (0.0025) Loss: 0.0330 (0.0621)
[2023/01/16 01:40] | TRAIN(033): [2100/2211] Batch: 0.1129 (0.0994) Data: 0.0020 (0.0024) Loss: 0.0045 (0.0621)
[2023/01/16 01:40] | TRAIN(033): [2150/2211] Batch: 0.0945 (0.0993) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0619)
[2023/01/16 01:40] | TRAIN(033): [2200/2211] Batch: 0.0878 (0.0991) Data: 0.0016 (0.0024) Loss: 0.1081 (0.0618)
[2023/01/16 01:40] | ------------------------------------------------------------
[2023/01/16 01:40] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:40] | ------------------------------------------------------------
[2023/01/16 01:40] |    TRAIN(33)     0:03:39     0:00:05     0:03:33      0.0616
[2023/01/16 01:40] | ------------------------------------------------------------
[2023/01/16 01:40] | **************************************************
[2023/01/16 01:40] | TRAIN(034): [  50/2211] Batch: 0.0867 (0.1205) Data: 0.0017 (0.0249) Loss: 0.0001 (0.0330)
[2023/01/16 01:40] | TRAIN(034): [ 100/2211] Batch: 0.0874 (0.1096) Data: 0.0018 (0.0134) Loss: 0.0000 (0.0451)
[2023/01/16 01:40] | TRAIN(034): [ 150/2211] Batch: 0.0877 (0.1035) Data: 0.0018 (0.0096) Loss: 0.0000 (0.0465)
[2023/01/16 01:40] | TRAIN(034): [ 200/2211] Batch: 0.0979 (0.1005) Data: 0.0020 (0.0076) Loss: 0.1643 (0.0522)
[2023/01/16 01:40] | TRAIN(034): [ 250/2211] Batch: 0.1167 (0.1040) Data: 0.0019 (0.0065) Loss: 0.0000 (0.0655)
[2023/01/16 01:40] | TRAIN(034): [ 300/2211] Batch: 0.0882 (0.1037) Data: 0.0017 (0.0057) Loss: 0.0579 (0.0680)
[2023/01/16 01:40] | TRAIN(034): [ 350/2211] Batch: 0.0941 (0.1023) Data: 0.0018 (0.0052) Loss: 0.0434 (0.0683)
[2023/01/16 01:40] | TRAIN(034): [ 400/2211] Batch: 0.1173 (0.1019) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0677)
[2023/01/16 01:41] | TRAIN(034): [ 450/2211] Batch: 0.1186 (0.1030) Data: 0.0020 (0.0044) Loss: 0.1286 (0.0692)
[2023/01/16 01:41] | TRAIN(034): [ 500/2211] Batch: 0.0889 (0.1022) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0711)
[2023/01/16 01:41] | TRAIN(034): [ 550/2211] Batch: 0.1177 (0.1021) Data: 0.0021 (0.0040) Loss: 0.0278 (0.0714)
[2023/01/16 01:41] | TRAIN(034): [ 600/2211] Batch: 0.1220 (0.1031) Data: 0.0021 (0.0038) Loss: 0.0000 (0.0691)
[2023/01/16 01:41] | TRAIN(034): [ 650/2211] Batch: 0.0885 (0.1028) Data: 0.0017 (0.0037) Loss: 0.0001 (0.0686)
[2023/01/16 01:41] | TRAIN(034): [ 700/2211] Batch: 0.0908 (0.1020) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0694)
[2023/01/16 01:41] | TRAIN(034): [ 750/2211] Batch: 0.1175 (0.1025) Data: 0.0021 (0.0034) Loss: 0.0000 (0.0685)
[2023/01/16 01:41] | TRAIN(034): [ 800/2211] Batch: 0.0963 (0.1023) Data: 0.0019 (0.0033) Loss: 0.0000 (0.0668)
[2023/01/16 01:41] | TRAIN(034): [ 850/2211] Batch: 0.0876 (0.1017) Data: 0.0017 (0.0032) Loss: 0.0038 (0.0663)
[2023/01/16 01:41] | TRAIN(034): [ 900/2211] Batch: 0.1171 (0.1012) Data: 0.0020 (0.0031) Loss: 0.0555 (0.0653)
[2023/01/16 01:41] | TRAIN(034): [ 950/2211] Batch: 0.1296 (0.1007) Data: 0.0019 (0.0031) Loss: 0.0126 (0.0642)
[2023/01/16 01:41] | TRAIN(034): [1000/2211] Batch: 0.0879 (0.1005) Data: 0.0018 (0.0030) Loss: 0.0053 (0.0627)
[2023/01/16 01:42] | TRAIN(034): [1050/2211] Batch: 0.0994 (0.1001) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0626)
[2023/01/16 01:42] | TRAIN(034): [1100/2211] Batch: 0.0930 (0.0996) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0626)
[2023/01/16 01:42] | TRAIN(034): [1150/2211] Batch: 0.0893 (0.0993) Data: 0.0017 (0.0028) Loss: 0.5676 (0.0627)
[2023/01/16 01:42] | TRAIN(034): [1200/2211] Batch: 0.0932 (0.0991) Data: 0.0017 (0.0028) Loss: 0.0554 (0.0623)
[2023/01/16 01:42] | TRAIN(034): [1250/2211] Batch: 0.0907 (0.0989) Data: 0.0016 (0.0028) Loss: 0.1245 (0.0622)
[2023/01/16 01:42] | TRAIN(034): [1300/2211] Batch: 0.0878 (0.0986) Data: 0.0015 (0.0027) Loss: 0.1605 (0.0627)
[2023/01/16 01:42] | TRAIN(034): [1350/2211] Batch: 0.1277 (0.0989) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0618)
[2023/01/16 01:42] | TRAIN(034): [1400/2211] Batch: 0.1228 (0.0988) Data: 0.0021 (0.0027) Loss: 0.0777 (0.0623)
[2023/01/16 01:42] | TRAIN(034): [1450/2211] Batch: 0.1178 (0.0995) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0629)
[2023/01/16 01:42] | TRAIN(034): [1500/2211] Batch: 0.1001 (0.1001) Data: 0.0017 (0.0026) Loss: 0.0849 (0.0631)
[2023/01/16 01:42] | TRAIN(034): [1550/2211] Batch: 0.0985 (0.0999) Data: 0.0018 (0.0026) Loss: 0.1190 (0.0626)
[2023/01/16 01:42] | TRAIN(034): [1600/2211] Batch: 0.0882 (0.0997) Data: 0.0018 (0.0026) Loss: 0.0067 (0.0626)
[2023/01/16 01:42] | TRAIN(034): [1650/2211] Batch: 0.0881 (0.0996) Data: 0.0016 (0.0025) Loss: 0.0034 (0.0622)
[2023/01/16 01:43] | TRAIN(034): [1700/2211] Batch: 0.0876 (0.0996) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0621)
[2023/01/16 01:43] | TRAIN(034): [1750/2211] Batch: 0.0983 (0.0993) Data: 0.0018 (0.0025) Loss: 0.2206 (0.0622)
[2023/01/16 01:43] | TRAIN(034): [1800/2211] Batch: 0.0949 (0.0994) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0621)
[2023/01/16 01:43] | TRAIN(034): [1850/2211] Batch: 0.0916 (0.0994) Data: 0.0016 (0.0025) Loss: 0.1476 (0.0621)
[2023/01/16 01:43] | TRAIN(034): [1900/2211] Batch: 0.1184 (0.0995) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0620)
[2023/01/16 01:43] | TRAIN(034): [1950/2211] Batch: 0.1189 (0.0998) Data: 0.0019 (0.0024) Loss: 0.1151 (0.0622)
[2023/01/16 01:43] | TRAIN(034): [2000/2211] Batch: 0.1181 (0.1001) Data: 0.0021 (0.0024) Loss: 0.0288 (0.0622)
[2023/01/16 01:43] | TRAIN(034): [2050/2211] Batch: 0.0974 (0.1001) Data: 0.0018 (0.0024) Loss: 0.0262 (0.0618)
[2023/01/16 01:43] | TRAIN(034): [2100/2211] Batch: 0.0887 (0.1002) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0617)
[2023/01/16 01:43] | TRAIN(034): [2150/2211] Batch: 0.0922 (0.1001) Data: 0.0014 (0.0024) Loss: 0.0165 (0.0615)
[2023/01/16 01:43] | TRAIN(034): [2200/2211] Batch: 0.0870 (0.1003) Data: 0.0015 (0.0023) Loss: 0.0000 (0.0616)
[2023/01/16 01:43] | ------------------------------------------------------------
[2023/01/16 01:43] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:43] | ------------------------------------------------------------
[2023/01/16 01:43] |    TRAIN(34)     0:03:41     0:00:05     0:03:36      0.0619
[2023/01/16 01:43] | ------------------------------------------------------------
[2023/01/16 01:43] | **************************************************
[2023/01/16 01:44] | TRAIN(035): [  50/2211] Batch: 0.0994 (0.1232) Data: 0.0019 (0.0209) Loss: 0.2191 (0.0510)
[2023/01/16 01:44] | TRAIN(035): [ 100/2211] Batch: 0.0955 (0.1140) Data: 0.0018 (0.0114) Loss: 0.0053 (0.0589)
[2023/01/16 01:44] | TRAIN(035): [ 150/2211] Batch: 0.1098 (0.1174) Data: 0.0018 (0.0084) Loss: 0.0123 (0.0612)
[2023/01/16 01:44] | TRAIN(035): [ 200/2211] Batch: 0.1001 (0.1220) Data: 0.0014 (0.0068) Loss: 0.0836 (0.0606)
[2023/01/16 01:44] | TRAIN(035): [ 250/2211] Batch: 0.1371 (0.1224) Data: 0.0023 (0.0059) Loss: 0.0029 (0.0594)
[2023/01/16 01:44] | TRAIN(035): [ 300/2211] Batch: 0.1184 (0.1217) Data: 0.0023 (0.0053) Loss: 0.0000 (0.0606)
[2023/01/16 01:44] | TRAIN(035): [ 350/2211] Batch: 0.1261 (0.1217) Data: 0.0022 (0.0048) Loss: 0.0000 (0.0583)
[2023/01/16 01:44] | TRAIN(035): [ 400/2211] Batch: 0.1225 (0.1206) Data: 0.0022 (0.0045) Loss: 0.1843 (0.0582)
[2023/01/16 01:44] | TRAIN(035): [ 450/2211] Batch: 0.1282 (0.1201) Data: 0.0022 (0.0042) Loss: 0.0000 (0.0566)
[2023/01/16 01:44] | TRAIN(035): [ 500/2211] Batch: 0.1003 (0.1177) Data: 0.0019 (0.0040) Loss: 0.0000 (0.0556)
[2023/01/16 01:45] | TRAIN(035): [ 550/2211] Batch: 0.0902 (0.1156) Data: 0.0018 (0.0038) Loss: 0.0194 (0.0549)
[2023/01/16 01:45] | TRAIN(035): [ 600/2211] Batch: 0.0952 (0.1138) Data: 0.0019 (0.0036) Loss: 0.0000 (0.0548)
[2023/01/16 01:45] | TRAIN(035): [ 650/2211] Batch: 0.0971 (0.1124) Data: 0.0018 (0.0035) Loss: 0.0828 (0.0552)
[2023/01/16 01:45] | TRAIN(035): [ 700/2211] Batch: 0.0896 (0.1111) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0548)
[2023/01/16 01:45] | TRAIN(035): [ 750/2211] Batch: 0.0923 (0.1102) Data: 0.0018 (0.0033) Loss: 0.0579 (0.0552)
[2023/01/16 01:45] | TRAIN(035): [ 800/2211] Batch: 0.0903 (0.1091) Data: 0.0017 (0.0032) Loss: 0.1636 (0.0559)
[2023/01/16 01:45] | TRAIN(035): [ 850/2211] Batch: 0.0910 (0.1083) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0551)
[2023/01/16 01:45] | TRAIN(035): [ 900/2211] Batch: 0.0904 (0.1082) Data: 0.0018 (0.0031) Loss: 0.1491 (0.0549)
[2023/01/16 01:45] | TRAIN(035): [ 950/2211] Batch: 0.1019 (0.1074) Data: 0.0015 (0.0030) Loss: 0.0000 (0.0548)
[2023/01/16 01:45] | TRAIN(035): [1000/2211] Batch: 0.0886 (0.1067) Data: 0.0017 (0.0029) Loss: 0.1254 (0.0560)
[2023/01/16 01:45] | TRAIN(035): [1050/2211] Batch: 0.1219 (0.1070) Data: 0.0027 (0.0029) Loss: 0.0000 (0.0561)
[2023/01/16 01:45] | TRAIN(035): [1100/2211] Batch: 0.0968 (0.1067) Data: 0.0018 (0.0028) Loss: 0.1058 (0.0555)
[2023/01/16 01:45] | TRAIN(035): [1150/2211] Batch: 0.1180 (0.1064) Data: 0.0022 (0.0028) Loss: 0.0000 (0.0550)
[2023/01/16 01:46] | TRAIN(035): [1200/2211] Batch: 0.0895 (0.1059) Data: 0.0018 (0.0028) Loss: 0.1535 (0.0557)
[2023/01/16 01:46] | TRAIN(035): [1250/2211] Batch: 0.1028 (0.1055) Data: 0.0017 (0.0027) Loss: 0.0001 (0.0560)
[2023/01/16 01:46] | TRAIN(035): [1300/2211] Batch: 0.0884 (0.1053) Data: 0.0018 (0.0027) Loss: 0.0635 (0.0566)
[2023/01/16 01:46] | TRAIN(035): [1350/2211] Batch: 0.1176 (0.1051) Data: 0.0021 (0.0027) Loss: 0.0658 (0.0564)
[2023/01/16 01:46] | TRAIN(035): [1400/2211] Batch: 0.1243 (0.1049) Data: 0.0022 (0.0026) Loss: 0.1375 (0.0560)
[2023/01/16 01:46] | TRAIN(035): [1450/2211] Batch: 0.0945 (0.1044) Data: 0.0017 (0.0026) Loss: 0.2862 (0.0563)
[2023/01/16 01:46] | TRAIN(035): [1500/2211] Batch: 0.0870 (0.1044) Data: 0.0018 (0.0026) Loss: 0.0553 (0.0570)
[2023/01/16 01:46] | TRAIN(035): [1550/2211] Batch: 0.0884 (0.1041) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0572)
[2023/01/16 01:46] | TRAIN(035): [1600/2211] Batch: 0.0906 (0.1037) Data: 0.0018 (0.0025) Loss: 0.0082 (0.0573)
[2023/01/16 01:46] | TRAIN(035): [1650/2211] Batch: 0.0895 (0.1034) Data: 0.0017 (0.0025) Loss: 0.0579 (0.0575)
[2023/01/16 01:46] | TRAIN(035): [1700/2211] Batch: 0.0811 (0.1028) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0578)
[2023/01/16 01:46] | TRAIN(035): [1750/2211] Batch: 0.0828 (0.1023) Data: 0.0018 (0.0025) Loss: 0.0045 (0.0577)
[2023/01/16 01:47] | TRAIN(035): [1800/2211] Batch: 0.0836 (0.1018) Data: 0.0018 (0.0025) Loss: 0.1664 (0.0575)
[2023/01/16 01:47] | TRAIN(035): [1850/2211] Batch: 0.0876 (0.1013) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0574)
[2023/01/16 01:47] | TRAIN(035): [1900/2211] Batch: 0.0894 (0.1011) Data: 0.0017 (0.0024) Loss: 0.2985 (0.0576)
[2023/01/16 01:47] | TRAIN(035): [1950/2211] Batch: 0.0825 (0.1007) Data: 0.0017 (0.0024) Loss: 0.0321 (0.0575)
[2023/01/16 01:47] | TRAIN(035): [2000/2211] Batch: 0.1014 (0.1005) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0575)
[2023/01/16 01:47] | TRAIN(035): [2050/2211] Batch: 0.0976 (0.1009) Data: 0.0017 (0.0024) Loss: 0.2287 (0.0575)
[2023/01/16 01:47] | TRAIN(035): [2100/2211] Batch: 0.0979 (0.1008) Data: 0.0019 (0.0024) Loss: 0.0394 (0.0576)
[2023/01/16 01:47] | TRAIN(035): [2150/2211] Batch: 0.1266 (0.1008) Data: 0.0022 (0.0024) Loss: 0.0495 (0.0576)
[2023/01/16 01:47] | TRAIN(035): [2200/2211] Batch: 0.1015 (0.1009) Data: 0.0016 (0.0024) Loss: 0.1596 (0.0574)
[2023/01/16 01:47] | ------------------------------------------------------------
[2023/01/16 01:47] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:47] | ------------------------------------------------------------
[2023/01/16 01:47] |    TRAIN(35)     0:03:42     0:00:05     0:03:37      0.0572
[2023/01/16 01:47] | ------------------------------------------------------------
[2023/01/16 01:47] | **************************************************
[2023/01/16 01:47] | TRAIN(036): [  50/2211] Batch: 0.0996 (0.1255) Data: 0.0019 (0.0247) Loss: 0.2232 (0.0741)
[2023/01/16 01:47] | TRAIN(036): [ 100/2211] Batch: 0.0978 (0.1095) Data: 0.0019 (0.0134) Loss: 0.0000 (0.0600)
[2023/01/16 01:47] | TRAIN(036): [ 150/2211] Batch: 0.1100 (0.1058) Data: 0.0021 (0.0096) Loss: 0.2719 (0.0573)
[2023/01/16 01:48] | TRAIN(036): [ 200/2211] Batch: 0.0909 (0.1063) Data: 0.0018 (0.0077) Loss: 0.0989 (0.0561)
[2023/01/16 01:48] | TRAIN(036): [ 250/2211] Batch: 0.0941 (0.1035) Data: 0.0018 (0.0066) Loss: 0.0025 (0.0533)
[2023/01/16 01:48] | TRAIN(036): [ 300/2211] Batch: 0.1254 (0.1049) Data: 0.0021 (0.0058) Loss: 0.1432 (0.0536)
[2023/01/16 01:48] | TRAIN(036): [ 350/2211] Batch: 0.0893 (0.1046) Data: 0.0018 (0.0053) Loss: 0.0000 (0.0552)
[2023/01/16 01:48] | TRAIN(036): [ 400/2211] Batch: 0.1129 (0.1040) Data: 0.0018 (0.0049) Loss: 0.0312 (0.0556)
[2023/01/16 01:48] | TRAIN(036): [ 450/2211] Batch: 0.0929 (0.1028) Data: 0.0019 (0.0045) Loss: 0.0000 (0.0558)
[2023/01/16 01:48] | TRAIN(036): [ 500/2211] Batch: 0.0963 (0.1020) Data: 0.0017 (0.0042) Loss: 0.0308 (0.0573)
[2023/01/16 01:48] | TRAIN(036): [ 550/2211] Batch: 0.0980 (0.1010) Data: 0.0018 (0.0040) Loss: 0.1943 (0.0583)
[2023/01/16 01:48] | TRAIN(036): [ 600/2211] Batch: 0.0878 (0.1004) Data: 0.0018 (0.0038) Loss: 0.0989 (0.0583)
[2023/01/16 01:48] | TRAIN(036): [ 650/2211] Batch: 0.0915 (0.1002) Data: 0.0017 (0.0037) Loss: 0.0000 (0.0578)
[2023/01/16 01:48] | TRAIN(036): [ 700/2211] Batch: 0.0987 (0.0999) Data: 0.0017 (0.0035) Loss: 0.1200 (0.0580)
[2023/01/16 01:48] | TRAIN(036): [ 750/2211] Batch: 0.1183 (0.1003) Data: 0.0021 (0.0034) Loss: 0.0000 (0.0586)
[2023/01/16 01:49] | TRAIN(036): [ 800/2211] Batch: 0.0885 (0.1005) Data: 0.0017 (0.0034) Loss: 0.1028 (0.0579)
[2023/01/16 01:49] | TRAIN(036): [ 850/2211] Batch: 0.0984 (0.1009) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0577)
[2023/01/16 01:49] | TRAIN(036): [ 900/2211] Batch: 0.0927 (0.1006) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0578)
[2023/01/16 01:49] | TRAIN(036): [ 950/2211] Batch: 0.1262 (0.1005) Data: 0.0022 (0.0031) Loss: 0.0813 (0.0581)
[2023/01/16 01:49] | TRAIN(036): [1000/2211] Batch: 0.0923 (0.1006) Data: 0.0022 (0.0031) Loss: 0.0000 (0.0577)
[2023/01/16 01:49] | TRAIN(036): [1050/2211] Batch: 0.1316 (0.1007) Data: 0.0025 (0.0030) Loss: 0.1051 (0.0574)
[2023/01/16 01:49] | TRAIN(036): [1100/2211] Batch: 0.0888 (0.1011) Data: 0.0018 (0.0030) Loss: 0.0001 (0.0576)
[2023/01/16 01:49] | TRAIN(036): [1150/2211] Batch: 0.0866 (0.1008) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0572)
[2023/01/16 01:49] | TRAIN(036): [1200/2211] Batch: 0.0891 (0.1005) Data: 0.0018 (0.0029) Loss: 0.0807 (0.0567)
[2023/01/16 01:49] | TRAIN(036): [1250/2211] Batch: 0.1069 (0.1001) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0560)
[2023/01/16 01:49] | TRAIN(036): [1300/2211] Batch: 0.0922 (0.1001) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0554)
[2023/01/16 01:49] | TRAIN(036): [1350/2211] Batch: 0.1011 (0.0999) Data: 0.0017 (0.0028) Loss: 0.0856 (0.0551)
[2023/01/16 01:50] | TRAIN(036): [1400/2211] Batch: 0.1246 (0.1000) Data: 0.0024 (0.0027) Loss: 0.0001 (0.0554)
[2023/01/16 01:50] | TRAIN(036): [1450/2211] Batch: 0.0884 (0.1001) Data: 0.0018 (0.0027) Loss: 0.0391 (0.0560)
[2023/01/16 01:50] | TRAIN(036): [1500/2211] Batch: 0.0977 (0.0998) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0561)
[2023/01/16 01:50] | TRAIN(036): [1550/2211] Batch: 0.1036 (0.0998) Data: 0.0016 (0.0026) Loss: 0.0000 (0.0561)
[2023/01/16 01:50] | TRAIN(036): [1600/2211] Batch: 0.0989 (0.0997) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0558)
[2023/01/16 01:50] | TRAIN(036): [1650/2211] Batch: 0.0912 (0.0995) Data: 0.0018 (0.0026) Loss: 0.3092 (0.0561)
[2023/01/16 01:50] | TRAIN(036): [1700/2211] Batch: 0.0871 (0.0995) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0563)
[2023/01/16 01:50] | TRAIN(036): [1750/2211] Batch: 0.0900 (0.0993) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0557)
[2023/01/16 01:50] | TRAIN(036): [1800/2211] Batch: 0.1029 (0.0991) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0558)
[2023/01/16 01:50] | TRAIN(036): [1850/2211] Batch: 0.1167 (0.0995) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0560)
[2023/01/16 01:50] | TRAIN(036): [1900/2211] Batch: 0.0897 (0.0997) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0561)
[2023/01/16 01:50] | TRAIN(036): [1950/2211] Batch: 0.0899 (0.0996) Data: 0.0017 (0.0025) Loss: 0.1972 (0.0561)
[2023/01/16 01:50] | TRAIN(036): [2000/2211] Batch: 0.0890 (0.0995) Data: 0.0018 (0.0025) Loss: 0.1343 (0.0561)
[2023/01/16 01:51] | TRAIN(036): [2050/2211] Batch: 0.0877 (0.0993) Data: 0.0018 (0.0025) Loss: 0.0942 (0.0560)
[2023/01/16 01:51] | TRAIN(036): [2100/2211] Batch: 0.0966 (0.0992) Data: 0.0017 (0.0024) Loss: 0.0239 (0.0558)
[2023/01/16 01:51] | TRAIN(036): [2150/2211] Batch: 0.0890 (0.0997) Data: 0.0017 (0.0024) Loss: 0.0307 (0.0557)
[2023/01/16 01:51] | TRAIN(036): [2200/2211] Batch: 0.0869 (0.0995) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0559)
[2023/01/16 01:51] | ------------------------------------------------------------
[2023/01/16 01:51] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:51] | ------------------------------------------------------------
[2023/01/16 01:51] |    TRAIN(36)     0:03:40     0:00:05     0:03:34      0.0559
[2023/01/16 01:51] | ------------------------------------------------------------
[2023/01/16 01:51] | **************************************************
[2023/01/16 01:51] | TRAIN(037): [  50/2211] Batch: 0.1280 (0.1519) Data: 0.0021 (0.0237) Loss: 0.0000 (0.0662)
[2023/01/16 01:51] | TRAIN(037): [ 100/2211] Batch: 0.0977 (0.1264) Data: 0.0037 (0.0129) Loss: 0.0000 (0.0706)
[2023/01/16 01:51] | TRAIN(037): [ 150/2211] Batch: 0.1264 (0.1184) Data: 0.0022 (0.0093) Loss: 0.0000 (0.0643)
[2023/01/16 01:51] | TRAIN(037): [ 200/2211] Batch: 0.1232 (0.1200) Data: 0.0023 (0.0075) Loss: 0.0000 (0.0660)
[2023/01/16 01:51] | TRAIN(037): [ 250/2211] Batch: 0.0923 (0.1223) Data: 0.0014 (0.0065) Loss: 0.0000 (0.0638)
[2023/01/16 01:51] | TRAIN(037): [ 300/2211] Batch: 0.1145 (0.1206) Data: 0.0015 (0.0057) Loss: 0.1413 (0.0622)
[2023/01/16 01:52] | TRAIN(037): [ 350/2211] Batch: 0.1149 (0.1198) Data: 0.0017 (0.0051) Loss: 0.0000 (0.0646)
[2023/01/16 01:52] | TRAIN(037): [ 400/2211] Batch: 0.1061 (0.1188) Data: 0.0014 (0.0047) Loss: 0.0372 (0.0628)
[2023/01/16 01:52] | TRAIN(037): [ 450/2211] Batch: 0.1019 (0.1173) Data: 0.0018 (0.0044) Loss: 0.1474 (0.0625)
[2023/01/16 01:52] | TRAIN(037): [ 500/2211] Batch: 0.1102 (0.1168) Data: 0.0016 (0.0041) Loss: 0.0169 (0.0600)
[2023/01/16 01:52] | TRAIN(037): [ 550/2211] Batch: 0.1184 (0.1167) Data: 0.0018 (0.0039) Loss: 0.1636 (0.0603)
[2023/01/16 01:52] | TRAIN(037): [ 600/2211] Batch: 0.1079 (0.1162) Data: 0.0016 (0.0037) Loss: 0.1512 (0.0621)
[2023/01/16 01:52] | TRAIN(037): [ 650/2211] Batch: 0.1161 (0.1165) Data: 0.0016 (0.0036) Loss: 0.0590 (0.0617)
[2023/01/16 01:52] | TRAIN(037): [ 700/2211] Batch: 0.1253 (0.1157) Data: 0.0023 (0.0034) Loss: 0.1946 (0.0638)
[2023/01/16 01:52] | TRAIN(037): [ 750/2211] Batch: 0.0867 (0.1142) Data: 0.0018 (0.0033) Loss: 0.0242 (0.0652)
[2023/01/16 01:52] | TRAIN(037): [ 800/2211] Batch: 0.1305 (0.1135) Data: 0.0024 (0.0032) Loss: 0.0000 (0.0636)
[2023/01/16 01:52] | TRAIN(037): [ 850/2211] Batch: 0.1166 (0.1139) Data: 0.0023 (0.0032) Loss: 0.0000 (0.0631)
[2023/01/16 01:53] | TRAIN(037): [ 900/2211] Batch: 0.1305 (0.1139) Data: 0.0020 (0.0031) Loss: 0.0295 (0.0629)
[2023/01/16 01:53] | TRAIN(037): [ 950/2211] Batch: 0.0985 (0.1134) Data: 0.0018 (0.0031) Loss: 0.1080 (0.0629)
[2023/01/16 01:53] | TRAIN(037): [1000/2211] Batch: 0.1054 (0.1125) Data: 0.0018 (0.0030) Loss: 0.0999 (0.0619)
[2023/01/16 01:53] | TRAIN(037): [1050/2211] Batch: 0.0922 (0.1119) Data: 0.0016 (0.0029) Loss: 0.0075 (0.0618)
[2023/01/16 01:53] | TRAIN(037): [1100/2211] Batch: 0.0881 (0.1111) Data: 0.0019 (0.0029) Loss: 0.0889 (0.0611)
[2023/01/16 01:53] | TRAIN(037): [1150/2211] Batch: 0.0957 (0.1109) Data: 0.0020 (0.0028) Loss: 0.1862 (0.0613)
[2023/01/16 01:53] | TRAIN(037): [1200/2211] Batch: 0.0889 (0.1101) Data: 0.0019 (0.0028) Loss: 0.0814 (0.0605)
[2023/01/16 01:53] | TRAIN(037): [1250/2211] Batch: 0.0880 (0.1095) Data: 0.0018 (0.0028) Loss: 0.0492 (0.0605)
[2023/01/16 01:53] | TRAIN(037): [1300/2211] Batch: 0.1190 (0.1090) Data: 0.0021 (0.0027) Loss: 0.0119 (0.0604)
[2023/01/16 01:53] | TRAIN(037): [1350/2211] Batch: 0.0980 (0.1086) Data: 0.0018 (0.0027) Loss: 0.0118 (0.0607)
[2023/01/16 01:53] | TRAIN(037): [1400/2211] Batch: 0.0890 (0.1080) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0606)
[2023/01/16 01:53] | TRAIN(037): [1450/2211] Batch: 0.0866 (0.1074) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0604)
[2023/01/16 01:54] | TRAIN(037): [1500/2211] Batch: 0.0903 (0.1070) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0603)
[2023/01/16 01:54] | TRAIN(037): [1550/2211] Batch: 0.0997 (0.1065) Data: 0.0019 (0.0026) Loss: 0.0127 (0.0603)
[2023/01/16 01:54] | TRAIN(037): [1600/2211] Batch: 0.0908 (0.1061) Data: 0.0018 (0.0025) Loss: 0.1146 (0.0596)
[2023/01/16 01:54] | TRAIN(037): [1650/2211] Batch: 0.0905 (0.1057) Data: 0.0019 (0.0025) Loss: 0.1102 (0.0597)
[2023/01/16 01:54] | TRAIN(037): [1700/2211] Batch: 0.0875 (0.1052) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0592)
[2023/01/16 01:54] | TRAIN(037): [1750/2211] Batch: 0.0902 (0.1049) Data: 0.0018 (0.0025) Loss: 0.0165 (0.0589)
[2023/01/16 01:54] | TRAIN(037): [1800/2211] Batch: 0.0975 (0.1045) Data: 0.0018 (0.0025) Loss: 0.0232 (0.0583)
[2023/01/16 01:54] | TRAIN(037): [1850/2211] Batch: 0.1093 (0.1043) Data: 0.0019 (0.0024) Loss: 0.0520 (0.0581)
[2023/01/16 01:54] | TRAIN(037): [1900/2211] Batch: 0.1183 (0.1044) Data: 0.0022 (0.0024) Loss: 0.0826 (0.0580)
[2023/01/16 01:54] | TRAIN(037): [1950/2211] Batch: 0.1177 (0.1048) Data: 0.0023 (0.0024) Loss: 0.0000 (0.0579)
[2023/01/16 01:54] | TRAIN(037): [2000/2211] Batch: 0.0878 (0.1045) Data: 0.0018 (0.0024) Loss: 0.0159 (0.0578)
[2023/01/16 01:54] | TRAIN(037): [2050/2211] Batch: 0.0879 (0.1041) Data: 0.0018 (0.0024) Loss: 0.0257 (0.0575)
[2023/01/16 01:54] | TRAIN(037): [2100/2211] Batch: 0.0873 (0.1038) Data: 0.0018 (0.0024) Loss: 0.0933 (0.0573)
[2023/01/16 01:55] | TRAIN(037): [2150/2211] Batch: 0.1039 (0.1035) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0572)
[2023/01/16 01:55] | TRAIN(037): [2200/2211] Batch: 0.1032 (0.1036) Data: 0.0030 (0.0024) Loss: 0.0656 (0.0569)
[2023/01/16 01:55] | ------------------------------------------------------------
[2023/01/16 01:55] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:55] | ------------------------------------------------------------
[2023/01/16 01:55] |    TRAIN(37)     0:03:49     0:00:05     0:03:43      0.0570
[2023/01/16 01:55] | ------------------------------------------------------------
[2023/01/16 01:55] | **************************************************
[2023/01/16 01:55] | TRAIN(038): [  50/2211] Batch: 0.0982 (0.1254) Data: 0.0015 (0.0256) Loss: 0.0441 (0.0421)
[2023/01/16 01:55] | TRAIN(038): [ 100/2211] Batch: 0.0829 (0.1075) Data: 0.0018 (0.0138) Loss: 0.0235 (0.0468)
[2023/01/16 01:55] | TRAIN(038): [ 150/2211] Batch: 0.1261 (0.1039) Data: 0.0023 (0.0099) Loss: 0.0000 (0.0442)
[2023/01/16 01:55] | TRAIN(038): [ 200/2211] Batch: 0.1197 (0.1093) Data: 0.0022 (0.0080) Loss: 0.0000 (0.0520)
[2023/01/16 01:55] | TRAIN(038): [ 250/2211] Batch: 0.1166 (0.1125) Data: 0.0020 (0.0069) Loss: 0.0000 (0.0484)
[2023/01/16 01:55] | TRAIN(038): [ 300/2211] Batch: 0.1175 (0.1141) Data: 0.0022 (0.0061) Loss: 0.0000 (0.0489)
[2023/01/16 01:55] | TRAIN(038): [ 350/2211] Batch: 0.1197 (0.1133) Data: 0.0022 (0.0055) Loss: 0.0031 (0.0471)
[2023/01/16 01:55] | TRAIN(038): [ 400/2211] Batch: 0.0939 (0.1137) Data: 0.0024 (0.0051) Loss: 0.0106 (0.0455)
[2023/01/16 01:56] | TRAIN(038): [ 450/2211] Batch: 0.0873 (0.1140) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0459)
[2023/01/16 01:56] | TRAIN(038): [ 500/2211] Batch: 0.1036 (0.1121) Data: 0.0017 (0.0045) Loss: 0.0000 (0.0465)
[2023/01/16 01:56] | TRAIN(038): [ 550/2211] Batch: 0.0948 (0.1117) Data: 0.0019 (0.0043) Loss: 0.0077 (0.0453)
[2023/01/16 01:56] | TRAIN(038): [ 600/2211] Batch: 0.0957 (0.1101) Data: 0.0017 (0.0041) Loss: 0.0000 (0.0447)
[2023/01/16 01:56] | TRAIN(038): [ 650/2211] Batch: 0.0889 (0.1087) Data: 0.0018 (0.0039) Loss: 0.0789 (0.0445)
[2023/01/16 01:56] | TRAIN(038): [ 700/2211] Batch: 0.0957 (0.1075) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0461)
[2023/01/16 01:56] | TRAIN(038): [ 750/2211] Batch: 0.0880 (0.1065) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0472)
[2023/01/16 01:56] | TRAIN(038): [ 800/2211] Batch: 0.0882 (0.1055) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0471)
[2023/01/16 01:56] | TRAIN(038): [ 850/2211] Batch: 0.1048 (0.1050) Data: 0.0018 (0.0034) Loss: 0.0271 (0.0464)
[2023/01/16 01:56] | TRAIN(038): [ 900/2211] Batch: 0.0952 (0.1042) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0471)
[2023/01/16 01:56] | TRAIN(038): [ 950/2211] Batch: 0.0877 (0.1039) Data: 0.0018 (0.0032) Loss: 0.0490 (0.0481)
[2023/01/16 01:56] | TRAIN(038): [1000/2211] Batch: 0.0942 (0.1034) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0484)
[2023/01/16 01:56] | TRAIN(038): [1050/2211] Batch: 0.0845 (0.1029) Data: 0.0018 (0.0031) Loss: 0.0159 (0.0508)
[2023/01/16 01:57] | TRAIN(038): [1100/2211] Batch: 0.0904 (0.1023) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0512)
[2023/01/16 01:57] | TRAIN(038): [1150/2211] Batch: 0.1007 (0.1019) Data: 0.0016 (0.0030) Loss: 0.0000 (0.0514)
[2023/01/16 01:57] | TRAIN(038): [1200/2211] Batch: 0.0857 (0.1017) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0514)
[2023/01/16 01:57] | TRAIN(038): [1250/2211] Batch: 0.0951 (0.1013) Data: 0.0019 (0.0029) Loss: 0.0326 (0.0516)
[2023/01/16 01:57] | TRAIN(038): [1300/2211] Batch: 0.1241 (0.1013) Data: 0.0020 (0.0028) Loss: 0.0000 (0.0519)
[2023/01/16 01:57] | TRAIN(038): [1350/2211] Batch: 0.0876 (0.1010) Data: 0.0017 (0.0028) Loss: 0.0101 (0.0520)
[2023/01/16 01:57] | TRAIN(038): [1400/2211] Batch: 0.0880 (0.1006) Data: 0.0016 (0.0028) Loss: 0.0202 (0.0526)
[2023/01/16 01:57] | TRAIN(038): [1450/2211] Batch: 0.0959 (0.1005) Data: 0.0018 (0.0027) Loss: 0.1509 (0.0525)
[2023/01/16 01:57] | TRAIN(038): [1500/2211] Batch: 0.0904 (0.1001) Data: 0.0016 (0.0027) Loss: 0.2495 (0.0543)
[2023/01/16 01:57] | TRAIN(038): [1550/2211] Batch: 0.1011 (0.0998) Data: 0.0016 (0.0027) Loss: 0.1651 (0.0547)
[2023/01/16 01:57] | TRAIN(038): [1600/2211] Batch: 0.0986 (0.0996) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0549)
[2023/01/16 01:57] | TRAIN(038): [1650/2211] Batch: 0.0952 (0.0994) Data: 0.0017 (0.0026) Loss: 0.0024 (0.0546)
[2023/01/16 01:57] | TRAIN(038): [1700/2211] Batch: 0.1197 (0.0994) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0542)
[2023/01/16 01:58] | TRAIN(038): [1750/2211] Batch: 0.0959 (0.0994) Data: 0.0018 (0.0026) Loss: 0.1130 (0.0552)
[2023/01/16 01:58] | TRAIN(038): [1800/2211] Batch: 0.0875 (0.0993) Data: 0.0020 (0.0025) Loss: 0.0094 (0.0549)
[2023/01/16 01:58] | TRAIN(038): [1850/2211] Batch: 0.0921 (0.0994) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0549)
[2023/01/16 01:58] | TRAIN(038): [1900/2211] Batch: 0.0968 (0.0995) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0547)
[2023/01/16 01:58] | TRAIN(038): [1950/2211] Batch: 0.0919 (0.0994) Data: 0.0018 (0.0025) Loss: 0.1060 (0.0556)
[2023/01/16 01:58] | TRAIN(038): [2000/2211] Batch: 0.0922 (0.0994) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0556)
[2023/01/16 01:58] | TRAIN(038): [2050/2211] Batch: 0.0911 (0.0993) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0566)
[2023/01/16 01:58] | TRAIN(038): [2100/2211] Batch: 0.0880 (0.0992) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0562)
[2023/01/16 01:58] | TRAIN(038): [2150/2211] Batch: 0.0898 (0.0990) Data: 0.0018 (0.0024) Loss: 0.0648 (0.0568)
[2023/01/16 01:58] | TRAIN(038): [2200/2211] Batch: 0.0856 (0.0989) Data: 0.0016 (0.0024) Loss: 0.0044 (0.0571)
[2023/01/16 01:58] | ------------------------------------------------------------
[2023/01/16 01:58] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 01:58] | ------------------------------------------------------------
[2023/01/16 01:58] |    TRAIN(38)     0:03:38     0:00:05     0:03:33      0.0571
[2023/01/16 01:58] | ------------------------------------------------------------
[2023/01/16 01:58] | **************************************************
[2023/01/16 01:58] | TRAIN(039): [  50/2211] Batch: 0.1275 (0.1226) Data: 0.0022 (0.0259) Loss: 0.0000 (0.0567)
[2023/01/16 01:58] | TRAIN(039): [ 100/2211] Batch: 0.0996 (0.1087) Data: 0.0019 (0.0140) Loss: 0.0000 (0.0474)
[2023/01/16 01:59] | TRAIN(039): [ 150/2211] Batch: 0.0883 (0.1028) Data: 0.0018 (0.0099) Loss: 0.0000 (0.0492)
[2023/01/16 01:59] | TRAIN(039): [ 200/2211] Batch: 0.0874 (0.0996) Data: 0.0017 (0.0079) Loss: 0.1396 (0.0493)
[2023/01/16 01:59] | TRAIN(039): [ 250/2211] Batch: 0.0877 (0.0975) Data: 0.0016 (0.0067) Loss: 0.0282 (0.0495)
[2023/01/16 01:59] | TRAIN(039): [ 300/2211] Batch: 0.0913 (0.0965) Data: 0.0018 (0.0059) Loss: 0.1197 (0.0494)
[2023/01/16 01:59] | TRAIN(039): [ 350/2211] Batch: 0.0915 (0.0963) Data: 0.0016 (0.0053) Loss: 0.1230 (0.0486)
[2023/01/16 01:59] | TRAIN(039): [ 400/2211] Batch: 0.0968 (0.0958) Data: 0.0018 (0.0048) Loss: 0.0846 (0.0480)
[2023/01/16 01:59] | TRAIN(039): [ 450/2211] Batch: 0.0906 (0.0961) Data: 0.0019 (0.0045) Loss: 0.0831 (0.0483)
[2023/01/16 01:59] | TRAIN(039): [ 500/2211] Batch: 0.0918 (0.0957) Data: 0.0018 (0.0042) Loss: 0.0700 (0.0468)
[2023/01/16 01:59] | TRAIN(039): [ 550/2211] Batch: 0.0882 (0.0956) Data: 0.0018 (0.0040) Loss: 0.4010 (0.0481)
[2023/01/16 01:59] | TRAIN(039): [ 600/2211] Batch: 0.0947 (0.0955) Data: 0.0019 (0.0038) Loss: 0.0675 (0.0488)
[2023/01/16 01:59] | TRAIN(039): [ 650/2211] Batch: 0.0868 (0.0955) Data: 0.0016 (0.0037) Loss: 0.0000 (0.0502)
[2023/01/16 01:59] | TRAIN(039): [ 700/2211] Batch: 0.1015 (0.0952) Data: 0.0018 (0.0035) Loss: 0.1925 (0.0505)
[2023/01/16 01:59] | TRAIN(039): [ 750/2211] Batch: 0.0880 (0.0951) Data: 0.0016 (0.0034) Loss: 0.0000 (0.0508)
[2023/01/16 02:00] | TRAIN(039): [ 800/2211] Batch: 0.0885 (0.0949) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0512)
[2023/01/16 02:00] | TRAIN(039): [ 850/2211] Batch: 0.0950 (0.0947) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0505)
[2023/01/16 02:00] | TRAIN(039): [ 900/2211] Batch: 0.0966 (0.0945) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0507)
[2023/01/16 02:00] | TRAIN(039): [ 950/2211] Batch: 0.0886 (0.0943) Data: 0.0017 (0.0031) Loss: 0.2020 (0.0521)
[2023/01/16 02:00] | TRAIN(039): [1000/2211] Batch: 0.0966 (0.0941) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0529)
[2023/01/16 02:00] | TRAIN(039): [1050/2211] Batch: 0.1288 (0.0943) Data: 0.0030 (0.0030) Loss: 0.0000 (0.0526)
[2023/01/16 02:00] | TRAIN(039): [1100/2211] Batch: 0.1453 (0.0956) Data: 0.0028 (0.0029) Loss: 0.0000 (0.0526)
[2023/01/16 02:00] | TRAIN(039): [1150/2211] Batch: 0.1538 (0.0967) Data: 0.0121 (0.0030) Loss: 0.0468 (0.0526)
[2023/01/16 02:00] | TRAIN(039): [1200/2211] Batch: 0.1572 (0.0975) Data: 0.0027 (0.0030) Loss: 0.0598 (0.0531)
[2023/01/16 02:00] | TRAIN(039): [1250/2211] Batch: 0.1234 (0.0991) Data: 0.0024 (0.0030) Loss: 0.1685 (0.0540)
[2023/01/16 02:00] | TRAIN(039): [1300/2211] Batch: 0.0969 (0.0997) Data: 0.0018 (0.0030) Loss: 0.2236 (0.0537)
[2023/01/16 02:01] | TRAIN(039): [1350/2211] Batch: 0.1126 (0.1000) Data: 0.0021 (0.0030) Loss: 0.0664 (0.0533)
[2023/01/16 02:01] | TRAIN(039): [1400/2211] Batch: 0.1682 (0.1003) Data: 0.0059 (0.0030) Loss: 0.0282 (0.0532)
[2023/01/16 02:01] | TRAIN(039): [1450/2211] Batch: 0.1467 (0.1009) Data: 0.0028 (0.0030) Loss: 0.0013 (0.0530)
[2023/01/16 02:01] | TRAIN(039): [1500/2211] Batch: 0.1062 (0.1021) Data: 0.0020 (0.0030) Loss: 0.1040 (0.0526)
[2023/01/16 02:01] | TRAIN(039): [1550/2211] Batch: 0.0985 (0.1027) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0522)
[2023/01/16 02:01] | TRAIN(039): [1600/2211] Batch: 0.0962 (0.1027) Data: 0.0021 (0.0030) Loss: 0.0724 (0.0518)
[2023/01/16 02:01] | TRAIN(039): [1650/2211] Batch: 0.1137 (0.1029) Data: 0.0020 (0.0030) Loss: 0.0213 (0.0519)
[2023/01/16 02:01] | TRAIN(039): [1700/2211] Batch: 0.1446 (0.1032) Data: 0.0027 (0.0030) Loss: 0.0000 (0.0517)
[2023/01/16 02:01] | TRAIN(039): [1750/2211] Batch: 0.1205 (0.1035) Data: 0.0023 (0.0030) Loss: 0.0000 (0.0513)
[2023/01/16 02:01] | TRAIN(039): [1800/2211] Batch: 0.1011 (0.1038) Data: 0.0020 (0.0030) Loss: 0.0650 (0.0513)
[2023/01/16 02:02] | TRAIN(039): [1850/2211] Batch: 0.1081 (0.1039) Data: 0.0022 (0.0030) Loss: 0.0000 (0.0512)
[2023/01/16 02:02] | TRAIN(039): [1900/2211] Batch: 0.1180 (0.1043) Data: 0.0020 (0.0030) Loss: 0.0018 (0.0516)
[2023/01/16 02:02] | TRAIN(039): [1950/2211] Batch: 0.1004 (0.1044) Data: 0.0024 (0.0030) Loss: 0.1463 (0.0519)
[2023/01/16 02:02] | TRAIN(039): [2000/2211] Batch: 0.1094 (0.1047) Data: 0.0021 (0.0030) Loss: 0.1956 (0.0524)
[2023/01/16 02:02] | TRAIN(039): [2050/2211] Batch: 0.1329 (0.1047) Data: 0.0018 (0.0030) Loss: 0.0056 (0.0525)
[2023/01/16 02:02] | TRAIN(039): [2100/2211] Batch: 0.1529 (0.1050) Data: 0.0028 (0.0030) Loss: 0.0000 (0.0523)
[2023/01/16 02:02] | TRAIN(039): [2150/2211] Batch: 0.1389 (0.1060) Data: 0.0025 (0.0030) Loss: 0.0000 (0.0522)
[2023/01/16 02:02] | TRAIN(039): [2200/2211] Batch: 0.0836 (0.1061) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0524)
[2023/01/16 02:02] | ------------------------------------------------------------
[2023/01/16 02:02] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:02] | ------------------------------------------------------------
[2023/01/16 02:02] |    TRAIN(39)     0:03:54     0:00:06     0:03:48      0.0525
[2023/01/16 02:02] | ------------------------------------------------------------
[2023/01/16 02:02] | **************************************************
[2023/01/16 02:02] | TRAIN(040): [  50/2211] Batch: 0.1369 (0.1658) Data: 0.0024 (0.0285) Loss: 0.0000 (0.0587)
[2023/01/16 02:02] | TRAIN(040): [ 100/2211] Batch: 0.0891 (0.1442) Data: 0.0014 (0.0168) Loss: 0.0000 (0.0471)
[2023/01/16 02:03] | TRAIN(040): [ 150/2211] Batch: 0.1045 (0.1335) Data: 0.0022 (0.0120) Loss: 0.0000 (0.0455)
[2023/01/16 02:03] | TRAIN(040): [ 200/2211] Batch: 0.0905 (0.1312) Data: 0.0019 (0.0098) Loss: 0.1467 (0.0451)
[2023/01/16 02:03] | TRAIN(040): [ 250/2211] Batch: 0.0962 (0.1283) Data: 0.0021 (0.0086) Loss: 0.0000 (0.0497)
[2023/01/16 02:03] | TRAIN(040): [ 300/2211] Batch: 0.0905 (0.1266) Data: 0.0018 (0.0076) Loss: 0.0000 (0.0524)
[2023/01/16 02:03] | TRAIN(040): [ 350/2211] Batch: 0.0988 (0.1238) Data: 0.0020 (0.0068) Loss: 0.0000 (0.0557)
[2023/01/16 02:03] | TRAIN(040): [ 400/2211] Batch: 0.1108 (0.1227) Data: 0.0020 (0.0065) Loss: 0.0000 (0.0548)
[2023/01/16 02:03] | TRAIN(040): [ 450/2211] Batch: 0.0853 (0.1193) Data: 0.0018 (0.0060) Loss: 0.2780 (0.0554)
[2023/01/16 02:03] | TRAIN(040): [ 500/2211] Batch: 0.0951 (0.1176) Data: 0.0017 (0.0056) Loss: 0.2620 (0.0549)
[2023/01/16 02:03] | TRAIN(040): [ 550/2211] Batch: 0.1593 (0.1169) Data: 0.0027 (0.0054) Loss: 0.3844 (0.0547)
[2023/01/16 02:03] | TRAIN(040): [ 600/2211] Batch: 0.0914 (0.1167) Data: 0.0022 (0.0052) Loss: 0.0096 (0.0544)
[2023/01/16 02:03] | TRAIN(040): [ 650/2211] Batch: 0.1267 (0.1176) Data: 0.0119 (0.0050) Loss: 0.0981 (0.0560)
[2023/01/16 02:04] | TRAIN(040): [ 700/2211] Batch: 0.1422 (0.1175) Data: 0.0026 (0.0048) Loss: 0.0325 (0.0547)
[2023/01/16 02:04] | TRAIN(040): [ 750/2211] Batch: 0.1158 (0.1175) Data: 0.0022 (0.0047) Loss: 0.0982 (0.0547)
[2023/01/16 02:04] | TRAIN(040): [ 800/2211] Batch: 0.1206 (0.1171) Data: 0.0021 (0.0046) Loss: 0.0000 (0.0555)
[2023/01/16 02:04] | TRAIN(040): [ 850/2211] Batch: 0.1339 (0.1179) Data: 0.0031 (0.0045) Loss: 0.0000 (0.0566)
[2023/01/16 02:04] | TRAIN(040): [ 900/2211] Batch: 0.0919 (0.1184) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0571)
[2023/01/16 02:04] | TRAIN(040): [ 950/2211] Batch: 0.1650 (0.1186) Data: 0.0028 (0.0043) Loss: 0.0664 (0.0576)
[2023/01/16 02:04] | TRAIN(040): [1000/2211] Batch: 0.1458 (0.1198) Data: 0.0123 (0.0044) Loss: 0.0020 (0.0580)
[2023/01/16 02:04] | TRAIN(040): [1050/2211] Batch: 0.1050 (0.1194) Data: 0.0024 (0.0043) Loss: 0.0000 (0.0577)
[2023/01/16 02:04] | TRAIN(040): [1100/2211] Batch: 0.1393 (0.1195) Data: 0.0028 (0.0042) Loss: 0.0000 (0.0574)
[2023/01/16 02:05] | TRAIN(040): [1150/2211] Batch: 0.0986 (0.1195) Data: 0.0017 (0.0042) Loss: 0.1700 (0.0575)
[2023/01/16 02:05] | TRAIN(040): [1200/2211] Batch: 0.1211 (0.1193) Data: 0.0023 (0.0041) Loss: 0.0234 (0.0569)
[2023/01/16 02:05] | TRAIN(040): [1250/2211] Batch: 0.0984 (0.1188) Data: 0.0017 (0.0040) Loss: 0.0000 (0.0566)
[2023/01/16 02:05] | TRAIN(040): [1300/2211] Batch: 0.0915 (0.1187) Data: 0.0019 (0.0041) Loss: 0.0791 (0.0564)
[2023/01/16 02:05] | TRAIN(040): [1350/2211] Batch: 0.0886 (0.1180) Data: 0.0017 (0.0040) Loss: 0.0045 (0.0562)
[2023/01/16 02:05] | TRAIN(040): [1400/2211] Batch: 0.0857 (0.1176) Data: 0.0019 (0.0039) Loss: 0.0000 (0.0560)
[2023/01/16 02:05] | TRAIN(040): [1450/2211] Batch: 0.0892 (0.1174) Data: 0.0018 (0.0039) Loss: 0.0003 (0.0552)
[2023/01/16 02:05] | TRAIN(040): [1500/2211] Batch: 0.0989 (0.1172) Data: 0.0019 (0.0039) Loss: 0.0000 (0.0543)
[2023/01/16 02:05] | TRAIN(040): [1550/2211] Batch: 0.1056 (0.1166) Data: 0.0021 (0.0038) Loss: 0.0000 (0.0542)
[2023/01/16 02:05] | TRAIN(040): [1600/2211] Batch: 0.1066 (0.1163) Data: 0.0023 (0.0038) Loss: 0.0000 (0.0543)
[2023/01/16 02:05] | TRAIN(040): [1650/2211] Batch: 0.1080 (0.1159) Data: 0.0021 (0.0037) Loss: 0.0027 (0.0540)
[2023/01/16 02:05] | TRAIN(040): [1700/2211] Batch: 0.1117 (0.1154) Data: 0.0022 (0.0037) Loss: 0.1098 (0.0542)
[2023/01/16 02:06] | TRAIN(040): [1750/2211] Batch: 0.0982 (0.1152) Data: 0.0022 (0.0037) Loss: 0.0000 (0.0544)
[2023/01/16 02:06] | TRAIN(040): [1800/2211] Batch: 0.1354 (0.1154) Data: 0.0025 (0.0036) Loss: 0.0000 (0.0547)
[2023/01/16 02:06] | TRAIN(040): [1850/2211] Batch: 0.1199 (0.1151) Data: 0.0023 (0.0036) Loss: 0.0000 (0.0549)
[2023/01/16 02:06] | TRAIN(040): [1900/2211] Batch: 0.1355 (0.1150) Data: 0.0025 (0.0036) Loss: 0.0688 (0.0550)
[2023/01/16 02:06] | TRAIN(040): [1950/2211] Batch: 0.0894 (0.1148) Data: 0.0013 (0.0035) Loss: 0.0000 (0.0549)
[2023/01/16 02:06] | TRAIN(040): [2000/2211] Batch: 0.0939 (0.1149) Data: 0.0036 (0.0035) Loss: 0.0002 (0.0549)
[2023/01/16 02:06] | TRAIN(040): [2050/2211] Batch: 0.1216 (0.1149) Data: 0.0024 (0.0035) Loss: 0.0213 (0.0547)
[2023/01/16 02:06] | TRAIN(040): [2100/2211] Batch: 0.0994 (0.1149) Data: 0.0026 (0.0035) Loss: 0.0000 (0.0548)
[2023/01/16 02:06] | TRAIN(040): [2150/2211] Batch: 0.0968 (0.1148) Data: 0.0022 (0.0035) Loss: 0.0000 (0.0547)
[2023/01/16 02:06] | TRAIN(040): [2200/2211] Batch: 0.1126 (0.1149) Data: 0.0023 (0.0035) Loss: 0.0000 (0.0547)
[2023/01/16 02:06] | ------------------------------------------------------------
[2023/01/16 02:06] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:06] | ------------------------------------------------------------
[2023/01/16 02:06] |    TRAIN(40)     0:04:13     0:00:07     0:04:06      0.0547
[2023/01/16 02:06] | ------------------------------------------------------------
[2023/01/16 02:06] | **************************************************
[2023/01/16 02:07] | TRAIN(041): [  50/2211] Batch: 0.0961 (0.1572) Data: 0.0019 (0.0388) Loss: 0.0000 (0.0347)
[2023/01/16 02:07] | TRAIN(041): [ 100/2211] Batch: 0.1278 (0.1393) Data: 0.0024 (0.0208) Loss: 0.0000 (0.0425)
[2023/01/16 02:07] | TRAIN(041): [ 150/2211] Batch: 0.1018 (0.1374) Data: 0.0021 (0.0148) Loss: 0.2104 (0.0417)
[2023/01/16 02:07] | TRAIN(041): [ 200/2211] Batch: 0.1133 (0.1299) Data: 0.0024 (0.0122) Loss: 0.0000 (0.0435)
[2023/01/16 02:07] | TRAIN(041): [ 250/2211] Batch: 0.1308 (0.1305) Data: 0.0028 (0.0103) Loss: 0.0224 (0.0472)
[2023/01/16 02:07] | TRAIN(041): [ 300/2211] Batch: 0.1134 (0.1296) Data: 0.0018 (0.0095) Loss: 0.0409 (0.0494)
[2023/01/16 02:07] | TRAIN(041): [ 350/2211] Batch: 0.0997 (0.1282) Data: 0.0022 (0.0085) Loss: 0.1371 (0.0518)
[2023/01/16 02:07] | TRAIN(041): [ 400/2211] Batch: 0.0989 (0.1262) Data: 0.0024 (0.0077) Loss: 0.0617 (0.0520)
[2023/01/16 02:07] | TRAIN(041): [ 450/2211] Batch: 0.1374 (0.1267) Data: 0.0022 (0.0073) Loss: 0.0000 (0.0524)
[2023/01/16 02:08] | TRAIN(041): [ 500/2211] Batch: 0.1320 (0.1257) Data: 0.0022 (0.0068) Loss: 0.0060 (0.0538)
[2023/01/16 02:08] | TRAIN(041): [ 550/2211] Batch: 0.0897 (0.1247) Data: 0.0018 (0.0064) Loss: 0.0747 (0.0535)
[2023/01/16 02:08] | TRAIN(041): [ 600/2211] Batch: 0.1101 (0.1238) Data: 0.0022 (0.0061) Loss: 0.0000 (0.0529)
[2023/01/16 02:08] | TRAIN(041): [ 650/2211] Batch: 0.1060 (0.1243) Data: 0.0069 (0.0058) Loss: 0.0243 (0.0535)
[2023/01/16 02:08] | TRAIN(041): [ 700/2211] Batch: 0.1193 (0.1225) Data: 0.0020 (0.0056) Loss: 0.0630 (0.0531)
[2023/01/16 02:08] | TRAIN(041): [ 750/2211] Batch: 0.1271 (0.1228) Data: 0.0028 (0.0054) Loss: 0.0064 (0.0531)
[2023/01/16 02:08] | TRAIN(041): [ 800/2211] Batch: 0.0979 (0.1224) Data: 0.0023 (0.0053) Loss: 0.0000 (0.0537)
[2023/01/16 02:08] | TRAIN(041): [ 850/2211] Batch: 0.1331 (0.1217) Data: 0.0029 (0.0051) Loss: 0.0000 (0.0543)
[2023/01/16 02:08] | TRAIN(041): [ 900/2211] Batch: 0.1484 (0.1213) Data: 0.0122 (0.0050) Loss: 0.0111 (0.0554)
[2023/01/16 02:08] | TRAIN(041): [ 950/2211] Batch: 0.0919 (0.1207) Data: 0.0019 (0.0049) Loss: 0.0177 (0.0562)
[2023/01/16 02:08] | TRAIN(041): [1000/2211] Batch: 0.0884 (0.1199) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0551)
[2023/01/16 02:09] | TRAIN(041): [1050/2211] Batch: 0.1436 (0.1201) Data: 0.0122 (0.0048) Loss: 0.0000 (0.0555)
[2023/01/16 02:09] | TRAIN(041): [1100/2211] Batch: 0.1236 (0.1197) Data: 0.0025 (0.0047) Loss: 0.0000 (0.0549)
[2023/01/16 02:09] | TRAIN(041): [1150/2211] Batch: 0.1412 (0.1198) Data: 0.0024 (0.0046) Loss: 0.1146 (0.0547)
[2023/01/16 02:09] | TRAIN(041): [1200/2211] Batch: 0.0916 (0.1197) Data: 0.0021 (0.0046) Loss: 0.1088 (0.0539)
[2023/01/16 02:09] | TRAIN(041): [1250/2211] Batch: 0.0885 (0.1186) Data: 0.0020 (0.0045) Loss: 0.1072 (0.0543)
[2023/01/16 02:09] | TRAIN(041): [1300/2211] Batch: 0.0922 (0.1176) Data: 0.0020 (0.0044) Loss: 0.0159 (0.0534)
[2023/01/16 02:09] | TRAIN(041): [1350/2211] Batch: 0.1196 (0.1180) Data: 0.0020 (0.0044) Loss: 0.0000 (0.0536)
[2023/01/16 02:09] | TRAIN(041): [1400/2211] Batch: 0.1326 (0.1184) Data: 0.0028 (0.0043) Loss: 0.0000 (0.0531)
[2023/01/16 02:09] | TRAIN(041): [1450/2211] Batch: 0.1375 (0.1185) Data: 0.0022 (0.0043) Loss: 0.0368 (0.0524)
[2023/01/16 02:09] | TRAIN(041): [1500/2211] Batch: 0.0891 (0.1181) Data: 0.0018 (0.0042) Loss: 0.0001 (0.0520)
[2023/01/16 02:10] | TRAIN(041): [1550/2211] Batch: 0.0996 (0.1179) Data: 0.0021 (0.0042) Loss: 0.1202 (0.0519)
[2023/01/16 02:10] | TRAIN(041): [1600/2211] Batch: 0.1232 (0.1177) Data: 0.0022 (0.0041) Loss: 0.0000 (0.0512)
[2023/01/16 02:10] | TRAIN(041): [1650/2211] Batch: 0.0924 (0.1181) Data: 0.0020 (0.0041) Loss: 0.2210 (0.0512)
[2023/01/16 02:10] | TRAIN(041): [1700/2211] Batch: 0.1013 (0.1179) Data: 0.0017 (0.0040) Loss: 0.2558 (0.0511)
[2023/01/16 02:10] | TRAIN(041): [1750/2211] Batch: 0.1219 (0.1177) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0518)
[2023/01/16 02:10] | TRAIN(041): [1800/2211] Batch: 0.1193 (0.1178) Data: 0.0022 (0.0039) Loss: 0.0024 (0.0522)
[2023/01/16 02:10] | TRAIN(041): [1850/2211] Batch: 0.0914 (0.1175) Data: 0.0020 (0.0039) Loss: 0.0000 (0.0519)
[2023/01/16 02:10] | TRAIN(041): [1900/2211] Batch: 0.0918 (0.1169) Data: 0.0018 (0.0038) Loss: 0.2271 (0.0517)
[2023/01/16 02:10] | TRAIN(041): [1950/2211] Batch: 0.0901 (0.1163) Data: 0.0017 (0.0038) Loss: 0.1246 (0.0522)
[2023/01/16 02:10] | TRAIN(041): [2000/2211] Batch: 0.0940 (0.1158) Data: 0.0019 (0.0037) Loss: 0.0000 (0.0521)
[2023/01/16 02:10] | TRAIN(041): [2050/2211] Batch: 0.1092 (0.1153) Data: 0.0019 (0.0037) Loss: 0.0000 (0.0537)
[2023/01/16 02:10] | TRAIN(041): [2100/2211] Batch: 0.0879 (0.1148) Data: 0.0016 (0.0036) Loss: 0.0000 (0.0543)
[2023/01/16 02:11] | TRAIN(041): [2150/2211] Batch: 0.0921 (0.1146) Data: 0.0017 (0.0036) Loss: 0.1898 (0.0541)
[2023/01/16 02:11] | TRAIN(041): [2200/2211] Batch: 0.0873 (0.1142) Data: 0.0016 (0.0036) Loss: 0.2992 (0.0547)
[2023/01/16 02:11] | ------------------------------------------------------------
[2023/01/16 02:11] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:11] | ------------------------------------------------------------
[2023/01/16 02:11] |    TRAIN(41)     0:04:12     0:00:07     0:04:04      0.0548
[2023/01/16 02:11] | ------------------------------------------------------------
[2023/01/16 02:11] | **************************************************
[2023/01/16 02:11] | TRAIN(042): [  50/2211] Batch: 0.0918 (0.1324) Data: 0.0018 (0.0239) Loss: 0.0000 (0.0623)
[2023/01/16 02:11] | TRAIN(042): [ 100/2211] Batch: 0.1014 (0.1152) Data: 0.0019 (0.0130) Loss: 0.0074 (0.0501)
[2023/01/16 02:11] | TRAIN(042): [ 150/2211] Batch: 0.0988 (0.1085) Data: 0.0016 (0.0093) Loss: 0.0000 (0.0462)
[2023/01/16 02:11] | TRAIN(042): [ 200/2211] Batch: 0.0904 (0.1051) Data: 0.0017 (0.0074) Loss: 0.1094 (0.0522)
[2023/01/16 02:11] | TRAIN(042): [ 250/2211] Batch: 0.0943 (0.1029) Data: 0.0018 (0.0063) Loss: 0.0000 (0.0490)
[2023/01/16 02:11] | TRAIN(042): [ 300/2211] Batch: 0.0946 (0.1023) Data: 0.0017 (0.0056) Loss: 0.0000 (0.0498)
[2023/01/16 02:11] | TRAIN(042): [ 350/2211] Batch: 0.0916 (0.1010) Data: 0.0017 (0.0050) Loss: 0.0447 (0.0515)
[2023/01/16 02:11] | TRAIN(042): [ 400/2211] Batch: 0.0921 (0.1004) Data: 0.0017 (0.0046) Loss: 0.0668 (0.0524)
[2023/01/16 02:11] | TRAIN(042): [ 450/2211] Batch: 0.1051 (0.1005) Data: 0.0020 (0.0043) Loss: 0.0099 (0.0536)
[2023/01/16 02:11] | TRAIN(042): [ 500/2211] Batch: 0.0883 (0.1000) Data: 0.0017 (0.0041) Loss: 0.1372 (0.0549)
[2023/01/16 02:12] | TRAIN(042): [ 550/2211] Batch: 0.0901 (0.0992) Data: 0.0018 (0.0039) Loss: 0.1014 (0.0558)
[2023/01/16 02:12] | TRAIN(042): [ 600/2211] Batch: 0.1071 (0.0993) Data: 0.0020 (0.0037) Loss: 0.0000 (0.0554)
[2023/01/16 02:12] | TRAIN(042): [ 650/2211] Batch: 0.1000 (0.0990) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0555)
[2023/01/16 02:12] | TRAIN(042): [ 700/2211] Batch: 0.0911 (0.0985) Data: 0.0017 (0.0034) Loss: 0.0166 (0.0561)
[2023/01/16 02:12] | TRAIN(042): [ 750/2211] Batch: 0.0971 (0.0985) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0552)
[2023/01/16 02:12] | TRAIN(042): [ 800/2211] Batch: 0.1062 (0.0983) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0557)
[2023/01/16 02:12] | TRAIN(042): [ 850/2211] Batch: 0.0945 (0.0981) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0545)
[2023/01/16 02:12] | TRAIN(042): [ 900/2211] Batch: 0.0922 (0.0979) Data: 0.0017 (0.0031) Loss: 0.0800 (0.0544)
[2023/01/16 02:12] | TRAIN(042): [ 950/2211] Batch: 0.0907 (0.0978) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0538)
[2023/01/16 02:12] | TRAIN(042): [1000/2211] Batch: 0.0967 (0.0977) Data: 0.0019 (0.0029) Loss: 0.0003 (0.0523)
[2023/01/16 02:12] | TRAIN(042): [1050/2211] Batch: 0.1457 (0.0978) Data: 0.0023 (0.0029) Loss: 0.0012 (0.0517)
[2023/01/16 02:12] | TRAIN(042): [1100/2211] Batch: 0.0898 (0.0980) Data: 0.0017 (0.0029) Loss: 0.0521 (0.0515)
[2023/01/16 02:13] | TRAIN(042): [1150/2211] Batch: 0.1237 (0.0982) Data: 0.0019 (0.0028) Loss: 0.0584 (0.0517)
[2023/01/16 02:13] | TRAIN(042): [1200/2211] Batch: 0.0915 (0.0980) Data: 0.0018 (0.0028) Loss: 0.0991 (0.0510)
[2023/01/16 02:13] | TRAIN(042): [1250/2211] Batch: 0.0995 (0.0979) Data: 0.0019 (0.0027) Loss: 0.2020 (0.0512)
[2023/01/16 02:13] | TRAIN(042): [1300/2211] Batch: 0.0868 (0.0980) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0511)
[2023/01/16 02:13] | TRAIN(042): [1350/2211] Batch: 0.1145 (0.0982) Data: 0.0017 (0.0027) Loss: 0.0024 (0.0510)
[2023/01/16 02:13] | TRAIN(042): [1400/2211] Batch: 0.0979 (0.0981) Data: 0.0018 (0.0026) Loss: 0.0085 (0.0513)
[2023/01/16 02:13] | TRAIN(042): [1450/2211] Batch: 0.1189 (0.0985) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0516)
[2023/01/16 02:13] | TRAIN(042): [1500/2211] Batch: 0.1035 (0.0987) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0516)
[2023/01/16 02:13] | TRAIN(042): [1550/2211] Batch: 0.0929 (0.0986) Data: 0.0019 (0.0026) Loss: 0.0036 (0.0514)
[2023/01/16 02:13] | TRAIN(042): [1600/2211] Batch: 0.0998 (0.0986) Data: 0.0018 (0.0025) Loss: 0.0476 (0.0514)
[2023/01/16 02:13] | TRAIN(042): [1650/2211] Batch: 0.0910 (0.0985) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0510)
[2023/01/16 02:13] | TRAIN(042): [1700/2211] Batch: 0.0914 (0.0983) Data: 0.0018 (0.0025) Loss: 0.0815 (0.0507)
[2023/01/16 02:14] | TRAIN(042): [1750/2211] Batch: 0.0989 (0.0983) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0509)
[2023/01/16 02:14] | TRAIN(042): [1800/2211] Batch: 0.0913 (0.0984) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0510)
[2023/01/16 02:14] | TRAIN(042): [1850/2211] Batch: 0.0969 (0.0982) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0514)
[2023/01/16 02:14] | TRAIN(042): [1900/2211] Batch: 0.0891 (0.0981) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0515)
[2023/01/16 02:14] | TRAIN(042): [1950/2211] Batch: 0.0921 (0.0979) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0512)
[2023/01/16 02:14] | TRAIN(042): [2000/2211] Batch: 0.0954 (0.0978) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0511)
[2023/01/16 02:14] | TRAIN(042): [2050/2211] Batch: 0.0912 (0.0977) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0517)
[2023/01/16 02:14] | TRAIN(042): [2100/2211] Batch: 0.0953 (0.0976) Data: 0.0017 (0.0024) Loss: 0.1034 (0.0517)
[2023/01/16 02:14] | TRAIN(042): [2150/2211] Batch: 0.0929 (0.0977) Data: 0.0020 (0.0024) Loss: 0.0917 (0.0521)
[2023/01/16 02:14] | TRAIN(042): [2200/2211] Batch: 0.0904 (0.0976) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0521)
[2023/01/16 02:14] | ------------------------------------------------------------
[2023/01/16 02:14] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:14] | ------------------------------------------------------------
[2023/01/16 02:14] |    TRAIN(42)     0:03:35     0:00:05     0:03:30      0.0520
[2023/01/16 02:14] | ------------------------------------------------------------
[2023/01/16 02:14] | **************************************************
[2023/01/16 02:14] | TRAIN(043): [  50/2211] Batch: 0.1197 (0.1256) Data: 0.0022 (0.0251) Loss: 0.0000 (0.0383)
[2023/01/16 02:14] | TRAIN(043): [ 100/2211] Batch: 0.0956 (0.1144) Data: 0.0018 (0.0137) Loss: 0.2029 (0.0514)
[2023/01/16 02:15] | TRAIN(043): [ 150/2211] Batch: 0.0912 (0.1070) Data: 0.0018 (0.0098) Loss: 0.0049 (0.0494)
[2023/01/16 02:15] | TRAIN(043): [ 200/2211] Batch: 0.0971 (0.1054) Data: 0.0018 (0.0078) Loss: 0.2387 (0.0498)
[2023/01/16 02:15] | TRAIN(043): [ 250/2211] Batch: 0.0929 (0.1030) Data: 0.0018 (0.0066) Loss: 0.0000 (0.0497)
[2023/01/16 02:15] | TRAIN(043): [ 300/2211] Batch: 0.0917 (0.1011) Data: 0.0018 (0.0058) Loss: 0.1692 (0.0520)
[2023/01/16 02:15] | TRAIN(043): [ 350/2211] Batch: 0.0921 (0.1006) Data: 0.0018 (0.0053) Loss: 0.0164 (0.0540)
[2023/01/16 02:15] | TRAIN(043): [ 400/2211] Batch: 0.0915 (0.1010) Data: 0.0018 (0.0048) Loss: 0.1257 (0.0547)
[2023/01/16 02:15] | TRAIN(043): [ 450/2211] Batch: 0.0956 (0.1004) Data: 0.0019 (0.0045) Loss: 0.0000 (0.0531)
[2023/01/16 02:15] | TRAIN(043): [ 500/2211] Batch: 0.0904 (0.0997) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0532)
[2023/01/16 02:15] | TRAIN(043): [ 550/2211] Batch: 0.0887 (0.0990) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0526)
[2023/01/16 02:15] | TRAIN(043): [ 600/2211] Batch: 0.0961 (0.0983) Data: 0.0020 (0.0038) Loss: 0.0000 (0.0527)
[2023/01/16 02:15] | TRAIN(043): [ 650/2211] Batch: 0.0964 (0.0982) Data: 0.0018 (0.0037) Loss: 0.1890 (0.0527)
[2023/01/16 02:15] | TRAIN(043): [ 700/2211] Batch: 0.0985 (0.0988) Data: 0.0016 (0.0036) Loss: 0.0831 (0.0526)
[2023/01/16 02:15] | TRAIN(043): [ 750/2211] Batch: 0.0911 (0.0984) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0522)
[2023/01/16 02:16] | TRAIN(043): [ 800/2211] Batch: 0.0927 (0.0983) Data: 0.0018 (0.0033) Loss: 0.0764 (0.0531)
[2023/01/16 02:16] | TRAIN(043): [ 850/2211] Batch: 0.0905 (0.0981) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0533)
[2023/01/16 02:16] | TRAIN(043): [ 900/2211] Batch: 0.1210 (0.0986) Data: 0.0022 (0.0032) Loss: 0.1811 (0.0533)
[2023/01/16 02:16] | TRAIN(043): [ 950/2211] Batch: 0.1222 (0.0988) Data: 0.0022 (0.0031) Loss: 0.0519 (0.0549)
[2023/01/16 02:16] | TRAIN(043): [1000/2211] Batch: 0.1100 (0.0991) Data: 0.0020 (0.0031) Loss: 0.0607 (0.0541)
[2023/01/16 02:16] | TRAIN(043): [1050/2211] Batch: 0.0936 (0.0990) Data: 0.0019 (0.0030) Loss: 0.1486 (0.0538)
[2023/01/16 02:16] | TRAIN(043): [1100/2211] Batch: 0.0904 (0.0992) Data: 0.0017 (0.0030) Loss: 0.0038 (0.0540)
[2023/01/16 02:16] | TRAIN(043): [1150/2211] Batch: 0.0911 (0.0997) Data: 0.0018 (0.0029) Loss: 0.1099 (0.0536)
[2023/01/16 02:16] | TRAIN(043): [1200/2211] Batch: 0.0953 (0.0994) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0538)
[2023/01/16 02:16] | TRAIN(043): [1250/2211] Batch: 0.0892 (0.0996) Data: 0.0018 (0.0028) Loss: 0.2382 (0.0535)
[2023/01/16 02:16] | TRAIN(043): [1300/2211] Batch: 0.0956 (0.0994) Data: 0.0017 (0.0028) Loss: 0.0770 (0.0529)
[2023/01/16 02:17] | TRAIN(043): [1350/2211] Batch: 0.1192 (0.0993) Data: 0.0022 (0.0028) Loss: 0.0000 (0.0526)
[2023/01/16 02:17] | TRAIN(043): [1400/2211] Batch: 0.1202 (0.0996) Data: 0.0024 (0.0027) Loss: 0.0034 (0.0524)
[2023/01/16 02:17] | TRAIN(043): [1450/2211] Batch: 0.0958 (0.0996) Data: 0.0018 (0.0027) Loss: 0.1376 (0.0528)
[2023/01/16 02:17] | TRAIN(043): [1500/2211] Batch: 0.0895 (0.0996) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0530)
[2023/01/16 02:17] | TRAIN(043): [1550/2211] Batch: 0.0939 (0.0993) Data: 0.0017 (0.0026) Loss: 0.0044 (0.0523)
[2023/01/16 02:17] | TRAIN(043): [1600/2211] Batch: 0.0931 (0.0993) Data: 0.0018 (0.0026) Loss: 0.1080 (0.0524)
[2023/01/16 02:17] | TRAIN(043): [1650/2211] Batch: 0.0905 (0.0992) Data: 0.0018 (0.0026) Loss: 0.0354 (0.0519)
[2023/01/16 02:17] | TRAIN(043): [1700/2211] Batch: 0.0882 (0.0991) Data: 0.0018 (0.0026) Loss: 0.0693 (0.0516)
[2023/01/16 02:17] | TRAIN(043): [1750/2211] Batch: 0.1085 (0.0992) Data: 0.0018 (0.0026) Loss: 0.0079 (0.0514)
[2023/01/16 02:17] | TRAIN(043): [1800/2211] Batch: 0.0981 (0.0991) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0515)
[2023/01/16 02:17] | TRAIN(043): [1850/2211] Batch: 0.1011 (0.0991) Data: 0.0018 (0.0025) Loss: 0.0236 (0.0521)
[2023/01/16 02:17] | TRAIN(043): [1900/2211] Batch: 0.0967 (0.0990) Data: 0.0017 (0.0025) Loss: 0.2168 (0.0524)
[2023/01/16 02:17] | TRAIN(043): [1950/2211] Batch: 0.0984 (0.0989) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0522)
[2023/01/16 02:18] | TRAIN(043): [2000/2211] Batch: 0.0980 (0.0988) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0521)
[2023/01/16 02:18] | TRAIN(043): [2050/2211] Batch: 0.0829 (0.0988) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0519)
[2023/01/16 02:18] | TRAIN(043): [2100/2211] Batch: 0.0881 (0.0986) Data: 0.0018 (0.0024) Loss: 0.1122 (0.0519)
[2023/01/16 02:18] | TRAIN(043): [2150/2211] Batch: 0.0886 (0.0985) Data: 0.0018 (0.0024) Loss: 0.0142 (0.0518)
[2023/01/16 02:18] | TRAIN(043): [2200/2211] Batch: 0.1189 (0.0987) Data: 0.0020 (0.0024) Loss: 0.0000 (0.0518)
[2023/01/16 02:18] | ------------------------------------------------------------
[2023/01/16 02:18] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:18] | ------------------------------------------------------------
[2023/01/16 02:18] |    TRAIN(43)     0:03:38     0:00:05     0:03:33      0.0519
[2023/01/16 02:18] | ------------------------------------------------------------
[2023/01/16 02:18] | **************************************************
[2023/01/16 02:18] | TRAIN(044): [  50/2211] Batch: 0.1008 (0.1448) Data: 0.0018 (0.0240) Loss: 0.0000 (0.0459)
[2023/01/16 02:18] | TRAIN(044): [ 100/2211] Batch: 0.0871 (0.1199) Data: 0.0018 (0.0130) Loss: 0.0118 (0.0393)
[2023/01/16 02:18] | TRAIN(044): [ 150/2211] Batch: 0.0900 (0.1120) Data: 0.0016 (0.0093) Loss: 0.0314 (0.0493)
[2023/01/16 02:18] | TRAIN(044): [ 200/2211] Batch: 0.0878 (0.1071) Data: 0.0018 (0.0074) Loss: 0.0676 (0.0470)
[2023/01/16 02:18] | TRAIN(044): [ 250/2211] Batch: 0.1075 (0.1053) Data: 0.0016 (0.0063) Loss: 0.0100 (0.0464)
[2023/01/16 02:18] | TRAIN(044): [ 300/2211] Batch: 0.1005 (0.1073) Data: 0.0017 (0.0056) Loss: 0.0000 (0.0474)
[2023/01/16 02:19] | TRAIN(044): [ 350/2211] Batch: 0.1208 (0.1089) Data: 0.0022 (0.0051) Loss: 0.0000 (0.0475)
[2023/01/16 02:19] | TRAIN(044): [ 400/2211] Batch: 0.0901 (0.1096) Data: 0.0017 (0.0048) Loss: 0.0000 (0.0469)
[2023/01/16 02:19] | TRAIN(044): [ 450/2211] Batch: 0.0931 (0.1078) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0454)
[2023/01/16 02:19] | TRAIN(044): [ 500/2211] Batch: 0.0967 (0.1071) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0457)
[2023/01/16 02:19] | TRAIN(044): [ 550/2211] Batch: 0.0940 (0.1074) Data: 0.0017 (0.0040) Loss: 0.0042 (0.0470)
[2023/01/16 02:19] | TRAIN(044): [ 600/2211] Batch: 0.0971 (0.1064) Data: 0.0018 (0.0038) Loss: 0.1289 (0.0480)
[2023/01/16 02:19] | TRAIN(044): [ 650/2211] Batch: 0.0888 (0.1056) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0475)
[2023/01/16 02:19] | TRAIN(044): [ 700/2211] Batch: 0.1002 (0.1051) Data: 0.0018 (0.0035) Loss: 0.0967 (0.0480)
[2023/01/16 02:19] | TRAIN(044): [ 750/2211] Batch: 0.0892 (0.1043) Data: 0.0017 (0.0034) Loss: 0.0072 (0.0485)
[2023/01/16 02:19] | TRAIN(044): [ 800/2211] Batch: 0.0906 (0.1037) Data: 0.0018 (0.0033) Loss: 0.0024 (0.0479)
[2023/01/16 02:19] | TRAIN(044): [ 850/2211] Batch: 0.0883 (0.1030) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0470)
[2023/01/16 02:19] | TRAIN(044): [ 900/2211] Batch: 0.0950 (0.1024) Data: 0.0018 (0.0031) Loss: 0.1632 (0.0474)
[2023/01/16 02:20] | TRAIN(044): [ 950/2211] Batch: 0.0902 (0.1021) Data: 0.0018 (0.0031) Loss: 0.0731 (0.0475)
[2023/01/16 02:20] | TRAIN(044): [1000/2211] Batch: 0.1001 (0.1017) Data: 0.0018 (0.0030) Loss: 0.0641 (0.0470)
[2023/01/16 02:20] | TRAIN(044): [1050/2211] Batch: 0.0904 (0.1015) Data: 0.0017 (0.0030) Loss: 0.0201 (0.0468)
[2023/01/16 02:20] | TRAIN(044): [1100/2211] Batch: 0.0889 (0.1010) Data: 0.0018 (0.0029) Loss: 0.0045 (0.0466)
[2023/01/16 02:20] | TRAIN(044): [1150/2211] Batch: 0.1184 (0.1008) Data: 0.0023 (0.0029) Loss: 0.0000 (0.0465)
[2023/01/16 02:20] | TRAIN(044): [1200/2211] Batch: 0.1216 (0.1010) Data: 0.0022 (0.0028) Loss: 0.0000 (0.0465)
[2023/01/16 02:20] | TRAIN(044): [1250/2211] Batch: 0.1287 (0.1011) Data: 0.0021 (0.0028) Loss: 0.0054 (0.0461)
[2023/01/16 02:20] | TRAIN(044): [1300/2211] Batch: 0.0993 (0.1011) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0465)
[2023/01/16 02:20] | TRAIN(044): [1350/2211] Batch: 0.0918 (0.1009) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0461)
[2023/01/16 02:20] | TRAIN(044): [1400/2211] Batch: 0.0894 (0.1007) Data: 0.0018 (0.0027) Loss: 0.0140 (0.0464)
[2023/01/16 02:20] | TRAIN(044): [1450/2211] Batch: 0.1016 (0.1004) Data: 0.0017 (0.0026) Loss: 0.2300 (0.0469)
[2023/01/16 02:20] | TRAIN(044): [1500/2211] Batch: 0.0930 (0.1002) Data: 0.0017 (0.0026) Loss: 0.0100 (0.0466)
[2023/01/16 02:20] | TRAIN(044): [1550/2211] Batch: 0.0935 (0.0999) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0474)
[2023/01/16 02:21] | TRAIN(044): [1600/2211] Batch: 0.1082 (0.0998) Data: 0.0018 (0.0026) Loss: 0.0966 (0.0476)
[2023/01/16 02:21] | TRAIN(044): [1650/2211] Batch: 0.0926 (0.0997) Data: 0.0019 (0.0025) Loss: 0.0025 (0.0478)
[2023/01/16 02:21] | TRAIN(044): [1700/2211] Batch: 0.1188 (0.0998) Data: 0.0022 (0.0025) Loss: 0.0789 (0.0482)
[2023/01/16 02:21] | TRAIN(044): [1750/2211] Batch: 0.1018 (0.0999) Data: 0.0018 (0.0025) Loss: 0.0927 (0.0487)
[2023/01/16 02:21] | TRAIN(044): [1800/2211] Batch: 0.0883 (0.0996) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0484)
[2023/01/16 02:21] | TRAIN(044): [1850/2211] Batch: 0.1171 (0.0995) Data: 0.0022 (0.0025) Loss: 0.2046 (0.0485)
[2023/01/16 02:21] | TRAIN(044): [1900/2211] Batch: 0.0919 (0.0996) Data: 0.0017 (0.0025) Loss: 0.0564 (0.0485)
[2023/01/16 02:21] | TRAIN(044): [1950/2211] Batch: 0.0917 (0.0995) Data: 0.0019 (0.0024) Loss: 0.0275 (0.0488)
[2023/01/16 02:21] | TRAIN(044): [2000/2211] Batch: 0.0916 (0.0993) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0488)
[2023/01/16 02:21] | TRAIN(044): [2050/2211] Batch: 0.0907 (0.0992) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0488)
[2023/01/16 02:21] | TRAIN(044): [2100/2211] Batch: 0.0920 (0.0992) Data: 0.0017 (0.0024) Loss: 0.0360 (0.0490)
[2023/01/16 02:21] | TRAIN(044): [2150/2211] Batch: 0.0938 (0.0992) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0490)
[2023/01/16 02:22] | TRAIN(044): [2200/2211] Batch: 0.0948 (0.0993) Data: 0.0016 (0.0024) Loss: 0.1129 (0.0489)
[2023/01/16 02:22] | ------------------------------------------------------------
[2023/01/16 02:22] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:22] | ------------------------------------------------------------
[2023/01/16 02:22] |    TRAIN(44)     0:03:39     0:00:05     0:03:34      0.0490
[2023/01/16 02:22] | ------------------------------------------------------------
[2023/01/16 02:22] | **************************************************
[2023/01/16 02:22] | TRAIN(045): [  50/2211] Batch: 0.0914 (0.1302) Data: 0.0018 (0.0252) Loss: 0.1378 (0.0544)
[2023/01/16 02:22] | TRAIN(045): [ 100/2211] Batch: 0.1195 (0.1259) Data: 0.0021 (0.0139) Loss: 0.1585 (0.0715)
[2023/01/16 02:22] | TRAIN(045): [ 150/2211] Batch: 0.0932 (0.1241) Data: 0.0018 (0.0100) Loss: 0.0089 (0.0672)
[2023/01/16 02:22] | TRAIN(045): [ 200/2211] Batch: 0.0966 (0.1168) Data: 0.0017 (0.0080) Loss: 0.0000 (0.0612)
[2023/01/16 02:22] | TRAIN(045): [ 250/2211] Batch: 0.0918 (0.1127) Data: 0.0016 (0.0067) Loss: 0.0939 (0.0593)
[2023/01/16 02:22] | TRAIN(045): [ 300/2211] Batch: 0.0875 (0.1096) Data: 0.0018 (0.0059) Loss: 0.1117 (0.0610)
[2023/01/16 02:22] | TRAIN(045): [ 350/2211] Batch: 0.0860 (0.1075) Data: 0.0016 (0.0053) Loss: 0.0000 (0.0596)
[2023/01/16 02:22] | TRAIN(045): [ 400/2211] Batch: 0.0962 (0.1067) Data: 0.0018 (0.0049) Loss: 0.0830 (0.0592)
[2023/01/16 02:22] | TRAIN(045): [ 450/2211] Batch: 0.0929 (0.1061) Data: 0.0017 (0.0046) Loss: 0.0000 (0.0576)
[2023/01/16 02:22] | TRAIN(045): [ 500/2211] Batch: 0.0852 (0.1054) Data: 0.0018 (0.0043) Loss: 0.0937 (0.0552)
[2023/01/16 02:23] | TRAIN(045): [ 550/2211] Batch: 0.0920 (0.1042) Data: 0.0017 (0.0041) Loss: 0.0348 (0.0557)
[2023/01/16 02:23] | TRAIN(045): [ 600/2211] Batch: 0.1044 (0.1039) Data: 0.0018 (0.0039) Loss: 0.0206 (0.0550)
[2023/01/16 02:23] | TRAIN(045): [ 650/2211] Batch: 0.1034 (0.1052) Data: 0.0019 (0.0038) Loss: 0.0000 (0.0549)
[2023/01/16 02:23] | TRAIN(045): [ 700/2211] Batch: 0.0917 (0.1059) Data: 0.0016 (0.0036) Loss: 0.0000 (0.0556)
[2023/01/16 02:23] | TRAIN(045): [ 750/2211] Batch: 0.1142 (0.1070) Data: 0.0018 (0.0035) Loss: 0.1455 (0.0546)
[2023/01/16 02:23] | TRAIN(045): [ 800/2211] Batch: 0.0981 (0.1075) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0545)
[2023/01/16 02:23] | TRAIN(045): [ 850/2211] Batch: 0.0834 (0.1069) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0544)
[2023/01/16 02:23] | TRAIN(045): [ 900/2211] Batch: 0.1201 (0.1077) Data: 0.0021 (0.0033) Loss: 0.1234 (0.0538)
[2023/01/16 02:23] | TRAIN(045): [ 950/2211] Batch: 0.1282 (0.1079) Data: 0.0025 (0.0032) Loss: 0.0000 (0.0538)
[2023/01/16 02:23] | TRAIN(045): [1000/2211] Batch: 0.0878 (0.1077) Data: 0.0017 (0.0031) Loss: 0.1781 (0.0542)
[2023/01/16 02:23] | TRAIN(045): [1050/2211] Batch: 0.0967 (0.1072) Data: 0.0018 (0.0031) Loss: 0.1229 (0.0540)
[2023/01/16 02:24] | TRAIN(045): [1100/2211] Batch: 0.0887 (0.1067) Data: 0.0018 (0.0030) Loss: 0.0544 (0.0538)
[2023/01/16 02:24] | TRAIN(045): [1150/2211] Batch: 0.0910 (0.1063) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0541)
[2023/01/16 02:24] | TRAIN(045): [1200/2211] Batch: 0.0883 (0.1057) Data: 0.0017 (0.0029) Loss: 0.0612 (0.0547)
[2023/01/16 02:24] | TRAIN(045): [1250/2211] Batch: 0.0987 (0.1055) Data: 0.0018 (0.0029) Loss: 0.0088 (0.0543)
[2023/01/16 02:24] | TRAIN(045): [1300/2211] Batch: 0.0932 (0.1050) Data: 0.0022 (0.0029) Loss: 0.1764 (0.0538)
[2023/01/16 02:24] | TRAIN(045): [1350/2211] Batch: 0.0916 (0.1046) Data: 0.0018 (0.0028) Loss: 0.1389 (0.0535)
[2023/01/16 02:24] | TRAIN(045): [1400/2211] Batch: 0.0930 (0.1043) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0530)
[2023/01/16 02:24] | TRAIN(045): [1450/2211] Batch: 0.0983 (0.1040) Data: 0.0019 (0.0027) Loss: 0.0208 (0.0524)
[2023/01/16 02:24] | TRAIN(045): [1500/2211] Batch: 0.0876 (0.1038) Data: 0.0017 (0.0027) Loss: 0.1121 (0.0527)
[2023/01/16 02:24] | TRAIN(045): [1550/2211] Batch: 0.0946 (0.1035) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0521)
[2023/01/16 02:24] | TRAIN(045): [1600/2211] Batch: 0.0887 (0.1032) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0520)
[2023/01/16 02:24] | TRAIN(045): [1650/2211] Batch: 0.1030 (0.1029) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0515)
[2023/01/16 02:24] | TRAIN(045): [1700/2211] Batch: 0.0895 (0.1027) Data: 0.0017 (0.0026) Loss: 0.0047 (0.0514)
[2023/01/16 02:25] | TRAIN(045): [1750/2211] Batch: 0.0896 (0.1024) Data: 0.0017 (0.0026) Loss: 0.1032 (0.0513)
[2023/01/16 02:25] | TRAIN(045): [1800/2211] Batch: 0.0880 (0.1021) Data: 0.0017 (0.0026) Loss: 0.0917 (0.0510)
[2023/01/16 02:25] | TRAIN(045): [1850/2211] Batch: 0.0886 (0.1018) Data: 0.0018 (0.0025) Loss: 0.0596 (0.0512)
[2023/01/16 02:25] | TRAIN(045): [1900/2211] Batch: 0.0869 (0.1015) Data: 0.0017 (0.0025) Loss: 0.0312 (0.0513)
[2023/01/16 02:25] | TRAIN(045): [1950/2211] Batch: 0.0961 (0.1013) Data: 0.0018 (0.0025) Loss: 0.0047 (0.0513)
[2023/01/16 02:25] | TRAIN(045): [2000/2211] Batch: 0.0965 (0.1010) Data: 0.0017 (0.0025) Loss: 0.0106 (0.0509)
[2023/01/16 02:25] | TRAIN(045): [2050/2211] Batch: 0.1212 (0.1009) Data: 0.0025 (0.0025) Loss: 0.0827 (0.0511)
[2023/01/16 02:25] | TRAIN(045): [2100/2211] Batch: 0.0897 (0.1009) Data: 0.0018 (0.0025) Loss: 0.0636 (0.0515)
[2023/01/16 02:25] | TRAIN(045): [2150/2211] Batch: 0.1178 (0.1012) Data: 0.0021 (0.0024) Loss: 0.0193 (0.0518)
[2023/01/16 02:25] | TRAIN(045): [2200/2211] Batch: 0.1205 (0.1013) Data: 0.0020 (0.0024) Loss: 0.0753 (0.0518)
[2023/01/16 02:25] | ------------------------------------------------------------
[2023/01/16 02:25] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:25] | ------------------------------------------------------------
[2023/01/16 02:25] |    TRAIN(45)     0:03:43     0:00:05     0:03:38      0.0517
[2023/01/16 02:25] | ------------------------------------------------------------
[2023/01/16 02:25] | **************************************************
[2023/01/16 02:25] | TRAIN(046): [  50/2211] Batch: 0.0894 (0.1433) Data: 0.0018 (0.0250) Loss: 0.0000 (0.0836)
[2023/01/16 02:26] | TRAIN(046): [ 100/2211] Batch: 0.0915 (0.1191) Data: 0.0018 (0.0136) Loss: 0.1257 (0.0695)
[2023/01/16 02:26] | TRAIN(046): [ 150/2211] Batch: 0.0923 (0.1114) Data: 0.0018 (0.0097) Loss: 0.0000 (0.0673)
[2023/01/16 02:26] | TRAIN(046): [ 200/2211] Batch: 0.0927 (0.1077) Data: 0.0017 (0.0077) Loss: 0.0232 (0.0667)
[2023/01/16 02:26] | TRAIN(046): [ 250/2211] Batch: 0.0827 (0.1035) Data: 0.0018 (0.0065) Loss: 0.0040 (0.0601)
[2023/01/16 02:26] | TRAIN(046): [ 300/2211] Batch: 0.0851 (0.1012) Data: 0.0018 (0.0058) Loss: 0.3240 (0.0580)
[2023/01/16 02:26] | TRAIN(046): [ 350/2211] Batch: 0.0879 (0.0999) Data: 0.0017 (0.0052) Loss: 0.0000 (0.0552)
[2023/01/16 02:26] | TRAIN(046): [ 400/2211] Batch: 0.0929 (0.1009) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0551)
[2023/01/16 02:26] | TRAIN(046): [ 450/2211] Batch: 0.0897 (0.1003) Data: 0.0019 (0.0045) Loss: 0.0000 (0.0531)
[2023/01/16 02:26] | TRAIN(046): [ 500/2211] Batch: 0.0953 (0.0993) Data: 0.0018 (0.0042) Loss: 0.0052 (0.0530)
[2023/01/16 02:26] | TRAIN(046): [ 550/2211] Batch: 0.0888 (0.0987) Data: 0.0017 (0.0040) Loss: 0.0582 (0.0515)
[2023/01/16 02:26] | TRAIN(046): [ 600/2211] Batch: 0.1065 (0.0983) Data: 0.0019 (0.0038) Loss: 0.0556 (0.0518)
[2023/01/16 02:26] | TRAIN(046): [ 650/2211] Batch: 0.0924 (0.0981) Data: 0.0016 (0.0037) Loss: 0.0065 (0.0509)
[2023/01/16 02:26] | TRAIN(046): [ 700/2211] Batch: 0.0931 (0.0982) Data: 0.0018 (0.0035) Loss: 0.1394 (0.0521)
[2023/01/16 02:27] | TRAIN(046): [ 750/2211] Batch: 0.0970 (0.0984) Data: 0.0018 (0.0034) Loss: 0.0309 (0.0510)
[2023/01/16 02:27] | TRAIN(046): [ 800/2211] Batch: 0.0869 (0.0985) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0508)
[2023/01/16 02:27] | TRAIN(046): [ 850/2211] Batch: 0.0907 (0.0982) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0503)
[2023/01/16 02:27] | TRAIN(046): [ 900/2211] Batch: 0.0919 (0.0979) Data: 0.0018 (0.0032) Loss: 0.1301 (0.0501)
[2023/01/16 02:27] | TRAIN(046): [ 950/2211] Batch: 0.1086 (0.0979) Data: 0.0018 (0.0031) Loss: 0.0850 (0.0498)
[2023/01/16 02:27] | TRAIN(046): [1000/2211] Batch: 0.0931 (0.0982) Data: 0.0019 (0.0030) Loss: 0.0189 (0.0505)
[2023/01/16 02:27] | TRAIN(046): [1050/2211] Batch: 0.0915 (0.0979) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0501)
[2023/01/16 02:27] | TRAIN(046): [1100/2211] Batch: 0.0922 (0.0977) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0499)
[2023/01/16 02:27] | TRAIN(046): [1150/2211] Batch: 0.0896 (0.0976) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0500)
[2023/01/16 02:27] | TRAIN(046): [1200/2211] Batch: 0.0940 (0.0974) Data: 0.0018 (0.0028) Loss: 0.0572 (0.0501)
[2023/01/16 02:27] | TRAIN(046): [1250/2211] Batch: 0.0984 (0.0979) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0497)
[2023/01/16 02:27] | TRAIN(046): [1300/2211] Batch: 0.0911 (0.0977) Data: 0.0018 (0.0028) Loss: 0.1828 (0.0497)
[2023/01/16 02:27] | TRAIN(046): [1350/2211] Batch: 0.0904 (0.0974) Data: 0.0018 (0.0027) Loss: 0.0608 (0.0495)
[2023/01/16 02:28] | TRAIN(046): [1400/2211] Batch: 0.1179 (0.0973) Data: 0.0022 (0.0027) Loss: 0.1052 (0.0499)
[2023/01/16 02:28] | TRAIN(046): [1450/2211] Batch: 0.0950 (0.0984) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0505)
[2023/01/16 02:28] | TRAIN(046): [1500/2211] Batch: 0.0882 (0.0983) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0511)
[2023/01/16 02:28] | TRAIN(046): [1550/2211] Batch: 0.0910 (0.0981) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0507)
[2023/01/16 02:28] | TRAIN(046): [1600/2211] Batch: 0.0963 (0.0982) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0507)
[2023/01/16 02:28] | TRAIN(046): [1650/2211] Batch: 0.0878 (0.0980) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0506)
[2023/01/16 02:28] | TRAIN(046): [1700/2211] Batch: 0.0897 (0.0978) Data: 0.0018 (0.0025) Loss: 0.2030 (0.0507)
[2023/01/16 02:28] | TRAIN(046): [1750/2211] Batch: 0.0920 (0.0978) Data: 0.0017 (0.0025) Loss: 0.0825 (0.0504)
[2023/01/16 02:28] | TRAIN(046): [1800/2211] Batch: 0.0890 (0.0976) Data: 0.0018 (0.0025) Loss: 0.1263 (0.0503)
[2023/01/16 02:28] | TRAIN(046): [1850/2211] Batch: 0.0872 (0.0975) Data: 0.0018 (0.0025) Loss: 0.1782 (0.0502)
[2023/01/16 02:28] | TRAIN(046): [1900/2211] Batch: 0.0936 (0.0974) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0502)
[2023/01/16 02:28] | TRAIN(046): [1950/2211] Batch: 0.0918 (0.0974) Data: 0.0018 (0.0025) Loss: 0.0322 (0.0508)
[2023/01/16 02:29] | TRAIN(046): [2000/2211] Batch: 0.1010 (0.0974) Data: 0.0018 (0.0024) Loss: 0.0970 (0.0507)
[2023/01/16 02:29] | TRAIN(046): [2050/2211] Batch: 0.0948 (0.0973) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0506)
[2023/01/16 02:29] | TRAIN(046): [2100/2211] Batch: 0.0966 (0.0973) Data: 0.0018 (0.0024) Loss: 0.1649 (0.0510)
[2023/01/16 02:29] | TRAIN(046): [2150/2211] Batch: 0.0956 (0.0972) Data: 0.0017 (0.0024) Loss: 0.0445 (0.0510)
[2023/01/16 02:29] | TRAIN(046): [2200/2211] Batch: 0.0905 (0.0973) Data: 0.0016 (0.0024) Loss: 0.0590 (0.0508)
[2023/01/16 02:29] | ------------------------------------------------------------
[2023/01/16 02:29] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:29] | ------------------------------------------------------------
[2023/01/16 02:29] |    TRAIN(46)     0:03:35     0:00:05     0:03:29      0.0507
[2023/01/16 02:29] | ------------------------------------------------------------
[2023/01/16 02:29] | **************************************************
[2023/01/16 02:29] | TRAIN(047): [  50/2211] Batch: 0.0882 (0.1167) Data: 0.0017 (0.0231) Loss: 0.0000 (0.0350)
[2023/01/16 02:29] | TRAIN(047): [ 100/2211] Batch: 0.1206 (0.1079) Data: 0.0024 (0.0126) Loss: 0.1623 (0.0397)
[2023/01/16 02:29] | TRAIN(047): [ 150/2211] Batch: 0.0888 (0.1097) Data: 0.0016 (0.0091) Loss: 0.0294 (0.0421)
[2023/01/16 02:29] | TRAIN(047): [ 200/2211] Batch: 0.0951 (0.1061) Data: 0.0015 (0.0073) Loss: 0.1573 (0.0405)
[2023/01/16 02:29] | TRAIN(047): [ 250/2211] Batch: 0.0907 (0.1037) Data: 0.0017 (0.0062) Loss: 0.0763 (0.0428)
[2023/01/16 02:29] | TRAIN(047): [ 300/2211] Batch: 0.1130 (0.1024) Data: 0.0025 (0.0054) Loss: 0.1559 (0.0452)
[2023/01/16 02:29] | TRAIN(047): [ 350/2211] Batch: 0.0954 (0.1030) Data: 0.0018 (0.0049) Loss: 0.0000 (0.0452)
[2023/01/16 02:30] | TRAIN(047): [ 400/2211] Batch: 0.0879 (0.1029) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0455)
[2023/01/16 02:30] | TRAIN(047): [ 450/2211] Batch: 0.0941 (0.1020) Data: 0.0018 (0.0042) Loss: 0.0666 (0.0431)
[2023/01/16 02:30] | TRAIN(047): [ 500/2211] Batch: 0.0886 (0.1015) Data: 0.0018 (0.0040) Loss: 0.0581 (0.0432)
[2023/01/16 02:30] | TRAIN(047): [ 550/2211] Batch: 0.0919 (0.1007) Data: 0.0017 (0.0038) Loss: 0.0076 (0.0430)
[2023/01/16 02:30] | TRAIN(047): [ 600/2211] Batch: 0.0938 (0.1002) Data: 0.0017 (0.0036) Loss: 0.2382 (0.0450)
[2023/01/16 02:30] | TRAIN(047): [ 650/2211] Batch: 0.0884 (0.0999) Data: 0.0017 (0.0035) Loss: 0.1995 (0.0457)
[2023/01/16 02:30] | TRAIN(047): [ 700/2211] Batch: 0.0916 (0.0994) Data: 0.0016 (0.0034) Loss: 0.0000 (0.0468)
[2023/01/16 02:30] | TRAIN(047): [ 750/2211] Batch: 0.0962 (0.0995) Data: 0.0018 (0.0033) Loss: 0.0988 (0.0485)
[2023/01/16 02:30] | TRAIN(047): [ 800/2211] Batch: 0.1028 (0.0996) Data: 0.0018 (0.0032) Loss: 0.0777 (0.0493)
[2023/01/16 02:30] | TRAIN(047): [ 850/2211] Batch: 0.0887 (0.0993) Data: 0.0017 (0.0031) Loss: 0.0599 (0.0491)
[2023/01/16 02:30] | TRAIN(047): [ 900/2211] Batch: 0.1192 (0.0993) Data: 0.0021 (0.0030) Loss: 0.0222 (0.0488)
[2023/01/16 02:30] | TRAIN(047): [ 950/2211] Batch: 0.1177 (0.0990) Data: 0.0021 (0.0030) Loss: 0.0557 (0.0485)
[2023/01/16 02:31] | TRAIN(047): [1000/2211] Batch: 0.0959 (0.0990) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0485)
[2023/01/16 02:31] | TRAIN(047): [1050/2211] Batch: 0.0876 (0.0986) Data: 0.0017 (0.0028) Loss: 0.0567 (0.0489)
[2023/01/16 02:31] | TRAIN(047): [1100/2211] Batch: 0.0915 (0.0984) Data: 0.0018 (0.0028) Loss: 0.1163 (0.0486)
[2023/01/16 02:31] | TRAIN(047): [1150/2211] Batch: 0.0906 (0.0982) Data: 0.0018 (0.0028) Loss: 0.0934 (0.0479)
[2023/01/16 02:31] | TRAIN(047): [1200/2211] Batch: 0.0891 (0.0984) Data: 0.0018 (0.0027) Loss: 0.0067 (0.0485)
[2023/01/16 02:31] | TRAIN(047): [1250/2211] Batch: 0.0885 (0.0981) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0486)
[2023/01/16 02:31] | TRAIN(047): [1300/2211] Batch: 0.0866 (0.0977) Data: 0.0017 (0.0026) Loss: 0.1771 (0.0485)
[2023/01/16 02:31] | TRAIN(047): [1350/2211] Batch: 0.0934 (0.0981) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0481)
[2023/01/16 02:31] | TRAIN(047): [1400/2211] Batch: 0.0905 (0.0978) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0481)
[2023/01/16 02:31] | TRAIN(047): [1450/2211] Batch: 0.0944 (0.0976) Data: 0.0018 (0.0026) Loss: 0.1704 (0.0481)
[2023/01/16 02:31] | TRAIN(047): [1500/2211] Batch: 0.0934 (0.0975) Data: 0.0018 (0.0025) Loss: 0.1002 (0.0481)
[2023/01/16 02:31] | TRAIN(047): [1550/2211] Batch: 0.0873 (0.0974) Data: 0.0017 (0.0025) Loss: 0.1665 (0.0478)
[2023/01/16 02:31] | TRAIN(047): [1600/2211] Batch: 0.1179 (0.0973) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0476)
[2023/01/16 02:32] | TRAIN(047): [1650/2211] Batch: 0.0898 (0.0973) Data: 0.0017 (0.0025) Loss: 0.0748 (0.0475)
[2023/01/16 02:32] | TRAIN(047): [1700/2211] Batch: 0.0885 (0.0972) Data: 0.0017 (0.0025) Loss: 0.0037 (0.0483)
[2023/01/16 02:32] | TRAIN(047): [1750/2211] Batch: 0.1115 (0.0972) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0481)
[2023/01/16 02:32] | TRAIN(047): [1800/2211] Batch: 0.1041 (0.0973) Data: 0.0018 (0.0024) Loss: 0.0035 (0.0482)
[2023/01/16 02:32] | TRAIN(047): [1850/2211] Batch: 0.1182 (0.0973) Data: 0.0022 (0.0024) Loss: 0.0030 (0.0481)
[2023/01/16 02:32] | TRAIN(047): [1900/2211] Batch: 0.1016 (0.0975) Data: 0.0016 (0.0024) Loss: 0.0123 (0.0482)
[2023/01/16 02:32] | TRAIN(047): [1950/2211] Batch: 0.0970 (0.0975) Data: 0.0017 (0.0024) Loss: 0.0200 (0.0477)
[2023/01/16 02:32] | TRAIN(047): [2000/2211] Batch: 0.0926 (0.0975) Data: 0.0018 (0.0024) Loss: 0.0618 (0.0479)
[2023/01/16 02:32] | TRAIN(047): [2050/2211] Batch: 0.1035 (0.0974) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0480)
[2023/01/16 02:32] | TRAIN(047): [2100/2211] Batch: 0.1060 (0.0973) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0481)
[2023/01/16 02:32] | TRAIN(047): [2150/2211] Batch: 0.0874 (0.0972) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0481)
[2023/01/16 02:32] | TRAIN(047): [2200/2211] Batch: 0.0956 (0.0972) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0483)
[2023/01/16 02:32] | ------------------------------------------------------------
[2023/01/16 02:32] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:32] | ------------------------------------------------------------
[2023/01/16 02:32] |    TRAIN(47)     0:03:34     0:00:05     0:03:29      0.0484
[2023/01/16 02:32] | ------------------------------------------------------------
[2023/01/16 02:32] | **************************************************
[2023/01/16 02:33] | TRAIN(048): [  50/2211] Batch: 0.1048 (0.1255) Data: 0.0018 (0.0238) Loss: 0.1432 (0.0409)
[2023/01/16 02:33] | TRAIN(048): [ 100/2211] Batch: 0.0873 (0.1142) Data: 0.0016 (0.0129) Loss: 0.0000 (0.0358)
[2023/01/16 02:33] | TRAIN(048): [ 150/2211] Batch: 0.0870 (0.1062) Data: 0.0018 (0.0092) Loss: 0.0635 (0.0395)
[2023/01/16 02:33] | TRAIN(048): [ 200/2211] Batch: 0.0911 (0.1023) Data: 0.0018 (0.0074) Loss: 0.1285 (0.0438)
[2023/01/16 02:33] | TRAIN(048): [ 250/2211] Batch: 0.0865 (0.1000) Data: 0.0017 (0.0063) Loss: 0.0000 (0.0427)
[2023/01/16 02:33] | TRAIN(048): [ 300/2211] Batch: 0.0925 (0.0984) Data: 0.0018 (0.0055) Loss: 0.0000 (0.0437)
[2023/01/16 02:33] | TRAIN(048): [ 350/2211] Batch: 0.0819 (0.0971) Data: 0.0017 (0.0050) Loss: 0.0612 (0.0450)
[2023/01/16 02:33] | TRAIN(048): [ 400/2211] Batch: 0.1182 (0.0981) Data: 0.0022 (0.0046) Loss: 0.0158 (0.0456)
[2023/01/16 02:33] | TRAIN(048): [ 450/2211] Batch: 0.1274 (0.0997) Data: 0.0021 (0.0043) Loss: 0.0304 (0.0463)
[2023/01/16 02:33] | TRAIN(048): [ 500/2211] Batch: 0.0908 (0.1009) Data: 0.0017 (0.0041) Loss: 0.0000 (0.0457)
[2023/01/16 02:33] | TRAIN(048): [ 550/2211] Batch: 0.0900 (0.1022) Data: 0.0018 (0.0039) Loss: 0.0915 (0.0451)
[2023/01/16 02:33] | TRAIN(048): [ 600/2211] Batch: 0.0896 (0.1013) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0455)
[2023/01/16 02:34] | TRAIN(048): [ 650/2211] Batch: 0.0867 (0.1003) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0449)
[2023/01/16 02:34] | TRAIN(048): [ 700/2211] Batch: 0.0898 (0.1001) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0446)
[2023/01/16 02:34] | TRAIN(048): [ 750/2211] Batch: 0.0883 (0.0998) Data: 0.0017 (0.0034) Loss: 0.0399 (0.0442)
[2023/01/16 02:34] | TRAIN(048): [ 800/2211] Batch: 0.0886 (0.0992) Data: 0.0017 (0.0033) Loss: 0.0059 (0.0433)
[2023/01/16 02:34] | TRAIN(048): [ 850/2211] Batch: 0.0917 (0.0989) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0433)
[2023/01/16 02:34] | TRAIN(048): [ 900/2211] Batch: 0.0996 (0.0987) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0435)
[2023/01/16 02:34] | TRAIN(048): [ 950/2211] Batch: 0.0857 (0.0984) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0433)
[2023/01/16 02:34] | TRAIN(048): [1000/2211] Batch: 0.0983 (0.0988) Data: 0.0019 (0.0030) Loss: 0.1726 (0.0435)
[2023/01/16 02:34] | TRAIN(048): [1050/2211] Batch: 0.0954 (0.0988) Data: 0.0017 (0.0029) Loss: 0.0085 (0.0435)
[2023/01/16 02:34] | TRAIN(048): [1100/2211] Batch: 0.0872 (0.0989) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0437)
[2023/01/16 02:34] | TRAIN(048): [1150/2211] Batch: 0.0879 (0.0985) Data: 0.0018 (0.0028) Loss: 0.0262 (0.0450)
[2023/01/16 02:34] | TRAIN(048): [1200/2211] Batch: 0.1176 (0.0984) Data: 0.0022 (0.0028) Loss: 0.0700 (0.0455)
[2023/01/16 02:35] | TRAIN(048): [1250/2211] Batch: 0.1012 (0.0983) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0462)
[2023/01/16 02:35] | TRAIN(048): [1300/2211] Batch: 0.0975 (0.0982) Data: 0.0021 (0.0027) Loss: 0.2198 (0.0459)
[2023/01/16 02:35] | TRAIN(048): [1350/2211] Batch: 0.0909 (0.0981) Data: 0.0018 (0.0027) Loss: 0.1432 (0.0464)
[2023/01/16 02:35] | TRAIN(048): [1400/2211] Batch: 0.1180 (0.0987) Data: 0.0024 (0.0027) Loss: 0.0647 (0.0464)
[2023/01/16 02:35] | TRAIN(048): [1450/2211] Batch: 0.0984 (0.0987) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0463)
[2023/01/16 02:35] | TRAIN(048): [1500/2211] Batch: 0.0940 (0.0985) Data: 0.0018 (0.0026) Loss: 0.0735 (0.0466)
[2023/01/16 02:35] | TRAIN(048): [1550/2211] Batch: 0.0872 (0.0983) Data: 0.0018 (0.0026) Loss: 0.2069 (0.0471)
[2023/01/16 02:35] | TRAIN(048): [1600/2211] Batch: 0.0872 (0.0982) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0471)
[2023/01/16 02:35] | TRAIN(048): [1650/2211] Batch: 0.0883 (0.0980) Data: 0.0018 (0.0025) Loss: 0.0571 (0.0469)
[2023/01/16 02:35] | TRAIN(048): [1700/2211] Batch: 0.1025 (0.0978) Data: 0.0013 (0.0025) Loss: 0.0399 (0.0469)
[2023/01/16 02:35] | TRAIN(048): [1750/2211] Batch: 0.0882 (0.0980) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0470)
[2023/01/16 02:35] | TRAIN(048): [1800/2211] Batch: 0.0994 (0.0979) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0470)
[2023/01/16 02:35] | TRAIN(048): [1850/2211] Batch: 0.0917 (0.0979) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0467)
[2023/01/16 02:36] | TRAIN(048): [1900/2211] Batch: 0.0889 (0.0977) Data: 0.0013 (0.0024) Loss: 0.0000 (0.0466)
[2023/01/16 02:36] | TRAIN(048): [1950/2211] Batch: 0.1173 (0.0978) Data: 0.0022 (0.0024) Loss: 0.1242 (0.0470)
[2023/01/16 02:36] | TRAIN(048): [2000/2211] Batch: 0.0888 (0.0979) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0471)
[2023/01/16 02:36] | TRAIN(048): [2050/2211] Batch: 0.0946 (0.0978) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0472)
[2023/01/16 02:36] | TRAIN(048): [2100/2211] Batch: 0.0894 (0.0976) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0476)
[2023/01/16 02:36] | TRAIN(048): [2150/2211] Batch: 0.0911 (0.0975) Data: 0.0016 (0.0024) Loss: 0.0279 (0.0478)
[2023/01/16 02:36] | TRAIN(048): [2200/2211] Batch: 0.0982 (0.0975) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0482)
[2023/01/16 02:36] | ------------------------------------------------------------
[2023/01/16 02:36] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:36] | ------------------------------------------------------------
[2023/01/16 02:36] |    TRAIN(48)     0:03:35     0:00:05     0:03:30      0.0483
[2023/01/16 02:36] | ------------------------------------------------------------
[2023/01/16 02:36] | **************************************************
[2023/01/16 02:36] | TRAIN(049): [  50/2211] Batch: 0.0987 (0.1287) Data: 0.0018 (0.0251) Loss: 0.0836 (0.0716)
[2023/01/16 02:36] | TRAIN(049): [ 100/2211] Batch: 0.1175 (0.1119) Data: 0.0022 (0.0137) Loss: 0.0000 (0.0528)
[2023/01/16 02:36] | TRAIN(049): [ 150/2211] Batch: 0.0952 (0.1072) Data: 0.0018 (0.0098) Loss: 0.0000 (0.0510)
[2023/01/16 02:36] | TRAIN(049): [ 200/2211] Batch: 0.0977 (0.1043) Data: 0.0017 (0.0078) Loss: 0.0000 (0.0517)
[2023/01/16 02:36] | TRAIN(049): [ 250/2211] Batch: 0.1177 (0.1029) Data: 0.0022 (0.0066) Loss: 0.2212 (0.0507)
[2023/01/16 02:37] | TRAIN(049): [ 300/2211] Batch: 0.0983 (0.1034) Data: 0.0017 (0.0059) Loss: 0.0285 (0.0495)
[2023/01/16 02:37] | TRAIN(049): [ 350/2211] Batch: 0.0915 (0.1021) Data: 0.0019 (0.0053) Loss: 0.1230 (0.0482)
[2023/01/16 02:37] | TRAIN(049): [ 400/2211] Batch: 0.1047 (0.1031) Data: 0.0018 (0.0049) Loss: 0.1540 (0.0480)
[2023/01/16 02:37] | TRAIN(049): [ 450/2211] Batch: 0.0955 (0.1017) Data: 0.0018 (0.0045) Loss: 0.0604 (0.0492)
[2023/01/16 02:37] | TRAIN(049): [ 500/2211] Batch: 0.0877 (0.1005) Data: 0.0032 (0.0043) Loss: 0.1087 (0.0489)
[2023/01/16 02:37] | TRAIN(049): [ 550/2211] Batch: 0.0902 (0.1010) Data: 0.0019 (0.0041) Loss: 0.1681 (0.0495)
[2023/01/16 02:37] | TRAIN(049): [ 600/2211] Batch: 0.0989 (0.1001) Data: 0.0018 (0.0039) Loss: 0.0405 (0.0488)
[2023/01/16 02:37] | TRAIN(049): [ 650/2211] Batch: 0.0990 (0.0996) Data: 0.0018 (0.0037) Loss: 0.1232 (0.0488)
[2023/01/16 02:37] | TRAIN(049): [ 700/2211] Batch: 0.0925 (0.0992) Data: 0.0019 (0.0036) Loss: 0.0086 (0.0487)
[2023/01/16 02:37] | TRAIN(049): [ 750/2211] Batch: 0.0990 (0.0990) Data: 0.0016 (0.0035) Loss: 0.0000 (0.0488)
[2023/01/16 02:37] | TRAIN(049): [ 800/2211] Batch: 0.0973 (0.0990) Data: 0.0018 (0.0034) Loss: 0.0303 (0.0483)
[2023/01/16 02:37] | TRAIN(049): [ 850/2211] Batch: 0.0962 (0.0988) Data: 0.0019 (0.0033) Loss: 0.0031 (0.0476)
[2023/01/16 02:38] | TRAIN(049): [ 900/2211] Batch: 0.0871 (0.0984) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0478)
[2023/01/16 02:38] | TRAIN(049): [ 950/2211] Batch: 0.1229 (0.0984) Data: 0.0025 (0.0031) Loss: 0.1265 (0.0471)
[2023/01/16 02:38] | TRAIN(049): [1000/2211] Batch: 0.0876 (0.0985) Data: 0.0019 (0.0030) Loss: 0.0020 (0.0469)
[2023/01/16 02:38] | TRAIN(049): [1050/2211] Batch: 0.0874 (0.0981) Data: 0.0018 (0.0030) Loss: 0.2282 (0.0471)
[2023/01/16 02:38] | TRAIN(049): [1100/2211] Batch: 0.0919 (0.0982) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0474)
[2023/01/16 02:38] | TRAIN(049): [1150/2211] Batch: 0.0926 (0.0980) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0475)
[2023/01/16 02:38] | TRAIN(049): [1200/2211] Batch: 0.0922 (0.0979) Data: 0.0019 (0.0028) Loss: 0.2109 (0.0475)
[2023/01/16 02:38] | TRAIN(049): [1250/2211] Batch: 0.1002 (0.0978) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0486)
[2023/01/16 02:38] | TRAIN(049): [1300/2211] Batch: 0.0878 (0.0977) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0485)
[2023/01/16 02:38] | TRAIN(049): [1350/2211] Batch: 0.0878 (0.0974) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0481)
[2023/01/16 02:38] | TRAIN(049): [1400/2211] Batch: 0.0910 (0.0971) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0478)
[2023/01/16 02:38] | TRAIN(049): [1450/2211] Batch: 0.0876 (0.0969) Data: 0.0018 (0.0027) Loss: 0.2102 (0.0479)
[2023/01/16 02:38] | TRAIN(049): [1500/2211] Batch: 0.1187 (0.0970) Data: 0.0022 (0.0026) Loss: 0.1454 (0.0478)
[2023/01/16 02:39] | TRAIN(049): [1550/2211] Batch: 0.1160 (0.0978) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0475)
[2023/01/16 02:39] | TRAIN(049): [1600/2211] Batch: 0.1218 (0.0984) Data: 0.0019 (0.0026) Loss: 0.0749 (0.0475)
[2023/01/16 02:39] | TRAIN(049): [1650/2211] Batch: 0.0878 (0.0985) Data: 0.0018 (0.0026) Loss: 0.0975 (0.0480)
[2023/01/16 02:39] | TRAIN(049): [1700/2211] Batch: 0.0968 (0.0985) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0488)
[2023/01/16 02:39] | TRAIN(049): [1750/2211] Batch: 0.1004 (0.0985) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0491)
[2023/01/16 02:39] | TRAIN(049): [1800/2211] Batch: 0.0999 (0.0984) Data: 0.0019 (0.0025) Loss: 0.0184 (0.0488)
[2023/01/16 02:39] | TRAIN(049): [1850/2211] Batch: 0.0886 (0.0982) Data: 0.0018 (0.0025) Loss: 0.0598 (0.0485)
[2023/01/16 02:39] | TRAIN(049): [1900/2211] Batch: 0.0885 (0.0980) Data: 0.0018 (0.0025) Loss: 0.0077 (0.0482)
[2023/01/16 02:39] | TRAIN(049): [1950/2211] Batch: 0.0870 (0.0978) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0482)
[2023/01/16 02:39] | TRAIN(049): [2000/2211] Batch: 0.0884 (0.0976) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0482)
[2023/01/16 02:39] | TRAIN(049): [2050/2211] Batch: 0.0888 (0.0975) Data: 0.0019 (0.0024) Loss: 0.1687 (0.0483)
[2023/01/16 02:39] | TRAIN(049): [2100/2211] Batch: 0.1015 (0.0974) Data: 0.0016 (0.0024) Loss: 0.0072 (0.0484)
[2023/01/16 02:40] | TRAIN(049): [2150/2211] Batch: 0.0979 (0.0973) Data: 0.0018 (0.0024) Loss: 0.1726 (0.0487)
[2023/01/16 02:40] | TRAIN(049): [2200/2211] Batch: 0.0862 (0.0973) Data: 0.0017 (0.0024) Loss: 0.0814 (0.0488)
[2023/01/16 02:40] | ------------------------------------------------------------
[2023/01/16 02:40] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:40] | ------------------------------------------------------------
[2023/01/16 02:40] |    TRAIN(49)     0:03:34     0:00:05     0:03:29      0.0488
[2023/01/16 02:40] | ------------------------------------------------------------
[2023/01/16 02:40] | **************************************************
[2023/01/16 02:40] | TRAIN(050): [  50/2211] Batch: 0.0874 (0.1211) Data: 0.0018 (0.0248) Loss: 0.1239 (0.0530)
[2023/01/16 02:40] | TRAIN(050): [ 100/2211] Batch: 0.0881 (0.1070) Data: 0.0018 (0.0134) Loss: 0.1014 (0.0468)
[2023/01/16 02:40] | TRAIN(050): [ 150/2211] Batch: 0.0876 (0.1021) Data: 0.0017 (0.0096) Loss: 0.1575 (0.0510)
[2023/01/16 02:40] | TRAIN(050): [ 200/2211] Batch: 0.0929 (0.1003) Data: 0.0018 (0.0076) Loss: 0.0000 (0.0517)
[2023/01/16 02:40] | TRAIN(050): [ 250/2211] Batch: 0.0875 (0.0985) Data: 0.0018 (0.0065) Loss: 0.1208 (0.0514)
[2023/01/16 02:40] | TRAIN(050): [ 300/2211] Batch: 0.0894 (0.0993) Data: 0.0018 (0.0057) Loss: 0.1142 (0.0549)
[2023/01/16 02:40] | TRAIN(050): [ 350/2211] Batch: 0.1180 (0.0988) Data: 0.0021 (0.0052) Loss: 0.1353 (0.0525)
[2023/01/16 02:40] | TRAIN(050): [ 400/2211] Batch: 0.0854 (0.0974) Data: 0.0018 (0.0048) Loss: 0.0140 (0.0541)
[2023/01/16 02:40] | TRAIN(050): [ 450/2211] Batch: 0.1004 (0.0966) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0522)
[2023/01/16 02:40] | TRAIN(050): [ 500/2211] Batch: 0.0872 (0.0967) Data: 0.0018 (0.0042) Loss: 0.0069 (0.0503)
[2023/01/16 02:41] | TRAIN(050): [ 550/2211] Batch: 0.0881 (0.0963) Data: 0.0019 (0.0040) Loss: 0.0000 (0.0499)
[2023/01/16 02:41] | TRAIN(050): [ 600/2211] Batch: 0.0983 (0.0964) Data: 0.0020 (0.0038) Loss: 0.0000 (0.0500)
[2023/01/16 02:41] | TRAIN(050): [ 650/2211] Batch: 0.0864 (0.0960) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0497)
[2023/01/16 02:41] | TRAIN(050): [ 700/2211] Batch: 0.0870 (0.0957) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0495)
[2023/01/16 02:41] | TRAIN(050): [ 750/2211] Batch: 0.1193 (0.0958) Data: 0.0022 (0.0034) Loss: 0.0567 (0.0488)
[2023/01/16 02:41] | TRAIN(050): [ 800/2211] Batch: 0.0881 (0.0957) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0489)
[2023/01/16 02:41] | TRAIN(050): [ 850/2211] Batch: 0.1173 (0.0961) Data: 0.0022 (0.0032) Loss: 0.0398 (0.0494)
[2023/01/16 02:41] | TRAIN(050): [ 900/2211] Batch: 0.0882 (0.0969) Data: 0.0018 (0.0032) Loss: 0.1414 (0.0495)
[2023/01/16 02:41] | TRAIN(050): [ 950/2211] Batch: 0.0923 (0.0969) Data: 0.0018 (0.0031) Loss: 0.0569 (0.0496)
[2023/01/16 02:41] | TRAIN(050): [1000/2211] Batch: 0.0895 (0.0967) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0492)
[2023/01/16 02:41] | TRAIN(050): [1050/2211] Batch: 0.1005 (0.0966) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0497)
[2023/01/16 02:41] | TRAIN(050): [1100/2211] Batch: 0.0896 (0.0965) Data: 0.0018 (0.0029) Loss: 0.0034 (0.0506)
[2023/01/16 02:42] | TRAIN(050): [1150/2211] Batch: 0.0902 (0.0964) Data: 0.0018 (0.0029) Loss: 0.0132 (0.0507)
[2023/01/16 02:42] | TRAIN(050): [1200/2211] Batch: 0.0884 (0.0962) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0500)
[2023/01/16 02:42] | TRAIN(050): [1250/2211] Batch: 0.0872 (0.0959) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0506)
[2023/01/16 02:42] | TRAIN(050): [1300/2211] Batch: 0.0874 (0.0957) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0502)
[2023/01/16 02:42] | TRAIN(050): [1350/2211] Batch: 0.1171 (0.0957) Data: 0.0021 (0.0027) Loss: 0.0786 (0.0498)
[2023/01/16 02:42] | TRAIN(050): [1400/2211] Batch: 0.0886 (0.0955) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0499)
[2023/01/16 02:42] | TRAIN(050): [1450/2211] Batch: 0.0924 (0.0957) Data: 0.0017 (0.0026) Loss: 0.0368 (0.0500)
[2023/01/16 02:42] | TRAIN(050): [1500/2211] Batch: 0.0879 (0.0956) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0496)
[2023/01/16 02:42] | TRAIN(050): [1550/2211] Batch: 0.0945 (0.0956) Data: 0.0018 (0.0026) Loss: 0.1740 (0.0495)
[2023/01/16 02:42] | TRAIN(050): [1600/2211] Batch: 0.1348 (0.0957) Data: 0.0023 (0.0026) Loss: 0.0000 (0.0491)
[2023/01/16 02:42] | TRAIN(050): [1650/2211] Batch: 0.0964 (0.0962) Data: 0.0017 (0.0025) Loss: 0.1659 (0.0489)
[2023/01/16 02:42] | TRAIN(050): [1700/2211] Batch: 0.0872 (0.0961) Data: 0.0018 (0.0025) Loss: 0.0138 (0.0485)
[2023/01/16 02:42] | TRAIN(050): [1750/2211] Batch: 0.0980 (0.0959) Data: 0.0018 (0.0025) Loss: 0.1347 (0.0486)
[2023/01/16 02:43] | TRAIN(050): [1800/2211] Batch: 0.0894 (0.0957) Data: 0.0018 (0.0025) Loss: 0.0087 (0.0484)
[2023/01/16 02:43] | TRAIN(050): [1850/2211] Batch: 0.0880 (0.0956) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0479)
[2023/01/16 02:43] | TRAIN(050): [1900/2211] Batch: 0.0882 (0.0955) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0476)
[2023/01/16 02:43] | TRAIN(050): [1950/2211] Batch: 0.0944 (0.0954) Data: 0.0017 (0.0024) Loss: 0.0077 (0.0475)
[2023/01/16 02:43] | TRAIN(050): [2000/2211] Batch: 0.1140 (0.0955) Data: 0.0019 (0.0024) Loss: 0.0397 (0.0475)
[2023/01/16 02:43] | TRAIN(050): [2050/2211] Batch: 0.0873 (0.0955) Data: 0.0018 (0.0024) Loss: 0.0647 (0.0473)
[2023/01/16 02:43] | TRAIN(050): [2100/2211] Batch: 0.0947 (0.0954) Data: 0.0019 (0.0024) Loss: 0.0602 (0.0474)
[2023/01/16 02:43] | TRAIN(050): [2150/2211] Batch: 0.0890 (0.0953) Data: 0.0019 (0.0024) Loss: 0.1858 (0.0479)
[2023/01/16 02:43] | TRAIN(050): [2200/2211] Batch: 0.0868 (0.0953) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0481)
[2023/01/16 02:43] | ------------------------------------------------------------
[2023/01/16 02:43] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:43] | ------------------------------------------------------------
[2023/01/16 02:43] |    TRAIN(50)     0:03:31     0:00:05     0:03:25      0.0481
[2023/01/16 02:43] | ------------------------------------------------------------
[2023/01/16 02:43] | **************************************************
[2023/01/16 02:43] | TRAIN(051): [  50/2211] Batch: 0.1140 (0.1325) Data: 0.0018 (0.0248) Loss: 0.0597 (0.0429)
[2023/01/16 02:43] | TRAIN(051): [ 100/2211] Batch: 0.0871 (0.1205) Data: 0.0018 (0.0134) Loss: 0.0431 (0.0414)
[2023/01/16 02:43] | TRAIN(051): [ 150/2211] Batch: 0.1046 (0.1153) Data: 0.0013 (0.0096) Loss: 0.0828 (0.0400)
[2023/01/16 02:44] | TRAIN(051): [ 200/2211] Batch: 0.0920 (0.1108) Data: 0.0018 (0.0076) Loss: 0.0528 (0.0421)
[2023/01/16 02:44] | TRAIN(051): [ 250/2211] Batch: 0.1037 (0.1082) Data: 0.0018 (0.0064) Loss: 0.1142 (0.0438)
[2023/01/16 02:44] | TRAIN(051): [ 300/2211] Batch: 0.0876 (0.1068) Data: 0.0017 (0.0057) Loss: 0.1030 (0.0454)
[2023/01/16 02:44] | TRAIN(051): [ 350/2211] Batch: 0.0908 (0.1051) Data: 0.0016 (0.0051) Loss: 0.3645 (0.0450)
[2023/01/16 02:44] | TRAIN(051): [ 400/2211] Batch: 0.0920 (0.1039) Data: 0.0019 (0.0047) Loss: 0.1185 (0.0454)
[2023/01/16 02:44] | TRAIN(051): [ 450/2211] Batch: 0.1000 (0.1030) Data: 0.0018 (0.0044) Loss: 0.0148 (0.0449)
[2023/01/16 02:44] | TRAIN(051): [ 500/2211] Batch: 0.0881 (0.1018) Data: 0.0017 (0.0041) Loss: 0.0019 (0.0457)
[2023/01/16 02:44] | TRAIN(051): [ 550/2211] Batch: 0.0885 (0.1012) Data: 0.0017 (0.0039) Loss: 0.0000 (0.0443)
[2023/01/16 02:44] | TRAIN(051): [ 600/2211] Batch: 0.0942 (0.1005) Data: 0.0016 (0.0037) Loss: 0.0000 (0.0443)
[2023/01/16 02:44] | TRAIN(051): [ 650/2211] Batch: 0.0884 (0.1000) Data: 0.0018 (0.0036) Loss: 0.0494 (0.0442)
[2023/01/16 02:44] | TRAIN(051): [ 700/2211] Batch: 0.1177 (0.0996) Data: 0.0022 (0.0034) Loss: 0.0000 (0.0438)
[2023/01/16 02:44] | TRAIN(051): [ 750/2211] Batch: 0.0883 (0.1001) Data: 0.0017 (0.0034) Loss: 0.0677 (0.0449)
[2023/01/16 02:45] | TRAIN(051): [ 800/2211] Batch: 0.0977 (0.0998) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0455)
[2023/01/16 02:45] | TRAIN(051): [ 850/2211] Batch: 0.0913 (0.0994) Data: 0.0018 (0.0032) Loss: 0.2881 (0.0465)
[2023/01/16 02:45] | TRAIN(051): [ 900/2211] Batch: 0.1050 (0.0992) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0470)
[2023/01/16 02:45] | TRAIN(051): [ 950/2211] Batch: 0.0913 (0.0990) Data: 0.0017 (0.0030) Loss: 0.1053 (0.0471)
[2023/01/16 02:45] | TRAIN(051): [1000/2211] Batch: 0.1040 (0.0990) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0474)
[2023/01/16 02:45] | TRAIN(051): [1050/2211] Batch: 0.0947 (0.0986) Data: 0.0019 (0.0029) Loss: 0.0578 (0.0473)
[2023/01/16 02:45] | TRAIN(051): [1100/2211] Batch: 0.0868 (0.0983) Data: 0.0018 (0.0029) Loss: 0.0727 (0.0476)
[2023/01/16 02:45] | TRAIN(051): [1150/2211] Batch: 0.0901 (0.0981) Data: 0.0017 (0.0028) Loss: 0.0617 (0.0477)
[2023/01/16 02:45] | TRAIN(051): [1200/2211] Batch: 0.1186 (0.0983) Data: 0.0021 (0.0028) Loss: 0.0841 (0.0484)
[2023/01/16 02:45] | TRAIN(051): [1250/2211] Batch: 0.0990 (0.0983) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0493)
[2023/01/16 02:45] | TRAIN(051): [1300/2211] Batch: 0.0885 (0.0982) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0492)
[2023/01/16 02:45] | TRAIN(051): [1350/2211] Batch: 0.0877 (0.0979) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0492)
[2023/01/16 02:45] | TRAIN(051): [1400/2211] Batch: 0.1050 (0.0977) Data: 0.0019 (0.0026) Loss: 0.0154 (0.0488)
[2023/01/16 02:46] | TRAIN(051): [1450/2211] Batch: 0.0907 (0.0976) Data: 0.0018 (0.0026) Loss: 0.0316 (0.0489)
[2023/01/16 02:46] | TRAIN(051): [1500/2211] Batch: 0.0866 (0.0974) Data: 0.0018 (0.0026) Loss: 0.2451 (0.0492)
[2023/01/16 02:46] | TRAIN(051): [1550/2211] Batch: 0.1278 (0.0973) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0491)
[2023/01/16 02:46] | TRAIN(051): [1600/2211] Batch: 0.0873 (0.0974) Data: 0.0017 (0.0025) Loss: 0.0632 (0.0490)
[2023/01/16 02:46] | TRAIN(051): [1650/2211] Batch: 0.0868 (0.0975) Data: 0.0017 (0.0025) Loss: 0.1571 (0.0486)
[2023/01/16 02:46] | TRAIN(051): [1700/2211] Batch: 0.1068 (0.0973) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0487)
[2023/01/16 02:46] | TRAIN(051): [1750/2211] Batch: 0.0902 (0.0973) Data: 0.0017 (0.0025) Loss: 0.1515 (0.0485)
[2023/01/16 02:46] | TRAIN(051): [1800/2211] Batch: 0.0896 (0.0971) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0483)
[2023/01/16 02:46] | TRAIN(051): [1850/2211] Batch: 0.0892 (0.0970) Data: 0.0022 (0.0024) Loss: 0.0029 (0.0482)
[2023/01/16 02:46] | TRAIN(051): [1900/2211] Batch: 0.0860 (0.0968) Data: 0.0018 (0.0024) Loss: 0.1051 (0.0486)
[2023/01/16 02:46] | TRAIN(051): [1950/2211] Batch: 0.0890 (0.0967) Data: 0.0017 (0.0024) Loss: 0.1620 (0.0486)
[2023/01/16 02:46] | TRAIN(051): [2000/2211] Batch: 0.1000 (0.0966) Data: 0.0015 (0.0024) Loss: 0.0163 (0.0488)
[2023/01/16 02:46] | TRAIN(051): [2050/2211] Batch: 0.0891 (0.0965) Data: 0.0018 (0.0024) Loss: 0.1616 (0.0491)
[2023/01/16 02:47] | TRAIN(051): [2100/2211] Batch: 0.0942 (0.0964) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0491)
[2023/01/16 02:47] | TRAIN(051): [2150/2211] Batch: 0.0988 (0.0962) Data: 0.0017 (0.0024) Loss: 0.0087 (0.0487)
[2023/01/16 02:47] | TRAIN(051): [2200/2211] Batch: 0.1227 (0.0966) Data: 0.0021 (0.0024) Loss: 0.0440 (0.0490)
[2023/01/16 02:47] | ------------------------------------------------------------
[2023/01/16 02:47] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:47] | ------------------------------------------------------------
[2023/01/16 02:47] |    TRAIN(51)     0:03:33     0:00:05     0:03:28      0.0490
[2023/01/16 02:47] | ------------------------------------------------------------
[2023/01/16 02:47] | **************************************************
[2023/01/16 02:47] | TRAIN(052): [  50/2211] Batch: 0.0920 (0.1274) Data: 0.0017 (0.0241) Loss: 0.0000 (0.0440)
[2023/01/16 02:47] | TRAIN(052): [ 100/2211] Batch: 0.0899 (0.1118) Data: 0.0017 (0.0131) Loss: 0.0025 (0.0495)
[2023/01/16 02:47] | TRAIN(052): [ 150/2211] Batch: 0.0982 (0.1076) Data: 0.0019 (0.0094) Loss: 0.0917 (0.0495)
[2023/01/16 02:47] | TRAIN(052): [ 200/2211] Batch: 0.0901 (0.1062) Data: 0.0016 (0.0075) Loss: 0.0000 (0.0449)
[2023/01/16 02:47] | TRAIN(052): [ 250/2211] Batch: 0.0940 (0.1031) Data: 0.0019 (0.0064) Loss: 0.1392 (0.0448)
[2023/01/16 02:47] | TRAIN(052): [ 300/2211] Batch: 0.1216 (0.1022) Data: 0.0022 (0.0056) Loss: 0.0000 (0.0440)
[2023/01/16 02:47] | TRAIN(052): [ 350/2211] Batch: 0.1019 (0.1022) Data: 0.0019 (0.0051) Loss: 0.0000 (0.0454)
[2023/01/16 02:47] | TRAIN(052): [ 400/2211] Batch: 0.0875 (0.1009) Data: 0.0018 (0.0047) Loss: 0.1539 (0.0473)
[2023/01/16 02:47] | TRAIN(052): [ 450/2211] Batch: 0.1166 (0.1005) Data: 0.0022 (0.0044) Loss: 0.1278 (0.0465)
[2023/01/16 02:48] | TRAIN(052): [ 500/2211] Batch: 0.0923 (0.1007) Data: 0.0017 (0.0041) Loss: 0.0374 (0.0456)
[2023/01/16 02:48] | TRAIN(052): [ 550/2211] Batch: 0.0898 (0.1003) Data: 0.0018 (0.0039) Loss: 0.0000 (0.0465)
[2023/01/16 02:48] | TRAIN(052): [ 600/2211] Batch: 0.0893 (0.0996) Data: 0.0019 (0.0037) Loss: 0.0559 (0.0470)
[2023/01/16 02:48] | TRAIN(052): [ 650/2211] Batch: 0.0903 (0.0989) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0452)
[2023/01/16 02:48] | TRAIN(052): [ 700/2211] Batch: 0.0950 (0.0988) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0446)
[2023/01/16 02:48] | TRAIN(052): [ 750/2211] Batch: 0.0921 (0.0994) Data: 0.0019 (0.0034) Loss: 0.0612 (0.0435)
[2023/01/16 02:48] | TRAIN(052): [ 800/2211] Batch: 0.0846 (0.0990) Data: 0.0014 (0.0033) Loss: 0.0343 (0.0431)
[2023/01/16 02:48] | TRAIN(052): [ 850/2211] Batch: 0.0973 (0.0987) Data: 0.0018 (0.0032) Loss: 0.0083 (0.0427)
[2023/01/16 02:48] | TRAIN(052): [ 900/2211] Batch: 0.1001 (0.0985) Data: 0.0025 (0.0031) Loss: 0.0794 (0.0424)
[2023/01/16 02:48] | TRAIN(052): [ 950/2211] Batch: 0.0899 (0.0983) Data: 0.0019 (0.0030) Loss: 0.0851 (0.0430)
[2023/01/16 02:48] | TRAIN(052): [1000/2211] Batch: 0.0974 (0.0988) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0437)
[2023/01/16 02:48] | TRAIN(052): [1050/2211] Batch: 0.0947 (0.0988) Data: 0.0018 (0.0029) Loss: 0.1060 (0.0438)
[2023/01/16 02:49] | TRAIN(052): [1100/2211] Batch: 0.0976 (0.0985) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0439)
[2023/01/16 02:49] | TRAIN(052): [1150/2211] Batch: 0.0885 (0.0982) Data: 0.0018 (0.0028) Loss: 0.1222 (0.0435)
[2023/01/16 02:49] | TRAIN(052): [1200/2211] Batch: 0.0914 (0.0983) Data: 0.0018 (0.0028) Loss: 0.1389 (0.0439)
[2023/01/16 02:49] | TRAIN(052): [1250/2211] Batch: 0.0983 (0.0981) Data: 0.0018 (0.0028) Loss: 0.0087 (0.0449)
[2023/01/16 02:49] | TRAIN(052): [1300/2211] Batch: 0.0962 (0.0981) Data: 0.0020 (0.0027) Loss: 0.0098 (0.0452)
[2023/01/16 02:49] | TRAIN(052): [1350/2211] Batch: 0.0915 (0.0979) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0450)
[2023/01/16 02:49] | TRAIN(052): [1400/2211] Batch: 0.0955 (0.0978) Data: 0.0018 (0.0027) Loss: 0.0033 (0.0454)
[2023/01/16 02:49] | TRAIN(052): [1450/2211] Batch: 0.0882 (0.0977) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0454)
[2023/01/16 02:49] | TRAIN(052): [1500/2211] Batch: 0.1013 (0.0976) Data: 0.0019 (0.0026) Loss: 0.0055 (0.0457)
[2023/01/16 02:49] | TRAIN(052): [1550/2211] Batch: 0.0917 (0.0977) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0457)
[2023/01/16 02:49] | TRAIN(052): [1600/2211] Batch: 0.1022 (0.0977) Data: 0.0019 (0.0026) Loss: 0.0050 (0.0454)
[2023/01/16 02:49] | TRAIN(052): [1650/2211] Batch: 0.0928 (0.0978) Data: 0.0020 (0.0025) Loss: 0.0274 (0.0456)
[2023/01/16 02:50] | TRAIN(052): [1700/2211] Batch: 0.1005 (0.0976) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0457)
[2023/01/16 02:50] | TRAIN(052): [1750/2211] Batch: 0.0937 (0.0975) Data: 0.0018 (0.0025) Loss: 0.0774 (0.0458)
[2023/01/16 02:50] | TRAIN(052): [1800/2211] Batch: 0.0884 (0.0974) Data: 0.0018 (0.0025) Loss: 0.0173 (0.0457)
[2023/01/16 02:50] | TRAIN(052): [1850/2211] Batch: 0.1098 (0.0973) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0455)
[2023/01/16 02:50] | TRAIN(052): [1900/2211] Batch: 0.0889 (0.0971) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0454)
[2023/01/16 02:50] | TRAIN(052): [1950/2211] Batch: 0.1058 (0.0971) Data: 0.0019 (0.0024) Loss: 0.0064 (0.0454)
[2023/01/16 02:50] | TRAIN(052): [2000/2211] Batch: 0.0941 (0.0972) Data: 0.0018 (0.0024) Loss: 0.1066 (0.0452)
[2023/01/16 02:50] | TRAIN(052): [2050/2211] Batch: 0.1090 (0.0972) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0454)
[2023/01/16 02:50] | TRAIN(052): [2100/2211] Batch: 0.0866 (0.0971) Data: 0.0017 (0.0024) Loss: 0.0283 (0.0455)
[2023/01/16 02:50] | TRAIN(052): [2150/2211] Batch: 0.0900 (0.0969) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0455)
[2023/01/16 02:50] | TRAIN(052): [2200/2211] Batch: 0.0922 (0.0970) Data: 0.0017 (0.0024) Loss: 0.0207 (0.0453)
[2023/01/16 02:50] | ------------------------------------------------------------
[2023/01/16 02:50] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:50] | ------------------------------------------------------------
[2023/01/16 02:50] |    TRAIN(52)     0:03:34     0:00:05     0:03:29      0.0453
[2023/01/16 02:50] | ------------------------------------------------------------
[2023/01/16 02:50] | **************************************************
[2023/01/16 02:50] | TRAIN(053): [  50/2211] Batch: 0.0961 (0.1206) Data: 0.0018 (0.0249) Loss: 0.0000 (0.0468)
[2023/01/16 02:50] | TRAIN(053): [ 100/2211] Batch: 0.0889 (0.1103) Data: 0.0017 (0.0135) Loss: 0.0065 (0.0437)
[2023/01/16 02:51] | TRAIN(053): [ 150/2211] Batch: 0.0882 (0.1047) Data: 0.0019 (0.0096) Loss: 0.0266 (0.0412)
[2023/01/16 02:51] | TRAIN(053): [ 200/2211] Batch: 0.0952 (0.1034) Data: 0.0018 (0.0077) Loss: 0.0000 (0.0376)
[2023/01/16 02:51] | TRAIN(053): [ 250/2211] Batch: 0.0884 (0.1020) Data: 0.0018 (0.0065) Loss: 0.0000 (0.0352)
[2023/01/16 02:51] | TRAIN(053): [ 300/2211] Batch: 0.0883 (0.1006) Data: 0.0017 (0.0057) Loss: 0.0043 (0.0332)
[2023/01/16 02:51] | TRAIN(053): [ 350/2211] Batch: 0.0914 (0.0999) Data: 0.0016 (0.0052) Loss: 0.0000 (0.0372)
[2023/01/16 02:51] | TRAIN(053): [ 400/2211] Batch: 0.0916 (0.1008) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0388)
[2023/01/16 02:51] | TRAIN(053): [ 450/2211] Batch: 0.1187 (0.1001) Data: 0.0023 (0.0044) Loss: 0.0079 (0.0399)
[2023/01/16 02:51] | TRAIN(053): [ 500/2211] Batch: 0.0874 (0.0999) Data: 0.0017 (0.0042) Loss: 0.0016 (0.0412)
[2023/01/16 02:51] | TRAIN(053): [ 550/2211] Batch: 0.0889 (0.0990) Data: 0.0018 (0.0040) Loss: 0.1022 (0.0416)
[2023/01/16 02:51] | TRAIN(053): [ 600/2211] Batch: 0.0948 (0.0991) Data: 0.0017 (0.0038) Loss: 0.0000 (0.0431)
[2023/01/16 02:51] | TRAIN(053): [ 650/2211] Batch: 0.1061 (0.0993) Data: 0.0022 (0.0036) Loss: 0.2011 (0.0436)
[2023/01/16 02:51] | TRAIN(053): [ 700/2211] Batch: 0.0903 (0.0989) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0445)
[2023/01/16 02:52] | TRAIN(053): [ 750/2211] Batch: 0.0920 (0.0985) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0449)
[2023/01/16 02:52] | TRAIN(053): [ 800/2211] Batch: 0.0956 (0.0989) Data: 0.0013 (0.0033) Loss: 0.0000 (0.0459)
[2023/01/16 02:52] | TRAIN(053): [ 850/2211] Batch: 0.0941 (0.0986) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0446)
[2023/01/16 02:52] | TRAIN(053): [ 900/2211] Batch: 0.1209 (0.0986) Data: 0.0016 (0.0031) Loss: 0.0000 (0.0444)
[2023/01/16 02:52] | TRAIN(053): [ 950/2211] Batch: 0.1095 (0.0990) Data: 0.0014 (0.0030) Loss: 0.1591 (0.0448)
[2023/01/16 02:52] | TRAIN(053): [1000/2211] Batch: 0.0964 (0.0989) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0452)
[2023/01/16 02:52] | TRAIN(053): [1050/2211] Batch: 0.1243 (0.0991) Data: 0.0044 (0.0029) Loss: 0.1586 (0.0458)
[2023/01/16 02:52] | TRAIN(053): [1100/2211] Batch: 0.1312 (0.0996) Data: 0.0022 (0.0029) Loss: 0.0000 (0.0467)
[2023/01/16 02:52] | TRAIN(053): [1150/2211] Batch: 0.1004 (0.0995) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0468)
[2023/01/16 02:52] | TRAIN(053): [1200/2211] Batch: 0.0921 (0.0994) Data: 0.0016 (0.0028) Loss: 0.0110 (0.0468)
[2023/01/16 02:52] | TRAIN(053): [1250/2211] Batch: 0.0918 (0.0992) Data: 0.0018 (0.0028) Loss: 0.0066 (0.0471)
[2023/01/16 02:52] | TRAIN(053): [1300/2211] Batch: 0.1030 (0.0991) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0468)
[2023/01/16 02:53] | TRAIN(053): [1350/2211] Batch: 0.0953 (0.0989) Data: 0.0018 (0.0027) Loss: 0.2459 (0.0471)
[2023/01/16 02:53] | TRAIN(053): [1400/2211] Batch: 0.0973 (0.0989) Data: 0.0015 (0.0026) Loss: 0.1695 (0.0474)
[2023/01/16 02:53] | TRAIN(053): [1450/2211] Batch: 0.0927 (0.0989) Data: 0.0018 (0.0026) Loss: 0.0045 (0.0472)
[2023/01/16 02:53] | TRAIN(053): [1500/2211] Batch: 0.1042 (0.0989) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0466)
[2023/01/16 02:53] | TRAIN(053): [1550/2211] Batch: 0.0948 (0.0989) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0463)
[2023/01/16 02:53] | TRAIN(053): [1600/2211] Batch: 0.1057 (0.0989) Data: 0.0017 (0.0025) Loss: 0.0321 (0.0469)
[2023/01/16 02:53] | TRAIN(053): [1650/2211] Batch: 0.0880 (0.0990) Data: 0.0018 (0.0025) Loss: 0.2142 (0.0472)
[2023/01/16 02:53] | TRAIN(053): [1700/2211] Batch: 0.1260 (0.0989) Data: 0.0022 (0.0025) Loss: 0.1623 (0.0472)
[2023/01/16 02:53] | TRAIN(053): [1750/2211] Batch: 0.0987 (0.0989) Data: 0.0017 (0.0025) Loss: 0.0155 (0.0476)
[2023/01/16 02:53] | TRAIN(053): [1800/2211] Batch: 0.0916 (0.0988) Data: 0.0018 (0.0025) Loss: 0.0906 (0.0477)
[2023/01/16 02:53] | TRAIN(053): [1850/2211] Batch: 0.0918 (0.0985) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0477)
[2023/01/16 02:53] | TRAIN(053): [1900/2211] Batch: 0.0968 (0.0985) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0476)
[2023/01/16 02:54] | TRAIN(053): [1950/2211] Batch: 0.0893 (0.0983) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0477)
[2023/01/16 02:54] | TRAIN(053): [2000/2211] Batch: 0.0895 (0.0981) Data: 0.0017 (0.0024) Loss: 0.0076 (0.0476)
[2023/01/16 02:54] | TRAIN(053): [2050/2211] Batch: 0.0981 (0.0980) Data: 0.0018 (0.0024) Loss: 0.1289 (0.0475)
[2023/01/16 02:54] | TRAIN(053): [2100/2211] Batch: 0.0950 (0.0980) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0471)
[2023/01/16 02:54] | TRAIN(053): [2150/2211] Batch: 0.0910 (0.0979) Data: 0.0017 (0.0023) Loss: 0.1178 (0.0468)
[2023/01/16 02:54] | TRAIN(053): [2200/2211] Batch: 0.0892 (0.0978) Data: 0.0015 (0.0023) Loss: 0.0000 (0.0466)
[2023/01/16 02:54] | ------------------------------------------------------------
[2023/01/16 02:54] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:54] | ------------------------------------------------------------
[2023/01/16 02:54] |    TRAIN(53)     0:03:36     0:00:05     0:03:31      0.0468
[2023/01/16 02:54] | ------------------------------------------------------------
[2023/01/16 02:54] | **************************************************
[2023/01/16 02:54] | TRAIN(054): [  50/2211] Batch: 0.0952 (0.1231) Data: 0.0019 (0.0241) Loss: 0.0000 (0.0133)
[2023/01/16 02:54] | TRAIN(054): [ 100/2211] Batch: 0.0914 (0.1085) Data: 0.0017 (0.0131) Loss: 0.0044 (0.0347)
[2023/01/16 02:54] | TRAIN(054): [ 150/2211] Batch: 0.1118 (0.1039) Data: 0.0019 (0.0093) Loss: 0.0061 (0.0403)
[2023/01/16 02:54] | TRAIN(054): [ 200/2211] Batch: 0.1176 (0.1020) Data: 0.0022 (0.0075) Loss: 0.0540 (0.0440)
[2023/01/16 02:54] | TRAIN(054): [ 250/2211] Batch: 0.0998 (0.1028) Data: 0.0018 (0.0064) Loss: 0.1530 (0.0442)
[2023/01/16 02:54] | TRAIN(054): [ 300/2211] Batch: 0.0947 (0.1015) Data: 0.0017 (0.0056) Loss: 0.0119 (0.0420)
[2023/01/16 02:55] | TRAIN(054): [ 350/2211] Batch: 0.0928 (0.1022) Data: 0.0019 (0.0051) Loss: 0.1165 (0.0424)
[2023/01/16 02:55] | TRAIN(054): [ 400/2211] Batch: 0.1014 (0.1014) Data: 0.0019 (0.0047) Loss: 0.0000 (0.0426)
[2023/01/16 02:55] | TRAIN(054): [ 450/2211] Batch: 0.0916 (0.1013) Data: 0.0018 (0.0044) Loss: 0.0234 (0.0456)
[2023/01/16 02:55] | TRAIN(054): [ 500/2211] Batch: 0.0946 (0.1008) Data: 0.0016 (0.0041) Loss: 0.0000 (0.0450)
[2023/01/16 02:55] | TRAIN(054): [ 550/2211] Batch: 0.0826 (0.1003) Data: 0.0019 (0.0039) Loss: 0.0680 (0.0450)
[2023/01/16 02:55] | TRAIN(054): [ 600/2211] Batch: 0.0877 (0.0997) Data: 0.0018 (0.0037) Loss: 0.3630 (0.0462)
[2023/01/16 02:55] | TRAIN(054): [ 650/2211] Batch: 0.0977 (0.0990) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0471)
[2023/01/16 02:55] | TRAIN(054): [ 700/2211] Batch: 0.0917 (0.0988) Data: 0.0015 (0.0035) Loss: 0.1634 (0.0465)
[2023/01/16 02:55] | TRAIN(054): [ 750/2211] Batch: 0.0896 (0.0985) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0469)
[2023/01/16 02:55] | TRAIN(054): [ 800/2211] Batch: 0.0900 (0.0983) Data: 0.0018 (0.0032) Loss: 0.1235 (0.0473)
[2023/01/16 02:55] | TRAIN(054): [ 850/2211] Batch: 0.0915 (0.0979) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0472)
[2023/01/16 02:55] | TRAIN(054): [ 900/2211] Batch: 0.0909 (0.0979) Data: 0.0016 (0.0031) Loss: 0.0000 (0.0478)
[2023/01/16 02:55] | TRAIN(054): [ 950/2211] Batch: 0.1020 (0.0981) Data: 0.0019 (0.0030) Loss: 0.0293 (0.0489)
[2023/01/16 02:56] | TRAIN(054): [1000/2211] Batch: 0.0887 (0.0981) Data: 0.0018 (0.0030) Loss: 0.0261 (0.0496)
[2023/01/16 02:56] | TRAIN(054): [1050/2211] Batch: 0.1247 (0.0981) Data: 0.0021 (0.0029) Loss: 0.0651 (0.0498)
[2023/01/16 02:56] | TRAIN(054): [1100/2211] Batch: 0.0935 (0.0979) Data: 0.0017 (0.0029) Loss: 0.1117 (0.0497)
[2023/01/16 02:56] | TRAIN(054): [1150/2211] Batch: 0.1173 (0.0977) Data: 0.0022 (0.0028) Loss: 0.1843 (0.0497)
[2023/01/16 02:56] | TRAIN(054): [1200/2211] Batch: 0.0882 (0.0979) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0494)
[2023/01/16 02:56] | TRAIN(054): [1250/2211] Batch: 0.0901 (0.0980) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0490)
[2023/01/16 02:56] | TRAIN(054): [1300/2211] Batch: 0.0951 (0.0980) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0491)
[2023/01/16 02:56] | TRAIN(054): [1350/2211] Batch: 0.1001 (0.0980) Data: 0.0017 (0.0027) Loss: 0.0164 (0.0487)
[2023/01/16 02:56] | TRAIN(054): [1400/2211] Batch: 0.0910 (0.0979) Data: 0.0019 (0.0026) Loss: 0.0040 (0.0480)
[2023/01/16 02:56] | TRAIN(054): [1450/2211] Batch: 0.0979 (0.0980) Data: 0.0017 (0.0026) Loss: 0.0327 (0.0477)
[2023/01/16 02:56] | TRAIN(054): [1500/2211] Batch: 0.0973 (0.0978) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0476)
[2023/01/16 02:56] | TRAIN(054): [1550/2211] Batch: 0.0856 (0.0978) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0474)
[2023/01/16 02:57] | TRAIN(054): [1600/2211] Batch: 0.0978 (0.0977) Data: 0.0018 (0.0025) Loss: 0.0010 (0.0472)
[2023/01/16 02:57] | TRAIN(054): [1650/2211] Batch: 0.0916 (0.0978) Data: 0.0017 (0.0025) Loss: 0.0048 (0.0475)
[2023/01/16 02:57] | TRAIN(054): [1700/2211] Batch: 0.1211 (0.0979) Data: 0.0024 (0.0025) Loss: 0.0000 (0.0477)
[2023/01/16 02:57] | TRAIN(054): [1750/2211] Batch: 0.0893 (0.0983) Data: 0.0018 (0.0025) Loss: 0.1479 (0.0477)
[2023/01/16 02:57] | TRAIN(054): [1800/2211] Batch: 0.0946 (0.0983) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0477)
[2023/01/16 02:57] | TRAIN(054): [1850/2211] Batch: 0.0951 (0.0984) Data: 0.0017 (0.0024) Loss: 0.0055 (0.0475)
[2023/01/16 02:57] | TRAIN(054): [1900/2211] Batch: 0.0944 (0.0985) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0472)
[2023/01/16 02:57] | TRAIN(054): [1950/2211] Batch: 0.0956 (0.0984) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0478)
[2023/01/16 02:57] | TRAIN(054): [2000/2211] Batch: 0.0920 (0.0984) Data: 0.0018 (0.0024) Loss: 0.0811 (0.0483)
[2023/01/16 02:57] | TRAIN(054): [2050/2211] Batch: 0.0949 (0.0983) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0485)
[2023/01/16 02:57] | TRAIN(054): [2100/2211] Batch: 0.0914 (0.0983) Data: 0.0018 (0.0024) Loss: 0.0176 (0.0487)
[2023/01/16 02:57] | TRAIN(054): [2150/2211] Batch: 0.1148 (0.0982) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0487)
[2023/01/16 02:58] | TRAIN(054): [2200/2211] Batch: 0.1053 (0.0982) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0487)
[2023/01/16 02:58] | ------------------------------------------------------------
[2023/01/16 02:58] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 02:58] | ------------------------------------------------------------
[2023/01/16 02:58] |    TRAIN(54)     0:03:37     0:00:05     0:03:32      0.0485
[2023/01/16 02:58] | ------------------------------------------------------------
[2023/01/16 02:58] | **************************************************
[2023/01/16 02:58] | TRAIN(055): [  50/2211] Batch: 0.0952 (0.1265) Data: 0.0016 (0.0252) Loss: 0.1395 (0.0552)
[2023/01/16 02:58] | TRAIN(055): [ 100/2211] Batch: 0.0877 (0.1102) Data: 0.0017 (0.0136) Loss: 0.0295 (0.0513)
[2023/01/16 02:58] | TRAIN(055): [ 150/2211] Batch: 0.0903 (0.1054) Data: 0.0018 (0.0097) Loss: 0.0697 (0.0512)
[2023/01/16 02:58] | TRAIN(055): [ 200/2211] Batch: 0.0930 (0.1046) Data: 0.0018 (0.0078) Loss: 0.0630 (0.0540)
[2023/01/16 02:58] | TRAIN(055): [ 250/2211] Batch: 0.0897 (0.1025) Data: 0.0017 (0.0066) Loss: 0.0000 (0.0521)
[2023/01/16 02:58] | TRAIN(055): [ 300/2211] Batch: 0.1008 (0.1010) Data: 0.0019 (0.0058) Loss: 0.0000 (0.0531)
[2023/01/16 02:58] | TRAIN(055): [ 350/2211] Batch: 0.1560 (0.1010) Data: 0.0029 (0.0052) Loss: 0.1361 (0.0544)
[2023/01/16 02:58] | TRAIN(055): [ 400/2211] Batch: 0.0981 (0.1022) Data: 0.0019 (0.0048) Loss: 0.0910 (0.0519)
[2023/01/16 02:58] | TRAIN(055): [ 450/2211] Batch: 0.0898 (0.1014) Data: 0.0019 (0.0045) Loss: 0.0231 (0.0516)
[2023/01/16 02:58] | TRAIN(055): [ 500/2211] Batch: 0.0961 (0.1010) Data: 0.0020 (0.0042) Loss: 0.0000 (0.0507)
[2023/01/16 02:58] | TRAIN(055): [ 550/2211] Batch: 0.0926 (0.1007) Data: 0.0018 (0.0040) Loss: 0.1184 (0.0521)
[2023/01/16 02:59] | TRAIN(055): [ 600/2211] Batch: 0.1006 (0.1001) Data: 0.0017 (0.0038) Loss: 0.0548 (0.0518)
[2023/01/16 02:59] | TRAIN(055): [ 650/2211] Batch: 0.0920 (0.1000) Data: 0.0018 (0.0037) Loss: 0.0046 (0.0505)
[2023/01/16 02:59] | TRAIN(055): [ 700/2211] Batch: 0.0939 (0.0999) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0500)
[2023/01/16 02:59] | TRAIN(055): [ 750/2211] Batch: 0.0915 (0.0995) Data: 0.0017 (0.0034) Loss: 0.0140 (0.0493)
[2023/01/16 02:59] | TRAIN(055): [ 800/2211] Batch: 0.0974 (0.0994) Data: 0.0017 (0.0033) Loss: 0.0101 (0.0493)
[2023/01/16 02:59] | TRAIN(055): [ 850/2211] Batch: 0.1087 (0.0992) Data: 0.0019 (0.0032) Loss: 0.0976 (0.0489)
[2023/01/16 02:59] | TRAIN(055): [ 900/2211] Batch: 0.0923 (0.0990) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0485)
[2023/01/16 02:59] | TRAIN(055): [ 950/2211] Batch: 0.0880 (0.0987) Data: 0.0018 (0.0031) Loss: 0.0416 (0.0476)
[2023/01/16 02:59] | TRAIN(055): [1000/2211] Batch: 0.0863 (0.0983) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0477)
[2023/01/16 02:59] | TRAIN(055): [1050/2211] Batch: 0.0881 (0.0979) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0489)
[2023/01/16 02:59] | TRAIN(055): [1100/2211] Batch: 0.0867 (0.0974) Data: 0.0021 (0.0029) Loss: 0.0001 (0.0490)
[2023/01/16 02:59] | TRAIN(055): [1150/2211] Batch: 0.0874 (0.0969) Data: 0.0019 (0.0029) Loss: 0.2285 (0.0486)
[2023/01/16 02:59] | TRAIN(055): [1200/2211] Batch: 0.1052 (0.0965) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0485)
[2023/01/16 03:00] | TRAIN(055): [1250/2211] Batch: 0.0823 (0.0960) Data: 0.0019 (0.0028) Loss: 0.0918 (0.0482)
[2023/01/16 03:00] | TRAIN(055): [1300/2211] Batch: 0.0865 (0.0957) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0482)
[2023/01/16 03:00] | TRAIN(055): [1350/2211] Batch: 0.0878 (0.0955) Data: 0.0019 (0.0027) Loss: 0.1163 (0.0484)
[2023/01/16 03:00] | TRAIN(055): [1400/2211] Batch: 0.0955 (0.0952) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0480)
[2023/01/16 03:00] | TRAIN(055): [1450/2211] Batch: 0.0980 (0.0954) Data: 0.0018 (0.0027) Loss: 0.0019 (0.0478)
[2023/01/16 03:00] | TRAIN(055): [1500/2211] Batch: 0.1006 (0.0956) Data: 0.0018 (0.0026) Loss: 0.2276 (0.0479)
[2023/01/16 03:00] | TRAIN(055): [1550/2211] Batch: 0.1184 (0.0957) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0477)
[2023/01/16 03:00] | TRAIN(055): [1600/2211] Batch: 0.0974 (0.0957) Data: 0.0018 (0.0026) Loss: 0.1101 (0.0475)
[2023/01/16 03:00] | TRAIN(055): [1650/2211] Batch: 0.1007 (0.0957) Data: 0.0015 (0.0026) Loss: 0.0000 (0.0479)
[2023/01/16 03:00] | TRAIN(055): [1700/2211] Batch: 0.0950 (0.0957) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0481)
[2023/01/16 03:00] | TRAIN(055): [1750/2211] Batch: 0.0922 (0.0957) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0476)
[2023/01/16 03:00] | TRAIN(055): [1800/2211] Batch: 0.0906 (0.0956) Data: 0.0015 (0.0025) Loss: 0.0285 (0.0476)
[2023/01/16 03:00] | TRAIN(055): [1850/2211] Batch: 0.0947 (0.0957) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0473)
[2023/01/16 03:01] | TRAIN(055): [1900/2211] Batch: 0.0959 (0.0957) Data: 0.0019 (0.0025) Loss: 0.1522 (0.0475)
[2023/01/16 03:01] | TRAIN(055): [1950/2211] Batch: 0.0967 (0.0957) Data: 0.0019 (0.0024) Loss: 0.0418 (0.0475)
[2023/01/16 03:01] | TRAIN(055): [2000/2211] Batch: 0.0931 (0.0957) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0477)
[2023/01/16 03:01] | TRAIN(055): [2050/2211] Batch: 0.0956 (0.0958) Data: 0.0017 (0.0024) Loss: 0.1296 (0.0475)
[2023/01/16 03:01] | TRAIN(055): [2100/2211] Batch: 0.1035 (0.0959) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0472)
[2023/01/16 03:01] | TRAIN(055): [2150/2211] Batch: 0.1091 (0.0959) Data: 0.0017 (0.0024) Loss: 0.0156 (0.0471)
[2023/01/16 03:01] | TRAIN(055): [2200/2211] Batch: 0.0996 (0.0959) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0470)
[2023/01/16 03:01] | ------------------------------------------------------------
[2023/01/16 03:01] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:01] | ------------------------------------------------------------
[2023/01/16 03:01] |    TRAIN(55)     0:03:32     0:00:05     0:03:26      0.0470
[2023/01/16 03:01] | ------------------------------------------------------------
[2023/01/16 03:01] | **************************************************
[2023/01/16 03:01] | TRAIN(056): [  50/2211] Batch: 0.0935 (0.1268) Data: 0.0020 (0.0237) Loss: 0.0958 (0.0643)
[2023/01/16 03:01] | TRAIN(056): [ 100/2211] Batch: 0.1015 (0.1126) Data: 0.0014 (0.0129) Loss: 0.0000 (0.0525)
[2023/01/16 03:01] | TRAIN(056): [ 150/2211] Batch: 0.0924 (0.1095) Data: 0.0018 (0.0092) Loss: 0.0146 (0.0502)
[2023/01/16 03:01] | TRAIN(056): [ 200/2211] Batch: 0.0887 (0.1059) Data: 0.0018 (0.0074) Loss: 0.0732 (0.0472)
[2023/01/16 03:02] | TRAIN(056): [ 250/2211] Batch: 0.1147 (0.1046) Data: 0.0019 (0.0063) Loss: 0.0000 (0.0446)
[2023/01/16 03:02] | TRAIN(056): [ 300/2211] Batch: 0.1020 (0.1041) Data: 0.0019 (0.0055) Loss: 0.1649 (0.0435)
[2023/01/16 03:02] | TRAIN(056): [ 350/2211] Batch: 0.0988 (0.1037) Data: 0.0018 (0.0050) Loss: 0.0051 (0.0411)
[2023/01/16 03:02] | TRAIN(056): [ 400/2211] Batch: 0.0877 (0.1030) Data: 0.0018 (0.0046) Loss: 0.0566 (0.0415)
[2023/01/16 03:02] | TRAIN(056): [ 450/2211] Batch: 0.1184 (0.1029) Data: 0.0022 (0.0043) Loss: 0.0001 (0.0427)
[2023/01/16 03:02] | TRAIN(056): [ 500/2211] Batch: 0.0954 (0.1020) Data: 0.0019 (0.0041) Loss: 0.0000 (0.0434)
[2023/01/16 03:02] | TRAIN(056): [ 550/2211] Batch: 0.0948 (0.1012) Data: 0.0018 (0.0039) Loss: 0.0037 (0.0418)
[2023/01/16 03:02] | TRAIN(056): [ 600/2211] Batch: 0.0914 (0.1006) Data: 0.0018 (0.0037) Loss: 0.0844 (0.0416)
[2023/01/16 03:02] | TRAIN(056): [ 650/2211] Batch: 0.1201 (0.1011) Data: 0.0022 (0.0036) Loss: 0.0144 (0.0414)
[2023/01/16 03:02] | TRAIN(056): [ 700/2211] Batch: 0.1260 (0.1023) Data: 0.0020 (0.0034) Loss: 0.0000 (0.0410)
[2023/01/16 03:02] | TRAIN(056): [ 750/2211] Batch: 0.0979 (0.1019) Data: 0.0018 (0.0033) Loss: 0.0188 (0.0414)
[2023/01/16 03:02] | TRAIN(056): [ 800/2211] Batch: 0.0868 (0.1013) Data: 0.0018 (0.0032) Loss: 0.0018 (0.0415)
[2023/01/16 03:03] | TRAIN(056): [ 850/2211] Batch: 0.0922 (0.1008) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0418)
[2023/01/16 03:03] | TRAIN(056): [ 900/2211] Batch: 0.0884 (0.1005) Data: 0.0018 (0.0031) Loss: 0.0284 (0.0421)
[2023/01/16 03:03] | TRAIN(056): [ 950/2211] Batch: 0.0946 (0.1000) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0423)
[2023/01/16 03:03] | TRAIN(056): [1000/2211] Batch: 0.0908 (0.0995) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0427)
[2023/01/16 03:03] | TRAIN(056): [1050/2211] Batch: 0.0947 (0.0993) Data: 0.0015 (0.0029) Loss: 0.0000 (0.0426)
[2023/01/16 03:03] | TRAIN(056): [1100/2211] Batch: 0.0925 (0.0990) Data: 0.0018 (0.0029) Loss: 0.0889 (0.0426)
[2023/01/16 03:03] | TRAIN(056): [1150/2211] Batch: 0.0994 (0.0990) Data: 0.0018 (0.0028) Loss: 0.1240 (0.0425)
[2023/01/16 03:03] | TRAIN(056): [1200/2211] Batch: 0.0877 (0.0989) Data: 0.0018 (0.0028) Loss: 0.0569 (0.0425)
[2023/01/16 03:03] | TRAIN(056): [1250/2211] Batch: 0.0959 (0.0994) Data: 0.0019 (0.0028) Loss: 0.0350 (0.0427)
[2023/01/16 03:03] | TRAIN(056): [1300/2211] Batch: 0.0865 (0.0993) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0426)
[2023/01/16 03:03] | TRAIN(056): [1350/2211] Batch: 0.0920 (0.0996) Data: 0.0018 (0.0027) Loss: 0.0093 (0.0427)
[2023/01/16 03:03] | TRAIN(056): [1400/2211] Batch: 0.1111 (0.0999) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0428)
[2023/01/16 03:03] | TRAIN(056): [1450/2211] Batch: 0.0933 (0.0999) Data: 0.0018 (0.0026) Loss: 0.0138 (0.0426)
[2023/01/16 03:04] | TRAIN(056): [1500/2211] Batch: 0.0914 (0.0998) Data: 0.0018 (0.0026) Loss: 0.1164 (0.0426)
[2023/01/16 03:04] | TRAIN(056): [1550/2211] Batch: 0.0891 (0.0997) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0427)
[2023/01/16 03:04] | TRAIN(056): [1600/2211] Batch: 0.0934 (0.0996) Data: 0.0018 (0.0026) Loss: 0.0026 (0.0426)
[2023/01/16 03:04] | TRAIN(056): [1650/2211] Batch: 0.0946 (0.0995) Data: 0.0018 (0.0025) Loss: 0.0370 (0.0421)
[2023/01/16 03:04] | TRAIN(056): [1700/2211] Batch: 0.1028 (0.0993) Data: 0.0018 (0.0025) Loss: 0.0956 (0.0421)
[2023/01/16 03:04] | TRAIN(056): [1750/2211] Batch: 0.0876 (0.0992) Data: 0.0018 (0.0025) Loss: 0.0035 (0.0422)
[2023/01/16 03:04] | TRAIN(056): [1800/2211] Batch: 0.0911 (0.0991) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0423)
[2023/01/16 03:04] | TRAIN(056): [1850/2211] Batch: 0.0997 (0.0989) Data: 0.0018 (0.0025) Loss: 0.0161 (0.0419)
[2023/01/16 03:04] | TRAIN(056): [1900/2211] Batch: 0.0876 (0.0987) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0418)
[2023/01/16 03:04] | TRAIN(056): [1950/2211] Batch: 0.1022 (0.0987) Data: 0.0019 (0.0024) Loss: 0.0265 (0.0417)
[2023/01/16 03:04] | TRAIN(056): [2000/2211] Batch: 0.0937 (0.0985) Data: 0.0017 (0.0024) Loss: 0.1977 (0.0419)
[2023/01/16 03:04] | TRAIN(056): [2050/2211] Batch: 0.0918 (0.0985) Data: 0.0016 (0.0024) Loss: 0.1235 (0.0420)
[2023/01/16 03:05] | TRAIN(056): [2100/2211] Batch: 0.0882 (0.0984) Data: 0.0017 (0.0024) Loss: 0.0707 (0.0423)
[2023/01/16 03:05] | TRAIN(056): [2150/2211] Batch: 0.0937 (0.0984) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0422)
[2023/01/16 03:05] | TRAIN(056): [2200/2211] Batch: 0.0880 (0.0984) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0424)
[2023/01/16 03:05] | ------------------------------------------------------------
[2023/01/16 03:05] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:05] | ------------------------------------------------------------
[2023/01/16 03:05] |    TRAIN(56)     0:03:37     0:00:05     0:03:32      0.0425
[2023/01/16 03:05] | ------------------------------------------------------------
[2023/01/16 03:05] | **************************************************
[2023/01/16 03:05] | TRAIN(057): [  50/2211] Batch: 0.1081 (0.1282) Data: 0.0017 (0.0252) Loss: 0.1472 (0.0304)
[2023/01/16 03:05] | TRAIN(057): [ 100/2211] Batch: 0.1184 (0.1298) Data: 0.0022 (0.0139) Loss: 0.0000 (0.0368)
[2023/01/16 03:05] | TRAIN(057): [ 150/2211] Batch: 0.0903 (0.1199) Data: 0.0018 (0.0099) Loss: 0.0580 (0.0413)
[2023/01/16 03:05] | TRAIN(057): [ 200/2211] Batch: 0.0884 (0.1153) Data: 0.0019 (0.0079) Loss: 0.0059 (0.0465)
[2023/01/16 03:05] | TRAIN(057): [ 250/2211] Batch: 0.0922 (0.1120) Data: 0.0018 (0.0067) Loss: 0.1859 (0.0491)
[2023/01/16 03:05] | TRAIN(057): [ 300/2211] Batch: 0.0923 (0.1095) Data: 0.0017 (0.0059) Loss: 0.0000 (0.0491)
[2023/01/16 03:05] | TRAIN(057): [ 350/2211] Batch: 0.0910 (0.1083) Data: 0.0017 (0.0053) Loss: 0.0000 (0.0495)
[2023/01/16 03:05] | TRAIN(057): [ 400/2211] Batch: 0.0867 (0.1059) Data: 0.0018 (0.0049) Loss: 0.0000 (0.0489)
[2023/01/16 03:06] | TRAIN(057): [ 450/2211] Batch: 0.0890 (0.1058) Data: 0.0018 (0.0046) Loss: 0.0000 (0.0467)
[2023/01/16 03:06] | TRAIN(057): [ 500/2211] Batch: 0.0879 (0.1049) Data: 0.0016 (0.0043) Loss: 0.0023 (0.0466)
[2023/01/16 03:06] | TRAIN(057): [ 550/2211] Batch: 0.0890 (0.1039) Data: 0.0017 (0.0041) Loss: 0.0558 (0.0464)
[2023/01/16 03:06] | TRAIN(057): [ 600/2211] Batch: 0.0903 (0.1033) Data: 0.0019 (0.0039) Loss: 0.0555 (0.0468)
[2023/01/16 03:06] | TRAIN(057): [ 650/2211] Batch: 0.0956 (0.1023) Data: 0.0018 (0.0037) Loss: 0.0191 (0.0474)
[2023/01/16 03:06] | TRAIN(057): [ 700/2211] Batch: 0.0877 (0.1018) Data: 0.0015 (0.0036) Loss: 0.0000 (0.0470)
[2023/01/16 03:06] | TRAIN(057): [ 750/2211] Batch: 0.0871 (0.1010) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0466)
[2023/01/16 03:06] | TRAIN(057): [ 800/2211] Batch: 0.1164 (0.1005) Data: 0.0023 (0.0034) Loss: 0.1304 (0.0471)
[2023/01/16 03:06] | TRAIN(057): [ 850/2211] Batch: 0.0920 (0.1004) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0470)
[2023/01/16 03:06] | TRAIN(057): [ 900/2211] Batch: 0.0930 (0.0999) Data: 0.0018 (0.0032) Loss: 0.1482 (0.0469)
[2023/01/16 03:06] | TRAIN(057): [ 950/2211] Batch: 0.0901 (0.0996) Data: 0.0017 (0.0031) Loss: 0.1553 (0.0476)
[2023/01/16 03:06] | TRAIN(057): [1000/2211] Batch: 0.0886 (0.0992) Data: 0.0018 (0.0031) Loss: 0.2421 (0.0478)
[2023/01/16 03:06] | TRAIN(057): [1050/2211] Batch: 0.0876 (0.0988) Data: 0.0018 (0.0030) Loss: 0.0027 (0.0483)
[2023/01/16 03:07] | TRAIN(057): [1100/2211] Batch: 0.0895 (0.0991) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0485)
[2023/01/16 03:07] | TRAIN(057): [1150/2211] Batch: 0.0876 (0.0987) Data: 0.0017 (0.0029) Loss: 0.0019 (0.0489)
[2023/01/16 03:07] | TRAIN(057): [1200/2211] Batch: 0.0875 (0.0987) Data: 0.0017 (0.0029) Loss: 0.0995 (0.0483)
[2023/01/16 03:07] | TRAIN(057): [1250/2211] Batch: 0.0983 (0.0984) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0485)
[2023/01/16 03:07] | TRAIN(057): [1300/2211] Batch: 0.0903 (0.0982) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0491)
[2023/01/16 03:07] | TRAIN(057): [1350/2211] Batch: 0.0875 (0.0980) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0488)
[2023/01/16 03:07] | TRAIN(057): [1400/2211] Batch: 0.0893 (0.0982) Data: 0.0017 (0.0027) Loss: 0.1557 (0.0485)
[2023/01/16 03:07] | TRAIN(057): [1450/2211] Batch: 0.0880 (0.0979) Data: 0.0018 (0.0027) Loss: 0.0149 (0.0484)
[2023/01/16 03:07] | TRAIN(057): [1500/2211] Batch: 0.1206 (0.0979) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0479)
[2023/01/16 03:07] | TRAIN(057): [1550/2211] Batch: 0.0878 (0.0980) Data: 0.0018 (0.0026) Loss: 0.0033 (0.0477)
[2023/01/16 03:07] | TRAIN(057): [1600/2211] Batch: 0.0878 (0.0981) Data: 0.0018 (0.0026) Loss: 0.0982 (0.0480)
[2023/01/16 03:07] | TRAIN(057): [1650/2211] Batch: 0.0894 (0.0983) Data: 0.0017 (0.0026) Loss: 0.2036 (0.0478)
[2023/01/16 03:07] | TRAIN(057): [1700/2211] Batch: 0.0926 (0.0982) Data: 0.0017 (0.0026) Loss: 0.1560 (0.0475)
[2023/01/16 03:08] | TRAIN(057): [1750/2211] Batch: 0.0982 (0.0981) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0470)
[2023/01/16 03:08] | TRAIN(057): [1800/2211] Batch: 0.0997 (0.0980) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0470)
[2023/01/16 03:08] | TRAIN(057): [1850/2211] Batch: 0.0879 (0.0982) Data: 0.0017 (0.0025) Loss: 0.0918 (0.0470)
[2023/01/16 03:08] | TRAIN(057): [1900/2211] Batch: 0.1019 (0.0982) Data: 0.0014 (0.0025) Loss: 0.0000 (0.0467)
[2023/01/16 03:08] | TRAIN(057): [1950/2211] Batch: 0.0914 (0.0981) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0461)
[2023/01/16 03:08] | TRAIN(057): [2000/2211] Batch: 0.0891 (0.0979) Data: 0.0018 (0.0024) Loss: 0.0871 (0.0456)
[2023/01/16 03:08] | TRAIN(057): [2050/2211] Batch: 0.0901 (0.0980) Data: 0.0018 (0.0024) Loss: 0.0538 (0.0458)
[2023/01/16 03:08] | TRAIN(057): [2100/2211] Batch: 0.1200 (0.0979) Data: 0.0022 (0.0024) Loss: 0.0000 (0.0461)
[2023/01/16 03:08] | TRAIN(057): [2150/2211] Batch: 0.0949 (0.0980) Data: 0.0018 (0.0024) Loss: 0.0174 (0.0462)
[2023/01/16 03:08] | TRAIN(057): [2200/2211] Batch: 0.0942 (0.0979) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0459)
[2023/01/16 03:08] | ------------------------------------------------------------
[2023/01/16 03:08] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:08] | ------------------------------------------------------------
[2023/01/16 03:08] |    TRAIN(57)     0:03:36     0:00:05     0:03:31      0.0460
[2023/01/16 03:08] | ------------------------------------------------------------
[2023/01/16 03:08] | **************************************************
[2023/01/16 03:08] | TRAIN(058): [  50/2211] Batch: 0.0878 (0.1291) Data: 0.0017 (0.0245) Loss: 0.0886 (0.0574)
[2023/01/16 03:09] | TRAIN(058): [ 100/2211] Batch: 0.1179 (0.1286) Data: 0.0024 (0.0135) Loss: 0.0035 (0.0559)
[2023/01/16 03:09] | TRAIN(058): [ 150/2211] Batch: 0.1289 (0.1271) Data: 0.0023 (0.0098) Loss: 0.0000 (0.0510)
[2023/01/16 03:09] | TRAIN(058): [ 200/2211] Batch: 0.0877 (0.1242) Data: 0.0017 (0.0079) Loss: 0.2059 (0.0526)
[2023/01/16 03:09] | TRAIN(058): [ 250/2211] Batch: 0.0941 (0.1196) Data: 0.0017 (0.0067) Loss: 0.0000 (0.0470)
[2023/01/16 03:09] | TRAIN(058): [ 300/2211] Batch: 0.0918 (0.1159) Data: 0.0017 (0.0059) Loss: 0.1150 (0.0484)
[2023/01/16 03:09] | TRAIN(058): [ 350/2211] Batch: 0.0902 (0.1124) Data: 0.0020 (0.0053) Loss: 0.0135 (0.0467)
[2023/01/16 03:09] | TRAIN(058): [ 400/2211] Batch: 0.0920 (0.1099) Data: 0.0017 (0.0049) Loss: 0.0015 (0.0478)
[2023/01/16 03:09] | TRAIN(058): [ 450/2211] Batch: 0.0954 (0.1081) Data: 0.0022 (0.0045) Loss: 0.0000 (0.0461)
[2023/01/16 03:09] | TRAIN(058): [ 500/2211] Batch: 0.0897 (0.1065) Data: 0.0019 (0.0042) Loss: 0.0000 (0.0456)
[2023/01/16 03:09] | TRAIN(058): [ 550/2211] Batch: 0.0927 (0.1063) Data: 0.0017 (0.0040) Loss: 0.0000 (0.0443)
[2023/01/16 03:09] | TRAIN(058): [ 600/2211] Batch: 0.0908 (0.1063) Data: 0.0019 (0.0039) Loss: 0.0782 (0.0444)
[2023/01/16 03:09] | TRAIN(058): [ 650/2211] Batch: 0.0923 (0.1054) Data: 0.0018 (0.0037) Loss: 0.0595 (0.0437)
[2023/01/16 03:10] | TRAIN(058): [ 700/2211] Batch: 0.0923 (0.1047) Data: 0.0018 (0.0036) Loss: 0.0107 (0.0429)
[2023/01/16 03:10] | TRAIN(058): [ 750/2211] Batch: 0.0992 (0.1045) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0418)
[2023/01/16 03:10] | TRAIN(058): [ 800/2211] Batch: 0.0882 (0.1038) Data: 0.0018 (0.0034) Loss: 0.0227 (0.0419)
[2023/01/16 03:10] | TRAIN(058): [ 850/2211] Batch: 0.0920 (0.1035) Data: 0.0017 (0.0033) Loss: 0.0632 (0.0418)
[2023/01/16 03:10] | TRAIN(058): [ 900/2211] Batch: 0.0968 (0.1030) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0425)
[2023/01/16 03:10] | TRAIN(058): [ 950/2211] Batch: 0.0883 (0.1024) Data: 0.0018 (0.0031) Loss: 0.0198 (0.0423)
[2023/01/16 03:10] | TRAIN(058): [1000/2211] Batch: 0.1224 (0.1019) Data: 0.0022 (0.0030) Loss: 0.0000 (0.0414)
[2023/01/16 03:10] | TRAIN(058): [1050/2211] Batch: 0.0880 (0.1019) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0415)
[2023/01/16 03:10] | TRAIN(058): [1100/2211] Batch: 0.0880 (0.1014) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0421)
[2023/01/16 03:10] | TRAIN(058): [1150/2211] Batch: 0.0953 (0.1010) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0419)
[2023/01/16 03:10] | TRAIN(058): [1200/2211] Batch: 0.0960 (0.1011) Data: 0.0018 (0.0028) Loss: 0.1849 (0.0423)
[2023/01/16 03:10] | TRAIN(058): [1250/2211] Batch: 0.0914 (0.1012) Data: 0.0017 (0.0028) Loss: 0.0314 (0.0432)
[2023/01/16 03:11] | TRAIN(058): [1300/2211] Batch: 0.0996 (0.1009) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0433)
[2023/01/16 03:11] | TRAIN(058): [1350/2211] Batch: 0.0910 (0.1005) Data: 0.0018 (0.0027) Loss: 0.1668 (0.0434)
[2023/01/16 03:11] | TRAIN(058): [1400/2211] Batch: 0.1242 (0.1006) Data: 0.0022 (0.0027) Loss: 0.0546 (0.0442)
[2023/01/16 03:11] | TRAIN(058): [1450/2211] Batch: 0.0876 (0.1005) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0441)
[2023/01/16 03:11] | TRAIN(058): [1500/2211] Batch: 0.1002 (0.1007) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0437)
[2023/01/16 03:11] | TRAIN(058): [1550/2211] Batch: 0.0890 (0.1004) Data: 0.0018 (0.0026) Loss: 0.0042 (0.0441)
[2023/01/16 03:11] | TRAIN(058): [1600/2211] Batch: 0.0974 (0.1001) Data: 0.0018 (0.0026) Loss: 0.0191 (0.0443)
[2023/01/16 03:11] | TRAIN(058): [1650/2211] Batch: 0.1011 (0.0999) Data: 0.0016 (0.0026) Loss: 0.0003 (0.0441)
[2023/01/16 03:11] | TRAIN(058): [1700/2211] Batch: 0.1011 (0.0999) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0437)
[2023/01/16 03:11] | TRAIN(058): [1750/2211] Batch: 0.0932 (0.0997) Data: 0.0014 (0.0025) Loss: 0.0000 (0.0438)
[2023/01/16 03:11] | TRAIN(058): [1800/2211] Batch: 0.0849 (0.0998) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0441)
[2023/01/16 03:11] | TRAIN(058): [1850/2211] Batch: 0.0885 (0.0996) Data: 0.0015 (0.0025) Loss: 0.0530 (0.0447)
[2023/01/16 03:11] | TRAIN(058): [1900/2211] Batch: 0.0887 (0.0995) Data: 0.0018 (0.0024) Loss: 0.1461 (0.0450)
[2023/01/16 03:12] | TRAIN(058): [1950/2211] Batch: 0.0996 (0.0993) Data: 0.0018 (0.0024) Loss: 0.1576 (0.0456)
[2023/01/16 03:12] | TRAIN(058): [2000/2211] Batch: 0.0953 (0.0992) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0453)
[2023/01/16 03:12] | TRAIN(058): [2050/2211] Batch: 0.0886 (0.0990) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0454)
[2023/01/16 03:12] | TRAIN(058): [2100/2211] Batch: 0.0842 (0.0988) Data: 0.0018 (0.0024) Loss: 0.0823 (0.0458)
[2023/01/16 03:12] | TRAIN(058): [2150/2211] Batch: 0.0883 (0.0986) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0457)
[2023/01/16 03:12] | TRAIN(058): [2200/2211] Batch: 0.0899 (0.0986) Data: 0.0015 (0.0024) Loss: 0.0000 (0.0457)
[2023/01/16 03:12] | ------------------------------------------------------------
[2023/01/16 03:12] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:12] | ------------------------------------------------------------
[2023/01/16 03:12] |    TRAIN(58)     0:03:37     0:00:05     0:03:32      0.0458
[2023/01/16 03:12] | ------------------------------------------------------------
[2023/01/16 03:12] | **************************************************
[2023/01/16 03:12] | TRAIN(059): [  50/2211] Batch: 0.1182 (0.1182) Data: 0.0023 (0.0234) Loss: 0.0548 (0.0553)
[2023/01/16 03:12] | TRAIN(059): [ 100/2211] Batch: 0.0981 (0.1124) Data: 0.0020 (0.0128) Loss: 0.0000 (0.0483)
[2023/01/16 03:12] | TRAIN(059): [ 150/2211] Batch: 0.1454 (0.1146) Data: 0.0020 (0.0092) Loss: 0.0000 (0.0399)
[2023/01/16 03:12] | TRAIN(059): [ 200/2211] Batch: 0.1415 (0.1177) Data: 0.0022 (0.0075) Loss: 0.0424 (0.0387)
[2023/01/16 03:12] | TRAIN(059): [ 250/2211] Batch: 0.1213 (0.1189) Data: 0.0021 (0.0064) Loss: 0.0705 (0.0414)
[2023/01/16 03:13] | TRAIN(059): [ 300/2211] Batch: 0.1172 (0.1191) Data: 0.0020 (0.0057) Loss: 0.0111 (0.0439)
[2023/01/16 03:13] | TRAIN(059): [ 350/2211] Batch: 0.0896 (0.1172) Data: 0.0017 (0.0051) Loss: 0.0000 (0.0436)
[2023/01/16 03:13] | TRAIN(059): [ 400/2211] Batch: 0.1024 (0.1151) Data: 0.0020 (0.0047) Loss: 0.0000 (0.0460)
[2023/01/16 03:13] | TRAIN(059): [ 450/2211] Batch: 0.0960 (0.1130) Data: 0.0018 (0.0044) Loss: 0.0077 (0.0470)
[2023/01/16 03:13] | TRAIN(059): [ 500/2211] Batch: 0.0915 (0.1131) Data: 0.0017 (0.0041) Loss: 0.1074 (0.0458)
[2023/01/16 03:13] | TRAIN(059): [ 550/2211] Batch: 0.0867 (0.1110) Data: 0.0017 (0.0039) Loss: 0.0143 (0.0446)
[2023/01/16 03:13] | TRAIN(059): [ 600/2211] Batch: 0.0866 (0.1092) Data: 0.0017 (0.0037) Loss: 0.0000 (0.0437)
[2023/01/16 03:13] | TRAIN(059): [ 650/2211] Batch: 0.1179 (0.1092) Data: 0.0021 (0.0036) Loss: 0.0723 (0.0442)
[2023/01/16 03:13] | TRAIN(059): [ 700/2211] Batch: 0.0908 (0.1084) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0447)
[2023/01/16 03:13] | TRAIN(059): [ 750/2211] Batch: 0.0911 (0.1074) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0442)
[2023/01/16 03:13] | TRAIN(059): [ 800/2211] Batch: 0.0953 (0.1077) Data: 0.0016 (0.0033) Loss: 0.0000 (0.0443)
[2023/01/16 03:13] | TRAIN(059): [ 850/2211] Batch: 0.0920 (0.1069) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0446)
[2023/01/16 03:14] | TRAIN(059): [ 900/2211] Batch: 0.1175 (0.1062) Data: 0.0020 (0.0031) Loss: 0.0000 (0.0448)
[2023/01/16 03:14] | TRAIN(059): [ 950/2211] Batch: 0.0873 (0.1061) Data: 0.0016 (0.0030) Loss: 0.0000 (0.0448)
[2023/01/16 03:14] | TRAIN(059): [1000/2211] Batch: 0.0906 (0.1060) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0444)
[2023/01/16 03:14] | TRAIN(059): [1050/2211] Batch: 0.0895 (0.1053) Data: 0.0017 (0.0029) Loss: 0.1220 (0.0450)
[2023/01/16 03:14] | TRAIN(059): [1100/2211] Batch: 0.0958 (0.1047) Data: 0.0017 (0.0029) Loss: 0.0831 (0.0453)
[2023/01/16 03:14] | TRAIN(059): [1150/2211] Batch: 0.0934 (0.1044) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0447)
[2023/01/16 03:14] | TRAIN(059): [1200/2211] Batch: 0.0964 (0.1039) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0445)
[2023/01/16 03:14] | TRAIN(059): [1250/2211] Batch: 0.0975 (0.1035) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0445)
[2023/01/16 03:14] | TRAIN(059): [1300/2211] Batch: 0.0910 (0.1032) Data: 0.0016 (0.0027) Loss: 0.0402 (0.0446)
[2023/01/16 03:14] | TRAIN(059): [1350/2211] Batch: 0.0892 (0.1028) Data: 0.0017 (0.0027) Loss: 0.0187 (0.0443)
[2023/01/16 03:14] | TRAIN(059): [1400/2211] Batch: 0.0924 (0.1030) Data: 0.0018 (0.0026) Loss: 0.0571 (0.0443)
[2023/01/16 03:14] | TRAIN(059): [1450/2211] Batch: 0.1243 (0.1028) Data: 0.0021 (0.0026) Loss: 0.0000 (0.0445)
[2023/01/16 03:15] | TRAIN(059): [1500/2211] Batch: 0.0887 (0.1027) Data: 0.0016 (0.0026) Loss: 0.0585 (0.0446)
[2023/01/16 03:15] | TRAIN(059): [1550/2211] Batch: 0.0897 (0.1025) Data: 0.0017 (0.0025) Loss: 0.1959 (0.0449)
[2023/01/16 03:15] | TRAIN(059): [1600/2211] Batch: 0.0893 (0.1025) Data: 0.0017 (0.0025) Loss: 0.1090 (0.0447)
[2023/01/16 03:15] | TRAIN(059): [1650/2211] Batch: 0.0957 (0.1021) Data: 0.0017 (0.0025) Loss: 0.1344 (0.0449)
[2023/01/16 03:15] | TRAIN(059): [1700/2211] Batch: 0.1179 (0.1019) Data: 0.0020 (0.0025) Loss: 0.0951 (0.0448)
[2023/01/16 03:15] | TRAIN(059): [1750/2211] Batch: 0.0889 (0.1018) Data: 0.0017 (0.0025) Loss: 0.0320 (0.0446)
[2023/01/16 03:15] | TRAIN(059): [1800/2211] Batch: 0.0963 (0.1016) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0446)
[2023/01/16 03:15] | TRAIN(059): [1850/2211] Batch: 0.0910 (0.1014) Data: 0.0017 (0.0024) Loss: 0.0132 (0.0447)
[2023/01/16 03:15] | TRAIN(059): [1900/2211] Batch: 0.0894 (0.1012) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0450)
[2023/01/16 03:15] | TRAIN(059): [1950/2211] Batch: 0.1053 (0.1010) Data: 0.0017 (0.0024) Loss: 0.0144 (0.0450)
[2023/01/16 03:15] | TRAIN(059): [2000/2211] Batch: 0.0895 (0.1008) Data: 0.0018 (0.0024) Loss: 0.0493 (0.0451)
[2023/01/16 03:15] | TRAIN(059): [2050/2211] Batch: 0.0977 (0.1006) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0450)
[2023/01/16 03:15] | TRAIN(059): [2100/2211] Batch: 0.1224 (0.1006) Data: 0.0020 (0.0023) Loss: 0.0000 (0.0450)
[2023/01/16 03:16] | TRAIN(059): [2150/2211] Batch: 0.1025 (0.1008) Data: 0.0017 (0.0023) Loss: 0.0061 (0.0448)
[2023/01/16 03:16] | TRAIN(059): [2200/2211] Batch: 0.0927 (0.1007) Data: 0.0016 (0.0023) Loss: 0.0021 (0.0449)
[2023/01/16 03:16] | ------------------------------------------------------------
[2023/01/16 03:16] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:16] | ------------------------------------------------------------
[2023/01/16 03:16] |    TRAIN(59)     0:03:42     0:00:05     0:03:37      0.0449
[2023/01/16 03:16] | ------------------------------------------------------------
[2023/01/16 03:16] | **************************************************
[2023/01/16 03:16] | TRAIN(060): [  50/2211] Batch: 0.0882 (0.1190) Data: 0.0018 (0.0258) Loss: 0.0167 (0.0422)
[2023/01/16 03:16] | TRAIN(060): [ 100/2211] Batch: 0.0888 (0.1066) Data: 0.0017 (0.0139) Loss: 0.0000 (0.0404)
[2023/01/16 03:16] | TRAIN(060): [ 150/2211] Batch: 0.0992 (0.1017) Data: 0.0017 (0.0099) Loss: 0.0000 (0.0456)
[2023/01/16 03:16] | TRAIN(060): [ 200/2211] Batch: 0.0881 (0.0993) Data: 0.0018 (0.0079) Loss: 0.1096 (0.0428)
[2023/01/16 03:16] | TRAIN(060): [ 250/2211] Batch: 0.0895 (0.0976) Data: 0.0019 (0.0067) Loss: 0.0667 (0.0418)
[2023/01/16 03:16] | TRAIN(060): [ 300/2211] Batch: 0.0962 (0.0970) Data: 0.0017 (0.0059) Loss: 0.0000 (0.0419)
[2023/01/16 03:16] | TRAIN(060): [ 350/2211] Batch: 0.0988 (0.0971) Data: 0.0018 (0.0053) Loss: 0.0310 (0.0409)
[2023/01/16 03:16] | TRAIN(060): [ 400/2211] Batch: 0.1000 (0.0972) Data: 0.0019 (0.0048) Loss: 0.0000 (0.0407)
[2023/01/16 03:16] | TRAIN(060): [ 450/2211] Batch: 0.0880 (0.0969) Data: 0.0018 (0.0045) Loss: 0.0337 (0.0417)
[2023/01/16 03:16] | TRAIN(060): [ 500/2211] Batch: 0.0901 (0.0966) Data: 0.0018 (0.0042) Loss: 0.0061 (0.0412)
[2023/01/16 03:17] | TRAIN(060): [ 550/2211] Batch: 0.0908 (0.0968) Data: 0.0018 (0.0040) Loss: 0.0092 (0.0412)
[2023/01/16 03:17] | TRAIN(060): [ 600/2211] Batch: 0.0895 (0.0963) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0416)
[2023/01/16 03:17] | TRAIN(060): [ 650/2211] Batch: 0.0879 (0.0958) Data: 0.0018 (0.0037) Loss: 0.1404 (0.0424)
[2023/01/16 03:17] | TRAIN(060): [ 700/2211] Batch: 0.0951 (0.0958) Data: 0.0017 (0.0035) Loss: 0.0091 (0.0425)
[2023/01/16 03:17] | TRAIN(060): [ 750/2211] Batch: 0.0891 (0.0954) Data: 0.0018 (0.0034) Loss: 0.0110 (0.0426)
[2023/01/16 03:17] | TRAIN(060): [ 800/2211] Batch: 0.0877 (0.0953) Data: 0.0018 (0.0033) Loss: 0.0082 (0.0429)
[2023/01/16 03:17] | TRAIN(060): [ 850/2211] Batch: 0.1218 (0.0955) Data: 0.0022 (0.0032) Loss: 0.0000 (0.0425)
[2023/01/16 03:17] | TRAIN(060): [ 900/2211] Batch: 0.0984 (0.0957) Data: 0.0018 (0.0032) Loss: 0.1150 (0.0422)
[2023/01/16 03:17] | TRAIN(060): [ 950/2211] Batch: 0.1039 (0.0956) Data: 0.0018 (0.0031) Loss: 0.0992 (0.0422)
[2023/01/16 03:17] | TRAIN(060): [1000/2211] Batch: 0.0957 (0.0958) Data: 0.0019 (0.0030) Loss: 0.2729 (0.0429)
[2023/01/16 03:17] | TRAIN(060): [1050/2211] Batch: 0.0986 (0.0957) Data: 0.0017 (0.0030) Loss: 0.0049 (0.0424)
[2023/01/16 03:17] | TRAIN(060): [1100/2211] Batch: 0.0985 (0.0956) Data: 0.0017 (0.0029) Loss: 0.0664 (0.0427)
[2023/01/16 03:17] | TRAIN(060): [1150/2211] Batch: 0.0870 (0.0954) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0432)
[2023/01/16 03:18] | TRAIN(060): [1200/2211] Batch: 0.0924 (0.0959) Data: 0.0017 (0.0028) Loss: 0.0648 (0.0429)
[2023/01/16 03:18] | TRAIN(060): [1250/2211] Batch: 0.1181 (0.0959) Data: 0.0022 (0.0028) Loss: 0.0451 (0.0437)
[2023/01/16 03:18] | TRAIN(060): [1300/2211] Batch: 0.1180 (0.0969) Data: 0.0021 (0.0028) Loss: 0.0505 (0.0431)
[2023/01/16 03:18] | TRAIN(060): [1350/2211] Batch: 0.0984 (0.0970) Data: 0.0018 (0.0027) Loss: 0.0058 (0.0438)
[2023/01/16 03:18] | TRAIN(060): [1400/2211] Batch: 0.0927 (0.0970) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0445)
[2023/01/16 03:18] | TRAIN(060): [1450/2211] Batch: 0.0913 (0.0969) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0445)
[2023/01/16 03:18] | TRAIN(060): [1500/2211] Batch: 0.0996 (0.0969) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0444)
[2023/01/16 03:18] | TRAIN(060): [1550/2211] Batch: 0.0909 (0.0968) Data: 0.0019 (0.0026) Loss: 0.1529 (0.0449)
[2023/01/16 03:18] | TRAIN(060): [1600/2211] Batch: 0.0892 (0.0967) Data: 0.0019 (0.0026) Loss: 0.0195 (0.0448)
[2023/01/16 03:18] | TRAIN(060): [1650/2211] Batch: 0.0896 (0.0966) Data: 0.0018 (0.0025) Loss: 0.0906 (0.0449)
[2023/01/16 03:18] | TRAIN(060): [1700/2211] Batch: 0.0959 (0.0964) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0446)
[2023/01/16 03:18] | TRAIN(060): [1750/2211] Batch: 0.0958 (0.0964) Data: 0.0019 (0.0025) Loss: 0.1273 (0.0454)
[2023/01/16 03:19] | TRAIN(060): [1800/2211] Batch: 0.0935 (0.0965) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0452)
[2023/01/16 03:19] | TRAIN(060): [1850/2211] Batch: 0.0887 (0.0964) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0455)
[2023/01/16 03:19] | TRAIN(060): [1900/2211] Batch: 0.0842 (0.0967) Data: 0.0017 (0.0025) Loss: 0.0429 (0.0458)
[2023/01/16 03:19] | TRAIN(060): [1950/2211] Batch: 0.1178 (0.0968) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0457)
[2023/01/16 03:19] | TRAIN(060): [2000/2211] Batch: 0.1030 (0.0971) Data: 0.0019 (0.0024) Loss: 0.0600 (0.0454)
[2023/01/16 03:19] | TRAIN(060): [2050/2211] Batch: 0.0908 (0.0971) Data: 0.0017 (0.0024) Loss: 0.0121 (0.0454)
[2023/01/16 03:19] | TRAIN(060): [2100/2211] Batch: 0.0883 (0.0970) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0453)
[2023/01/16 03:19] | TRAIN(060): [2150/2211] Batch: 0.0924 (0.0969) Data: 0.0018 (0.0024) Loss: 0.0318 (0.0458)
[2023/01/16 03:19] | TRAIN(060): [2200/2211] Batch: 0.0910 (0.0969) Data: 0.0017 (0.0024) Loss: 0.1162 (0.0455)
[2023/01/16 03:19] | ------------------------------------------------------------
[2023/01/16 03:19] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:19] | ------------------------------------------------------------
[2023/01/16 03:19] |    TRAIN(60)     0:03:34     0:00:05     0:03:29      0.0456
[2023/01/16 03:19] | ------------------------------------------------------------
[2023/01/16 03:19] | **************************************************
[2023/01/16 03:19] | TRAIN(061): [  50/2211] Batch: 0.1312 (0.1408) Data: 0.0022 (0.0251) Loss: 0.0000 (0.0260)
[2023/01/16 03:19] | TRAIN(061): [ 100/2211] Batch: 0.1033 (0.1242) Data: 0.0022 (0.0137) Loss: 0.0108 (0.0347)
[2023/01/16 03:20] | TRAIN(061): [ 150/2211] Batch: 0.0917 (0.1161) Data: 0.0018 (0.0098) Loss: 0.0000 (0.0405)
[2023/01/16 03:20] | TRAIN(061): [ 200/2211] Batch: 0.0953 (0.1115) Data: 0.0018 (0.0078) Loss: 0.0727 (0.0422)
[2023/01/16 03:20] | TRAIN(061): [ 250/2211] Batch: 0.0958 (0.1083) Data: 0.0014 (0.0066) Loss: 0.0181 (0.0401)
[2023/01/16 03:20] | TRAIN(061): [ 300/2211] Batch: 0.0959 (0.1055) Data: 0.0018 (0.0058) Loss: 0.0022 (0.0407)
[2023/01/16 03:20] | TRAIN(061): [ 350/2211] Batch: 0.0891 (0.1056) Data: 0.0018 (0.0053) Loss: 0.0000 (0.0394)
[2023/01/16 03:20] | TRAIN(061): [ 400/2211] Batch: 0.0851 (0.1039) Data: 0.0018 (0.0048) Loss: 0.1247 (0.0391)
[2023/01/16 03:20] | TRAIN(061): [ 450/2211] Batch: 0.0950 (0.1028) Data: 0.0017 (0.0045) Loss: 0.0017 (0.0387)
[2023/01/16 03:20] | TRAIN(061): [ 500/2211] Batch: 0.1012 (0.1022) Data: 0.0019 (0.0042) Loss: 0.0382 (0.0382)
[2023/01/16 03:20] | TRAIN(061): [ 550/2211] Batch: 0.0921 (0.1013) Data: 0.0017 (0.0040) Loss: 0.0000 (0.0398)
[2023/01/16 03:20] | TRAIN(061): [ 600/2211] Batch: 0.0997 (0.1006) Data: 0.0019 (0.0038) Loss: 0.0000 (0.0409)
[2023/01/16 03:20] | TRAIN(061): [ 650/2211] Batch: 0.0892 (0.1002) Data: 0.0018 (0.0036) Loss: 0.0720 (0.0420)
[2023/01/16 03:20] | TRAIN(061): [ 700/2211] Batch: 0.0879 (0.0995) Data: 0.0018 (0.0035) Loss: 0.0160 (0.0426)
[2023/01/16 03:20] | TRAIN(061): [ 750/2211] Batch: 0.0967 (0.0991) Data: 0.0019 (0.0034) Loss: 0.0000 (0.0415)
[2023/01/16 03:21] | TRAIN(061): [ 800/2211] Batch: 0.1092 (0.1001) Data: 0.0020 (0.0033) Loss: 0.0000 (0.0418)
[2023/01/16 03:21] | TRAIN(061): [ 850/2211] Batch: 0.1258 (0.0996) Data: 0.0023 (0.0032) Loss: 0.1168 (0.0422)
[2023/01/16 03:21] | TRAIN(061): [ 900/2211] Batch: 0.1082 (0.1008) Data: 0.0019 (0.0032) Loss: 0.1741 (0.0429)
[2023/01/16 03:21] | TRAIN(061): [ 950/2211] Batch: 0.0882 (0.1004) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0421)
[2023/01/16 03:21] | TRAIN(061): [1000/2211] Batch: 0.0952 (0.1000) Data: 0.0017 (0.0030) Loss: 0.0986 (0.0428)
[2023/01/16 03:21] | TRAIN(061): [1050/2211] Batch: 0.0973 (0.0997) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0423)
[2023/01/16 03:21] | TRAIN(061): [1100/2211] Batch: 0.1009 (0.0995) Data: 0.0019 (0.0029) Loss: 0.0320 (0.0422)
[2023/01/16 03:21] | TRAIN(061): [1150/2211] Batch: 0.0982 (0.0992) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0424)
[2023/01/16 03:21] | TRAIN(061): [1200/2211] Batch: 0.1214 (0.0989) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0427)
[2023/01/16 03:21] | TRAIN(061): [1250/2211] Batch: 0.1079 (0.0989) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0424)
[2023/01/16 03:21] | TRAIN(061): [1300/2211] Batch: 0.0879 (0.0986) Data: 0.0017 (0.0028) Loss: 0.0978 (0.0428)
[2023/01/16 03:21] | TRAIN(061): [1350/2211] Batch: 0.0885 (0.0983) Data: 0.0019 (0.0027) Loss: 0.0799 (0.0428)
[2023/01/16 03:22] | TRAIN(061): [1400/2211] Batch: 0.0891 (0.0980) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0427)
[2023/01/16 03:22] | TRAIN(061): [1450/2211] Batch: 0.0994 (0.0985) Data: 0.0016 (0.0027) Loss: 0.0363 (0.0430)
[2023/01/16 03:22] | TRAIN(061): [1500/2211] Batch: 0.0828 (0.0985) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0432)
[2023/01/16 03:22] | TRAIN(061): [1550/2211] Batch: 0.0864 (0.0980) Data: 0.0019 (0.0026) Loss: 0.0640 (0.0434)
[2023/01/16 03:22] | TRAIN(061): [1600/2211] Batch: 0.0885 (0.0977) Data: 0.0018 (0.0026) Loss: 0.0053 (0.0431)
[2023/01/16 03:22] | TRAIN(061): [1650/2211] Batch: 0.1171 (0.0976) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0431)
[2023/01/16 03:22] | TRAIN(061): [1700/2211] Batch: 0.0889 (0.0976) Data: 0.0018 (0.0025) Loss: 0.0432 (0.0429)
[2023/01/16 03:22] | TRAIN(061): [1750/2211] Batch: 0.0969 (0.0974) Data: 0.0018 (0.0025) Loss: 0.1757 (0.0430)
[2023/01/16 03:22] | TRAIN(061): [1800/2211] Batch: 0.0880 (0.0973) Data: 0.0018 (0.0025) Loss: 0.1164 (0.0432)
[2023/01/16 03:22] | TRAIN(061): [1850/2211] Batch: 0.0882 (0.0971) Data: 0.0017 (0.0025) Loss: 0.1354 (0.0436)
[2023/01/16 03:22] | TRAIN(061): [1900/2211] Batch: 0.0891 (0.0970) Data: 0.0019 (0.0025) Loss: 0.0611 (0.0435)
[2023/01/16 03:22] | TRAIN(061): [1950/2211] Batch: 0.0957 (0.0969) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0438)
[2023/01/16 03:22] | TRAIN(061): [2000/2211] Batch: 0.0882 (0.0968) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0439)
[2023/01/16 03:23] | TRAIN(061): [2050/2211] Batch: 0.0878 (0.0967) Data: 0.0017 (0.0024) Loss: 0.0391 (0.0437)
[2023/01/16 03:23] | TRAIN(061): [2100/2211] Batch: 0.0891 (0.0965) Data: 0.0018 (0.0024) Loss: 0.0507 (0.0438)
[2023/01/16 03:23] | TRAIN(061): [2150/2211] Batch: 0.0997 (0.0964) Data: 0.0019 (0.0024) Loss: 0.2109 (0.0439)
[2023/01/16 03:23] | TRAIN(061): [2200/2211] Batch: 0.0882 (0.0964) Data: 0.0017 (0.0024) Loss: 0.0308 (0.0439)
[2023/01/16 03:23] | ------------------------------------------------------------
[2023/01/16 03:23] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:23] | ------------------------------------------------------------
[2023/01/16 03:23] |    TRAIN(61)     0:03:33     0:00:05     0:03:27      0.0438
[2023/01/16 03:23] | ------------------------------------------------------------
[2023/01/16 03:23] | **************************************************
[2023/01/16 03:23] | TRAIN(062): [  50/2211] Batch: 0.0910 (0.1272) Data: 0.0018 (0.0245) Loss: 0.0253 (0.0462)
[2023/01/16 03:23] | TRAIN(062): [ 100/2211] Batch: 0.0834 (0.1059) Data: 0.0018 (0.0133) Loss: 0.0000 (0.0415)
[2023/01/16 03:23] | TRAIN(062): [ 150/2211] Batch: 0.0892 (0.1001) Data: 0.0018 (0.0095) Loss: 0.0000 (0.0408)
[2023/01/16 03:23] | TRAIN(062): [ 200/2211] Batch: 0.0830 (0.0970) Data: 0.0018 (0.0076) Loss: 0.0928 (0.0418)
[2023/01/16 03:23] | TRAIN(062): [ 250/2211] Batch: 0.0902 (0.0948) Data: 0.0018 (0.0064) Loss: 0.0000 (0.0432)
[2023/01/16 03:23] | TRAIN(062): [ 300/2211] Batch: 0.0884 (0.0940) Data: 0.0017 (0.0057) Loss: 0.0000 (0.0447)
[2023/01/16 03:23] | TRAIN(062): [ 350/2211] Batch: 0.0954 (0.0943) Data: 0.0017 (0.0051) Loss: 0.0000 (0.0456)
[2023/01/16 03:23] | TRAIN(062): [ 400/2211] Batch: 0.0992 (0.0949) Data: 0.0019 (0.0047) Loss: 0.0033 (0.0457)
[2023/01/16 03:24] | TRAIN(062): [ 450/2211] Batch: 0.0892 (0.0947) Data: 0.0019 (0.0044) Loss: 0.0134 (0.0457)
[2023/01/16 03:24] | TRAIN(062): [ 500/2211] Batch: 0.1176 (0.0952) Data: 0.0020 (0.0042) Loss: 0.1082 (0.0453)
[2023/01/16 03:24] | TRAIN(062): [ 550/2211] Batch: 0.0891 (0.0948) Data: 0.0017 (0.0039) Loss: 0.0375 (0.0456)
[2023/01/16 03:24] | TRAIN(062): [ 600/2211] Batch: 0.0892 (0.0957) Data: 0.0019 (0.0038) Loss: 0.1116 (0.0456)
[2023/01/16 03:24] | TRAIN(062): [ 650/2211] Batch: 0.1248 (0.0956) Data: 0.0020 (0.0036) Loss: 0.0000 (0.0463)
[2023/01/16 03:24] | TRAIN(062): [ 700/2211] Batch: 0.0918 (0.0956) Data: 0.0017 (0.0035) Loss: 0.0250 (0.0467)
[2023/01/16 03:24] | TRAIN(062): [ 750/2211] Batch: 0.0971 (0.0954) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0467)
[2023/01/16 03:24] | TRAIN(062): [ 800/2211] Batch: 0.0922 (0.0954) Data: 0.0018 (0.0033) Loss: 0.0255 (0.0451)
[2023/01/16 03:24] | TRAIN(062): [ 850/2211] Batch: 0.0956 (0.0953) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0445)
[2023/01/16 03:24] | TRAIN(062): [ 900/2211] Batch: 0.0986 (0.0952) Data: 0.0017 (0.0031) Loss: 0.0075 (0.0438)
[2023/01/16 03:24] | TRAIN(062): [ 950/2211] Batch: 0.0952 (0.0950) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0435)
[2023/01/16 03:24] | TRAIN(062): [1000/2211] Batch: 0.0944 (0.0948) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0440)
[2023/01/16 03:24] | TRAIN(062): [1050/2211] Batch: 0.0876 (0.0949) Data: 0.0017 (0.0029) Loss: 0.0076 (0.0438)
[2023/01/16 03:25] | TRAIN(062): [1100/2211] Batch: 0.1041 (0.0948) Data: 0.0018 (0.0029) Loss: 0.0122 (0.0440)
[2023/01/16 03:25] | TRAIN(062): [1150/2211] Batch: 0.0991 (0.0948) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0438)
[2023/01/16 03:25] | TRAIN(062): [1200/2211] Batch: 0.0997 (0.0948) Data: 0.0018 (0.0028) Loss: 0.0738 (0.0436)
[2023/01/16 03:25] | TRAIN(062): [1250/2211] Batch: 0.0901 (0.0948) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0430)
[2023/01/16 03:25] | TRAIN(062): [1300/2211] Batch: 0.0927 (0.0949) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0423)
[2023/01/16 03:25] | TRAIN(062): [1350/2211] Batch: 0.0987 (0.0949) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0422)
[2023/01/16 03:25] | TRAIN(062): [1400/2211] Batch: 0.0970 (0.0948) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0417)
[2023/01/16 03:25] | TRAIN(062): [1450/2211] Batch: 0.0873 (0.0947) Data: 0.0017 (0.0026) Loss: 0.0824 (0.0422)
[2023/01/16 03:25] | TRAIN(062): [1500/2211] Batch: 0.0959 (0.0946) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0418)
[2023/01/16 03:25] | TRAIN(062): [1550/2211] Batch: 0.0890 (0.0944) Data: 0.0018 (0.0026) Loss: 0.1700 (0.0417)
[2023/01/16 03:25] | TRAIN(062): [1600/2211] Batch: 0.0955 (0.0943) Data: 0.0019 (0.0025) Loss: 0.0139 (0.0416)
[2023/01/16 03:25] | TRAIN(062): [1650/2211] Batch: 0.0908 (0.0943) Data: 0.0018 (0.0025) Loss: 0.0051 (0.0416)
[2023/01/16 03:25] | TRAIN(062): [1700/2211] Batch: 0.0881 (0.0942) Data: 0.0017 (0.0025) Loss: 0.0443 (0.0413)
[2023/01/16 03:26] | TRAIN(062): [1750/2211] Batch: 0.1251 (0.0947) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0419)
[2023/01/16 03:26] | TRAIN(062): [1800/2211] Batch: 0.0896 (0.0946) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0419)
[2023/01/16 03:26] | TRAIN(062): [1850/2211] Batch: 0.1255 (0.0946) Data: 0.0024 (0.0024) Loss: 0.0000 (0.0418)
[2023/01/16 03:26] | TRAIN(062): [1900/2211] Batch: 0.0951 (0.0948) Data: 0.0017 (0.0024) Loss: 0.1370 (0.0420)
[2023/01/16 03:26] | TRAIN(062): [1950/2211] Batch: 0.1048 (0.0947) Data: 0.0018 (0.0024) Loss: 0.0323 (0.0420)
[2023/01/16 03:26] | TRAIN(062): [2000/2211] Batch: 0.0949 (0.0947) Data: 0.0018 (0.0024) Loss: 0.1626 (0.0420)
[2023/01/16 03:26] | TRAIN(062): [2050/2211] Batch: 0.0867 (0.0947) Data: 0.0017 (0.0024) Loss: 0.0112 (0.0421)
[2023/01/16 03:26] | TRAIN(062): [2100/2211] Batch: 0.0895 (0.0946) Data: 0.0022 (0.0024) Loss: 0.0901 (0.0420)
[2023/01/16 03:26] | TRAIN(062): [2150/2211] Batch: 0.0879 (0.0949) Data: 0.0018 (0.0024) Loss: 0.0340 (0.0420)
[2023/01/16 03:26] | TRAIN(062): [2200/2211] Batch: 0.0977 (0.0948) Data: 0.0015 (0.0023) Loss: 0.0000 (0.0420)
[2023/01/16 03:26] | ------------------------------------------------------------
[2023/01/16 03:26] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:26] | ------------------------------------------------------------
[2023/01/16 03:26] |    TRAIN(62)     0:03:29     0:00:05     0:03:24      0.0419
[2023/01/16 03:26] | ------------------------------------------------------------
[2023/01/16 03:26] | **************************************************
[2023/01/16 03:26] | TRAIN(063): [  50/2211] Batch: 0.1189 (0.1481) Data: 0.0023 (0.0246) Loss: 0.0000 (0.0470)
[2023/01/16 03:27] | TRAIN(063): [ 100/2211] Batch: 0.1283 (0.1319) Data: 0.0020 (0.0135) Loss: 0.2573 (0.0445)
[2023/01/16 03:27] | TRAIN(063): [ 150/2211] Batch: 0.0913 (0.1196) Data: 0.0019 (0.0096) Loss: 0.0377 (0.0491)
[2023/01/16 03:27] | TRAIN(063): [ 200/2211] Batch: 0.0885 (0.1154) Data: 0.0018 (0.0077) Loss: 0.0289 (0.0448)
[2023/01/16 03:27] | TRAIN(063): [ 250/2211] Batch: 0.0966 (0.1104) Data: 0.0018 (0.0065) Loss: 0.0190 (0.0437)
[2023/01/16 03:27] | TRAIN(063): [ 300/2211] Batch: 0.0914 (0.1079) Data: 0.0018 (0.0058) Loss: 0.0125 (0.0450)
[2023/01/16 03:27] | TRAIN(063): [ 350/2211] Batch: 0.0879 (0.1054) Data: 0.0018 (0.0052) Loss: 0.0860 (0.0448)
[2023/01/16 03:27] | TRAIN(063): [ 400/2211] Batch: 0.0890 (0.1036) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0454)
[2023/01/16 03:27] | TRAIN(063): [ 450/2211] Batch: 0.0880 (0.1023) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0456)
[2023/01/16 03:27] | TRAIN(063): [ 500/2211] Batch: 0.0958 (0.1016) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0456)
[2023/01/16 03:27] | TRAIN(063): [ 550/2211] Batch: 0.0985 (0.1006) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0459)
[2023/01/16 03:27] | TRAIN(063): [ 600/2211] Batch: 0.0876 (0.1003) Data: 0.0018 (0.0038) Loss: 0.1156 (0.0459)
[2023/01/16 03:27] | TRAIN(063): [ 650/2211] Batch: 0.0992 (0.0998) Data: 0.0019 (0.0036) Loss: 0.0000 (0.0463)
[2023/01/16 03:27] | TRAIN(063): [ 700/2211] Batch: 0.0891 (0.0993) Data: 0.0018 (0.0035) Loss: 0.1653 (0.0468)
[2023/01/16 03:28] | TRAIN(063): [ 750/2211] Batch: 0.0896 (0.0988) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0472)
[2023/01/16 03:28] | TRAIN(063): [ 800/2211] Batch: 0.0881 (0.0983) Data: 0.0017 (0.0033) Loss: 0.0024 (0.0470)
[2023/01/16 03:28] | TRAIN(063): [ 850/2211] Batch: 0.0963 (0.0979) Data: 0.0022 (0.0032) Loss: 0.0000 (0.0469)
[2023/01/16 03:28] | TRAIN(063): [ 900/2211] Batch: 0.0928 (0.0978) Data: 0.0018 (0.0031) Loss: 0.0342 (0.0469)
[2023/01/16 03:28] | TRAIN(063): [ 950/2211] Batch: 0.0980 (0.0976) Data: 0.0016 (0.0030) Loss: 0.0091 (0.0467)
[2023/01/16 03:28] | TRAIN(063): [1000/2211] Batch: 0.0932 (0.0975) Data: 0.0018 (0.0030) Loss: 0.1431 (0.0458)
[2023/01/16 03:28] | TRAIN(063): [1050/2211] Batch: 0.0925 (0.0975) Data: 0.0019 (0.0029) Loss: 0.0356 (0.0461)
[2023/01/16 03:28] | TRAIN(063): [1100/2211] Batch: 0.0899 (0.0973) Data: 0.0018 (0.0029) Loss: 0.0939 (0.0457)
[2023/01/16 03:28] | TRAIN(063): [1150/2211] Batch: 0.0897 (0.0971) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0461)
[2023/01/16 03:28] | TRAIN(063): [1200/2211] Batch: 0.0925 (0.0969) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0467)
[2023/01/16 03:28] | TRAIN(063): [1250/2211] Batch: 0.0955 (0.0968) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0463)
[2023/01/16 03:28] | TRAIN(063): [1300/2211] Batch: 0.0978 (0.0967) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0457)
[2023/01/16 03:28] | TRAIN(063): [1350/2211] Batch: 0.0963 (0.0967) Data: 0.0015 (0.0027) Loss: 0.0000 (0.0451)
[2023/01/16 03:29] | TRAIN(063): [1400/2211] Batch: 0.0992 (0.0968) Data: 0.0018 (0.0026) Loss: 0.0671 (0.0447)
[2023/01/16 03:29] | TRAIN(063): [1450/2211] Batch: 0.0872 (0.0967) Data: 0.0017 (0.0026) Loss: 0.0694 (0.0446)
[2023/01/16 03:29] | TRAIN(063): [1500/2211] Batch: 0.1196 (0.0972) Data: 0.0021 (0.0026) Loss: 0.1276 (0.0448)
[2023/01/16 03:29] | TRAIN(063): [1550/2211] Batch: 0.0928 (0.0973) Data: 0.0018 (0.0026) Loss: 0.0239 (0.0444)
[2023/01/16 03:29] | TRAIN(063): [1600/2211] Batch: 0.0890 (0.0972) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0440)
[2023/01/16 03:29] | TRAIN(063): [1650/2211] Batch: 0.0875 (0.0971) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0444)
[2023/01/16 03:29] | TRAIN(063): [1700/2211] Batch: 0.0876 (0.0971) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0446)
[2023/01/16 03:29] | TRAIN(063): [1750/2211] Batch: 0.0894 (0.0969) Data: 0.0018 (0.0025) Loss: 0.0029 (0.0447)
[2023/01/16 03:29] | TRAIN(063): [1800/2211] Batch: 0.0915 (0.0968) Data: 0.0018 (0.0025) Loss: 0.0915 (0.0448)
[2023/01/16 03:29] | TRAIN(063): [1850/2211] Batch: 0.1084 (0.0968) Data: 0.0018 (0.0024) Loss: 0.1393 (0.0448)
[2023/01/16 03:29] | TRAIN(063): [1900/2211] Batch: 0.0917 (0.0967) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0442)
[2023/01/16 03:29] | TRAIN(063): [1950/2211] Batch: 0.0980 (0.0967) Data: 0.0017 (0.0024) Loss: 0.0104 (0.0442)
[2023/01/16 03:30] | TRAIN(063): [2000/2211] Batch: 0.0826 (0.0965) Data: 0.0017 (0.0024) Loss: 0.0013 (0.0439)
[2023/01/16 03:30] | TRAIN(063): [2050/2211] Batch: 0.0899 (0.0964) Data: 0.0018 (0.0024) Loss: 0.0306 (0.0441)
[2023/01/16 03:30] | TRAIN(063): [2100/2211] Batch: 0.0883 (0.0963) Data: 0.0018 (0.0024) Loss: 0.2041 (0.0441)
[2023/01/16 03:30] | TRAIN(063): [2150/2211] Batch: 0.0849 (0.0962) Data: 0.0018 (0.0023) Loss: 0.0057 (0.0440)
[2023/01/16 03:30] | TRAIN(063): [2200/2211] Batch: 0.0889 (0.0961) Data: 0.0016 (0.0023) Loss: 0.0000 (0.0438)
[2023/01/16 03:30] | ------------------------------------------------------------
[2023/01/16 03:30] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:30] | ------------------------------------------------------------
[2023/01/16 03:30] |    TRAIN(63)     0:03:32     0:00:05     0:03:27      0.0437
[2023/01/16 03:30] | ------------------------------------------------------------
[2023/01/16 03:30] | **************************************************
[2023/01/16 03:30] | TRAIN(064): [  50/2211] Batch: 0.0885 (0.1187) Data: 0.0018 (0.0240) Loss: 0.0135 (0.0473)
[2023/01/16 03:30] | TRAIN(064): [ 100/2211] Batch: 0.0878 (0.1051) Data: 0.0017 (0.0130) Loss: 0.1956 (0.0451)
[2023/01/16 03:30] | TRAIN(064): [ 150/2211] Batch: 0.0979 (0.1013) Data: 0.0018 (0.0093) Loss: 0.0000 (0.0493)
[2023/01/16 03:30] | TRAIN(064): [ 200/2211] Batch: 0.0912 (0.0992) Data: 0.0017 (0.0074) Loss: 0.1935 (0.0458)
[2023/01/16 03:30] | TRAIN(064): [ 250/2211] Batch: 0.0888 (0.0978) Data: 0.0018 (0.0063) Loss: 0.0883 (0.0461)
[2023/01/16 03:30] | TRAIN(064): [ 300/2211] Batch: 0.0970 (0.0970) Data: 0.0025 (0.0055) Loss: 0.0000 (0.0468)
[2023/01/16 03:30] | TRAIN(064): [ 350/2211] Batch: 0.0879 (0.0963) Data: 0.0018 (0.0050) Loss: 0.0000 (0.0478)
[2023/01/16 03:30] | TRAIN(064): [ 400/2211] Batch: 0.0879 (0.0956) Data: 0.0017 (0.0046) Loss: 0.0110 (0.0453)
[2023/01/16 03:31] | TRAIN(064): [ 450/2211] Batch: 0.0879 (0.0952) Data: 0.0018 (0.0043) Loss: 0.1408 (0.0449)
[2023/01/16 03:31] | TRAIN(064): [ 500/2211] Batch: 0.0895 (0.0947) Data: 0.0018 (0.0040) Loss: 0.0973 (0.0432)
[2023/01/16 03:31] | TRAIN(064): [ 550/2211] Batch: 0.0972 (0.0947) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0430)
[2023/01/16 03:31] | TRAIN(064): [ 600/2211] Batch: 0.0873 (0.0947) Data: 0.0017 (0.0037) Loss: 0.0721 (0.0432)
[2023/01/16 03:31] | TRAIN(064): [ 650/2211] Batch: 0.0947 (0.0944) Data: 0.0018 (0.0035) Loss: 0.0053 (0.0424)
[2023/01/16 03:31] | TRAIN(064): [ 700/2211] Batch: 0.0874 (0.0953) Data: 0.0017 (0.0034) Loss: 0.0029 (0.0415)
[2023/01/16 03:31] | TRAIN(064): [ 750/2211] Batch: 0.0969 (0.0950) Data: 0.0018 (0.0033) Loss: 0.1447 (0.0407)
[2023/01/16 03:31] | TRAIN(064): [ 800/2211] Batch: 0.0895 (0.0947) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0410)
[2023/01/16 03:31] | TRAIN(064): [ 850/2211] Batch: 0.0910 (0.0946) Data: 0.0018 (0.0031) Loss: 0.0831 (0.0412)
[2023/01/16 03:31] | TRAIN(064): [ 900/2211] Batch: 0.0936 (0.0947) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0414)
[2023/01/16 03:31] | TRAIN(064): [ 950/2211] Batch: 0.0871 (0.0946) Data: 0.0017 (0.0030) Loss: 0.0978 (0.0412)
[2023/01/16 03:31] | TRAIN(064): [1000/2211] Batch: 0.0884 (0.0946) Data: 0.0017 (0.0029) Loss: 0.1848 (0.0419)
[2023/01/16 03:31] | TRAIN(064): [1050/2211] Batch: 0.0883 (0.0946) Data: 0.0017 (0.0029) Loss: 0.0573 (0.0419)
[2023/01/16 03:32] | TRAIN(064): [1100/2211] Batch: 0.0868 (0.0944) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0429)
[2023/01/16 03:32] | TRAIN(064): [1150/2211] Batch: 0.0878 (0.0942) Data: 0.0018 (0.0028) Loss: 0.2185 (0.0430)
[2023/01/16 03:32] | TRAIN(064): [1200/2211] Batch: 0.1016 (0.0942) Data: 0.0018 (0.0027) Loss: 0.0267 (0.0426)
[2023/01/16 03:32] | TRAIN(064): [1250/2211] Batch: 0.0998 (0.0940) Data: 0.0017 (0.0027) Loss: 0.0099 (0.0430)
[2023/01/16 03:32] | TRAIN(064): [1300/2211] Batch: 0.0825 (0.0939) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0434)
[2023/01/16 03:32] | TRAIN(064): [1350/2211] Batch: 0.1141 (0.0938) Data: 0.0018 (0.0026) Loss: 0.0075 (0.0437)
[2023/01/16 03:32] | TRAIN(064): [1400/2211] Batch: 0.0970 (0.0942) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0437)
[2023/01/16 03:32] | TRAIN(064): [1450/2211] Batch: 0.0928 (0.0942) Data: 0.0017 (0.0026) Loss: 0.0274 (0.0440)
[2023/01/16 03:32] | TRAIN(064): [1500/2211] Batch: 0.0899 (0.0944) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0438)
[2023/01/16 03:32] | TRAIN(064): [1550/2211] Batch: 0.1203 (0.0945) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0442)
[2023/01/16 03:32] | TRAIN(064): [1600/2211] Batch: 0.1294 (0.0947) Data: 0.0023 (0.0025) Loss: 0.1195 (0.0440)
[2023/01/16 03:32] | TRAIN(064): [1650/2211] Batch: 0.0986 (0.0949) Data: 0.0019 (0.0025) Loss: 0.0174 (0.0442)
[2023/01/16 03:33] | TRAIN(064): [1700/2211] Batch: 0.1105 (0.0952) Data: 0.0018 (0.0025) Loss: 0.0076 (0.0443)
[2023/01/16 03:33] | TRAIN(064): [1750/2211] Batch: 0.1090 (0.0953) Data: 0.0017 (0.0024) Loss: 0.0110 (0.0441)
[2023/01/16 03:33] | TRAIN(064): [1800/2211] Batch: 0.0912 (0.0953) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0440)
[2023/01/16 03:33] | TRAIN(064): [1850/2211] Batch: 0.0868 (0.0953) Data: 0.0018 (0.0024) Loss: 0.0304 (0.0440)
[2023/01/16 03:33] | TRAIN(064): [1900/2211] Batch: 0.0882 (0.0954) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0440)
[2023/01/16 03:33] | TRAIN(064): [1950/2211] Batch: 0.0872 (0.0953) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0440)
[2023/01/16 03:33] | TRAIN(064): [2000/2211] Batch: 0.0879 (0.0952) Data: 0.0017 (0.0024) Loss: 0.0707 (0.0442)
[2023/01/16 03:33] | TRAIN(064): [2050/2211] Batch: 0.0882 (0.0953) Data: 0.0019 (0.0023) Loss: 0.0869 (0.0439)
[2023/01/16 03:33] | TRAIN(064): [2100/2211] Batch: 0.0878 (0.0952) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0435)
[2023/01/16 03:33] | TRAIN(064): [2150/2211] Batch: 0.0958 (0.0951) Data: 0.0018 (0.0023) Loss: 0.1219 (0.0433)
[2023/01/16 03:33] | TRAIN(064): [2200/2211] Batch: 0.1024 (0.0953) Data: 0.0016 (0.0023) Loss: 0.0017 (0.0434)
[2023/01/16 03:33] | ------------------------------------------------------------
[2023/01/16 03:33] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:33] | ------------------------------------------------------------
[2023/01/16 03:33] |    TRAIN(64)     0:03:30     0:00:05     0:03:25      0.0434
[2023/01/16 03:33] | ------------------------------------------------------------
[2023/01/16 03:33] | **************************************************
[2023/01/16 03:33] | TRAIN(065): [  50/2211] Batch: 0.0843 (0.1248) Data: 0.0016 (0.0250) Loss: 0.0444 (0.0397)
[2023/01/16 03:34] | TRAIN(065): [ 100/2211] Batch: 0.0952 (0.1079) Data: 0.0017 (0.0135) Loss: 0.0582 (0.0459)
[2023/01/16 03:34] | TRAIN(065): [ 150/2211] Batch: 0.0974 (0.1020) Data: 0.0017 (0.0096) Loss: 0.0337 (0.0495)
[2023/01/16 03:34] | TRAIN(065): [ 200/2211] Batch: 0.1486 (0.1009) Data: 0.0026 (0.0077) Loss: 0.1771 (0.0489)
[2023/01/16 03:34] | TRAIN(065): [ 250/2211] Batch: 0.0825 (0.0981) Data: 0.0019 (0.0065) Loss: 0.0000 (0.0533)
[2023/01/16 03:34] | TRAIN(065): [ 300/2211] Batch: 0.1202 (0.0976) Data: 0.0022 (0.0058) Loss: 0.0000 (0.0512)
[2023/01/16 03:34] | TRAIN(065): [ 350/2211] Batch: 0.1294 (0.0978) Data: 0.0026 (0.0052) Loss: 0.0017 (0.0484)
[2023/01/16 03:34] | TRAIN(065): [ 400/2211] Batch: 0.0956 (0.0975) Data: 0.0019 (0.0048) Loss: 0.0000 (0.0488)
[2023/01/16 03:34] | TRAIN(065): [ 450/2211] Batch: 0.0910 (0.0973) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0492)
[2023/01/16 03:34] | TRAIN(065): [ 500/2211] Batch: 0.0876 (0.0967) Data: 0.0018 (0.0042) Loss: 0.0212 (0.0480)
[2023/01/16 03:34] | TRAIN(065): [ 550/2211] Batch: 0.1178 (0.0980) Data: 0.0022 (0.0040) Loss: 0.0792 (0.0481)
[2023/01/16 03:34] | TRAIN(065): [ 600/2211] Batch: 0.1207 (0.1002) Data: 0.0022 (0.0039) Loss: 0.0000 (0.0496)
[2023/01/16 03:34] | TRAIN(065): [ 650/2211] Batch: 0.1086 (0.1010) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0528)
[2023/01/16 03:35] | TRAIN(065): [ 700/2211] Batch: 0.1212 (0.1008) Data: 0.0023 (0.0036) Loss: 0.0789 (0.0512)
[2023/01/16 03:35] | TRAIN(065): [ 750/2211] Batch: 0.0891 (0.1020) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0507)
[2023/01/16 03:35] | TRAIN(065): [ 800/2211] Batch: 0.0898 (0.1014) Data: 0.0016 (0.0034) Loss: 0.0111 (0.0504)
[2023/01/16 03:35] | TRAIN(065): [ 850/2211] Batch: 0.0896 (0.1009) Data: 0.0019 (0.0033) Loss: 0.1163 (0.0512)
[2023/01/16 03:35] | TRAIN(065): [ 900/2211] Batch: 0.0877 (0.1005) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0497)
[2023/01/16 03:35] | TRAIN(065): [ 950/2211] Batch: 0.1038 (0.1005) Data: 0.0018 (0.0031) Loss: 0.0211 (0.0487)
[2023/01/16 03:35] | TRAIN(065): [1000/2211] Batch: 0.0884 (0.1001) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0482)
[2023/01/16 03:35] | TRAIN(065): [1050/2211] Batch: 0.0974 (0.0997) Data: 0.0017 (0.0030) Loss: 0.1480 (0.0481)
[2023/01/16 03:35] | TRAIN(065): [1100/2211] Batch: 0.0875 (0.0993) Data: 0.0018 (0.0030) Loss: 0.0433 (0.0477)
[2023/01/16 03:35] | TRAIN(065): [1150/2211] Batch: 0.0873 (0.0988) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0473)
[2023/01/16 03:35] | TRAIN(065): [1200/2211] Batch: 0.1045 (0.0987) Data: 0.0019 (0.0029) Loss: 0.0656 (0.0476)
[2023/01/16 03:35] | TRAIN(065): [1250/2211] Batch: 0.0928 (0.0986) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0465)
[2023/01/16 03:35] | TRAIN(065): [1300/2211] Batch: 0.0904 (0.0986) Data: 0.0018 (0.0028) Loss: 0.0148 (0.0463)
[2023/01/16 03:36] | TRAIN(065): [1350/2211] Batch: 0.0972 (0.0984) Data: 0.0018 (0.0028) Loss: 0.0831 (0.0459)
[2023/01/16 03:36] | TRAIN(065): [1400/2211] Batch: 0.0894 (0.0981) Data: 0.0018 (0.0027) Loss: 0.1630 (0.0455)
[2023/01/16 03:36] | TRAIN(065): [1450/2211] Batch: 0.0878 (0.0979) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0453)
[2023/01/16 03:36] | TRAIN(065): [1500/2211] Batch: 0.0868 (0.0976) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0448)
[2023/01/16 03:36] | TRAIN(065): [1550/2211] Batch: 0.0965 (0.0974) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0453)
[2023/01/16 03:36] | TRAIN(065): [1600/2211] Batch: 0.0878 (0.0972) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0450)
[2023/01/16 03:36] | TRAIN(065): [1650/2211] Batch: 0.0929 (0.0972) Data: 0.0019 (0.0026) Loss: 0.0019 (0.0445)
[2023/01/16 03:36] | TRAIN(065): [1700/2211] Batch: 0.0944 (0.0971) Data: 0.0018 (0.0026) Loss: 0.1520 (0.0448)
[2023/01/16 03:36] | TRAIN(065): [1750/2211] Batch: 0.0881 (0.0969) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0444)
[2023/01/16 03:36] | TRAIN(065): [1800/2211] Batch: 0.1036 (0.0969) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0446)
[2023/01/16 03:36] | TRAIN(065): [1850/2211] Batch: 0.0892 (0.0968) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0446)
[2023/01/16 03:36] | TRAIN(065): [1900/2211] Batch: 0.1011 (0.0967) Data: 0.0018 (0.0025) Loss: 0.0878 (0.0443)
[2023/01/16 03:36] | TRAIN(065): [1950/2211] Batch: 0.0989 (0.0967) Data: 0.0017 (0.0025) Loss: 0.0272 (0.0441)
[2023/01/16 03:37] | TRAIN(065): [2000/2211] Batch: 0.0975 (0.0965) Data: 0.0019 (0.0024) Loss: 0.0338 (0.0442)
[2023/01/16 03:37] | TRAIN(065): [2050/2211] Batch: 0.0872 (0.0964) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0441)
[2023/01/16 03:37] | TRAIN(065): [2100/2211] Batch: 0.0898 (0.0964) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0441)
[2023/01/16 03:37] | TRAIN(065): [2150/2211] Batch: 0.1184 (0.0963) Data: 0.0022 (0.0024) Loss: 0.0156 (0.0439)
[2023/01/16 03:37] | TRAIN(065): [2200/2211] Batch: 0.0866 (0.0966) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0440)
[2023/01/16 03:37] | ------------------------------------------------------------
[2023/01/16 03:37] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:37] | ------------------------------------------------------------
[2023/01/16 03:37] |    TRAIN(65)     0:03:33     0:00:05     0:03:28      0.0440
[2023/01/16 03:37] | ------------------------------------------------------------
[2023/01/16 03:37] | **************************************************
[2023/01/16 03:37] | TRAIN(066): [  50/2211] Batch: 0.0886 (0.1355) Data: 0.0019 (0.0242) Loss: 0.0000 (0.0381)
[2023/01/16 03:37] | TRAIN(066): [ 100/2211] Batch: 0.1275 (0.1262) Data: 0.0022 (0.0133) Loss: 0.0854 (0.0381)
[2023/01/16 03:37] | TRAIN(066): [ 150/2211] Batch: 0.0952 (0.1179) Data: 0.0056 (0.0096) Loss: 0.0042 (0.0378)
[2023/01/16 03:37] | TRAIN(066): [ 200/2211] Batch: 0.0922 (0.1119) Data: 0.0017 (0.0076) Loss: 0.0898 (0.0375)
[2023/01/16 03:37] | TRAIN(066): [ 250/2211] Batch: 0.1297 (0.1105) Data: 0.0023 (0.0065) Loss: 0.1391 (0.0384)
[2023/01/16 03:37] | TRAIN(066): [ 300/2211] Batch: 0.0888 (0.1111) Data: 0.0017 (0.0058) Loss: 0.0000 (0.0369)
[2023/01/16 03:38] | TRAIN(066): [ 350/2211] Batch: 0.1182 (0.1108) Data: 0.0022 (0.0053) Loss: 0.0000 (0.0365)
[2023/01/16 03:38] | TRAIN(066): [ 400/2211] Batch: 0.0899 (0.1091) Data: 0.0019 (0.0049) Loss: 0.2037 (0.0378)
[2023/01/16 03:38] | TRAIN(066): [ 450/2211] Batch: 0.1177 (0.1084) Data: 0.0023 (0.0045) Loss: 0.0000 (0.0367)
[2023/01/16 03:38] | TRAIN(066): [ 500/2211] Batch: 0.0943 (0.1080) Data: 0.0017 (0.0043) Loss: 0.0321 (0.0386)
[2023/01/16 03:38] | TRAIN(066): [ 550/2211] Batch: 0.0882 (0.1064) Data: 0.0020 (0.0041) Loss: 0.0523 (0.0393)
[2023/01/16 03:38] | TRAIN(066): [ 600/2211] Batch: 0.0903 (0.1061) Data: 0.0018 (0.0039) Loss: 0.0080 (0.0400)
[2023/01/16 03:38] | TRAIN(066): [ 650/2211] Batch: 0.1004 (0.1052) Data: 0.0018 (0.0037) Loss: 0.1348 (0.0412)
[2023/01/16 03:38] | TRAIN(066): [ 700/2211] Batch: 0.0904 (0.1043) Data: 0.0020 (0.0036) Loss: 0.1765 (0.0400)
[2023/01/16 03:38] | TRAIN(066): [ 750/2211] Batch: 0.0923 (0.1036) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0394)
[2023/01/16 03:38] | TRAIN(066): [ 800/2211] Batch: 0.0861 (0.1031) Data: 0.0018 (0.0034) Loss: 0.0345 (0.0402)
[2023/01/16 03:38] | TRAIN(066): [ 850/2211] Batch: 0.0968 (0.1023) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0402)
[2023/01/16 03:38] | TRAIN(066): [ 900/2211] Batch: 0.0962 (0.1017) Data: 0.0017 (0.0032) Loss: 0.2283 (0.0413)
[2023/01/16 03:39] | TRAIN(066): [ 950/2211] Batch: 0.0958 (0.1015) Data: 0.0018 (0.0031) Loss: 0.2015 (0.0420)
[2023/01/16 03:39] | TRAIN(066): [1000/2211] Batch: 0.0898 (0.1011) Data: 0.0031 (0.0031) Loss: 0.0855 (0.0428)
[2023/01/16 03:39] | TRAIN(066): [1050/2211] Batch: 0.1281 (0.1009) Data: 0.0020 (0.0030) Loss: 0.1969 (0.0430)
[2023/01/16 03:39] | TRAIN(066): [1100/2211] Batch: 0.1170 (0.1011) Data: 0.0028 (0.0030) Loss: 0.0032 (0.0431)
[2023/01/16 03:39] | TRAIN(066): [1150/2211] Batch: 0.1172 (0.1015) Data: 0.0021 (0.0029) Loss: 0.0369 (0.0430)
[2023/01/16 03:39] | TRAIN(066): [1200/2211] Batch: 0.0945 (0.1011) Data: 0.0017 (0.0029) Loss: 0.1300 (0.0428)
[2023/01/16 03:39] | TRAIN(066): [1250/2211] Batch: 0.0916 (0.1009) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0424)
[2023/01/16 03:39] | TRAIN(066): [1300/2211] Batch: 0.0898 (0.1006) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0418)
[2023/01/16 03:39] | TRAIN(066): [1350/2211] Batch: 0.0927 (0.1002) Data: 0.0017 (0.0028) Loss: 0.0310 (0.0416)
[2023/01/16 03:39] | TRAIN(066): [1400/2211] Batch: 0.0940 (0.1000) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0416)
[2023/01/16 03:39] | TRAIN(066): [1450/2211] Batch: 0.0913 (0.0999) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0416)
[2023/01/16 03:39] | TRAIN(066): [1500/2211] Batch: 0.1263 (0.0996) Data: 0.0023 (0.0027) Loss: 0.0000 (0.0413)
[2023/01/16 03:39] | TRAIN(066): [1550/2211] Batch: 0.1177 (0.0997) Data: 0.0021 (0.0026) Loss: 0.0178 (0.0411)
[2023/01/16 03:40] | TRAIN(066): [1600/2211] Batch: 0.1179 (0.0996) Data: 0.0022 (0.0026) Loss: 0.0314 (0.0406)
[2023/01/16 03:40] | TRAIN(066): [1650/2211] Batch: 0.0882 (0.0996) Data: 0.0017 (0.0026) Loss: 0.0088 (0.0403)
[2023/01/16 03:40] | TRAIN(066): [1700/2211] Batch: 0.0885 (0.0997) Data: 0.0018 (0.0026) Loss: 0.0018 (0.0404)
[2023/01/16 03:40] | TRAIN(066): [1750/2211] Batch: 0.0867 (0.0996) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0401)
[2023/01/16 03:40] | TRAIN(066): [1800/2211] Batch: 0.0877 (0.0994) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0400)
[2023/01/16 03:40] | TRAIN(066): [1850/2211] Batch: 0.1018 (0.0992) Data: 0.0020 (0.0025) Loss: 0.0464 (0.0402)
[2023/01/16 03:40] | TRAIN(066): [1900/2211] Batch: 0.0985 (0.0991) Data: 0.0016 (0.0025) Loss: 0.0033 (0.0402)
[2023/01/16 03:40] | TRAIN(066): [1950/2211] Batch: 0.0873 (0.0990) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0404)
[2023/01/16 03:40] | TRAIN(066): [2000/2211] Batch: 0.0978 (0.0988) Data: 0.0019 (0.0025) Loss: 0.0063 (0.0406)
[2023/01/16 03:40] | TRAIN(066): [2050/2211] Batch: 0.0891 (0.0987) Data: 0.0018 (0.0024) Loss: 0.0196 (0.0408)
[2023/01/16 03:40] | TRAIN(066): [2100/2211] Batch: 0.0929 (0.0985) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0412)
[2023/01/16 03:40] | TRAIN(066): [2150/2211] Batch: 0.0924 (0.0984) Data: 0.0021 (0.0024) Loss: 0.2106 (0.0415)
[2023/01/16 03:41] | TRAIN(066): [2200/2211] Batch: 0.0892 (0.0982) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0417)
[2023/01/16 03:41] | ------------------------------------------------------------
[2023/01/16 03:41] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:41] | ------------------------------------------------------------
[2023/01/16 03:41] |    TRAIN(66)     0:03:37     0:00:05     0:03:31      0.0417
[2023/01/16 03:41] | ------------------------------------------------------------
[2023/01/16 03:41] | **************************************************
[2023/01/16 03:41] | TRAIN(067): [  50/2211] Batch: 0.0881 (0.1201) Data: 0.0018 (0.0245) Loss: 0.0133 (0.0323)
[2023/01/16 03:41] | TRAIN(067): [ 100/2211] Batch: 0.0880 (0.1060) Data: 0.0017 (0.0133) Loss: 0.1082 (0.0367)
[2023/01/16 03:41] | TRAIN(067): [ 150/2211] Batch: 0.0877 (0.1009) Data: 0.0017 (0.0095) Loss: 0.1050 (0.0396)
[2023/01/16 03:41] | TRAIN(067): [ 200/2211] Batch: 0.0896 (0.0985) Data: 0.0015 (0.0076) Loss: 0.0000 (0.0360)
[2023/01/16 03:41] | TRAIN(067): [ 250/2211] Batch: 0.0882 (0.0969) Data: 0.0018 (0.0064) Loss: 0.0911 (0.0390)
[2023/01/16 03:41] | TRAIN(067): [ 300/2211] Batch: 0.0979 (0.0960) Data: 0.0018 (0.0057) Loss: 0.0016 (0.0381)
[2023/01/16 03:41] | TRAIN(067): [ 350/2211] Batch: 0.1128 (0.0972) Data: 0.0018 (0.0051) Loss: 0.0149 (0.0391)
[2023/01/16 03:41] | TRAIN(067): [ 400/2211] Batch: 0.0885 (0.0971) Data: 0.0017 (0.0047) Loss: 0.1119 (0.0394)
[2023/01/16 03:41] | TRAIN(067): [ 450/2211] Batch: 0.0881 (0.0971) Data: 0.0017 (0.0044) Loss: 0.0099 (0.0391)
[2023/01/16 03:41] | TRAIN(067): [ 500/2211] Batch: 0.0896 (0.0971) Data: 0.0018 (0.0041) Loss: 0.0058 (0.0381)
[2023/01/16 03:41] | TRAIN(067): [ 550/2211] Batch: 0.1024 (0.0970) Data: 0.0021 (0.0039) Loss: 0.1340 (0.0388)
[2023/01/16 03:41] | TRAIN(067): [ 600/2211] Batch: 0.0937 (0.0969) Data: 0.0018 (0.0038) Loss: 0.0508 (0.0388)
[2023/01/16 03:42] | TRAIN(067): [ 650/2211] Batch: 0.0923 (0.0967) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0396)
[2023/01/16 03:42] | TRAIN(067): [ 700/2211] Batch: 0.1152 (0.0965) Data: 0.0018 (0.0035) Loss: 0.0135 (0.0392)
[2023/01/16 03:42] | TRAIN(067): [ 750/2211] Batch: 0.0904 (0.0965) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0390)
[2023/01/16 03:42] | TRAIN(067): [ 800/2211] Batch: 0.0935 (0.0963) Data: 0.0048 (0.0033) Loss: 0.0000 (0.0383)
[2023/01/16 03:42] | TRAIN(067): [ 850/2211] Batch: 0.0918 (0.0967) Data: 0.0018 (0.0032) Loss: 0.0352 (0.0396)
[2023/01/16 03:42] | TRAIN(067): [ 900/2211] Batch: 0.0918 (0.0966) Data: 0.0018 (0.0031) Loss: 0.0331 (0.0393)
[2023/01/16 03:42] | TRAIN(067): [ 950/2211] Batch: 0.0893 (0.0966) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0395)
[2023/01/16 03:42] | TRAIN(067): [1000/2211] Batch: 0.0884 (0.0963) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0391)
[2023/01/16 03:42] | TRAIN(067): [1050/2211] Batch: 0.0898 (0.0960) Data: 0.0018 (0.0029) Loss: 0.0062 (0.0397)
[2023/01/16 03:42] | TRAIN(067): [1100/2211] Batch: 0.0936 (0.0959) Data: 0.0018 (0.0029) Loss: 0.0024 (0.0398)
[2023/01/16 03:42] | TRAIN(067): [1150/2211] Batch: 0.0894 (0.0958) Data: 0.0018 (0.0028) Loss: 0.0051 (0.0396)
[2023/01/16 03:42] | TRAIN(067): [1200/2211] Batch: 0.0906 (0.0958) Data: 0.0017 (0.0028) Loss: 0.1287 (0.0398)
[2023/01/16 03:43] | TRAIN(067): [1250/2211] Batch: 0.0905 (0.0958) Data: 0.0018 (0.0027) Loss: 0.0733 (0.0397)
[2023/01/16 03:43] | TRAIN(067): [1300/2211] Batch: 0.0915 (0.0957) Data: 0.0018 (0.0027) Loss: 0.0819 (0.0402)
[2023/01/16 03:43] | TRAIN(067): [1350/2211] Batch: 0.0874 (0.0956) Data: 0.0017 (0.0027) Loss: 0.0206 (0.0404)
[2023/01/16 03:43] | TRAIN(067): [1400/2211] Batch: 0.0888 (0.0957) Data: 0.0018 (0.0026) Loss: 0.0062 (0.0406)
[2023/01/16 03:43] | TRAIN(067): [1450/2211] Batch: 0.0889 (0.0955) Data: 0.0018 (0.0026) Loss: 0.0051 (0.0408)
[2023/01/16 03:43] | TRAIN(067): [1500/2211] Batch: 0.0891 (0.0953) Data: 0.0018 (0.0026) Loss: 0.1556 (0.0413)
[2023/01/16 03:43] | TRAIN(067): [1550/2211] Batch: 0.0876 (0.0953) Data: 0.0017 (0.0026) Loss: 0.0652 (0.0414)
[2023/01/16 03:43] | TRAIN(067): [1600/2211] Batch: 0.0850 (0.0952) Data: 0.0019 (0.0025) Loss: 0.0689 (0.0412)
[2023/01/16 03:43] | TRAIN(067): [1650/2211] Batch: 0.0878 (0.0953) Data: 0.0015 (0.0025) Loss: 0.0575 (0.0414)
[2023/01/16 03:43] | TRAIN(067): [1700/2211] Batch: 0.1074 (0.0952) Data: 0.0017 (0.0025) Loss: 0.0001 (0.0418)
[2023/01/16 03:43] | TRAIN(067): [1750/2211] Batch: 0.0843 (0.0951) Data: 0.0018 (0.0025) Loss: 0.0930 (0.0416)
[2023/01/16 03:43] | TRAIN(067): [1800/2211] Batch: 0.0880 (0.0950) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0421)
[2023/01/16 03:43] | TRAIN(067): [1850/2211] Batch: 0.0903 (0.0949) Data: 0.0018 (0.0024) Loss: 0.0126 (0.0425)
[2023/01/16 03:44] | TRAIN(067): [1900/2211] Batch: 0.0878 (0.0949) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0425)
[2023/01/16 03:44] | TRAIN(067): [1950/2211] Batch: 0.0874 (0.0948) Data: 0.0018 (0.0024) Loss: 0.1138 (0.0428)
[2023/01/16 03:44] | TRAIN(067): [2000/2211] Batch: 0.0889 (0.0947) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0428)
[2023/01/16 03:44] | TRAIN(067): [2050/2211] Batch: 0.0913 (0.0950) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0428)
[2023/01/16 03:44] | TRAIN(067): [2100/2211] Batch: 0.0945 (0.0949) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0427)
[2023/01/16 03:44] | TRAIN(067): [2150/2211] Batch: 0.0864 (0.0948) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0429)
[2023/01/16 03:44] | TRAIN(067): [2200/2211] Batch: 0.0864 (0.0947) Data: 0.0015 (0.0023) Loss: 0.1466 (0.0430)
[2023/01/16 03:44] | ------------------------------------------------------------
[2023/01/16 03:44] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:44] | ------------------------------------------------------------
[2023/01/16 03:44] |    TRAIN(67)     0:03:29     0:00:05     0:03:24      0.0430
[2023/01/16 03:44] | ------------------------------------------------------------
[2023/01/16 03:44] | **************************************************
[2023/01/16 03:44] | TRAIN(068): [  50/2211] Batch: 0.0887 (0.1253) Data: 0.0017 (0.0244) Loss: 0.0000 (0.0315)
[2023/01/16 03:44] | TRAIN(068): [ 100/2211] Batch: 0.1007 (0.1086) Data: 0.0019 (0.0132) Loss: 0.0526 (0.0318)
[2023/01/16 03:44] | TRAIN(068): [ 150/2211] Batch: 0.0892 (0.1040) Data: 0.0018 (0.0094) Loss: 0.1167 (0.0336)
[2023/01/16 03:44] | TRAIN(068): [ 200/2211] Batch: 0.0874 (0.1022) Data: 0.0017 (0.0075) Loss: 0.0793 (0.0343)
[2023/01/16 03:44] | TRAIN(068): [ 250/2211] Batch: 0.0933 (0.1004) Data: 0.0018 (0.0064) Loss: 0.0000 (0.0348)
[2023/01/16 03:45] | TRAIN(068): [ 300/2211] Batch: 0.0972 (0.0993) Data: 0.0018 (0.0056) Loss: 0.0000 (0.0364)
[2023/01/16 03:45] | TRAIN(068): [ 350/2211] Batch: 0.0884 (0.0981) Data: 0.0017 (0.0051) Loss: 0.0000 (0.0377)
[2023/01/16 03:45] | TRAIN(068): [ 400/2211] Batch: 0.0911 (0.0980) Data: 0.0014 (0.0047) Loss: 0.0000 (0.0385)
[2023/01/16 03:45] | TRAIN(068): [ 450/2211] Batch: 0.1200 (0.0972) Data: 0.0021 (0.0043) Loss: 0.0192 (0.0373)
[2023/01/16 03:45] | TRAIN(068): [ 500/2211] Batch: 0.0977 (0.0973) Data: 0.0018 (0.0041) Loss: 0.0024 (0.0375)
[2023/01/16 03:45] | TRAIN(068): [ 550/2211] Batch: 0.0892 (0.0966) Data: 0.0018 (0.0039) Loss: 0.0000 (0.0387)
[2023/01/16 03:45] | TRAIN(068): [ 600/2211] Batch: 0.0882 (0.0961) Data: 0.0018 (0.0037) Loss: 0.0024 (0.0385)
[2023/01/16 03:45] | TRAIN(068): [ 650/2211] Batch: 0.0966 (0.0957) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0385)
[2023/01/16 03:45] | TRAIN(068): [ 700/2211] Batch: 0.0881 (0.0955) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0382)
[2023/01/16 03:45] | TRAIN(068): [ 750/2211] Batch: 0.0952 (0.0953) Data: 0.0020 (0.0033) Loss: 0.1512 (0.0378)
[2023/01/16 03:45] | TRAIN(068): [ 800/2211] Batch: 0.0880 (0.0952) Data: 0.0018 (0.0032) Loss: 0.1337 (0.0393)
[2023/01/16 03:45] | TRAIN(068): [ 850/2211] Batch: 0.0907 (0.0952) Data: 0.0017 (0.0032) Loss: 0.0608 (0.0393)
[2023/01/16 03:45] | TRAIN(068): [ 900/2211] Batch: 0.0917 (0.0951) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0393)
[2023/01/16 03:46] | TRAIN(068): [ 950/2211] Batch: 0.0960 (0.0950) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0393)
[2023/01/16 03:46] | TRAIN(068): [1000/2211] Batch: 0.0918 (0.0949) Data: 0.0017 (0.0029) Loss: 0.0042 (0.0400)
[2023/01/16 03:46] | TRAIN(068): [1050/2211] Batch: 0.0948 (0.0949) Data: 0.0017 (0.0029) Loss: 0.1855 (0.0406)
[2023/01/16 03:46] | TRAIN(068): [1100/2211] Batch: 0.0893 (0.0950) Data: 0.0018 (0.0028) Loss: 0.0144 (0.0411)
[2023/01/16 03:46] | TRAIN(068): [1150/2211] Batch: 0.0897 (0.0949) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0411)
[2023/01/16 03:46] | TRAIN(068): [1200/2211] Batch: 0.0964 (0.0950) Data: 0.0018 (0.0028) Loss: 0.1982 (0.0417)
[2023/01/16 03:46] | TRAIN(068): [1250/2211] Batch: 0.0930 (0.0951) Data: 0.0018 (0.0027) Loss: 0.0258 (0.0425)
[2023/01/16 03:46] | TRAIN(068): [1300/2211] Batch: 0.0875 (0.0949) Data: 0.0018 (0.0027) Loss: 0.0860 (0.0431)
[2023/01/16 03:46] | TRAIN(068): [1350/2211] Batch: 0.0883 (0.0949) Data: 0.0018 (0.0026) Loss: 0.0267 (0.0432)
[2023/01/16 03:46] | TRAIN(068): [1400/2211] Batch: 0.0892 (0.0950) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0432)
[2023/01/16 03:46] | TRAIN(068): [1450/2211] Batch: 0.0868 (0.0950) Data: 0.0017 (0.0026) Loss: 0.1966 (0.0430)
[2023/01/16 03:46] | TRAIN(068): [1500/2211] Batch: 0.0934 (0.0948) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0427)
[2023/01/16 03:46] | TRAIN(068): [1550/2211] Batch: 0.0875 (0.0947) Data: 0.0018 (0.0025) Loss: 0.1799 (0.0423)
[2023/01/16 03:47] | TRAIN(068): [1600/2211] Batch: 0.0888 (0.0946) Data: 0.0018 (0.0025) Loss: 0.0176 (0.0423)
[2023/01/16 03:47] | TRAIN(068): [1650/2211] Batch: 0.0931 (0.0946) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0426)
[2023/01/16 03:47] | TRAIN(068): [1700/2211] Batch: 0.0861 (0.0945) Data: 0.0017 (0.0025) Loss: 0.0268 (0.0425)
[2023/01/16 03:47] | TRAIN(068): [1750/2211] Batch: 0.1219 (0.0952) Data: 0.0022 (0.0025) Loss: 0.1623 (0.0421)
[2023/01/16 03:47] | TRAIN(068): [1800/2211] Batch: 0.1179 (0.0959) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0418)
[2023/01/16 03:47] | TRAIN(068): [1850/2211] Batch: 0.1200 (0.0966) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0424)
[2023/01/16 03:47] | TRAIN(068): [1900/2211] Batch: 0.0885 (0.0966) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0425)
[2023/01/16 03:47] | TRAIN(068): [1950/2211] Batch: 0.0936 (0.0965) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0423)
[2023/01/16 03:47] | TRAIN(068): [2000/2211] Batch: 0.0880 (0.0964) Data: 0.0018 (0.0024) Loss: 0.1361 (0.0426)
[2023/01/16 03:47] | TRAIN(068): [2050/2211] Batch: 0.0879 (0.0963) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0424)
[2023/01/16 03:47] | TRAIN(068): [2100/2211] Batch: 0.0931 (0.0963) Data: 0.0018 (0.0024) Loss: 0.0255 (0.0426)
[2023/01/16 03:47] | TRAIN(068): [2150/2211] Batch: 0.0914 (0.0962) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0428)
[2023/01/16 03:48] | TRAIN(068): [2200/2211] Batch: 0.0898 (0.0962) Data: 0.0017 (0.0023) Loss: 0.0795 (0.0424)
[2023/01/16 03:48] | ------------------------------------------------------------
[2023/01/16 03:48] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:48] | ------------------------------------------------------------
[2023/01/16 03:48] |    TRAIN(68)     0:03:32     0:00:05     0:03:27      0.0424
[2023/01/16 03:48] | ------------------------------------------------------------
[2023/01/16 03:48] | **************************************************
[2023/01/16 03:48] | TRAIN(069): [  50/2211] Batch: 0.0869 (0.1228) Data: 0.0017 (0.0247) Loss: 0.0000 (0.0407)
[2023/01/16 03:48] | TRAIN(069): [ 100/2211] Batch: 0.1020 (0.1092) Data: 0.0018 (0.0133) Loss: 0.1517 (0.0470)
[2023/01/16 03:48] | TRAIN(069): [ 150/2211] Batch: 0.1092 (0.1038) Data: 0.0018 (0.0095) Loss: 0.0104 (0.0445)
[2023/01/16 03:48] | TRAIN(069): [ 200/2211] Batch: 0.0881 (0.1031) Data: 0.0020 (0.0077) Loss: 0.0029 (0.0444)
[2023/01/16 03:48] | TRAIN(069): [ 250/2211] Batch: 0.0948 (0.1008) Data: 0.0017 (0.0065) Loss: 0.0812 (0.0429)
[2023/01/16 03:48] | TRAIN(069): [ 300/2211] Batch: 0.0947 (0.1002) Data: 0.0019 (0.0057) Loss: 0.0033 (0.0413)
[2023/01/16 03:48] | TRAIN(069): [ 350/2211] Batch: 0.0889 (0.0994) Data: 0.0018 (0.0052) Loss: 0.1691 (0.0418)
[2023/01/16 03:48] | TRAIN(069): [ 400/2211] Batch: 0.0873 (0.0991) Data: 0.0018 (0.0047) Loss: 0.0022 (0.0409)
[2023/01/16 03:48] | TRAIN(069): [ 450/2211] Batch: 0.0879 (0.0982) Data: 0.0045 (0.0044) Loss: 0.0393 (0.0410)
[2023/01/16 03:48] | TRAIN(069): [ 500/2211] Batch: 0.0889 (0.0974) Data: 0.0019 (0.0042) Loss: 0.1646 (0.0402)
[2023/01/16 03:48] | TRAIN(069): [ 550/2211] Batch: 0.0956 (0.0973) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0389)
[2023/01/16 03:49] | TRAIN(069): [ 600/2211] Batch: 0.0872 (0.0971) Data: 0.0017 (0.0038) Loss: 0.0064 (0.0392)
[2023/01/16 03:49] | TRAIN(069): [ 650/2211] Batch: 0.0906 (0.0967) Data: 0.0017 (0.0036) Loss: 0.0500 (0.0412)
[2023/01/16 03:49] | TRAIN(069): [ 700/2211] Batch: 0.0924 (0.0972) Data: 0.0019 (0.0035) Loss: 0.0029 (0.0419)
[2023/01/16 03:49] | TRAIN(069): [ 750/2211] Batch: 0.0975 (0.0977) Data: 0.0018 (0.0034) Loss: 0.0103 (0.0422)
[2023/01/16 03:49] | TRAIN(069): [ 800/2211] Batch: 0.0970 (0.0975) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0415)
[2023/01/16 03:49] | TRAIN(069): [ 850/2211] Batch: 0.0999 (0.0973) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0427)
[2023/01/16 03:49] | TRAIN(069): [ 900/2211] Batch: 0.0874 (0.0972) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0431)
[2023/01/16 03:49] | TRAIN(069): [ 950/2211] Batch: 0.0875 (0.0969) Data: 0.0017 (0.0031) Loss: 0.0153 (0.0428)
[2023/01/16 03:49] | TRAIN(069): [1000/2211] Batch: 0.0952 (0.0972) Data: 0.0017 (0.0030) Loss: 0.1732 (0.0425)
[2023/01/16 03:49] | TRAIN(069): [1050/2211] Batch: 0.1191 (0.0974) Data: 0.0018 (0.0029) Loss: 0.0996 (0.0419)
[2023/01/16 03:49] | TRAIN(069): [1100/2211] Batch: 0.0949 (0.0977) Data: 0.0020 (0.0029) Loss: 0.0123 (0.0419)
[2023/01/16 03:49] | TRAIN(069): [1150/2211] Batch: 0.0877 (0.0974) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0417)
[2023/01/16 03:50] | TRAIN(069): [1200/2211] Batch: 0.0932 (0.0972) Data: 0.0019 (0.0028) Loss: 0.1016 (0.0422)
[2023/01/16 03:50] | TRAIN(069): [1250/2211] Batch: 0.0958 (0.0971) Data: 0.0017 (0.0028) Loss: 0.0584 (0.0418)
[2023/01/16 03:50] | TRAIN(069): [1300/2211] Batch: 0.0907 (0.0968) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0416)
[2023/01/16 03:50] | TRAIN(069): [1350/2211] Batch: 0.0881 (0.0966) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0412)
[2023/01/16 03:50] | TRAIN(069): [1400/2211] Batch: 0.0896 (0.0964) Data: 0.0018 (0.0027) Loss: 0.0174 (0.0412)
[2023/01/16 03:50] | TRAIN(069): [1450/2211] Batch: 0.0866 (0.0962) Data: 0.0018 (0.0026) Loss: 0.0745 (0.0409)
[2023/01/16 03:50] | TRAIN(069): [1500/2211] Batch: 0.1198 (0.0966) Data: 0.0022 (0.0026) Loss: 0.2222 (0.0406)
[2023/01/16 03:50] | TRAIN(069): [1550/2211] Batch: 0.0884 (0.0966) Data: 0.0017 (0.0026) Loss: 0.1751 (0.0407)
[2023/01/16 03:50] | TRAIN(069): [1600/2211] Batch: 0.0958 (0.0965) Data: 0.0018 (0.0026) Loss: 0.0069 (0.0404)
[2023/01/16 03:50] | TRAIN(069): [1650/2211] Batch: 0.0877 (0.0963) Data: 0.0016 (0.0025) Loss: 0.0542 (0.0402)
[2023/01/16 03:50] | TRAIN(069): [1700/2211] Batch: 0.0976 (0.0963) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0400)
[2023/01/16 03:50] | TRAIN(069): [1750/2211] Batch: 0.0891 (0.0962) Data: 0.0018 (0.0025) Loss: 0.0068 (0.0401)
[2023/01/16 03:50] | TRAIN(069): [1800/2211] Batch: 0.0883 (0.0961) Data: 0.0018 (0.0025) Loss: 0.1153 (0.0402)
[2023/01/16 03:51] | TRAIN(069): [1850/2211] Batch: 0.0882 (0.0960) Data: 0.0018 (0.0025) Loss: 0.0866 (0.0401)
[2023/01/16 03:51] | TRAIN(069): [1900/2211] Batch: 0.0946 (0.0959) Data: 0.0019 (0.0024) Loss: 0.0280 (0.0403)
[2023/01/16 03:51] | TRAIN(069): [1950/2211] Batch: 0.0889 (0.0958) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0402)
[2023/01/16 03:51] | TRAIN(069): [2000/2211] Batch: 0.0917 (0.0963) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0403)
[2023/01/16 03:51] | TRAIN(069): [2050/2211] Batch: 0.0871 (0.0962) Data: 0.0013 (0.0024) Loss: 0.0000 (0.0406)
[2023/01/16 03:51] | TRAIN(069): [2100/2211] Batch: 0.0989 (0.0963) Data: 0.0019 (0.0024) Loss: 0.0792 (0.0409)
[2023/01/16 03:51] | TRAIN(069): [2150/2211] Batch: 0.1018 (0.0962) Data: 0.0017 (0.0024) Loss: 0.0579 (0.0412)
[2023/01/16 03:51] | TRAIN(069): [2200/2211] Batch: 0.0915 (0.0962) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0412)
[2023/01/16 03:51] | ------------------------------------------------------------
[2023/01/16 03:51] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:51] | ------------------------------------------------------------
[2023/01/16 03:51] |    TRAIN(69)     0:03:32     0:00:05     0:03:27      0.0415
[2023/01/16 03:51] | ------------------------------------------------------------
[2023/01/16 03:51] | **************************************************
[2023/01/16 03:51] | TRAIN(070): [  50/2211] Batch: 0.0898 (0.1251) Data: 0.0018 (0.0253) Loss: 0.0379 (0.0456)
[2023/01/16 03:51] | TRAIN(070): [ 100/2211] Batch: 0.0896 (0.1086) Data: 0.0014 (0.0136) Loss: 0.1774 (0.0521)
[2023/01/16 03:51] | TRAIN(070): [ 150/2211] Batch: 0.0922 (0.1038) Data: 0.0014 (0.0096) Loss: 0.0000 (0.0444)
[2023/01/16 03:51] | TRAIN(070): [ 200/2211] Batch: 0.0889 (0.1016) Data: 0.0020 (0.0076) Loss: 0.1019 (0.0471)
[2023/01/16 03:52] | TRAIN(070): [ 250/2211] Batch: 0.0941 (0.1000) Data: 0.0016 (0.0064) Loss: 0.0000 (0.0453)
[2023/01/16 03:52] | TRAIN(070): [ 300/2211] Batch: 0.0993 (0.1000) Data: 0.0015 (0.0056) Loss: 0.0000 (0.0435)
[2023/01/16 03:52] | TRAIN(070): [ 350/2211] Batch: 0.0879 (0.0990) Data: 0.0014 (0.0050) Loss: 0.0843 (0.0426)
[2023/01/16 03:52] | TRAIN(070): [ 400/2211] Batch: 0.0974 (0.0986) Data: 0.0016 (0.0046) Loss: 0.0664 (0.0419)
[2023/01/16 03:52] | TRAIN(070): [ 450/2211] Batch: 0.0950 (0.0990) Data: 0.0019 (0.0043) Loss: 0.1074 (0.0411)
[2023/01/16 03:52] | TRAIN(070): [ 500/2211] Batch: 0.0973 (0.0994) Data: 0.0017 (0.0041) Loss: 0.0594 (0.0410)
[2023/01/16 03:52] | TRAIN(070): [ 550/2211] Batch: 0.0839 (0.0988) Data: 0.0018 (0.0039) Loss: 0.0000 (0.0413)
[2023/01/16 03:52] | TRAIN(070): [ 600/2211] Batch: 0.0962 (0.0981) Data: 0.0018 (0.0037) Loss: 0.0224 (0.0414)
[2023/01/16 03:52] | TRAIN(070): [ 650/2211] Batch: 0.0882 (0.0975) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0414)
[2023/01/16 03:52] | TRAIN(070): [ 700/2211] Batch: 0.0866 (0.0970) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0404)
[2023/01/16 03:52] | TRAIN(070): [ 750/2211] Batch: 0.0921 (0.0968) Data: 0.0016 (0.0033) Loss: 0.0000 (0.0403)
[2023/01/16 03:52] | TRAIN(070): [ 800/2211] Batch: 0.1045 (0.0966) Data: 0.0023 (0.0032) Loss: 0.0242 (0.0408)
[2023/01/16 03:52] | TRAIN(070): [ 850/2211] Batch: 0.0908 (0.0965) Data: 0.0020 (0.0031) Loss: 0.0487 (0.0407)
[2023/01/16 03:53] | TRAIN(070): [ 900/2211] Batch: 0.0870 (0.0961) Data: 0.0016 (0.0030) Loss: 0.0459 (0.0405)
[2023/01/16 03:53] | TRAIN(070): [ 950/2211] Batch: 0.0881 (0.0958) Data: 0.0016 (0.0030) Loss: 0.0000 (0.0407)
[2023/01/16 03:53] | TRAIN(070): [1000/2211] Batch: 0.0879 (0.0956) Data: 0.0018 (0.0029) Loss: 0.0981 (0.0405)
[2023/01/16 03:53] | TRAIN(070): [1050/2211] Batch: 0.0871 (0.0954) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0397)
[2023/01/16 03:53] | TRAIN(070): [1100/2211] Batch: 0.0953 (0.0952) Data: 0.0017 (0.0028) Loss: 0.0581 (0.0408)
[2023/01/16 03:53] | TRAIN(070): [1150/2211] Batch: 0.0888 (0.0950) Data: 0.0017 (0.0028) Loss: 0.3545 (0.0416)
[2023/01/16 03:53] | TRAIN(070): [1200/2211] Batch: 0.0911 (0.0949) Data: 0.0017 (0.0027) Loss: 0.0267 (0.0412)
[2023/01/16 03:53] | TRAIN(070): [1250/2211] Batch: 0.0920 (0.0948) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0407)
[2023/01/16 03:53] | TRAIN(070): [1300/2211] Batch: 0.0869 (0.0948) Data: 0.0017 (0.0026) Loss: 0.0152 (0.0407)
[2023/01/16 03:53] | TRAIN(070): [1350/2211] Batch: 0.0889 (0.0946) Data: 0.0018 (0.0026) Loss: 0.0363 (0.0409)
[2023/01/16 03:53] | TRAIN(070): [1400/2211] Batch: 0.0982 (0.0946) Data: 0.0019 (0.0026) Loss: 0.1071 (0.0413)
[2023/01/16 03:53] | TRAIN(070): [1450/2211] Batch: 0.0891 (0.0946) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0413)
[2023/01/16 03:53] | TRAIN(070): [1500/2211] Batch: 0.0864 (0.0945) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0413)
[2023/01/16 03:54] | TRAIN(070): [1550/2211] Batch: 0.0898 (0.0944) Data: 0.0018 (0.0025) Loss: 0.1373 (0.0411)
[2023/01/16 03:54] | TRAIN(070): [1600/2211] Batch: 0.0989 (0.0945) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0409)
[2023/01/16 03:54] | TRAIN(070): [1650/2211] Batch: 0.1158 (0.0946) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0410)
[2023/01/16 03:54] | TRAIN(070): [1700/2211] Batch: 0.0872 (0.0949) Data: 0.0017 (0.0024) Loss: 0.0696 (0.0413)
[2023/01/16 03:54] | TRAIN(070): [1750/2211] Batch: 0.1020 (0.0948) Data: 0.0023 (0.0024) Loss: 0.0354 (0.0412)
[2023/01/16 03:54] | TRAIN(070): [1800/2211] Batch: 0.0922 (0.0950) Data: 0.0019 (0.0024) Loss: 0.0870 (0.0415)
[2023/01/16 03:54] | TRAIN(070): [1850/2211] Batch: 0.0916 (0.0950) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0417)
[2023/01/16 03:54] | TRAIN(070): [1900/2211] Batch: 0.0876 (0.0951) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0420)
[2023/01/16 03:54] | TRAIN(070): [1950/2211] Batch: 0.0890 (0.0951) Data: 0.0019 (0.0024) Loss: 0.1749 (0.0416)
[2023/01/16 03:54] | TRAIN(070): [2000/2211] Batch: 0.0897 (0.0950) Data: 0.0018 (0.0024) Loss: 0.2351 (0.0419)
[2023/01/16 03:54] | TRAIN(070): [2050/2211] Batch: 0.0983 (0.0950) Data: 0.0018 (0.0023) Loss: 0.1717 (0.0418)
[2023/01/16 03:54] | TRAIN(070): [2100/2211] Batch: 0.0876 (0.0950) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0417)
[2023/01/16 03:55] | TRAIN(070): [2150/2211] Batch: 0.0894 (0.0949) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0416)
[2023/01/16 03:55] | TRAIN(070): [2200/2211] Batch: 0.0921 (0.0948) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0416)
[2023/01/16 03:55] | ------------------------------------------------------------
[2023/01/16 03:55] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:55] | ------------------------------------------------------------
[2023/01/16 03:55] |    TRAIN(70)     0:03:29     0:00:05     0:03:24      0.0417
[2023/01/16 03:55] | ------------------------------------------------------------
[2023/01/16 03:55] | **************************************************
[2023/01/16 03:55] | TRAIN(071): [  50/2211] Batch: 0.0933 (0.1302) Data: 0.0014 (0.0266) Loss: 0.0000 (0.0330)
[2023/01/16 03:55] | TRAIN(071): [ 100/2211] Batch: 0.1084 (0.1184) Data: 0.0015 (0.0142) Loss: 0.0309 (0.0459)
[2023/01/16 03:55] | TRAIN(071): [ 150/2211] Batch: 0.1252 (0.1183) Data: 0.0022 (0.0101) Loss: 0.0065 (0.0480)
[2023/01/16 03:55] | TRAIN(071): [ 200/2211] Batch: 0.1189 (0.1189) Data: 0.0023 (0.0081) Loss: 0.0835 (0.0495)
[2023/01/16 03:55] | TRAIN(071): [ 250/2211] Batch: 0.0916 (0.1171) Data: 0.0018 (0.0068) Loss: 0.0227 (0.0466)
[2023/01/16 03:55] | TRAIN(071): [ 300/2211] Batch: 0.0900 (0.1130) Data: 0.0017 (0.0060) Loss: 0.0508 (0.0453)
[2023/01/16 03:55] | TRAIN(071): [ 350/2211] Batch: 0.1198 (0.1127) Data: 0.0020 (0.0054) Loss: 0.2193 (0.0448)
[2023/01/16 03:55] | TRAIN(071): [ 400/2211] Batch: 0.0888 (0.1123) Data: 0.0018 (0.0050) Loss: 0.0000 (0.0434)
[2023/01/16 03:55] | TRAIN(071): [ 450/2211] Batch: 0.0900 (0.1101) Data: 0.0018 (0.0046) Loss: 0.1323 (0.0430)
[2023/01/16 03:56] | TRAIN(071): [ 500/2211] Batch: 0.0925 (0.1085) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0427)
[2023/01/16 03:56] | TRAIN(071): [ 550/2211] Batch: 0.0989 (0.1074) Data: 0.0018 (0.0041) Loss: 0.0000 (0.0431)
[2023/01/16 03:56] | TRAIN(071): [ 600/2211] Batch: 0.0985 (0.1078) Data: 0.0018 (0.0039) Loss: 0.2510 (0.0435)
[2023/01/16 03:56] | TRAIN(071): [ 650/2211] Batch: 0.0873 (0.1067) Data: 0.0018 (0.0038) Loss: 0.1772 (0.0449)
[2023/01/16 03:56] | TRAIN(071): [ 700/2211] Batch: 0.0889 (0.1055) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0451)
[2023/01/16 03:56] | TRAIN(071): [ 750/2211] Batch: 0.0892 (0.1048) Data: 0.0018 (0.0035) Loss: 0.0443 (0.0437)
[2023/01/16 03:56] | TRAIN(071): [ 800/2211] Batch: 0.0884 (0.1040) Data: 0.0019 (0.0034) Loss: 0.0028 (0.0431)
[2023/01/16 03:56] | TRAIN(071): [ 850/2211] Batch: 0.0888 (0.1032) Data: 0.0019 (0.0033) Loss: 0.1431 (0.0420)
[2023/01/16 03:56] | TRAIN(071): [ 900/2211] Batch: 0.0889 (0.1028) Data: 0.0017 (0.0032) Loss: 0.1837 (0.0422)
[2023/01/16 03:56] | TRAIN(071): [ 950/2211] Batch: 0.0955 (0.1022) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0423)
[2023/01/16 03:56] | TRAIN(071): [1000/2211] Batch: 0.0932 (0.1019) Data: 0.0018 (0.0031) Loss: 0.0427 (0.0427)
[2023/01/16 03:56] | TRAIN(071): [1050/2211] Batch: 0.1029 (0.1017) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0422)
[2023/01/16 03:56] | TRAIN(071): [1100/2211] Batch: 0.0880 (0.1013) Data: 0.0016 (0.0030) Loss: 0.1008 (0.0419)
[2023/01/16 03:57] | TRAIN(071): [1150/2211] Batch: 0.0946 (0.1011) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0416)
[2023/01/16 03:57] | TRAIN(071): [1200/2211] Batch: 0.0876 (0.1008) Data: 0.0017 (0.0029) Loss: 0.0950 (0.0411)
[2023/01/16 03:57] | TRAIN(071): [1250/2211] Batch: 0.0887 (0.1004) Data: 0.0016 (0.0028) Loss: 0.0000 (0.0411)
[2023/01/16 03:57] | TRAIN(071): [1300/2211] Batch: 0.0906 (0.1001) Data: 0.0017 (0.0028) Loss: 0.0412 (0.0406)
[2023/01/16 03:57] | TRAIN(071): [1350/2211] Batch: 0.0898 (0.1000) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0404)
[2023/01/16 03:57] | TRAIN(071): [1400/2211] Batch: 0.0927 (0.0997) Data: 0.0017 (0.0027) Loss: 0.1195 (0.0407)
[2023/01/16 03:57] | TRAIN(071): [1450/2211] Batch: 0.0896 (0.0995) Data: 0.0018 (0.0027) Loss: 0.1646 (0.0406)
[2023/01/16 03:57] | TRAIN(071): [1500/2211] Batch: 0.0962 (0.0993) Data: 0.0014 (0.0026) Loss: 0.0085 (0.0405)
[2023/01/16 03:57] | TRAIN(071): [1550/2211] Batch: 0.0898 (0.0991) Data: 0.0016 (0.0026) Loss: 0.0000 (0.0411)
[2023/01/16 03:57] | TRAIN(071): [1600/2211] Batch: 0.0893 (0.0989) Data: 0.0017 (0.0026) Loss: 0.0021 (0.0411)
[2023/01/16 03:57] | TRAIN(071): [1650/2211] Batch: 0.1059 (0.0988) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0414)
[2023/01/16 03:57] | TRAIN(071): [1700/2211] Batch: 0.0902 (0.0987) Data: 0.0014 (0.0025) Loss: 0.0000 (0.0413)
[2023/01/16 03:57] | TRAIN(071): [1750/2211] Batch: 0.1026 (0.0985) Data: 0.0017 (0.0025) Loss: 0.0722 (0.0411)
[2023/01/16 03:58] | TRAIN(071): [1800/2211] Batch: 0.0913 (0.0983) Data: 0.0018 (0.0025) Loss: 0.0804 (0.0415)
[2023/01/16 03:58] | TRAIN(071): [1850/2211] Batch: 0.0893 (0.0982) Data: 0.0017 (0.0025) Loss: 0.0144 (0.0418)
[2023/01/16 03:58] | TRAIN(071): [1900/2211] Batch: 0.0982 (0.0981) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0422)
[2023/01/16 03:58] | TRAIN(071): [1950/2211] Batch: 0.0990 (0.0981) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0423)
[2023/01/16 03:58] | TRAIN(071): [2000/2211] Batch: 0.0906 (0.0980) Data: 0.0017 (0.0024) Loss: 0.0261 (0.0424)
[2023/01/16 03:58] | TRAIN(071): [2050/2211] Batch: 0.0895 (0.0979) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0427)
[2023/01/16 03:58] | TRAIN(071): [2100/2211] Batch: 0.0918 (0.0982) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0433)
[2023/01/16 03:58] | TRAIN(071): [2150/2211] Batch: 0.0911 (0.0981) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0440)
[2023/01/16 03:58] | TRAIN(071): [2200/2211] Batch: 0.0861 (0.0980) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0445)
[2023/01/16 03:58] | ------------------------------------------------------------
[2023/01/16 03:58] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 03:58] | ------------------------------------------------------------
[2023/01/16 03:58] |    TRAIN(71)     0:03:36     0:00:05     0:03:31      0.0445
[2023/01/16 03:58] | ------------------------------------------------------------
[2023/01/16 03:58] | **************************************************
[2023/01/16 03:58] | TRAIN(072): [  50/2211] Batch: 0.1227 (0.1300) Data: 0.0023 (0.0241) Loss: 0.0000 (0.0501)
[2023/01/16 03:58] | TRAIN(072): [ 100/2211] Batch: 0.1176 (0.1272) Data: 0.0023 (0.0133) Loss: 0.0952 (0.0468)
[2023/01/16 03:59] | TRAIN(072): [ 150/2211] Batch: 0.1380 (0.1267) Data: 0.0022 (0.0097) Loss: 0.1074 (0.0427)
[2023/01/16 03:59] | TRAIN(072): [ 200/2211] Batch: 0.1422 (0.1296) Data: 0.0022 (0.0078) Loss: 0.0000 (0.0453)
[2023/01/16 03:59] | TRAIN(072): [ 250/2211] Batch: 0.1129 (0.1289) Data: 0.0015 (0.0067) Loss: 0.0000 (0.0434)
[2023/01/16 03:59] | TRAIN(072): [ 300/2211] Batch: 0.1123 (0.1270) Data: 0.0018 (0.0058) Loss: 0.0000 (0.0440)
[2023/01/16 03:59] | TRAIN(072): [ 350/2211] Batch: 0.1279 (0.1273) Data: 0.0023 (0.0053) Loss: 0.0000 (0.0441)
[2023/01/16 03:59] | TRAIN(072): [ 400/2211] Batch: 0.1310 (0.1272) Data: 0.0039 (0.0049) Loss: 0.1646 (0.0428)
[2023/01/16 03:59] | TRAIN(072): [ 450/2211] Batch: 0.1190 (0.1267) Data: 0.0022 (0.0046) Loss: 0.0000 (0.0424)
[2023/01/16 03:59] | TRAIN(072): [ 500/2211] Batch: 0.1189 (0.1259) Data: 0.0024 (0.0044) Loss: 0.1190 (0.0433)
[2023/01/16 03:59] | TRAIN(072): [ 550/2211] Batch: 0.1281 (0.1259) Data: 0.0023 (0.0042) Loss: 0.0103 (0.0420)
[2023/01/16 03:59] | TRAIN(072): [ 600/2211] Batch: 0.0884 (0.1237) Data: 0.0019 (0.0040) Loss: 0.1430 (0.0430)
[2023/01/16 04:00] | TRAIN(072): [ 650/2211] Batch: 0.0900 (0.1214) Data: 0.0026 (0.0038) Loss: 0.0000 (0.0431)
[2023/01/16 04:00] | TRAIN(072): [ 700/2211] Batch: 0.0921 (0.1196) Data: 0.0017 (0.0037) Loss: 0.0000 (0.0424)
[2023/01/16 04:00] | TRAIN(072): [ 750/2211] Batch: 0.0876 (0.1177) Data: 0.0020 (0.0036) Loss: 0.0023 (0.0421)
[2023/01/16 04:00] | TRAIN(072): [ 800/2211] Batch: 0.0876 (0.1162) Data: 0.0018 (0.0035) Loss: 0.0022 (0.0418)
[2023/01/16 04:00] | TRAIN(072): [ 850/2211] Batch: 0.0894 (0.1148) Data: 0.0018 (0.0034) Loss: 0.0316 (0.0416)
[2023/01/16 04:00] | TRAIN(072): [ 900/2211] Batch: 0.0887 (0.1136) Data: 0.0015 (0.0033) Loss: 0.1257 (0.0409)
[2023/01/16 04:00] | TRAIN(072): [ 950/2211] Batch: 0.0997 (0.1131) Data: 0.0024 (0.0032) Loss: 0.0185 (0.0415)
[2023/01/16 04:00] | TRAIN(072): [1000/2211] Batch: 0.0894 (0.1125) Data: 0.0017 (0.0031) Loss: 0.0053 (0.0417)
[2023/01/16 04:00] | TRAIN(072): [1050/2211] Batch: 0.0964 (0.1121) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0413)
[2023/01/16 04:00] | TRAIN(072): [1100/2211] Batch: 0.1179 (0.1113) Data: 0.0022 (0.0030) Loss: 0.0000 (0.0414)
[2023/01/16 04:00] | TRAIN(072): [1150/2211] Batch: 0.0997 (0.1105) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0413)
[2023/01/16 04:00] | TRAIN(072): [1200/2211] Batch: 0.0889 (0.1098) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0410)
[2023/01/16 04:00] | TRAIN(072): [1250/2211] Batch: 0.0894 (0.1091) Data: 0.0018 (0.0029) Loss: 0.1741 (0.0415)
[2023/01/16 04:01] | TRAIN(072): [1300/2211] Batch: 0.0883 (0.1084) Data: 0.0017 (0.0028) Loss: 0.0052 (0.0420)
[2023/01/16 04:01] | TRAIN(072): [1350/2211] Batch: 0.0956 (0.1078) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0422)
[2023/01/16 04:01] | TRAIN(072): [1400/2211] Batch: 0.0989 (0.1076) Data: 0.0018 (0.0028) Loss: 0.0014 (0.0419)
[2023/01/16 04:01] | TRAIN(072): [1450/2211] Batch: 0.0920 (0.1072) Data: 0.0019 (0.0027) Loss: 0.0066 (0.0422)
[2023/01/16 04:01] | TRAIN(072): [1500/2211] Batch: 0.0920 (0.1071) Data: 0.0018 (0.0027) Loss: 0.0257 (0.0420)
[2023/01/16 04:01] | TRAIN(072): [1550/2211] Batch: 0.0889 (0.1067) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0422)
[2023/01/16 04:01] | TRAIN(072): [1600/2211] Batch: 0.0983 (0.1062) Data: 0.0018 (0.0027) Loss: 0.1076 (0.0424)
[2023/01/16 04:01] | TRAIN(072): [1650/2211] Batch: 0.0890 (0.1057) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0421)
[2023/01/16 04:01] | TRAIN(072): [1700/2211] Batch: 0.0965 (0.1053) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0421)
[2023/01/16 04:01] | TRAIN(072): [1750/2211] Batch: 0.0932 (0.1049) Data: 0.0018 (0.0026) Loss: 0.0400 (0.0421)
[2023/01/16 04:01] | TRAIN(072): [1800/2211] Batch: 0.0915 (0.1047) Data: 0.0014 (0.0026) Loss: 0.0017 (0.0418)
[2023/01/16 04:01] | TRAIN(072): [1850/2211] Batch: 0.0980 (0.1045) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0414)
[2023/01/16 04:02] | TRAIN(072): [1900/2211] Batch: 0.0965 (0.1043) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0417)
[2023/01/16 04:02] | TRAIN(072): [1950/2211] Batch: 0.0954 (0.1040) Data: 0.0018 (0.0025) Loss: 0.0115 (0.0418)
[2023/01/16 04:02] | TRAIN(072): [2000/2211] Batch: 0.0892 (0.1038) Data: 0.0015 (0.0025) Loss: 0.0000 (0.0420)
[2023/01/16 04:02] | TRAIN(072): [2050/2211] Batch: 0.0898 (0.1036) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0424)
[2023/01/16 04:02] | TRAIN(072): [2100/2211] Batch: 0.0895 (0.1034) Data: 0.0018 (0.0024) Loss: 0.0002 (0.0422)
[2023/01/16 04:02] | TRAIN(072): [2150/2211] Batch: 0.0974 (0.1032) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0419)
[2023/01/16 04:02] | TRAIN(072): [2200/2211] Batch: 0.0881 (0.1031) Data: 0.0012 (0.0024) Loss: 0.0000 (0.0416)
[2023/01/16 04:02] | ------------------------------------------------------------
[2023/01/16 04:02] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:02] | ------------------------------------------------------------
[2023/01/16 04:02] |    TRAIN(72)     0:03:47     0:00:05     0:03:42      0.0416
[2023/01/16 04:02] | ------------------------------------------------------------
[2023/01/16 04:02] | **************************************************
[2023/01/16 04:02] | TRAIN(073): [  50/2211] Batch: 0.0984 (0.1240) Data: 0.0018 (0.0237) Loss: 0.0228 (0.0422)
[2023/01/16 04:02] | TRAIN(073): [ 100/2211] Batch: 0.0986 (0.1093) Data: 0.0017 (0.0129) Loss: 0.0000 (0.0461)
[2023/01/16 04:02] | TRAIN(073): [ 150/2211] Batch: 0.1003 (0.1040) Data: 0.0018 (0.0092) Loss: 0.0000 (0.0449)
[2023/01/16 04:02] | TRAIN(073): [ 200/2211] Batch: 0.0927 (0.1031) Data: 0.0017 (0.0074) Loss: 0.0000 (0.0407)
[2023/01/16 04:02] | TRAIN(073): [ 250/2211] Batch: 0.0895 (0.1010) Data: 0.0018 (0.0063) Loss: 0.0842 (0.0401)
[2023/01/16 04:03] | TRAIN(073): [ 300/2211] Batch: 0.0892 (0.0993) Data: 0.0018 (0.0055) Loss: 0.0678 (0.0398)
[2023/01/16 04:03] | TRAIN(073): [ 350/2211] Batch: 0.0897 (0.0981) Data: 0.0023 (0.0050) Loss: 0.0924 (0.0396)
[2023/01/16 04:03] | TRAIN(073): [ 400/2211] Batch: 0.0884 (0.0971) Data: 0.0017 (0.0046) Loss: 0.0176 (0.0402)
[2023/01/16 04:03] | TRAIN(073): [ 450/2211] Batch: 0.0903 (0.0964) Data: 0.0017 (0.0043) Loss: 0.1637 (0.0396)
[2023/01/16 04:03] | TRAIN(073): [ 500/2211] Batch: 0.0989 (0.0963) Data: 0.0017 (0.0040) Loss: 0.0000 (0.0390)
[2023/01/16 04:03] | TRAIN(073): [ 550/2211] Batch: 0.0889 (0.0960) Data: 0.0019 (0.0038) Loss: 0.0996 (0.0389)
[2023/01/16 04:03] | TRAIN(073): [ 600/2211] Batch: 0.0880 (0.0955) Data: 0.0017 (0.0037) Loss: 0.0455 (0.0398)
[2023/01/16 04:03] | TRAIN(073): [ 650/2211] Batch: 0.0911 (0.0954) Data: 0.0017 (0.0035) Loss: 0.0049 (0.0402)
[2023/01/16 04:03] | TRAIN(073): [ 700/2211] Batch: 0.0924 (0.0953) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0399)
[2023/01/16 04:03] | TRAIN(073): [ 750/2211] Batch: 0.0840 (0.0952) Data: 0.0018 (0.0033) Loss: 0.1322 (0.0401)
[2023/01/16 04:03] | TRAIN(073): [ 800/2211] Batch: 0.0887 (0.0953) Data: 0.0017 (0.0032) Loss: 0.0019 (0.0400)
[2023/01/16 04:03] | TRAIN(073): [ 850/2211] Batch: 0.0899 (0.0952) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0395)
[2023/01/16 04:03] | TRAIN(073): [ 900/2211] Batch: 0.1185 (0.0951) Data: 0.0023 (0.0030) Loss: 0.1028 (0.0396)
[2023/01/16 04:04] | TRAIN(073): [ 950/2211] Batch: 0.0984 (0.0954) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0391)
[2023/01/16 04:04] | TRAIN(073): [1000/2211] Batch: 0.1053 (0.0953) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0393)
[2023/01/16 04:04] | TRAIN(073): [1050/2211] Batch: 0.0878 (0.0965) Data: 0.0017 (0.0029) Loss: 0.1441 (0.0400)
[2023/01/16 04:04] | TRAIN(073): [1100/2211] Batch: 0.0839 (0.0963) Data: 0.0017 (0.0028) Loss: 0.0780 (0.0402)
[2023/01/16 04:04] | TRAIN(073): [1150/2211] Batch: 0.1286 (0.0965) Data: 0.0020 (0.0028) Loss: 0.0048 (0.0398)
[2023/01/16 04:04] | TRAIN(073): [1200/2211] Batch: 0.0929 (0.0967) Data: 0.0017 (0.0028) Loss: 0.1982 (0.0395)
[2023/01/16 04:04] | TRAIN(073): [1250/2211] Batch: 0.0912 (0.0966) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0392)
[2023/01/16 04:04] | TRAIN(073): [1300/2211] Batch: 0.1001 (0.0966) Data: 0.0019 (0.0027) Loss: 0.1452 (0.0391)
[2023/01/16 04:04] | TRAIN(073): [1350/2211] Batch: 0.0911 (0.0969) Data: 0.0019 (0.0027) Loss: 0.0053 (0.0390)
[2023/01/16 04:04] | TRAIN(073): [1400/2211] Batch: 0.0943 (0.0966) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0390)
[2023/01/16 04:04] | TRAIN(073): [1450/2211] Batch: 0.1011 (0.0966) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0392)
[2023/01/16 04:04] | TRAIN(073): [1500/2211] Batch: 0.0889 (0.0965) Data: 0.0018 (0.0026) Loss: 0.0135 (0.0390)
[2023/01/16 04:05] | TRAIN(073): [1550/2211] Batch: 0.0897 (0.0964) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0388)
[2023/01/16 04:05] | TRAIN(073): [1600/2211] Batch: 0.0908 (0.0963) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0385)
[2023/01/16 04:05] | TRAIN(073): [1650/2211] Batch: 0.1184 (0.0963) Data: 0.0021 (0.0025) Loss: 0.0598 (0.0389)
[2023/01/16 04:05] | TRAIN(073): [1700/2211] Batch: 0.0893 (0.0962) Data: 0.0018 (0.0025) Loss: 0.0926 (0.0389)
[2023/01/16 04:05] | TRAIN(073): [1750/2211] Batch: 0.0963 (0.0961) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0385)
[2023/01/16 04:05] | TRAIN(073): [1800/2211] Batch: 0.0964 (0.0959) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0385)
[2023/01/16 04:05] | TRAIN(073): [1850/2211] Batch: 0.0880 (0.0958) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0382)
[2023/01/16 04:05] | TRAIN(073): [1900/2211] Batch: 0.0900 (0.0956) Data: 0.0004 (0.0024) Loss: 0.0721 (0.0386)
[2023/01/16 04:05] | TRAIN(073): [1950/2211] Batch: 0.0884 (0.0955) Data: 0.0018 (0.0024) Loss: 0.0001 (0.0388)
[2023/01/16 04:05] | TRAIN(073): [2000/2211] Batch: 0.1171 (0.0954) Data: 0.0021 (0.0024) Loss: 0.1812 (0.0387)
[2023/01/16 04:05] | TRAIN(073): [2050/2211] Batch: 0.0875 (0.0953) Data: 0.0017 (0.0024) Loss: 0.0049 (0.0386)
[2023/01/16 04:05] | TRAIN(073): [2100/2211] Batch: 0.0990 (0.0952) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0389)
[2023/01/16 04:05] | TRAIN(073): [2150/2211] Batch: 0.0886 (0.0953) Data: 0.0019 (0.0023) Loss: 0.1027 (0.0387)
[2023/01/16 04:06] | TRAIN(073): [2200/2211] Batch: 0.0864 (0.0952) Data: 0.0016 (0.0023) Loss: 0.1232 (0.0386)
[2023/01/16 04:06] | ------------------------------------------------------------
[2023/01/16 04:06] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:06] | ------------------------------------------------------------
[2023/01/16 04:06] |    TRAIN(73)     0:03:30     0:00:05     0:03:25      0.0387
[2023/01/16 04:06] | ------------------------------------------------------------
[2023/01/16 04:06] | **************************************************
[2023/01/16 04:06] | TRAIN(074): [  50/2211] Batch: 0.0945 (0.1179) Data: 0.0019 (0.0242) Loss: 0.0000 (0.0346)
[2023/01/16 04:06] | TRAIN(074): [ 100/2211] Batch: 0.0941 (0.1052) Data: 0.0018 (0.0131) Loss: 0.0000 (0.0430)
[2023/01/16 04:06] | TRAIN(074): [ 150/2211] Batch: 0.0880 (0.1022) Data: 0.0018 (0.0093) Loss: 0.0261 (0.0414)
[2023/01/16 04:06] | TRAIN(074): [ 200/2211] Batch: 0.0952 (0.1043) Data: 0.0020 (0.0075) Loss: 0.0000 (0.0421)
[2023/01/16 04:06] | TRAIN(074): [ 250/2211] Batch: 0.0919 (0.1025) Data: 0.0015 (0.0064) Loss: 0.0687 (0.0417)
[2023/01/16 04:06] | TRAIN(074): [ 300/2211] Batch: 0.0943 (0.1015) Data: 0.0018 (0.0056) Loss: 0.0000 (0.0409)
[2023/01/16 04:06] | TRAIN(074): [ 350/2211] Batch: 0.0881 (0.1007) Data: 0.0016 (0.0051) Loss: 0.0000 (0.0394)
[2023/01/16 04:06] | TRAIN(074): [ 400/2211] Batch: 0.0927 (0.0997) Data: 0.0019 (0.0047) Loss: 0.0019 (0.0395)
[2023/01/16 04:06] | TRAIN(074): [ 450/2211] Batch: 0.0937 (0.0990) Data: 0.0018 (0.0043) Loss: 0.0019 (0.0393)
[2023/01/16 04:06] | TRAIN(074): [ 500/2211] Batch: 0.0915 (0.0985) Data: 0.0018 (0.0041) Loss: 0.0000 (0.0395)
[2023/01/16 04:06] | TRAIN(074): [ 550/2211] Batch: 0.0957 (0.0977) Data: 0.0018 (0.0039) Loss: 0.0240 (0.0396)
[2023/01/16 04:07] | TRAIN(074): [ 600/2211] Batch: 0.1171 (0.0978) Data: 0.0022 (0.0037) Loss: 0.0000 (0.0414)
[2023/01/16 04:07] | TRAIN(074): [ 650/2211] Batch: 0.0876 (0.0985) Data: 0.0017 (0.0036) Loss: 0.0690 (0.0409)
[2023/01/16 04:07] | TRAIN(074): [ 700/2211] Batch: 0.0916 (0.0981) Data: 0.0019 (0.0034) Loss: 0.1245 (0.0408)
[2023/01/16 04:07] | TRAIN(074): [ 750/2211] Batch: 0.0894 (0.0978) Data: 0.0017 (0.0033) Loss: 0.0249 (0.0399)
[2023/01/16 04:07] | TRAIN(074): [ 800/2211] Batch: 0.0979 (0.0974) Data: 0.0019 (0.0032) Loss: 0.1136 (0.0397)
[2023/01/16 04:07] | TRAIN(074): [ 850/2211] Batch: 0.0971 (0.0971) Data: 0.0018 (0.0032) Loss: 0.0052 (0.0390)
[2023/01/16 04:07] | TRAIN(074): [ 900/2211] Batch: 0.0872 (0.0968) Data: 0.0018 (0.0031) Loss: 0.0018 (0.0388)
[2023/01/16 04:07] | TRAIN(074): [ 950/2211] Batch: 0.0882 (0.0964) Data: 0.0017 (0.0030) Loss: 0.0566 (0.0392)
[2023/01/16 04:07] | TRAIN(074): [1000/2211] Batch: 0.0976 (0.0961) Data: 0.0023 (0.0030) Loss: 0.0000 (0.0386)
[2023/01/16 04:07] | TRAIN(074): [1050/2211] Batch: 0.0886 (0.0958) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0398)
[2023/01/16 04:07] | TRAIN(074): [1100/2211] Batch: 0.0882 (0.0956) Data: 0.0018 (0.0028) Loss: 0.0125 (0.0396)
[2023/01/16 04:07] | TRAIN(074): [1150/2211] Batch: 0.1189 (0.0960) Data: 0.0022 (0.0028) Loss: 0.0720 (0.0408)
[2023/01/16 04:07] | TRAIN(074): [1200/2211] Batch: 0.0888 (0.0961) Data: 0.0019 (0.0028) Loss: 0.1218 (0.0406)
[2023/01/16 04:08] | TRAIN(074): [1250/2211] Batch: 0.1238 (0.0964) Data: 0.0022 (0.0027) Loss: 0.0000 (0.0406)
[2023/01/16 04:08] | TRAIN(074): [1300/2211] Batch: 0.0872 (0.0967) Data: 0.0017 (0.0027) Loss: 0.0066 (0.0407)
[2023/01/16 04:08] | TRAIN(074): [1350/2211] Batch: 0.0941 (0.0965) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0409)
[2023/01/16 04:08] | TRAIN(074): [1400/2211] Batch: 0.0886 (0.0962) Data: 0.0019 (0.0026) Loss: 0.1058 (0.0409)
[2023/01/16 04:08] | TRAIN(074): [1450/2211] Batch: 0.0875 (0.0961) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0411)
[2023/01/16 04:08] | TRAIN(074): [1500/2211] Batch: 0.0900 (0.0960) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0409)
[2023/01/16 04:08] | TRAIN(074): [1550/2211] Batch: 0.0911 (0.0960) Data: 0.0017 (0.0026) Loss: 0.0242 (0.0411)
[2023/01/16 04:08] | TRAIN(074): [1600/2211] Batch: 0.0969 (0.0960) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0407)
[2023/01/16 04:08] | TRAIN(074): [1650/2211] Batch: 0.0912 (0.0959) Data: 0.0017 (0.0025) Loss: 0.1414 (0.0408)
[2023/01/16 04:08] | TRAIN(074): [1700/2211] Batch: 0.0875 (0.0957) Data: 0.0017 (0.0025) Loss: 0.1014 (0.0409)
[2023/01/16 04:08] | TRAIN(074): [1750/2211] Batch: 0.0923 (0.0957) Data: 0.0013 (0.0025) Loss: 0.0000 (0.0412)
[2023/01/16 04:08] | TRAIN(074): [1800/2211] Batch: 0.0969 (0.0956) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0414)
[2023/01/16 04:08] | TRAIN(074): [1850/2211] Batch: 0.0894 (0.0955) Data: 0.0017 (0.0024) Loss: 0.0073 (0.0412)
[2023/01/16 04:09] | TRAIN(074): [1900/2211] Batch: 0.0874 (0.0955) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0410)
[2023/01/16 04:09] | TRAIN(074): [1950/2211] Batch: 0.0958 (0.0954) Data: 0.0012 (0.0024) Loss: 0.0717 (0.0411)
[2023/01/16 04:09] | TRAIN(074): [2000/2211] Batch: 0.0889 (0.0957) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0411)
[2023/01/16 04:09] | TRAIN(074): [2050/2211] Batch: 0.0886 (0.0956) Data: 0.0016 (0.0024) Loss: 0.0472 (0.0410)
[2023/01/16 04:09] | TRAIN(074): [2100/2211] Batch: 0.0880 (0.0955) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0406)
[2023/01/16 04:09] | TRAIN(074): [2150/2211] Batch: 0.0966 (0.0956) Data: 0.0019 (0.0023) Loss: 0.1799 (0.0409)
[2023/01/16 04:09] | TRAIN(074): [2200/2211] Batch: 0.1182 (0.0957) Data: 0.0021 (0.0023) Loss: 0.1137 (0.0412)
[2023/01/16 04:09] | ------------------------------------------------------------
[2023/01/16 04:09] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:09] | ------------------------------------------------------------
[2023/01/16 04:09] |    TRAIN(74)     0:03:31     0:00:05     0:03:26      0.0413
[2023/01/16 04:09] | ------------------------------------------------------------
[2023/01/16 04:09] | **************************************************
[2023/01/16 04:09] | TRAIN(075): [  50/2211] Batch: 0.1165 (0.1143) Data: 0.0023 (0.0258) Loss: 0.1089 (0.0427)
[2023/01/16 04:09] | TRAIN(075): [ 100/2211] Batch: 0.0920 (0.1099) Data: 0.0019 (0.0140) Loss: 0.0000 (0.0405)
[2023/01/16 04:09] | TRAIN(075): [ 150/2211] Batch: 0.0923 (0.1047) Data: 0.0021 (0.0100) Loss: 0.0421 (0.0369)
[2023/01/16 04:09] | TRAIN(075): [ 200/2211] Batch: 0.0878 (0.1013) Data: 0.0015 (0.0079) Loss: 0.0000 (0.0396)
[2023/01/16 04:09] | TRAIN(075): [ 250/2211] Batch: 0.0871 (0.0991) Data: 0.0018 (0.0067) Loss: 0.3066 (0.0410)
[2023/01/16 04:10] | TRAIN(075): [ 300/2211] Batch: 0.0890 (0.0983) Data: 0.0018 (0.0059) Loss: 0.0885 (0.0391)
[2023/01/16 04:10] | TRAIN(075): [ 350/2211] Batch: 0.0894 (0.0973) Data: 0.0019 (0.0053) Loss: 0.0265 (0.0382)
[2023/01/16 04:10] | TRAIN(075): [ 400/2211] Batch: 0.0866 (0.0974) Data: 0.0017 (0.0049) Loss: 0.0255 (0.0375)
[2023/01/16 04:10] | TRAIN(075): [ 450/2211] Batch: 0.0880 (0.0972) Data: 0.0018 (0.0046) Loss: 0.0000 (0.0390)
[2023/01/16 04:10] | TRAIN(075): [ 500/2211] Batch: 0.0963 (0.0974) Data: 0.0018 (0.0043) Loss: 0.0000 (0.0405)
[2023/01/16 04:10] | TRAIN(075): [ 550/2211] Batch: 0.0911 (0.0975) Data: 0.0018 (0.0041) Loss: 0.0825 (0.0404)
[2023/01/16 04:10] | TRAIN(075): [ 600/2211] Batch: 0.0883 (0.0970) Data: 0.0017 (0.0039) Loss: 0.0017 (0.0412)
[2023/01/16 04:10] | TRAIN(075): [ 650/2211] Batch: 0.0871 (0.0965) Data: 0.0015 (0.0037) Loss: 0.0298 (0.0418)
[2023/01/16 04:10] | TRAIN(075): [ 700/2211] Batch: 0.0920 (0.0972) Data: 0.0019 (0.0036) Loss: 0.0499 (0.0422)
[2023/01/16 04:10] | TRAIN(075): [ 750/2211] Batch: 0.0879 (0.0972) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0420)
[2023/01/16 04:10] | TRAIN(075): [ 800/2211] Batch: 0.0908 (0.0972) Data: 0.0018 (0.0034) Loss: 0.1936 (0.0417)
[2023/01/16 04:10] | TRAIN(075): [ 850/2211] Batch: 0.1189 (0.0977) Data: 0.0022 (0.0033) Loss: 0.0000 (0.0423)
[2023/01/16 04:11] | TRAIN(075): [ 900/2211] Batch: 0.0912 (0.0987) Data: 0.0018 (0.0032) Loss: 0.0091 (0.0414)
[2023/01/16 04:11] | TRAIN(075): [ 950/2211] Batch: 0.0899 (0.0984) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0413)
[2023/01/16 04:11] | TRAIN(075): [1000/2211] Batch: 0.0868 (0.0980) Data: 0.0018 (0.0031) Loss: 0.0062 (0.0408)
[2023/01/16 04:11] | TRAIN(075): [1050/2211] Batch: 0.0870 (0.0982) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0410)
[2023/01/16 04:11] | TRAIN(075): [1100/2211] Batch: 0.0956 (0.0978) Data: 0.0018 (0.0030) Loss: 0.0290 (0.0413)
[2023/01/16 04:11] | TRAIN(075): [1150/2211] Batch: 0.0860 (0.0974) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0407)
[2023/01/16 04:11] | TRAIN(075): [1200/2211] Batch: 0.0873 (0.0971) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0400)
[2023/01/16 04:11] | TRAIN(075): [1250/2211] Batch: 0.0874 (0.0969) Data: 0.0019 (0.0028) Loss: 0.1092 (0.0399)
[2023/01/16 04:11] | TRAIN(075): [1300/2211] Batch: 0.0981 (0.0968) Data: 0.0018 (0.0028) Loss: 0.1734 (0.0400)
[2023/01/16 04:11] | TRAIN(075): [1350/2211] Batch: 0.0866 (0.0966) Data: 0.0018 (0.0028) Loss: 0.0104 (0.0400)
[2023/01/16 04:11] | TRAIN(075): [1400/2211] Batch: 0.0929 (0.0965) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0399)
[2023/01/16 04:11] | TRAIN(075): [1450/2211] Batch: 0.0909 (0.0963) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0396)
[2023/01/16 04:11] | TRAIN(075): [1500/2211] Batch: 0.0872 (0.0965) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0397)
[2023/01/16 04:12] | TRAIN(075): [1550/2211] Batch: 0.0964 (0.0964) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0391)
[2023/01/16 04:12] | TRAIN(075): [1600/2211] Batch: 0.0904 (0.0964) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0390)
[2023/01/16 04:12] | TRAIN(075): [1650/2211] Batch: 0.0893 (0.0962) Data: 0.0022 (0.0026) Loss: 0.0132 (0.0391)
[2023/01/16 04:12] | TRAIN(075): [1700/2211] Batch: 0.0886 (0.0960) Data: 0.0018 (0.0026) Loss: 0.0019 (0.0400)
[2023/01/16 04:12] | TRAIN(075): [1750/2211] Batch: 0.0891 (0.0958) Data: 0.0018 (0.0026) Loss: 0.0084 (0.0403)
[2023/01/16 04:12] | TRAIN(075): [1800/2211] Batch: 0.1014 (0.0959) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0402)
[2023/01/16 04:12] | TRAIN(075): [1850/2211] Batch: 0.0880 (0.0958) Data: 0.0017 (0.0025) Loss: 0.0711 (0.0402)
[2023/01/16 04:12] | TRAIN(075): [1900/2211] Batch: 0.0880 (0.0959) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0405)
[2023/01/16 04:12] | TRAIN(075): [1950/2211] Batch: 0.1173 (0.0962) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0407)
[2023/01/16 04:12] | TRAIN(075): [2000/2211] Batch: 0.0875 (0.0962) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0404)
[2023/01/16 04:12] | TRAIN(075): [2050/2211] Batch: 0.1175 (0.0961) Data: 0.0022 (0.0025) Loss: 0.0164 (0.0405)
[2023/01/16 04:12] | TRAIN(075): [2100/2211] Batch: 0.0899 (0.0962) Data: 0.0018 (0.0024) Loss: 0.0314 (0.0404)
[2023/01/16 04:13] | TRAIN(075): [2150/2211] Batch: 0.0889 (0.0961) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0402)
[2023/01/16 04:13] | TRAIN(075): [2200/2211] Batch: 0.0872 (0.0960) Data: 0.0016 (0.0024) Loss: 0.0023 (0.0406)
[2023/01/16 04:13] | ------------------------------------------------------------
[2023/01/16 04:13] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:13] | ------------------------------------------------------------
[2023/01/16 04:13] |    TRAIN(75)     0:03:32     0:00:05     0:03:26      0.0406
[2023/01/16 04:13] | ------------------------------------------------------------
[2023/01/16 04:13] | **************************************************
[2023/01/16 04:13] | TRAIN(076): [  50/2211] Batch: 0.1038 (0.1326) Data: 0.0014 (0.0260) Loss: 0.2016 (0.0457)
[2023/01/16 04:13] | TRAIN(076): [ 100/2211] Batch: 0.1022 (0.1144) Data: 0.0021 (0.0139) Loss: 0.0001 (0.0408)
[2023/01/16 04:13] | TRAIN(076): [ 150/2211] Batch: 0.0918 (0.1098) Data: 0.0017 (0.0099) Loss: 0.1159 (0.0379)
[2023/01/16 04:13] | TRAIN(076): [ 200/2211] Batch: 0.1085 (0.1061) Data: 0.0020 (0.0079) Loss: 0.0018 (0.0376)
[2023/01/16 04:13] | TRAIN(076): [ 250/2211] Batch: 0.0875 (0.1030) Data: 0.0018 (0.0067) Loss: 0.0000 (0.0397)
[2023/01/16 04:13] | TRAIN(076): [ 300/2211] Batch: 0.0876 (0.1009) Data: 0.0017 (0.0059) Loss: 0.0241 (0.0414)
[2023/01/16 04:13] | TRAIN(076): [ 350/2211] Batch: 0.1017 (0.1007) Data: 0.0018 (0.0053) Loss: 0.0125 (0.0398)
[2023/01/16 04:13] | TRAIN(076): [ 400/2211] Batch: 0.1188 (0.1006) Data: 0.0022 (0.0049) Loss: 0.1363 (0.0398)
[2023/01/16 04:13] | TRAIN(076): [ 450/2211] Batch: 0.0872 (0.1007) Data: 0.0018 (0.0046) Loss: 0.0000 (0.0393)
[2023/01/16 04:13] | TRAIN(076): [ 500/2211] Batch: 0.0926 (0.1006) Data: 0.0018 (0.0043) Loss: 0.1803 (0.0415)
[2023/01/16 04:14] | TRAIN(076): [ 550/2211] Batch: 0.0949 (0.1004) Data: 0.0019 (0.0041) Loss: 0.0000 (0.0414)
[2023/01/16 04:14] | TRAIN(076): [ 600/2211] Batch: 0.0949 (0.1000) Data: 0.0019 (0.0039) Loss: 0.0000 (0.0414)
[2023/01/16 04:14] | TRAIN(076): [ 650/2211] Batch: 0.0884 (0.0994) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0407)
[2023/01/16 04:14] | TRAIN(076): [ 700/2211] Batch: 0.1405 (0.0995) Data: 0.0022 (0.0036) Loss: 0.0000 (0.0403)
[2023/01/16 04:14] | TRAIN(076): [ 750/2211] Batch: 0.0903 (0.0998) Data: 0.0019 (0.0035) Loss: 0.0000 (0.0394)
[2023/01/16 04:14] | TRAIN(076): [ 800/2211] Batch: 0.0884 (0.0996) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0401)
[2023/01/16 04:14] | TRAIN(076): [ 850/2211] Batch: 0.0977 (0.0993) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0403)
[2023/01/16 04:14] | TRAIN(076): [ 900/2211] Batch: 0.1069 (0.0991) Data: 0.0030 (0.0032) Loss: 0.0642 (0.0407)
[2023/01/16 04:14] | TRAIN(076): [ 950/2211] Batch: 0.0948 (0.0989) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0400)
[2023/01/16 04:14] | TRAIN(076): [1000/2211] Batch: 0.0883 (0.0986) Data: 0.0018 (0.0031) Loss: 0.0002 (0.0405)
[2023/01/16 04:14] | TRAIN(076): [1050/2211] Batch: 0.0978 (0.0983) Data: 0.0017 (0.0030) Loss: 0.0318 (0.0403)
[2023/01/16 04:14] | TRAIN(076): [1100/2211] Batch: 0.0882 (0.0981) Data: 0.0018 (0.0030) Loss: 0.1094 (0.0408)
[2023/01/16 04:14] | TRAIN(076): [1150/2211] Batch: 0.0883 (0.0978) Data: 0.0018 (0.0029) Loss: 0.0731 (0.0409)
[2023/01/16 04:15] | TRAIN(076): [1200/2211] Batch: 0.0921 (0.0977) Data: 0.0018 (0.0029) Loss: 0.0045 (0.0408)
[2023/01/16 04:15] | TRAIN(076): [1250/2211] Batch: 0.0824 (0.0976) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0402)
[2023/01/16 04:15] | TRAIN(076): [1300/2211] Batch: 0.0886 (0.0972) Data: 0.0018 (0.0028) Loss: 0.0450 (0.0401)
[2023/01/16 04:15] | TRAIN(076): [1350/2211] Batch: 0.0851 (0.0970) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0398)
[2023/01/16 04:15] | TRAIN(076): [1400/2211] Batch: 0.0909 (0.0967) Data: 0.0017 (0.0027) Loss: 0.0523 (0.0396)
[2023/01/16 04:15] | TRAIN(076): [1450/2211] Batch: 0.0987 (0.0966) Data: 0.0018 (0.0027) Loss: 0.1106 (0.0393)
[2023/01/16 04:15] | TRAIN(076): [1500/2211] Batch: 0.0979 (0.0964) Data: 0.0018 (0.0026) Loss: 0.0188 (0.0394)
[2023/01/16 04:15] | TRAIN(076): [1550/2211] Batch: 0.0875 (0.0964) Data: 0.0018 (0.0026) Loss: 0.0863 (0.0394)
[2023/01/16 04:15] | TRAIN(076): [1600/2211] Batch: 0.0844 (0.0962) Data: 0.0018 (0.0026) Loss: 0.1179 (0.0396)
[2023/01/16 04:15] | TRAIN(076): [1650/2211] Batch: 0.0872 (0.0961) Data: 0.0018 (0.0026) Loss: 0.2003 (0.0395)
[2023/01/16 04:15] | TRAIN(076): [1700/2211] Batch: 0.0879 (0.0959) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0390)
[2023/01/16 04:15] | TRAIN(076): [1750/2211] Batch: 0.0880 (0.0959) Data: 0.0019 (0.0025) Loss: 0.1703 (0.0394)
[2023/01/16 04:15] | TRAIN(076): [1800/2211] Batch: 0.0980 (0.0960) Data: 0.0018 (0.0025) Loss: 0.0040 (0.0393)
[2023/01/16 04:16] | TRAIN(076): [1850/2211] Batch: 0.0896 (0.0961) Data: 0.0018 (0.0025) Loss: 0.0607 (0.0395)
[2023/01/16 04:16] | TRAIN(076): [1900/2211] Batch: 0.0941 (0.0961) Data: 0.0016 (0.0025) Loss: 0.0678 (0.0393)
[2023/01/16 04:16] | TRAIN(076): [1950/2211] Batch: 0.0882 (0.0960) Data: 0.0019 (0.0025) Loss: 0.0016 (0.0393)
[2023/01/16 04:16] | TRAIN(076): [2000/2211] Batch: 0.0952 (0.0959) Data: 0.0016 (0.0024) Loss: 0.0721 (0.0393)
[2023/01/16 04:16] | TRAIN(076): [2050/2211] Batch: 0.0888 (0.0959) Data: 0.0017 (0.0024) Loss: 0.0037 (0.0391)
[2023/01/16 04:16] | TRAIN(076): [2100/2211] Batch: 0.0874 (0.0958) Data: 0.0018 (0.0024) Loss: 0.0971 (0.0392)
[2023/01/16 04:16] | TRAIN(076): [2150/2211] Batch: 0.0897 (0.0957) Data: 0.0018 (0.0024) Loss: 0.0707 (0.0394)
[2023/01/16 04:16] | TRAIN(076): [2200/2211] Batch: 0.0862 (0.0956) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0395)
[2023/01/16 04:16] | ------------------------------------------------------------
[2023/01/16 04:16] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:16] | ------------------------------------------------------------
[2023/01/16 04:16] |    TRAIN(76)     0:03:31     0:00:05     0:03:26      0.0395
[2023/01/16 04:16] | ------------------------------------------------------------
[2023/01/16 04:16] | **************************************************
[2023/01/16 04:16] | TRAIN(077): [  50/2211] Batch: 0.1184 (0.1386) Data: 0.0022 (0.0254) Loss: 0.3107 (0.0473)
[2023/01/16 04:16] | TRAIN(077): [ 100/2211] Batch: 0.1181 (0.1295) Data: 0.0022 (0.0139) Loss: 0.0000 (0.0486)
[2023/01/16 04:16] | TRAIN(077): [ 150/2211] Batch: 0.1010 (0.1204) Data: 0.0018 (0.0099) Loss: 0.0257 (0.0456)
[2023/01/16 04:17] | TRAIN(077): [ 200/2211] Batch: 0.0925 (0.1137) Data: 0.0018 (0.0079) Loss: 0.0000 (0.0431)
[2023/01/16 04:17] | TRAIN(077): [ 250/2211] Batch: 0.0833 (0.1100) Data: 0.0017 (0.0067) Loss: 0.1219 (0.0439)
[2023/01/16 04:17] | TRAIN(077): [ 300/2211] Batch: 0.1176 (0.1065) Data: 0.0022 (0.0059) Loss: 0.0000 (0.0417)
[2023/01/16 04:17] | TRAIN(077): [ 350/2211] Batch: 0.1193 (0.1066) Data: 0.0022 (0.0054) Loss: 0.0031 (0.0413)
[2023/01/16 04:17] | TRAIN(077): [ 400/2211] Batch: 0.0887 (0.1071) Data: 0.0018 (0.0050) Loss: 0.0000 (0.0420)
[2023/01/16 04:17] | TRAIN(077): [ 450/2211] Batch: 0.0889 (0.1056) Data: 0.0018 (0.0046) Loss: 0.1867 (0.0394)
[2023/01/16 04:17] | TRAIN(077): [ 500/2211] Batch: 0.0890 (0.1044) Data: 0.0017 (0.0043) Loss: 0.0165 (0.0388)
[2023/01/16 04:17] | TRAIN(077): [ 550/2211] Batch: 0.0888 (0.1035) Data: 0.0018 (0.0041) Loss: 0.0055 (0.0390)
[2023/01/16 04:17] | TRAIN(077): [ 600/2211] Batch: 0.1185 (0.1032) Data: 0.0023 (0.0039) Loss: 0.0000 (0.0391)
[2023/01/16 04:17] | TRAIN(077): [ 650/2211] Batch: 0.0879 (0.1029) Data: 0.0018 (0.0038) Loss: 0.1478 (0.0394)
[2023/01/16 04:17] | TRAIN(077): [ 700/2211] Batch: 0.0896 (0.1023) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0384)
[2023/01/16 04:17] | TRAIN(077): [ 750/2211] Batch: 0.0942 (0.1016) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0378)
[2023/01/16 04:17] | TRAIN(077): [ 800/2211] Batch: 0.0955 (0.1013) Data: 0.0018 (0.0034) Loss: 0.0542 (0.0373)
[2023/01/16 04:18] | TRAIN(077): [ 850/2211] Batch: 0.0913 (0.1007) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0368)
[2023/01/16 04:18] | TRAIN(077): [ 900/2211] Batch: 0.0908 (0.1004) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0372)
[2023/01/16 04:18] | TRAIN(077): [ 950/2211] Batch: 0.0915 (0.1002) Data: 0.0017 (0.0031) Loss: 0.0199 (0.0372)
[2023/01/16 04:18] | TRAIN(077): [1000/2211] Batch: 0.0904 (0.0999) Data: 0.0018 (0.0031) Loss: 0.1279 (0.0377)
[2023/01/16 04:18] | TRAIN(077): [1050/2211] Batch: 0.1005 (0.0995) Data: 0.0018 (0.0030) Loss: 0.0927 (0.0382)
[2023/01/16 04:18] | TRAIN(077): [1100/2211] Batch: 0.1076 (0.0992) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0382)
[2023/01/16 04:18] | TRAIN(077): [1150/2211] Batch: 0.0890 (0.0992) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0383)
[2023/01/16 04:18] | TRAIN(077): [1200/2211] Batch: 0.0909 (0.0989) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0387)
[2023/01/16 04:18] | TRAIN(077): [1250/2211] Batch: 0.0964 (0.0987) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0384)
[2023/01/16 04:18] | TRAIN(077): [1300/2211] Batch: 0.0902 (0.0985) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0386)
[2023/01/16 04:18] | TRAIN(077): [1350/2211] Batch: 0.0975 (0.0982) Data: 0.0017 (0.0028) Loss: 0.1392 (0.0389)
[2023/01/16 04:18] | TRAIN(077): [1400/2211] Batch: 0.0951 (0.0980) Data: 0.0020 (0.0027) Loss: 0.1121 (0.0389)
[2023/01/16 04:19] | TRAIN(077): [1450/2211] Batch: 0.1196 (0.0983) Data: 0.0022 (0.0027) Loss: 0.0332 (0.0389)
[2023/01/16 04:19] | TRAIN(077): [1500/2211] Batch: 0.0894 (0.0986) Data: 0.0017 (0.0027) Loss: 0.0469 (0.0387)
[2023/01/16 04:19] | TRAIN(077): [1550/2211] Batch: 0.0901 (0.0990) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0388)
[2023/01/16 04:19] | TRAIN(077): [1600/2211] Batch: 0.0888 (0.0991) Data: 0.0018 (0.0026) Loss: 0.0088 (0.0389)
[2023/01/16 04:19] | TRAIN(077): [1650/2211] Batch: 0.0877 (0.0989) Data: 0.0017 (0.0026) Loss: 0.0155 (0.0391)
[2023/01/16 04:19] | TRAIN(077): [1700/2211] Batch: 0.0990 (0.0987) Data: 0.0018 (0.0026) Loss: 0.0433 (0.0393)
[2023/01/16 04:19] | TRAIN(077): [1750/2211] Batch: 0.0885 (0.0984) Data: 0.0017 (0.0025) Loss: 0.1443 (0.0392)
[2023/01/16 04:19] | TRAIN(077): [1800/2211] Batch: 0.0876 (0.0983) Data: 0.0018 (0.0025) Loss: 0.0093 (0.0393)
[2023/01/16 04:19] | TRAIN(077): [1850/2211] Batch: 0.0989 (0.0981) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0390)
[2023/01/16 04:19] | TRAIN(077): [1900/2211] Batch: 0.0967 (0.0979) Data: 0.0017 (0.0025) Loss: 0.0031 (0.0389)
[2023/01/16 04:19] | TRAIN(077): [1950/2211] Batch: 0.0904 (0.0978) Data: 0.0015 (0.0025) Loss: 0.1240 (0.0391)
[2023/01/16 04:19] | TRAIN(077): [2000/2211] Batch: 0.0879 (0.0976) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0391)
[2023/01/16 04:19] | TRAIN(077): [2050/2211] Batch: 0.1013 (0.0975) Data: 0.0018 (0.0024) Loss: 0.0116 (0.0392)
[2023/01/16 04:20] | TRAIN(077): [2100/2211] Batch: 0.1031 (0.0974) Data: 0.0017 (0.0024) Loss: 0.0125 (0.0393)
[2023/01/16 04:20] | TRAIN(077): [2150/2211] Batch: 0.0921 (0.0973) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0392)
[2023/01/16 04:20] | TRAIN(077): [2200/2211] Batch: 0.0879 (0.0972) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0392)
[2023/01/16 04:20] | ------------------------------------------------------------
[2023/01/16 04:20] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:20] | ------------------------------------------------------------
[2023/01/16 04:20] |    TRAIN(77)     0:03:34     0:00:05     0:03:29      0.0392
[2023/01/16 04:20] | ------------------------------------------------------------
[2023/01/16 04:20] | **************************************************
[2023/01/16 04:20] | TRAIN(078): [  50/2211] Batch: 0.0949 (0.1154) Data: 0.0018 (0.0240) Loss: 0.0000 (0.0423)
[2023/01/16 04:20] | TRAIN(078): [ 100/2211] Batch: 0.0953 (0.1029) Data: 0.0018 (0.0130) Loss: 0.0000 (0.0344)
[2023/01/16 04:20] | TRAIN(078): [ 150/2211] Batch: 0.0933 (0.0993) Data: 0.0017 (0.0092) Loss: 0.0000 (0.0352)
[2023/01/16 04:20] | TRAIN(078): [ 200/2211] Batch: 0.0892 (0.0978) Data: 0.0018 (0.0074) Loss: 0.1049 (0.0369)
[2023/01/16 04:20] | TRAIN(078): [ 250/2211] Batch: 0.0881 (0.0982) Data: 0.0018 (0.0063) Loss: 0.0662 (0.0365)
[2023/01/16 04:20] | TRAIN(078): [ 300/2211] Batch: 0.0887 (0.0971) Data: 0.0018 (0.0056) Loss: 0.2369 (0.0369)
[2023/01/16 04:20] | TRAIN(078): [ 350/2211] Batch: 0.1025 (0.0961) Data: 0.0017 (0.0050) Loss: 0.0000 (0.0372)
[2023/01/16 04:20] | TRAIN(078): [ 400/2211] Batch: 0.0919 (0.0959) Data: 0.0018 (0.0046) Loss: 0.0802 (0.0374)
[2023/01/16 04:20] | TRAIN(078): [ 450/2211] Batch: 0.0937 (0.0954) Data: 0.0018 (0.0043) Loss: 0.0313 (0.0369)
[2023/01/16 04:21] | TRAIN(078): [ 500/2211] Batch: 0.0876 (0.0952) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0387)
[2023/01/16 04:21] | TRAIN(078): [ 550/2211] Batch: 0.0896 (0.0949) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0386)
[2023/01/16 04:21] | TRAIN(078): [ 600/2211] Batch: 0.0871 (0.0948) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0384)
[2023/01/16 04:21] | TRAIN(078): [ 650/2211] Batch: 0.0877 (0.0944) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0383)
[2023/01/16 04:21] | TRAIN(078): [ 700/2211] Batch: 0.0876 (0.0941) Data: 0.0017 (0.0034) Loss: 0.0382 (0.0377)
[2023/01/16 04:21] | TRAIN(078): [ 750/2211] Batch: 0.0962 (0.0939) Data: 0.0017 (0.0033) Loss: 0.0031 (0.0387)
[2023/01/16 04:21] | TRAIN(078): [ 800/2211] Batch: 0.0898 (0.0940) Data: 0.0018 (0.0032) Loss: 0.1524 (0.0392)
[2023/01/16 04:21] | TRAIN(078): [ 850/2211] Batch: 0.1196 (0.0944) Data: 0.0022 (0.0031) Loss: 0.0000 (0.0391)
[2023/01/16 04:21] | TRAIN(078): [ 900/2211] Batch: 0.0975 (0.0947) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0387)
[2023/01/16 04:21] | TRAIN(078): [ 950/2211] Batch: 0.0881 (0.0945) Data: 0.0017 (0.0030) Loss: 0.0386 (0.0394)
[2023/01/16 04:21] | TRAIN(078): [1000/2211] Batch: 0.0958 (0.0943) Data: 0.0018 (0.0029) Loss: 0.0091 (0.0394)
[2023/01/16 04:21] | TRAIN(078): [1050/2211] Batch: 0.0995 (0.0948) Data: 0.0018 (0.0029) Loss: 0.0127 (0.0393)
[2023/01/16 04:21] | TRAIN(078): [1100/2211] Batch: 0.0880 (0.0948) Data: 0.0018 (0.0028) Loss: 0.0027 (0.0390)
[2023/01/16 04:22] | TRAIN(078): [1150/2211] Batch: 0.0931 (0.0949) Data: 0.0018 (0.0028) Loss: 0.0926 (0.0391)
[2023/01/16 04:22] | TRAIN(078): [1200/2211] Batch: 0.0956 (0.0948) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0393)
[2023/01/16 04:22] | TRAIN(078): [1250/2211] Batch: 0.0834 (0.0946) Data: 0.0017 (0.0027) Loss: 0.0117 (0.0395)
[2023/01/16 04:22] | TRAIN(078): [1300/2211] Batch: 0.0949 (0.0945) Data: 0.0017 (0.0027) Loss: 0.0450 (0.0394)
[2023/01/16 04:22] | TRAIN(078): [1350/2211] Batch: 0.0878 (0.0943) Data: 0.0019 (0.0026) Loss: 0.0100 (0.0388)
[2023/01/16 04:22] | TRAIN(078): [1400/2211] Batch: 0.0981 (0.0942) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0387)
[2023/01/16 04:22] | TRAIN(078): [1450/2211] Batch: 0.0821 (0.0941) Data: 0.0017 (0.0026) Loss: 0.0215 (0.0392)
[2023/01/16 04:22] | TRAIN(078): [1500/2211] Batch: 0.0879 (0.0940) Data: 0.0017 (0.0025) Loss: 0.1261 (0.0390)
[2023/01/16 04:22] | TRAIN(078): [1550/2211] Batch: 0.0876 (0.0939) Data: 0.0017 (0.0025) Loss: 0.1245 (0.0391)
[2023/01/16 04:22] | TRAIN(078): [1600/2211] Batch: 0.0958 (0.0940) Data: 0.0018 (0.0025) Loss: 0.0045 (0.0395)
[2023/01/16 04:22] | TRAIN(078): [1650/2211] Batch: 0.0878 (0.0939) Data: 0.0016 (0.0025) Loss: 0.0191 (0.0389)
[2023/01/16 04:22] | TRAIN(078): [1700/2211] Batch: 0.0883 (0.0942) Data: 0.0017 (0.0025) Loss: 0.1302 (0.0393)
[2023/01/16 04:22] | TRAIN(078): [1750/2211] Batch: 0.0882 (0.0941) Data: 0.0017 (0.0024) Loss: 0.0033 (0.0391)
[2023/01/16 04:23] | TRAIN(078): [1800/2211] Batch: 0.0947 (0.0941) Data: 0.0018 (0.0024) Loss: 0.0269 (0.0394)
[2023/01/16 04:23] | TRAIN(078): [1850/2211] Batch: 0.0905 (0.0941) Data: 0.0017 (0.0024) Loss: 0.1749 (0.0396)
[2023/01/16 04:23] | TRAIN(078): [1900/2211] Batch: 0.0885 (0.0941) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0396)
[2023/01/16 04:23] | TRAIN(078): [1950/2211] Batch: 0.1012 (0.0941) Data: 0.0015 (0.0024) Loss: 0.0000 (0.0395)
[2023/01/16 04:23] | TRAIN(078): [2000/2211] Batch: 0.0883 (0.0941) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0396)
[2023/01/16 04:23] | TRAIN(078): [2050/2211] Batch: 0.0898 (0.0941) Data: 0.0017 (0.0023) Loss: 0.0211 (0.0397)
[2023/01/16 04:23] | TRAIN(078): [2100/2211] Batch: 0.0956 (0.0941) Data: 0.0017 (0.0023) Loss: 0.0040 (0.0395)
[2023/01/16 04:23] | TRAIN(078): [2150/2211] Batch: 0.0841 (0.0940) Data: 0.0018 (0.0023) Loss: 0.0652 (0.0396)
[2023/01/16 04:23] | TRAIN(078): [2200/2211] Batch: 0.0805 (0.0937) Data: 0.0016 (0.0023) Loss: 0.0000 (0.0404)
[2023/01/16 04:23] | ------------------------------------------------------------
[2023/01/16 04:23] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:23] | ------------------------------------------------------------
[2023/01/16 04:23] |    TRAIN(78)     0:03:27     0:00:05     0:03:21      0.0404
[2023/01/16 04:23] | ------------------------------------------------------------
[2023/01/16 04:23] | **************************************************
[2023/01/16 04:23] | TRAIN(079): [  50/2211] Batch: 0.0888 (0.1179) Data: 0.0018 (0.0254) Loss: 0.1320 (0.0468)
[2023/01/16 04:23] | TRAIN(079): [ 100/2211] Batch: 0.0873 (0.1062) Data: 0.0016 (0.0137) Loss: 0.0000 (0.0440)
[2023/01/16 04:23] | TRAIN(079): [ 150/2211] Batch: 0.1085 (0.1027) Data: 0.0018 (0.0098) Loss: 0.0000 (0.0452)
[2023/01/16 04:24] | TRAIN(079): [ 200/2211] Batch: 0.0921 (0.1012) Data: 0.0018 (0.0078) Loss: 0.1103 (0.0440)
[2023/01/16 04:24] | TRAIN(079): [ 250/2211] Batch: 0.0885 (0.1000) Data: 0.0018 (0.0066) Loss: 0.0000 (0.0432)
[2023/01/16 04:24] | TRAIN(079): [ 300/2211] Batch: 0.0881 (0.0986) Data: 0.0017 (0.0058) Loss: 0.0000 (0.0429)
[2023/01/16 04:24] | TRAIN(079): [ 350/2211] Batch: 0.0889 (0.0987) Data: 0.0018 (0.0052) Loss: 0.1367 (0.0437)
[2023/01/16 04:24] | TRAIN(079): [ 400/2211] Batch: 0.0931 (0.1013) Data: 0.0018 (0.0049) Loss: 0.0145 (0.0432)
[2023/01/16 04:24] | TRAIN(079): [ 450/2211] Batch: 0.0879 (0.1003) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0438)
[2023/01/16 04:24] | TRAIN(079): [ 500/2211] Batch: 0.0895 (0.0995) Data: 0.0018 (0.0043) Loss: 0.0081 (0.0436)
[2023/01/16 04:24] | TRAIN(079): [ 550/2211] Batch: 0.0890 (0.0987) Data: 0.0017 (0.0040) Loss: 0.0000 (0.0416)
[2023/01/16 04:24] | TRAIN(079): [ 600/2211] Batch: 0.0897 (0.0982) Data: 0.0018 (0.0039) Loss: 0.0304 (0.0408)
[2023/01/16 04:24] | TRAIN(079): [ 650/2211] Batch: 0.0924 (0.0982) Data: 0.0018 (0.0037) Loss: 0.0954 (0.0410)
[2023/01/16 04:24] | TRAIN(079): [ 700/2211] Batch: 0.0893 (0.0978) Data: 0.0019 (0.0036) Loss: 0.0000 (0.0415)
[2023/01/16 04:24] | TRAIN(079): [ 750/2211] Batch: 0.0901 (0.0974) Data: 0.0018 (0.0035) Loss: 0.0339 (0.0413)
[2023/01/16 04:24] | TRAIN(079): [ 800/2211] Batch: 0.0877 (0.0972) Data: 0.0020 (0.0034) Loss: 0.0121 (0.0420)
[2023/01/16 04:25] | TRAIN(079): [ 850/2211] Batch: 0.0888 (0.0969) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0408)
[2023/01/16 04:25] | TRAIN(079): [ 900/2211] Batch: 0.0895 (0.0965) Data: 0.0026 (0.0032) Loss: 0.0027 (0.0404)
[2023/01/16 04:25] | TRAIN(079): [ 950/2211] Batch: 0.0885 (0.0963) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0401)
[2023/01/16 04:25] | TRAIN(079): [1000/2211] Batch: 0.0882 (0.0959) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0404)
[2023/01/16 04:25] | TRAIN(079): [1050/2211] Batch: 0.0881 (0.0959) Data: 0.0018 (0.0030) Loss: 0.0147 (0.0402)
[2023/01/16 04:25] | TRAIN(079): [1100/2211] Batch: 0.0887 (0.0956) Data: 0.0017 (0.0029) Loss: 0.0237 (0.0400)
[2023/01/16 04:25] | TRAIN(079): [1150/2211] Batch: 0.0915 (0.0958) Data: 0.0019 (0.0029) Loss: 0.0002 (0.0396)
[2023/01/16 04:25] | TRAIN(079): [1200/2211] Batch: 0.0900 (0.0957) Data: 0.0018 (0.0028) Loss: 0.0108 (0.0396)
[2023/01/16 04:25] | TRAIN(079): [1250/2211] Batch: 0.0981 (0.0956) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0397)
[2023/01/16 04:25] | TRAIN(079): [1300/2211] Batch: 0.0977 (0.0958) Data: 0.0018 (0.0028) Loss: 0.0520 (0.0397)
[2023/01/16 04:25] | TRAIN(079): [1350/2211] Batch: 0.0884 (0.0957) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0400)
[2023/01/16 04:25] | TRAIN(079): [1400/2211] Batch: 0.0943 (0.0957) Data: 0.0018 (0.0027) Loss: 0.0577 (0.0399)
[2023/01/16 04:25] | TRAIN(079): [1450/2211] Batch: 0.0970 (0.0963) Data: 0.0017 (0.0027) Loss: 0.0022 (0.0400)
[2023/01/16 04:26] | TRAIN(079): [1500/2211] Batch: 0.0959 (0.0963) Data: 0.0018 (0.0027) Loss: 0.0880 (0.0400)
[2023/01/16 04:26] | TRAIN(079): [1550/2211] Batch: 0.1191 (0.0965) Data: 0.0021 (0.0026) Loss: 0.0000 (0.0404)
[2023/01/16 04:26] | TRAIN(079): [1600/2211] Batch: 0.0880 (0.0966) Data: 0.0018 (0.0026) Loss: 0.0019 (0.0399)
[2023/01/16 04:26] | TRAIN(079): [1650/2211] Batch: 0.1317 (0.0966) Data: 0.0024 (0.0026) Loss: 0.1620 (0.0401)
[2023/01/16 04:26] | TRAIN(079): [1700/2211] Batch: 0.1232 (0.0972) Data: 0.0022 (0.0026) Loss: 0.0147 (0.0396)
[2023/01/16 04:26] | TRAIN(079): [1750/2211] Batch: 0.0871 (0.0974) Data: 0.0016 (0.0026) Loss: 0.0000 (0.0396)
[2023/01/16 04:26] | TRAIN(079): [1800/2211] Batch: 0.0873 (0.0978) Data: 0.0018 (0.0025) Loss: 0.0627 (0.0397)
[2023/01/16 04:26] | TRAIN(079): [1850/2211] Batch: 0.0894 (0.0976) Data: 0.0019 (0.0025) Loss: 0.0081 (0.0393)
[2023/01/16 04:26] | TRAIN(079): [1900/2211] Batch: 0.0890 (0.0976) Data: 0.0018 (0.0025) Loss: 0.2058 (0.0395)
[2023/01/16 04:26] | TRAIN(079): [1950/2211] Batch: 0.0886 (0.0976) Data: 0.0018 (0.0025) Loss: 0.0595 (0.0394)
[2023/01/16 04:26] | TRAIN(079): [2000/2211] Batch: 0.0958 (0.0976) Data: 0.0018 (0.0025) Loss: 0.0319 (0.0389)
[2023/01/16 04:26] | TRAIN(079): [2050/2211] Batch: 0.0984 (0.0975) Data: 0.0019 (0.0025) Loss: 0.0368 (0.0390)
[2023/01/16 04:27] | TRAIN(079): [2100/2211] Batch: 0.0884 (0.0973) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0388)
[2023/01/16 04:27] | TRAIN(079): [2150/2211] Batch: 0.1037 (0.0972) Data: 0.0018 (0.0024) Loss: 0.0288 (0.0388)
[2023/01/16 04:27] | TRAIN(079): [2200/2211] Batch: 0.0870 (0.0971) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0387)
[2023/01/16 04:27] | ------------------------------------------------------------
[2023/01/16 04:27] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:27] | ------------------------------------------------------------
[2023/01/16 04:27] |    TRAIN(79)     0:03:34     0:00:05     0:03:29      0.0389
[2023/01/16 04:27] | ------------------------------------------------------------
[2023/01/16 04:27] | **************************************************
[2023/01/16 04:27] | TRAIN(080): [  50/2211] Batch: 0.1011 (0.1327) Data: 0.0014 (0.0248) Loss: 0.0077 (0.0270)
[2023/01/16 04:27] | TRAIN(080): [ 100/2211] Batch: 0.0967 (0.1155) Data: 0.0018 (0.0134) Loss: 0.0654 (0.0340)
[2023/01/16 04:27] | TRAIN(080): [ 150/2211] Batch: 0.0870 (0.1079) Data: 0.0014 (0.0095) Loss: 0.1277 (0.0341)
[2023/01/16 04:27] | TRAIN(080): [ 200/2211] Batch: 0.0884 (0.1038) Data: 0.0018 (0.0076) Loss: 0.1109 (0.0352)
[2023/01/16 04:27] | TRAIN(080): [ 250/2211] Batch: 0.0972 (0.1025) Data: 0.0018 (0.0065) Loss: 0.1297 (0.0374)
[2023/01/16 04:27] | TRAIN(080): [ 300/2211] Batch: 0.1243 (0.1022) Data: 0.0023 (0.0057) Loss: 0.0028 (0.0379)
[2023/01/16 04:27] | TRAIN(080): [ 350/2211] Batch: 0.1186 (0.1051) Data: 0.0023 (0.0052) Loss: 0.0275 (0.0370)
[2023/01/16 04:27] | TRAIN(080): [ 400/2211] Batch: 0.0948 (0.1038) Data: 0.0017 (0.0048) Loss: 0.0000 (0.0367)
[2023/01/16 04:28] | TRAIN(080): [ 450/2211] Batch: 0.0895 (0.1024) Data: 0.0018 (0.0045) Loss: 0.0366 (0.0357)
[2023/01/16 04:28] | TRAIN(080): [ 500/2211] Batch: 0.0878 (0.1012) Data: 0.0019 (0.0042) Loss: 0.0051 (0.0356)
[2023/01/16 04:28] | TRAIN(080): [ 550/2211] Batch: 0.0911 (0.1005) Data: 0.0018 (0.0040) Loss: 0.0590 (0.0352)
[2023/01/16 04:28] | TRAIN(080): [ 600/2211] Batch: 0.0912 (0.0997) Data: 0.0021 (0.0038) Loss: 0.0065 (0.0356)
[2023/01/16 04:28] | TRAIN(080): [ 650/2211] Batch: 0.0959 (0.0990) Data: 0.0021 (0.0037) Loss: 0.1400 (0.0358)
[2023/01/16 04:28] | TRAIN(080): [ 700/2211] Batch: 0.0868 (0.0984) Data: 0.0018 (0.0035) Loss: 0.0091 (0.0352)
[2023/01/16 04:28] | TRAIN(080): [ 750/2211] Batch: 0.0969 (0.0978) Data: 0.0019 (0.0034) Loss: 0.0038 (0.0351)
[2023/01/16 04:28] | TRAIN(080): [ 800/2211] Batch: 0.0824 (0.0972) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0362)
[2023/01/16 04:28] | TRAIN(080): [ 850/2211] Batch: 0.0830 (0.0964) Data: 0.0019 (0.0032) Loss: 0.0424 (0.0365)
[2023/01/16 04:28] | TRAIN(080): [ 900/2211] Batch: 0.0821 (0.0957) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0370)
[2023/01/16 04:28] | TRAIN(080): [ 950/2211] Batch: 0.0834 (0.0950) Data: 0.0018 (0.0031) Loss: 0.1669 (0.0375)
[2023/01/16 04:28] | TRAIN(080): [1000/2211] Batch: 0.0828 (0.0944) Data: 0.0018 (0.0030) Loss: 0.0176 (0.0369)
[2023/01/16 04:28] | TRAIN(080): [1050/2211] Batch: 0.0955 (0.0945) Data: 0.0018 (0.0030) Loss: 0.0287 (0.0364)
[2023/01/16 04:28] | TRAIN(080): [1100/2211] Batch: 0.0922 (0.0952) Data: 0.0019 (0.0029) Loss: 0.0636 (0.0369)
[2023/01/16 04:29] | TRAIN(080): [1150/2211] Batch: 0.0886 (0.0950) Data: 0.0019 (0.0029) Loss: 0.0074 (0.0364)
[2023/01/16 04:29] | TRAIN(080): [1200/2211] Batch: 0.0973 (0.0950) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0366)
[2023/01/16 04:29] | TRAIN(080): [1250/2211] Batch: 0.1071 (0.0953) Data: 0.0018 (0.0028) Loss: 0.0497 (0.0369)
[2023/01/16 04:29] | TRAIN(080): [1300/2211] Batch: 0.0944 (0.0954) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0381)
[2023/01/16 04:29] | TRAIN(080): [1350/2211] Batch: 0.0919 (0.0953) Data: 0.0018 (0.0027) Loss: 0.0606 (0.0387)
[2023/01/16 04:29] | TRAIN(080): [1400/2211] Batch: 0.0958 (0.0952) Data: 0.0013 (0.0027) Loss: 0.0000 (0.0387)
[2023/01/16 04:29] | TRAIN(080): [1450/2211] Batch: 0.1066 (0.0953) Data: 0.0018 (0.0026) Loss: 0.0267 (0.0388)
[2023/01/16 04:29] | TRAIN(080): [1500/2211] Batch: 0.1307 (0.0953) Data: 0.0021 (0.0026) Loss: 0.0910 (0.0386)
[2023/01/16 04:29] | TRAIN(080): [1550/2211] Batch: 0.1097 (0.0953) Data: 0.0018 (0.0026) Loss: 0.1299 (0.0387)
[2023/01/16 04:29] | TRAIN(080): [1600/2211] Batch: 0.0978 (0.0953) Data: 0.0016 (0.0025) Loss: 0.1692 (0.0388)
[2023/01/16 04:29] | TRAIN(080): [1650/2211] Batch: 0.0939 (0.0951) Data: 0.0018 (0.0025) Loss: 0.0377 (0.0388)
[2023/01/16 04:29] | TRAIN(080): [1700/2211] Batch: 0.0977 (0.0952) Data: 0.0018 (0.0025) Loss: 0.1855 (0.0389)
[2023/01/16 04:30] | TRAIN(080): [1750/2211] Batch: 0.0883 (0.0953) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0387)
[2023/01/16 04:30] | TRAIN(080): [1800/2211] Batch: 0.0877 (0.0952) Data: 0.0015 (0.0024) Loss: 0.0000 (0.0389)
[2023/01/16 04:30] | TRAIN(080): [1850/2211] Batch: 0.0887 (0.0951) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0391)
[2023/01/16 04:30] | TRAIN(080): [1900/2211] Batch: 0.1032 (0.0950) Data: 0.0018 (0.0024) Loss: 0.0454 (0.0391)
[2023/01/16 04:30] | TRAIN(080): [1950/2211] Batch: 0.1000 (0.0951) Data: 0.0017 (0.0024) Loss: 0.1105 (0.0388)
[2023/01/16 04:30] | TRAIN(080): [2000/2211] Batch: 0.0874 (0.0950) Data: 0.0017 (0.0024) Loss: 0.0324 (0.0388)
[2023/01/16 04:30] | TRAIN(080): [2050/2211] Batch: 0.0923 (0.0950) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0390)
[2023/01/16 04:30] | TRAIN(080): [2100/2211] Batch: 0.0892 (0.0950) Data: 0.0017 (0.0023) Loss: 0.0800 (0.0388)
[2023/01/16 04:30] | TRAIN(080): [2150/2211] Batch: 0.0978 (0.0949) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0390)
[2023/01/16 04:30] | TRAIN(080): [2200/2211] Batch: 0.0953 (0.0950) Data: 0.0016 (0.0023) Loss: 0.0000 (0.0390)
[2023/01/16 04:30] | ------------------------------------------------------------
[2023/01/16 04:30] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:30] | ------------------------------------------------------------
[2023/01/16 04:30] |    TRAIN(80)     0:03:30     0:00:05     0:03:24      0.0390
[2023/01/16 04:30] | ------------------------------------------------------------
[2023/01/16 04:30] | **************************************************
[2023/01/16 04:30] | TRAIN(081): [  50/2211] Batch: 0.1035 (0.1304) Data: 0.0017 (0.0263) Loss: 0.0025 (0.0379)
[2023/01/16 04:30] | TRAIN(081): [ 100/2211] Batch: 0.0953 (0.1105) Data: 0.0018 (0.0142) Loss: 0.5997 (0.0552)
[2023/01/16 04:31] | TRAIN(081): [ 150/2211] Batch: 0.0871 (0.1040) Data: 0.0018 (0.0101) Loss: 0.0000 (0.0524)
[2023/01/16 04:31] | TRAIN(081): [ 200/2211] Batch: 0.0875 (0.1006) Data: 0.0018 (0.0080) Loss: 0.1070 (0.0512)
[2023/01/16 04:31] | TRAIN(081): [ 250/2211] Batch: 0.1243 (0.1007) Data: 0.0023 (0.0068) Loss: 0.2380 (0.0487)
[2023/01/16 04:31] | TRAIN(081): [ 300/2211] Batch: 0.1226 (0.1045) Data: 0.0043 (0.0060) Loss: 0.1705 (0.0468)
[2023/01/16 04:31] | TRAIN(081): [ 350/2211] Batch: 0.0904 (0.1059) Data: 0.0018 (0.0055) Loss: 0.2057 (0.0432)
[2023/01/16 04:31] | TRAIN(081): [ 400/2211] Batch: 0.0902 (0.1049) Data: 0.0019 (0.0050) Loss: 0.0000 (0.0422)
[2023/01/16 04:31] | TRAIN(081): [ 450/2211] Batch: 0.0980 (0.1040) Data: 0.0018 (0.0047) Loss: 0.0000 (0.0417)
[2023/01/16 04:31] | TRAIN(081): [ 500/2211] Batch: 0.0890 (0.1026) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0428)
[2023/01/16 04:31] | TRAIN(081): [ 550/2211] Batch: 0.0868 (0.1016) Data: 0.0017 (0.0042) Loss: 0.0000 (0.0427)
[2023/01/16 04:31] | TRAIN(081): [ 600/2211] Batch: 0.0968 (0.1007) Data: 0.0018 (0.0040) Loss: 0.0312 (0.0418)
[2023/01/16 04:31] | TRAIN(081): [ 650/2211] Batch: 0.0893 (0.0999) Data: 0.0017 (0.0038) Loss: 0.0354 (0.0412)
[2023/01/16 04:31] | TRAIN(081): [ 700/2211] Batch: 0.0914 (0.0997) Data: 0.0018 (0.0037) Loss: 0.0564 (0.0411)
[2023/01/16 04:31] | TRAIN(081): [ 750/2211] Batch: 0.0878 (0.0992) Data: 0.0017 (0.0035) Loss: 0.0839 (0.0412)
[2023/01/16 04:32] | TRAIN(081): [ 800/2211] Batch: 0.0911 (0.0989) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0412)
[2023/01/16 04:32] | TRAIN(081): [ 850/2211] Batch: 0.0878 (0.0985) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0412)
[2023/01/16 04:32] | TRAIN(081): [ 900/2211] Batch: 0.0933 (0.0983) Data: 0.0018 (0.0032) Loss: 0.0425 (0.0405)
[2023/01/16 04:32] | TRAIN(081): [ 950/2211] Batch: 0.0868 (0.0980) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0401)
[2023/01/16 04:32] | TRAIN(081): [1000/2211] Batch: 0.0980 (0.0976) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0397)
[2023/01/16 04:32] | TRAIN(081): [1050/2211] Batch: 0.0914 (0.0975) Data: 0.0018 (0.0030) Loss: 0.1189 (0.0396)
[2023/01/16 04:32] | TRAIN(081): [1100/2211] Batch: 0.0905 (0.0975) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0388)
[2023/01/16 04:32] | TRAIN(081): [1150/2211] Batch: 0.0870 (0.0973) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0387)
[2023/01/16 04:32] | TRAIN(081): [1200/2211] Batch: 0.0877 (0.0970) Data: 0.0018 (0.0029) Loss: 0.0372 (0.0385)
[2023/01/16 04:32] | TRAIN(081): [1250/2211] Batch: 0.1039 (0.0975) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0383)
[2023/01/16 04:32] | TRAIN(081): [1300/2211] Batch: 0.0958 (0.0973) Data: 0.0018 (0.0028) Loss: 0.0944 (0.0382)
[2023/01/16 04:32] | TRAIN(081): [1350/2211] Batch: 0.0873 (0.0973) Data: 0.0018 (0.0028) Loss: 0.0035 (0.0383)
[2023/01/16 04:33] | TRAIN(081): [1400/2211] Batch: 0.0911 (0.0972) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0381)
[2023/01/16 04:33] | TRAIN(081): [1450/2211] Batch: 0.1171 (0.0970) Data: 0.0022 (0.0027) Loss: 0.0000 (0.0379)
[2023/01/16 04:33] | TRAIN(081): [1500/2211] Batch: 0.0936 (0.0969) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0380)
[2023/01/16 04:33] | TRAIN(081): [1550/2211] Batch: 0.0908 (0.0967) Data: 0.0017 (0.0026) Loss: 0.0146 (0.0381)
[2023/01/16 04:33] | TRAIN(081): [1600/2211] Batch: 0.0971 (0.0968) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0378)
[2023/01/16 04:33] | TRAIN(081): [1650/2211] Batch: 0.0905 (0.0968) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0380)
[2023/01/16 04:33] | TRAIN(081): [1700/2211] Batch: 0.0975 (0.0968) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0380)
[2023/01/16 04:33] | TRAIN(081): [1750/2211] Batch: 0.0983 (0.0967) Data: 0.0028 (0.0026) Loss: 0.1896 (0.0387)
[2023/01/16 04:33] | TRAIN(081): [1800/2211] Batch: 0.0884 (0.0968) Data: 0.0018 (0.0025) Loss: 0.1054 (0.0390)
[2023/01/16 04:33] | TRAIN(081): [1850/2211] Batch: 0.0916 (0.0968) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0392)
[2023/01/16 04:33] | TRAIN(081): [1900/2211] Batch: 0.0880 (0.0966) Data: 0.0017 (0.0025) Loss: 0.0667 (0.0389)
[2023/01/16 04:33] | TRAIN(081): [1950/2211] Batch: 0.0827 (0.0964) Data: 0.0018 (0.0025) Loss: 0.0242 (0.0390)
[2023/01/16 04:33] | TRAIN(081): [2000/2211] Batch: 0.0867 (0.0962) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0389)
[2023/01/16 04:34] | TRAIN(081): [2050/2211] Batch: 0.0828 (0.0959) Data: 0.0018 (0.0024) Loss: 0.0124 (0.0391)
[2023/01/16 04:34] | TRAIN(081): [2100/2211] Batch: 0.0824 (0.0956) Data: 0.0017 (0.0024) Loss: 0.0803 (0.0394)
[2023/01/16 04:34] | TRAIN(081): [2150/2211] Batch: 0.1063 (0.0957) Data: 0.0018 (0.0024) Loss: 0.0564 (0.0399)
[2023/01/16 04:34] | TRAIN(081): [2200/2211] Batch: 0.0974 (0.0956) Data: 0.0016 (0.0024) Loss: 0.0580 (0.0399)
[2023/01/16 04:34] | ------------------------------------------------------------
[2023/01/16 04:34] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:34] | ------------------------------------------------------------
[2023/01/16 04:34] |    TRAIN(81)     0:03:31     0:00:05     0:03:26      0.0399
[2023/01/16 04:34] | ------------------------------------------------------------
[2023/01/16 04:34] | **************************************************
[2023/01/16 04:34] | TRAIN(082): [  50/2211] Batch: 0.0900 (0.1221) Data: 0.0014 (0.0253) Loss: 0.0000 (0.0514)
[2023/01/16 04:34] | TRAIN(082): [ 100/2211] Batch: 0.1226 (0.1196) Data: 0.0023 (0.0138) Loss: 0.0884 (0.0397)
[2023/01/16 04:34] | TRAIN(082): [ 150/2211] Batch: 0.1047 (0.1165) Data: 0.0019 (0.0098) Loss: 0.0000 (0.0389)
[2023/01/16 04:34] | TRAIN(082): [ 200/2211] Batch: 0.1080 (0.1133) Data: 0.0016 (0.0078) Loss: 0.0000 (0.0369)
[2023/01/16 04:34] | TRAIN(082): [ 250/2211] Batch: 0.1306 (0.1152) Data: 0.0025 (0.0066) Loss: 0.0306 (0.0359)
[2023/01/16 04:34] | TRAIN(082): [ 300/2211] Batch: 0.1237 (0.1155) Data: 0.0022 (0.0059) Loss: 0.0002 (0.0366)
[2023/01/16 04:34] | TRAIN(082): [ 350/2211] Batch: 0.0879 (0.1154) Data: 0.0018 (0.0053) Loss: 0.0141 (0.0353)
[2023/01/16 04:35] | TRAIN(082): [ 400/2211] Batch: 0.0892 (0.1129) Data: 0.0018 (0.0049) Loss: 0.0015 (0.0353)
[2023/01/16 04:35] | TRAIN(082): [ 450/2211] Batch: 0.0870 (0.1103) Data: 0.0018 (0.0046) Loss: 0.0000 (0.0352)
[2023/01/16 04:35] | TRAIN(082): [ 500/2211] Batch: 0.0871 (0.1087) Data: 0.0018 (0.0043) Loss: 0.0075 (0.0361)
[2023/01/16 04:35] | TRAIN(082): [ 550/2211] Batch: 0.1094 (0.1088) Data: 0.0020 (0.0040) Loss: 0.1029 (0.0373)
[2023/01/16 04:35] | TRAIN(082): [ 600/2211] Batch: 0.1225 (0.1092) Data: 0.0020 (0.0038) Loss: 0.0000 (0.0381)
[2023/01/16 04:35] | TRAIN(082): [ 650/2211] Batch: 0.0920 (0.1086) Data: 0.0014 (0.0037) Loss: 0.0030 (0.0379)
[2023/01/16 04:35] | TRAIN(082): [ 700/2211] Batch: 0.0872 (0.1074) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0381)
[2023/01/16 04:35] | TRAIN(082): [ 750/2211] Batch: 0.0893 (0.1064) Data: 0.0018 (0.0034) Loss: 0.0015 (0.0378)
[2023/01/16 04:35] | TRAIN(082): [ 800/2211] Batch: 0.0889 (0.1055) Data: 0.0014 (0.0033) Loss: 0.0991 (0.0383)
[2023/01/16 04:35] | TRAIN(082): [ 850/2211] Batch: 0.0888 (0.1051) Data: 0.0018 (0.0032) Loss: 0.0473 (0.0376)
[2023/01/16 04:35] | TRAIN(082): [ 900/2211] Batch: 0.0872 (0.1043) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0380)
[2023/01/16 04:35] | TRAIN(082): [ 950/2211] Batch: 0.0892 (0.1036) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0375)
[2023/01/16 04:35] | TRAIN(082): [1000/2211] Batch: 0.0971 (0.1034) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0379)
[2023/01/16 04:36] | TRAIN(082): [1050/2211] Batch: 0.0885 (0.1030) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0374)
[2023/01/16 04:36] | TRAIN(082): [1100/2211] Batch: 0.0875 (0.1024) Data: 0.0019 (0.0029) Loss: 0.0324 (0.0376)
[2023/01/16 04:36] | TRAIN(082): [1150/2211] Batch: 0.0891 (0.1019) Data: 0.0018 (0.0029) Loss: 0.0054 (0.0378)
[2023/01/16 04:36] | TRAIN(082): [1200/2211] Batch: 0.0888 (0.1015) Data: 0.0018 (0.0028) Loss: 0.0784 (0.0377)
[2023/01/16 04:36] | TRAIN(082): [1250/2211] Batch: 0.0967 (0.1011) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0379)
[2023/01/16 04:36] | TRAIN(082): [1300/2211] Batch: 0.0924 (0.1008) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0379)
[2023/01/16 04:36] | TRAIN(082): [1350/2211] Batch: 0.0892 (0.1005) Data: 0.0018 (0.0027) Loss: 0.0885 (0.0379)
[2023/01/16 04:36] | TRAIN(082): [1400/2211] Batch: 0.0945 (0.1002) Data: 0.0017 (0.0027) Loss: 0.0241 (0.0379)
[2023/01/16 04:36] | TRAIN(082): [1450/2211] Batch: 0.0886 (0.1000) Data: 0.0017 (0.0026) Loss: 0.0688 (0.0381)
[2023/01/16 04:36] | TRAIN(082): [1500/2211] Batch: 0.0893 (0.0999) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0376)
[2023/01/16 04:36] | TRAIN(082): [1550/2211] Batch: 0.0913 (0.0996) Data: 0.0019 (0.0026) Loss: 0.0087 (0.0369)
[2023/01/16 04:36] | TRAIN(082): [1600/2211] Batch: 0.1021 (0.0996) Data: 0.0018 (0.0026) Loss: 0.1364 (0.0371)
[2023/01/16 04:37] | TRAIN(082): [1650/2211] Batch: 0.0964 (0.0995) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0370)
[2023/01/16 04:37] | TRAIN(082): [1700/2211] Batch: 0.0878 (0.0993) Data: 0.0019 (0.0025) Loss: 0.0072 (0.0370)
[2023/01/16 04:37] | TRAIN(082): [1750/2211] Batch: 0.0974 (0.0990) Data: 0.0019 (0.0025) Loss: 0.0792 (0.0376)
[2023/01/16 04:37] | TRAIN(082): [1800/2211] Batch: 0.0899 (0.0988) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0377)
[2023/01/16 04:37] | TRAIN(082): [1850/2211] Batch: 0.0878 (0.0988) Data: 0.0017 (0.0025) Loss: 0.0408 (0.0373)
[2023/01/16 04:37] | TRAIN(082): [1900/2211] Batch: 0.0992 (0.0986) Data: 0.0018 (0.0024) Loss: 0.1368 (0.0371)
[2023/01/16 04:37] | TRAIN(082): [1950/2211] Batch: 0.1174 (0.0990) Data: 0.0023 (0.0024) Loss: 0.0150 (0.0373)
[2023/01/16 04:37] | TRAIN(082): [2000/2211] Batch: 0.1109 (0.0991) Data: 0.0019 (0.0024) Loss: 0.1181 (0.0373)
[2023/01/16 04:37] | TRAIN(082): [2050/2211] Batch: 0.1164 (0.0991) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0375)
[2023/01/16 04:37] | TRAIN(082): [2100/2211] Batch: 0.0884 (0.0990) Data: 0.0018 (0.0024) Loss: 0.0123 (0.0374)
[2023/01/16 04:37] | TRAIN(082): [2150/2211] Batch: 0.0907 (0.0989) Data: 0.0019 (0.0024) Loss: 0.0030 (0.0373)
[2023/01/16 04:37] | TRAIN(082): [2200/2211] Batch: 0.0883 (0.0989) Data: 0.0016 (0.0024) Loss: 0.0434 (0.0377)
[2023/01/16 04:37] | ------------------------------------------------------------
[2023/01/16 04:37] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:37] | ------------------------------------------------------------
[2023/01/16 04:37] |    TRAIN(82)     0:03:38     0:00:05     0:03:33      0.0377
[2023/01/16 04:37] | ------------------------------------------------------------
[2023/01/16 04:37] | **************************************************
[2023/01/16 04:38] | TRAIN(083): [  50/2211] Batch: 0.0917 (0.1207) Data: 0.0019 (0.0248) Loss: 0.0000 (0.0353)
[2023/01/16 04:38] | TRAIN(083): [ 100/2211] Batch: 0.0891 (0.1076) Data: 0.0018 (0.0134) Loss: 0.0000 (0.0374)
[2023/01/16 04:38] | TRAIN(083): [ 150/2211] Batch: 0.0886 (0.1023) Data: 0.0016 (0.0095) Loss: 0.1041 (0.0361)
[2023/01/16 04:38] | TRAIN(083): [ 200/2211] Batch: 0.0997 (0.0998) Data: 0.0015 (0.0076) Loss: 0.1004 (0.0358)
[2023/01/16 04:38] | TRAIN(083): [ 250/2211] Batch: 0.0902 (0.0990) Data: 0.0019 (0.0064) Loss: 0.0000 (0.0351)
[2023/01/16 04:38] | TRAIN(083): [ 300/2211] Batch: 0.0877 (0.0999) Data: 0.0018 (0.0057) Loss: 0.0046 (0.0356)
[2023/01/16 04:38] | TRAIN(083): [ 350/2211] Batch: 0.0909 (0.0993) Data: 0.0016 (0.0051) Loss: 0.0102 (0.0343)
[2023/01/16 04:38] | TRAIN(083): [ 400/2211] Batch: 0.1066 (0.0995) Data: 0.0018 (0.0047) Loss: 0.0000 (0.0368)
[2023/01/16 04:38] | TRAIN(083): [ 450/2211] Batch: 0.0884 (0.0989) Data: 0.0018 (0.0044) Loss: 0.0711 (0.0374)
[2023/01/16 04:38] | TRAIN(083): [ 500/2211] Batch: 0.0992 (0.0987) Data: 0.0018 (0.0041) Loss: 0.2047 (0.0375)
[2023/01/16 04:38] | TRAIN(083): [ 550/2211] Batch: 0.0874 (0.0983) Data: 0.0017 (0.0039) Loss: 0.0000 (0.0370)
[2023/01/16 04:38] | TRAIN(083): [ 600/2211] Batch: 0.0916 (0.0986) Data: 0.0015 (0.0037) Loss: 0.0000 (0.0365)
[2023/01/16 04:38] | TRAIN(083): [ 650/2211] Batch: 0.0955 (0.0981) Data: 0.0017 (0.0036) Loss: 0.0042 (0.0369)
[2023/01/16 04:39] | TRAIN(083): [ 700/2211] Batch: 0.0879 (0.0979) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0361)
[2023/01/16 04:39] | TRAIN(083): [ 750/2211] Batch: 0.0915 (0.0977) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0370)
[2023/01/16 04:39] | TRAIN(083): [ 800/2211] Batch: 0.0918 (0.0975) Data: 0.0018 (0.0032) Loss: 0.0213 (0.0370)
[2023/01/16 04:39] | TRAIN(083): [ 850/2211] Batch: 0.0888 (0.0973) Data: 0.0019 (0.0032) Loss: 0.0439 (0.0376)
[2023/01/16 04:39] | TRAIN(083): [ 900/2211] Batch: 0.0878 (0.0972) Data: 0.0019 (0.0031) Loss: 0.1429 (0.0379)
[2023/01/16 04:39] | TRAIN(083): [ 950/2211] Batch: 0.1175 (0.0972) Data: 0.0022 (0.0030) Loss: 0.1668 (0.0381)
[2023/01/16 04:39] | TRAIN(083): [1000/2211] Batch: 0.0924 (0.0979) Data: 0.0018 (0.0030) Loss: 0.0824 (0.0388)
[2023/01/16 04:39] | TRAIN(083): [1050/2211] Batch: 0.0877 (0.0976) Data: 0.0019 (0.0029) Loss: 0.0320 (0.0395)
[2023/01/16 04:39] | TRAIN(083): [1100/2211] Batch: 0.0879 (0.0973) Data: 0.0018 (0.0029) Loss: 0.0614 (0.0394)
[2023/01/16 04:39] | TRAIN(083): [1150/2211] Batch: 0.0945 (0.0974) Data: 0.0017 (0.0028) Loss: 0.0322 (0.0402)
[2023/01/16 04:39] | TRAIN(083): [1200/2211] Batch: 0.0994 (0.0975) Data: 0.0018 (0.0028) Loss: 0.0759 (0.0403)
[2023/01/16 04:39] | TRAIN(083): [1250/2211] Batch: 0.1186 (0.0974) Data: 0.0023 (0.0027) Loss: 0.0000 (0.0396)
[2023/01/16 04:40] | TRAIN(083): [1300/2211] Batch: 0.1014 (0.0974) Data: 0.0016 (0.0027) Loss: 0.0116 (0.0398)
[2023/01/16 04:40] | TRAIN(083): [1350/2211] Batch: 0.1245 (0.0973) Data: 0.0021 (0.0027) Loss: 0.0000 (0.0398)
[2023/01/16 04:40] | TRAIN(083): [1400/2211] Batch: 0.0966 (0.0974) Data: 0.0018 (0.0026) Loss: 0.0128 (0.0398)
[2023/01/16 04:40] | TRAIN(083): [1450/2211] Batch: 0.0879 (0.0974) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0395)
[2023/01/16 04:40] | TRAIN(083): [1500/2211] Batch: 0.1010 (0.0973) Data: 0.0017 (0.0026) Loss: 0.0056 (0.0395)
[2023/01/16 04:40] | TRAIN(083): [1550/2211] Batch: 0.0881 (0.0972) Data: 0.0017 (0.0025) Loss: 0.0674 (0.0392)
[2023/01/16 04:40] | TRAIN(083): [1600/2211] Batch: 0.0842 (0.0970) Data: 0.0018 (0.0025) Loss: 0.0525 (0.0394)
[2023/01/16 04:40] | TRAIN(083): [1650/2211] Batch: 0.0892 (0.0968) Data: 0.0019 (0.0025) Loss: 0.0815 (0.0392)
[2023/01/16 04:40] | TRAIN(083): [1700/2211] Batch: 0.0925 (0.0969) Data: 0.0018 (0.0025) Loss: 0.1185 (0.0391)
[2023/01/16 04:40] | TRAIN(083): [1750/2211] Batch: 0.0948 (0.0969) Data: 0.0018 (0.0025) Loss: 0.0861 (0.0391)
[2023/01/16 04:40] | TRAIN(083): [1800/2211] Batch: 0.0885 (0.0968) Data: 0.0018 (0.0024) Loss: 0.0831 (0.0391)
[2023/01/16 04:40] | TRAIN(083): [1850/2211] Batch: 0.0940 (0.0967) Data: 0.0018 (0.0024) Loss: 0.1508 (0.0392)
[2023/01/16 04:40] | TRAIN(083): [1900/2211] Batch: 0.0911 (0.0966) Data: 0.0017 (0.0024) Loss: 0.1176 (0.0389)
[2023/01/16 04:41] | TRAIN(083): [1950/2211] Batch: 0.0967 (0.0966) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0384)
[2023/01/16 04:41] | TRAIN(083): [2000/2211] Batch: 0.0903 (0.0966) Data: 0.0018 (0.0024) Loss: 0.1189 (0.0388)
[2023/01/16 04:41] | TRAIN(083): [2050/2211] Batch: 0.0996 (0.0965) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0385)
[2023/01/16 04:41] | TRAIN(083): [2100/2211] Batch: 0.0902 (0.0965) Data: 0.0018 (0.0023) Loss: 0.0369 (0.0391)
[2023/01/16 04:41] | TRAIN(083): [2150/2211] Batch: 0.0956 (0.0965) Data: 0.0017 (0.0023) Loss: 0.0751 (0.0391)
[2023/01/16 04:41] | TRAIN(083): [2200/2211] Batch: 0.0881 (0.0967) Data: 0.0017 (0.0023) Loss: 0.0523 (0.0389)
[2023/01/16 04:41] | ------------------------------------------------------------
[2023/01/16 04:41] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:41] | ------------------------------------------------------------
[2023/01/16 04:41] |    TRAIN(83)     0:03:34     0:00:05     0:03:28      0.0388
[2023/01/16 04:41] | ------------------------------------------------------------
[2023/01/16 04:41] | **************************************************
[2023/01/16 04:41] | TRAIN(084): [  50/2211] Batch: 0.1181 (0.1331) Data: 0.0024 (0.0244) Loss: 0.0605 (0.0361)
[2023/01/16 04:41] | TRAIN(084): [ 100/2211] Batch: 0.0925 (0.1210) Data: 0.0017 (0.0132) Loss: 0.0214 (0.0409)
[2023/01/16 04:41] | TRAIN(084): [ 150/2211] Batch: 0.0909 (0.1212) Data: 0.0017 (0.0095) Loss: 0.0134 (0.0410)
[2023/01/16 04:41] | TRAIN(084): [ 200/2211] Batch: 0.1181 (0.1201) Data: 0.0020 (0.0077) Loss: 0.0398 (0.0392)
[2023/01/16 04:41] | TRAIN(084): [ 250/2211] Batch: 0.1180 (0.1199) Data: 0.0022 (0.0066) Loss: 0.0000 (0.0378)
[2023/01/16 04:42] | TRAIN(084): [ 300/2211] Batch: 0.1101 (0.1199) Data: 0.0017 (0.0058) Loss: 0.1290 (0.0382)
[2023/01/16 04:42] | TRAIN(084): [ 350/2211] Batch: 0.0924 (0.1177) Data: 0.0019 (0.0053) Loss: 0.1037 (0.0386)
[2023/01/16 04:42] | TRAIN(084): [ 400/2211] Batch: 0.1177 (0.1173) Data: 0.0022 (0.0049) Loss: 0.0378 (0.0378)
[2023/01/16 04:42] | TRAIN(084): [ 450/2211] Batch: 0.0914 (0.1144) Data: 0.0017 (0.0045) Loss: 0.0613 (0.0400)
[2023/01/16 04:42] | TRAIN(084): [ 500/2211] Batch: 0.0905 (0.1123) Data: 0.0017 (0.0042) Loss: 0.0773 (0.0406)
[2023/01/16 04:42] | TRAIN(084): [ 550/2211] Batch: 0.1172 (0.1104) Data: 0.0020 (0.0040) Loss: 0.0000 (0.0399)
[2023/01/16 04:42] | TRAIN(084): [ 600/2211] Batch: 0.1006 (0.1099) Data: 0.0017 (0.0038) Loss: 0.0000 (0.0402)
[2023/01/16 04:42] | TRAIN(084): [ 650/2211] Batch: 0.0982 (0.1087) Data: 0.0018 (0.0037) Loss: 0.0453 (0.0401)
[2023/01/16 04:42] | TRAIN(084): [ 700/2211] Batch: 0.0958 (0.1084) Data: 0.0016 (0.0035) Loss: 0.0801 (0.0403)
[2023/01/16 04:42] | TRAIN(084): [ 750/2211] Batch: 0.0879 (0.1076) Data: 0.0017 (0.0034) Loss: 0.1408 (0.0399)
[2023/01/16 04:42] | TRAIN(084): [ 800/2211] Batch: 0.0948 (0.1067) Data: 0.0016 (0.0033) Loss: 0.0000 (0.0394)
[2023/01/16 04:42] | TRAIN(084): [ 850/2211] Batch: 0.0881 (0.1060) Data: 0.0018 (0.0032) Loss: 0.0143 (0.0390)
[2023/01/16 04:43] | TRAIN(084): [ 900/2211] Batch: 0.0888 (0.1052) Data: 0.0017 (0.0032) Loss: 0.0863 (0.0386)
[2023/01/16 04:43] | TRAIN(084): [ 950/2211] Batch: 0.0887 (0.1045) Data: 0.0019 (0.0031) Loss: 0.0202 (0.0383)
[2023/01/16 04:43] | TRAIN(084): [1000/2211] Batch: 0.0878 (0.1046) Data: 0.0018 (0.0030) Loss: 0.1868 (0.0385)
[2023/01/16 04:43] | TRAIN(084): [1050/2211] Batch: 0.1186 (0.1041) Data: 0.0020 (0.0030) Loss: 0.0879 (0.0386)
[2023/01/16 04:43] | TRAIN(084): [1100/2211] Batch: 0.0879 (0.1039) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0384)
[2023/01/16 04:43] | TRAIN(084): [1150/2211] Batch: 0.0895 (0.1033) Data: 0.0015 (0.0029) Loss: 0.0000 (0.0387)
[2023/01/16 04:43] | TRAIN(084): [1200/2211] Batch: 0.0879 (0.1029) Data: 0.0017 (0.0028) Loss: 0.0265 (0.0388)
[2023/01/16 04:43] | TRAIN(084): [1250/2211] Batch: 0.0915 (0.1024) Data: 0.0023 (0.0028) Loss: 0.0000 (0.0384)
[2023/01/16 04:43] | TRAIN(084): [1300/2211] Batch: 0.0905 (0.1021) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0382)
[2023/01/16 04:43] | TRAIN(084): [1350/2211] Batch: 0.0980 (0.1018) Data: 0.0016 (0.0027) Loss: 0.0553 (0.0385)
[2023/01/16 04:43] | TRAIN(084): [1400/2211] Batch: 0.0964 (0.1016) Data: 0.0018 (0.0027) Loss: 0.0092 (0.0391)
[2023/01/16 04:43] | TRAIN(084): [1450/2211] Batch: 0.0909 (0.1014) Data: 0.0016 (0.0026) Loss: 0.0095 (0.0390)
[2023/01/16 04:44] | TRAIN(084): [1500/2211] Batch: 0.0963 (0.1014) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0388)
[2023/01/16 04:44] | TRAIN(084): [1550/2211] Batch: 0.1078 (0.1011) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0387)
[2023/01/16 04:44] | TRAIN(084): [1600/2211] Batch: 0.0973 (0.1009) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0387)
[2023/01/16 04:44] | TRAIN(084): [1650/2211] Batch: 0.1001 (0.1007) Data: 0.0021 (0.0025) Loss: 0.0797 (0.0384)
[2023/01/16 04:44] | TRAIN(084): [1700/2211] Batch: 0.0979 (0.1006) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0381)
[2023/01/16 04:44] | TRAIN(084): [1750/2211] Batch: 0.0878 (0.1005) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0378)
[2023/01/16 04:44] | TRAIN(084): [1800/2211] Batch: 0.0885 (0.1003) Data: 0.0018 (0.0025) Loss: 0.0705 (0.0375)
[2023/01/16 04:44] | TRAIN(084): [1850/2211] Batch: 0.1053 (0.1004) Data: 0.0012 (0.0024) Loss: 0.0000 (0.0373)
[2023/01/16 04:44] | TRAIN(084): [1900/2211] Batch: 0.0902 (0.1002) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0370)
[2023/01/16 04:44] | TRAIN(084): [1950/2211] Batch: 0.0845 (0.1000) Data: 0.0019 (0.0024) Loss: 0.1294 (0.0371)
[2023/01/16 04:44] | TRAIN(084): [2000/2211] Batch: 0.0913 (0.0998) Data: 0.0018 (0.0024) Loss: 0.0189 (0.0370)
[2023/01/16 04:44] | TRAIN(084): [2050/2211] Batch: 0.0871 (0.0998) Data: 0.0013 (0.0024) Loss: 0.0000 (0.0370)
[2023/01/16 04:44] | TRAIN(084): [2100/2211] Batch: 0.0863 (0.0995) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0370)
[2023/01/16 04:45] | TRAIN(084): [2150/2211] Batch: 0.0885 (0.0993) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0368)
[2023/01/16 04:45] | TRAIN(084): [2200/2211] Batch: 0.0815 (0.0992) Data: 0.0025 (0.0023) Loss: 0.0047 (0.0369)
[2023/01/16 04:45] | ------------------------------------------------------------
[2023/01/16 04:45] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:45] | ------------------------------------------------------------
[2023/01/16 04:45] |    TRAIN(84)     0:03:39     0:00:05     0:03:34      0.0370
[2023/01/16 04:45] | ------------------------------------------------------------
[2023/01/16 04:45] | **************************************************
[2023/01/16 04:45] | TRAIN(085): [  50/2211] Batch: 0.1024 (0.1246) Data: 0.0019 (0.0250) Loss: 0.0064 (0.0284)
[2023/01/16 04:45] | TRAIN(085): [ 100/2211] Batch: 0.0880 (0.1132) Data: 0.0018 (0.0135) Loss: 0.0000 (0.0314)
[2023/01/16 04:45] | TRAIN(085): [ 150/2211] Batch: 0.0904 (0.1064) Data: 0.0018 (0.0096) Loss: 0.0931 (0.0346)
[2023/01/16 04:45] | TRAIN(085): [ 200/2211] Batch: 0.0874 (0.1024) Data: 0.0018 (0.0077) Loss: 0.0000 (0.0354)
[2023/01/16 04:45] | TRAIN(085): [ 250/2211] Batch: 0.0870 (0.1002) Data: 0.0017 (0.0065) Loss: 0.0000 (0.0407)
[2023/01/16 04:45] | TRAIN(085): [ 300/2211] Batch: 0.0924 (0.0997) Data: 0.0018 (0.0057) Loss: 0.0025 (0.0391)
[2023/01/16 04:45] | TRAIN(085): [ 350/2211] Batch: 0.0888 (0.0988) Data: 0.0017 (0.0052) Loss: 0.0000 (0.0365)
[2023/01/16 04:45] | TRAIN(085): [ 400/2211] Batch: 0.0914 (0.0980) Data: 0.0035 (0.0048) Loss: 0.0000 (0.0354)
[2023/01/16 04:45] | TRAIN(085): [ 450/2211] Batch: 0.0955 (0.0987) Data: 0.0018 (0.0044) Loss: 0.0121 (0.0357)
[2023/01/16 04:45] | TRAIN(085): [ 500/2211] Batch: 0.0945 (0.0984) Data: 0.0022 (0.0042) Loss: 0.0000 (0.0362)
[2023/01/16 04:46] | TRAIN(085): [ 550/2211] Batch: 0.1000 (0.0984) Data: 0.0017 (0.0040) Loss: 0.1127 (0.0377)
[2023/01/16 04:46] | TRAIN(085): [ 600/2211] Batch: 0.0922 (0.0981) Data: 0.0016 (0.0038) Loss: 0.0785 (0.0369)
[2023/01/16 04:46] | TRAIN(085): [ 650/2211] Batch: 0.0982 (0.0978) Data: 0.0017 (0.0036) Loss: 0.0064 (0.0373)
[2023/01/16 04:46] | TRAIN(085): [ 700/2211] Batch: 0.0921 (0.0974) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0378)
[2023/01/16 04:46] | TRAIN(085): [ 750/2211] Batch: 0.0901 (0.0970) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0386)
[2023/01/16 04:46] | TRAIN(085): [ 800/2211] Batch: 0.0888 (0.0977) Data: 0.0018 (0.0033) Loss: 0.1362 (0.0398)
[2023/01/16 04:46] | TRAIN(085): [ 850/2211] Batch: 0.0882 (0.0974) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0396)
[2023/01/16 04:46] | TRAIN(085): [ 900/2211] Batch: 0.1105 (0.0973) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0389)
[2023/01/16 04:46] | TRAIN(085): [ 950/2211] Batch: 0.0906 (0.0973) Data: 0.0018 (0.0031) Loss: 0.1625 (0.0391)
[2023/01/16 04:46] | TRAIN(085): [1000/2211] Batch: 0.0962 (0.0978) Data: 0.0017 (0.0030) Loss: 0.0025 (0.0386)
[2023/01/16 04:46] | TRAIN(085): [1050/2211] Batch: 0.1223 (0.0981) Data: 0.0022 (0.0030) Loss: 0.0000 (0.0385)
[2023/01/16 04:46] | TRAIN(085): [1100/2211] Batch: 0.0977 (0.0985) Data: 0.0019 (0.0029) Loss: 0.0025 (0.0382)
[2023/01/16 04:47] | TRAIN(085): [1150/2211] Batch: 0.0885 (0.0983) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0382)
[2023/01/16 04:47] | TRAIN(085): [1200/2211] Batch: 0.0979 (0.0980) Data: 0.0018 (0.0028) Loss: 0.0247 (0.0381)
[2023/01/16 04:47] | TRAIN(085): [1250/2211] Batch: 0.0882 (0.0979) Data: 0.0017 (0.0028) Loss: 0.0295 (0.0378)
[2023/01/16 04:47] | TRAIN(085): [1300/2211] Batch: 0.0940 (0.0977) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0377)
[2023/01/16 04:47] | TRAIN(085): [1350/2211] Batch: 0.0883 (0.0975) Data: 0.0018 (0.0027) Loss: 0.0439 (0.0379)
[2023/01/16 04:47] | TRAIN(085): [1400/2211] Batch: 0.0876 (0.0974) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0378)
[2023/01/16 04:47] | TRAIN(085): [1450/2211] Batch: 0.0975 (0.0974) Data: 0.0019 (0.0026) Loss: 0.0117 (0.0378)
[2023/01/16 04:47] | TRAIN(085): [1500/2211] Batch: 0.0911 (0.0974) Data: 0.0017 (0.0026) Loss: 0.0876 (0.0380)
[2023/01/16 04:47] | TRAIN(085): [1550/2211] Batch: 0.0885 (0.0972) Data: 0.0018 (0.0026) Loss: 0.0274 (0.0380)
[2023/01/16 04:47] | TRAIN(085): [1600/2211] Batch: 0.0978 (0.0970) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0379)
[2023/01/16 04:47] | TRAIN(085): [1650/2211] Batch: 0.1014 (0.0970) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0377)
[2023/01/16 04:47] | TRAIN(085): [1700/2211] Batch: 0.1179 (0.0976) Data: 0.0022 (0.0025) Loss: 0.0342 (0.0375)
[2023/01/16 04:47] | TRAIN(085): [1750/2211] Batch: 0.0971 (0.0975) Data: 0.0018 (0.0025) Loss: 0.0302 (0.0376)
[2023/01/16 04:48] | TRAIN(085): [1800/2211] Batch: 0.0887 (0.0973) Data: 0.0017 (0.0025) Loss: 0.1888 (0.0376)
[2023/01/16 04:48] | TRAIN(085): [1850/2211] Batch: 0.0961 (0.0972) Data: 0.0017 (0.0025) Loss: 0.0814 (0.0380)
[2023/01/16 04:48] | TRAIN(085): [1900/2211] Batch: 0.0897 (0.0970) Data: 0.0018 (0.0025) Loss: 0.0177 (0.0378)
[2023/01/16 04:48] | TRAIN(085): [1950/2211] Batch: 0.1015 (0.0970) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0381)
[2023/01/16 04:48] | TRAIN(085): [2000/2211] Batch: 0.0934 (0.0970) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0378)
[2023/01/16 04:48] | TRAIN(085): [2050/2211] Batch: 0.0896 (0.0970) Data: 0.0018 (0.0024) Loss: 0.0984 (0.0380)
[2023/01/16 04:48] | TRAIN(085): [2100/2211] Batch: 0.0893 (0.0969) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0377)
[2023/01/16 04:48] | TRAIN(085): [2150/2211] Batch: 0.0903 (0.0973) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0375)
[2023/01/16 04:48] | TRAIN(085): [2200/2211] Batch: 0.1161 (0.0972) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0375)
[2023/01/16 04:48] | ------------------------------------------------------------
[2023/01/16 04:48] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:48] | ------------------------------------------------------------
[2023/01/16 04:48] |    TRAIN(85)     0:03:35     0:00:05     0:03:29      0.0375
[2023/01/16 04:48] | ------------------------------------------------------------
[2023/01/16 04:48] | **************************************************
[2023/01/16 04:48] | TRAIN(086): [  50/2211] Batch: 0.1590 (0.1310) Data: 0.0030 (0.0243) Loss: 0.0937 (0.0429)
[2023/01/16 04:48] | TRAIN(086): [ 100/2211] Batch: 0.1562 (0.1271) Data: 0.0026 (0.0133) Loss: 0.0000 (0.0397)
[2023/01/16 04:49] | TRAIN(086): [ 150/2211] Batch: 0.1234 (0.1284) Data: 0.0024 (0.0096) Loss: 0.0720 (0.0354)
[2023/01/16 04:49] | TRAIN(086): [ 200/2211] Batch: 0.0990 (0.1237) Data: 0.0018 (0.0076) Loss: 0.0000 (0.0343)
[2023/01/16 04:49] | TRAIN(086): [ 250/2211] Batch: 0.1091 (0.1205) Data: 0.0017 (0.0064) Loss: 0.0000 (0.0358)
[2023/01/16 04:49] | TRAIN(086): [ 300/2211] Batch: 0.0914 (0.1193) Data: 0.0018 (0.0057) Loss: 0.0000 (0.0347)
[2023/01/16 04:49] | TRAIN(086): [ 350/2211] Batch: 0.1192 (0.1169) Data: 0.0025 (0.0051) Loss: 0.0672 (0.0370)
[2023/01/16 04:49] | TRAIN(086): [ 400/2211] Batch: 0.1231 (0.1178) Data: 0.0023 (0.0048) Loss: 0.0000 (0.0359)
[2023/01/16 04:49] | TRAIN(086): [ 450/2211] Batch: 0.1042 (0.1185) Data: 0.0016 (0.0045) Loss: 0.0000 (0.0377)
[2023/01/16 04:49] | TRAIN(086): [ 500/2211] Batch: 0.1181 (0.1184) Data: 0.0023 (0.0042) Loss: 0.1761 (0.0372)
[2023/01/16 04:49] | TRAIN(086): [ 550/2211] Batch: 0.1059 (0.1171) Data: 0.0018 (0.0040) Loss: 0.0630 (0.0376)
[2023/01/16 04:49] | TRAIN(086): [ 600/2211] Batch: 0.0948 (0.1152) Data: 0.0017 (0.0038) Loss: 0.0667 (0.0373)
[2023/01/16 04:49] | TRAIN(086): [ 650/2211] Batch: 0.1069 (0.1136) Data: 0.0021 (0.0037) Loss: 0.0764 (0.0371)
[2023/01/16 04:50] | TRAIN(086): [ 700/2211] Batch: 0.1182 (0.1124) Data: 0.0022 (0.0036) Loss: 0.0358 (0.0364)
[2023/01/16 04:50] | TRAIN(086): [ 750/2211] Batch: 0.1056 (0.1116) Data: 0.0019 (0.0034) Loss: 0.0117 (0.0362)
[2023/01/16 04:50] | TRAIN(086): [ 800/2211] Batch: 0.0882 (0.1106) Data: 0.0018 (0.0033) Loss: 0.0607 (0.0360)
[2023/01/16 04:50] | TRAIN(086): [ 850/2211] Batch: 0.0931 (0.1098) Data: 0.0022 (0.0033) Loss: 0.0376 (0.0361)
[2023/01/16 04:50] | TRAIN(086): [ 900/2211] Batch: 0.0986 (0.1095) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0357)
[2023/01/16 04:50] | TRAIN(086): [ 950/2211] Batch: 0.1049 (0.1087) Data: 0.0045 (0.0031) Loss: 0.0000 (0.0359)
[2023/01/16 04:50] | TRAIN(086): [1000/2211] Batch: 0.0905 (0.1080) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0363)
[2023/01/16 04:50] | TRAIN(086): [1050/2211] Batch: 0.1015 (0.1075) Data: 0.0018 (0.0030) Loss: 0.1736 (0.0366)
[2023/01/16 04:50] | TRAIN(086): [1100/2211] Batch: 0.0891 (0.1071) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0363)
[2023/01/16 04:50] | TRAIN(086): [1150/2211] Batch: 0.0899 (0.1065) Data: 0.0018 (0.0029) Loss: 0.0516 (0.0376)
[2023/01/16 04:50] | TRAIN(086): [1200/2211] Batch: 0.0866 (0.1059) Data: 0.0017 (0.0028) Loss: 0.0033 (0.0380)
[2023/01/16 04:50] | TRAIN(086): [1250/2211] Batch: 0.0869 (0.1053) Data: 0.0016 (0.0028) Loss: 0.0000 (0.0377)
[2023/01/16 04:51] | TRAIN(086): [1300/2211] Batch: 0.0904 (0.1049) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0381)
[2023/01/16 04:51] | TRAIN(086): [1350/2211] Batch: 0.0902 (0.1044) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0386)
[2023/01/16 04:51] | TRAIN(086): [1400/2211] Batch: 0.0881 (0.1039) Data: 0.0018 (0.0027) Loss: 0.0580 (0.0394)
[2023/01/16 04:51] | TRAIN(086): [1450/2211] Batch: 0.0952 (0.1035) Data: 0.0017 (0.0026) Loss: 0.0122 (0.0394)
[2023/01/16 04:51] | TRAIN(086): [1500/2211] Batch: 0.0891 (0.1031) Data: 0.0017 (0.0026) Loss: 0.1151 (0.0390)
[2023/01/16 04:51] | TRAIN(086): [1550/2211] Batch: 0.0898 (0.1027) Data: 0.0017 (0.0026) Loss: 0.0247 (0.0389)
[2023/01/16 04:51] | TRAIN(086): [1600/2211] Batch: 0.0886 (0.1026) Data: 0.0017 (0.0026) Loss: 0.0456 (0.0390)
[2023/01/16 04:51] | TRAIN(086): [1650/2211] Batch: 0.0880 (0.1023) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0387)
[2023/01/16 04:51] | TRAIN(086): [1700/2211] Batch: 0.0881 (0.1020) Data: 0.0017 (0.0025) Loss: 0.0796 (0.0386)
[2023/01/16 04:51] | TRAIN(086): [1750/2211] Batch: 0.0890 (0.1016) Data: 0.0018 (0.0025) Loss: 0.0925 (0.0384)
[2023/01/16 04:51] | TRAIN(086): [1800/2211] Batch: 0.0876 (0.1015) Data: 0.0017 (0.0025) Loss: 0.0108 (0.0385)
[2023/01/16 04:51] | TRAIN(086): [1850/2211] Batch: 0.0887 (0.1014) Data: 0.0018 (0.0025) Loss: 0.1885 (0.0386)
[2023/01/16 04:51] | TRAIN(086): [1900/2211] Batch: 0.0882 (0.1011) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0387)
[2023/01/16 04:52] | TRAIN(086): [1950/2211] Batch: 0.0979 (0.1009) Data: 0.0019 (0.0024) Loss: 0.0805 (0.0386)
[2023/01/16 04:52] | TRAIN(086): [2000/2211] Batch: 0.0879 (0.1006) Data: 0.0017 (0.0024) Loss: 0.0081 (0.0381)
[2023/01/16 04:52] | TRAIN(086): [2050/2211] Batch: 0.0876 (0.1004) Data: 0.0018 (0.0024) Loss: 0.0945 (0.0378)
[2023/01/16 04:52] | TRAIN(086): [2100/2211] Batch: 0.0996 (0.1003) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0377)
[2023/01/16 04:52] | TRAIN(086): [2150/2211] Batch: 0.0970 (0.1002) Data: 0.0017 (0.0024) Loss: 0.1366 (0.0377)
[2023/01/16 04:52] | TRAIN(086): [2200/2211] Batch: 0.0876 (0.1000) Data: 0.0016 (0.0024) Loss: 0.1633 (0.0377)
[2023/01/16 04:52] | ------------------------------------------------------------
[2023/01/16 04:52] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:52] | ------------------------------------------------------------
[2023/01/16 04:52] |    TRAIN(86)     0:03:40     0:00:05     0:03:35      0.0376
[2023/01/16 04:52] | ------------------------------------------------------------
[2023/01/16 04:52] | **************************************************
[2023/01/16 04:52] | TRAIN(087): [  50/2211] Batch: 0.0899 (0.1154) Data: 0.0017 (0.0244) Loss: 0.0013 (0.0324)
[2023/01/16 04:52] | TRAIN(087): [ 100/2211] Batch: 0.1205 (0.1056) Data: 0.0022 (0.0132) Loss: 0.0000 (0.0345)
[2023/01/16 04:52] | TRAIN(087): [ 150/2211] Batch: 0.0877 (0.1020) Data: 0.0017 (0.0095) Loss: 0.1100 (0.0313)
[2023/01/16 04:52] | TRAIN(087): [ 200/2211] Batch: 0.0882 (0.0992) Data: 0.0018 (0.0076) Loss: 0.0090 (0.0327)
[2023/01/16 04:52] | TRAIN(087): [ 250/2211] Batch: 0.0927 (0.0977) Data: 0.0019 (0.0064) Loss: 0.0000 (0.0322)
[2023/01/16 04:52] | TRAIN(087): [ 300/2211] Batch: 0.0890 (0.0967) Data: 0.0018 (0.0056) Loss: 0.0595 (0.0312)
[2023/01/16 04:52] | TRAIN(087): [ 350/2211] Batch: 0.1003 (0.0961) Data: 0.0020 (0.0051) Loss: 0.0016 (0.0313)
[2023/01/16 04:53] | TRAIN(087): [ 400/2211] Batch: 0.1181 (0.0965) Data: 0.0022 (0.0047) Loss: 0.0000 (0.0308)
[2023/01/16 04:53] | TRAIN(087): [ 450/2211] Batch: 0.0882 (0.0964) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0306)
[2023/01/16 04:53] | TRAIN(087): [ 500/2211] Batch: 0.0904 (0.0968) Data: 0.0018 (0.0041) Loss: 0.0000 (0.0293)
[2023/01/16 04:53] | TRAIN(087): [ 550/2211] Batch: 0.0890 (0.0967) Data: 0.0018 (0.0039) Loss: 0.1654 (0.0299)
[2023/01/16 04:53] | TRAIN(087): [ 600/2211] Batch: 0.0909 (0.0973) Data: 0.0018 (0.0038) Loss: 0.0822 (0.0297)
[2023/01/16 04:53] | TRAIN(087): [ 650/2211] Batch: 0.0991 (0.0972) Data: 0.0019 (0.0036) Loss: 0.0000 (0.0303)
[2023/01/16 04:53] | TRAIN(087): [ 700/2211] Batch: 0.0965 (0.0973) Data: 0.0019 (0.0035) Loss: 0.1153 (0.0304)
[2023/01/16 04:53] | TRAIN(087): [ 750/2211] Batch: 0.0889 (0.0970) Data: 0.0018 (0.0034) Loss: 0.1473 (0.0309)
[2023/01/16 04:53] | TRAIN(087): [ 800/2211] Batch: 0.0876 (0.0967) Data: 0.0016 (0.0033) Loss: 0.0310 (0.0306)
[2023/01/16 04:53] | TRAIN(087): [ 850/2211] Batch: 0.0901 (0.0964) Data: 0.0018 (0.0032) Loss: 0.0488 (0.0309)
[2023/01/16 04:53] | TRAIN(087): [ 900/2211] Batch: 0.0985 (0.0964) Data: 0.0017 (0.0031) Loss: 0.1003 (0.0309)
[2023/01/16 04:53] | TRAIN(087): [ 950/2211] Batch: 0.0889 (0.0962) Data: 0.0018 (0.0030) Loss: 0.0650 (0.0314)
[2023/01/16 04:54] | TRAIN(087): [1000/2211] Batch: 0.0971 (0.0963) Data: 0.0018 (0.0030) Loss: 0.2048 (0.0320)
[2023/01/16 04:54] | TRAIN(087): [1050/2211] Batch: 0.0984 (0.0965) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0320)
[2023/01/16 04:54] | TRAIN(087): [1100/2211] Batch: 0.0904 (0.0964) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0324)
[2023/01/16 04:54] | TRAIN(087): [1150/2211] Batch: 0.0894 (0.0962) Data: 0.0018 (0.0028) Loss: 0.1595 (0.0330)
[2023/01/16 04:54] | TRAIN(087): [1200/2211] Batch: 0.0974 (0.0960) Data: 0.0017 (0.0028) Loss: 0.0022 (0.0332)
[2023/01/16 04:54] | TRAIN(087): [1250/2211] Batch: 0.0821 (0.0957) Data: 0.0018 (0.0027) Loss: 0.0470 (0.0333)
[2023/01/16 04:54] | TRAIN(087): [1300/2211] Batch: 0.0829 (0.0953) Data: 0.0017 (0.0027) Loss: 0.0069 (0.0334)
[2023/01/16 04:54] | TRAIN(087): [1350/2211] Batch: 0.0983 (0.0952) Data: 0.0017 (0.0027) Loss: 0.1598 (0.0337)
[2023/01/16 04:54] | TRAIN(087): [1400/2211] Batch: 0.0940 (0.0951) Data: 0.0022 (0.0026) Loss: 0.1435 (0.0338)
[2023/01/16 04:54] | TRAIN(087): [1450/2211] Batch: 0.0902 (0.0952) Data: 0.0017 (0.0026) Loss: 0.0134 (0.0340)
[2023/01/16 04:54] | TRAIN(087): [1500/2211] Batch: 0.0961 (0.0950) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0341)
[2023/01/16 04:54] | TRAIN(087): [1550/2211] Batch: 0.1153 (0.0949) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0346)
[2023/01/16 04:54] | TRAIN(087): [1600/2211] Batch: 0.0909 (0.0948) Data: 0.0018 (0.0025) Loss: 0.0103 (0.0348)
[2023/01/16 04:55] | TRAIN(087): [1650/2211] Batch: 0.0854 (0.0948) Data: 0.0016 (0.0025) Loss: 0.0268 (0.0349)
[2023/01/16 04:55] | TRAIN(087): [1700/2211] Batch: 0.1201 (0.0949) Data: 0.0020 (0.0025) Loss: 0.0000 (0.0351)
[2023/01/16 04:55] | TRAIN(087): [1750/2211] Batch: 0.0877 (0.0949) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0353)
[2023/01/16 04:55] | TRAIN(087): [1800/2211] Batch: 0.0966 (0.0948) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0355)
[2023/01/16 04:55] | TRAIN(087): [1850/2211] Batch: 0.0820 (0.0948) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0353)
[2023/01/16 04:55] | TRAIN(087): [1900/2211] Batch: 0.0899 (0.0947) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0353)
[2023/01/16 04:55] | TRAIN(087): [1950/2211] Batch: 0.0919 (0.0947) Data: 0.0018 (0.0024) Loss: 0.0022 (0.0355)
[2023/01/16 04:55] | TRAIN(087): [2000/2211] Batch: 0.0840 (0.0946) Data: 0.0018 (0.0024) Loss: 0.0021 (0.0351)
[2023/01/16 04:55] | TRAIN(087): [2050/2211] Batch: 0.0825 (0.0945) Data: 0.0018 (0.0023) Loss: 0.0095 (0.0350)
[2023/01/16 04:55] | TRAIN(087): [2100/2211] Batch: 0.1201 (0.0949) Data: 0.0023 (0.0023) Loss: 0.0000 (0.0348)
[2023/01/16 04:55] | TRAIN(087): [2150/2211] Batch: 0.0876 (0.0953) Data: 0.0017 (0.0023) Loss: 0.0087 (0.0345)
[2023/01/16 04:55] | TRAIN(087): [2200/2211] Batch: 0.1175 (0.0952) Data: 0.0020 (0.0023) Loss: 0.0016 (0.0343)
[2023/01/16 04:55] | ------------------------------------------------------------
[2023/01/16 04:55] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:55] | ------------------------------------------------------------
[2023/01/16 04:55] |    TRAIN(87)     0:03:30     0:00:05     0:03:25      0.0343
[2023/01/16 04:55] | ------------------------------------------------------------
[2023/01/16 04:55] | **************************************************
[2023/01/16 04:56] | TRAIN(088): [  50/2211] Batch: 0.0874 (0.1220) Data: 0.0017 (0.0237) Loss: 0.0703 (0.0306)
[2023/01/16 04:56] | TRAIN(088): [ 100/2211] Batch: 0.1083 (0.1072) Data: 0.0020 (0.0128) Loss: 0.0000 (0.0332)
[2023/01/16 04:56] | TRAIN(088): [ 150/2211] Batch: 0.1277 (0.1036) Data: 0.0023 (0.0092) Loss: 0.0281 (0.0313)
[2023/01/16 04:56] | TRAIN(088): [ 200/2211] Batch: 0.0871 (0.1023) Data: 0.0015 (0.0073) Loss: 0.0000 (0.0309)
[2023/01/16 04:56] | TRAIN(088): [ 250/2211] Batch: 0.0870 (0.0999) Data: 0.0017 (0.0062) Loss: 0.0538 (0.0322)
[2023/01/16 04:56] | TRAIN(088): [ 300/2211] Batch: 0.0918 (0.0990) Data: 0.0017 (0.0055) Loss: 0.1298 (0.0323)
[2023/01/16 04:56] | TRAIN(088): [ 350/2211] Batch: 0.1178 (0.1000) Data: 0.0020 (0.0050) Loss: 0.0000 (0.0319)
[2023/01/16 04:56] | TRAIN(088): [ 400/2211] Batch: 0.1007 (0.1004) Data: 0.0016 (0.0046) Loss: 0.1376 (0.0316)
[2023/01/16 04:56] | TRAIN(088): [ 450/2211] Batch: 0.0873 (0.0997) Data: 0.0016 (0.0043) Loss: 0.0578 (0.0326)
[2023/01/16 04:56] | TRAIN(088): [ 500/2211] Batch: 0.0875 (0.0989) Data: 0.0017 (0.0040) Loss: 0.0000 (0.0332)
[2023/01/16 04:56] | TRAIN(088): [ 550/2211] Batch: 0.1013 (0.0986) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0333)
[2023/01/16 04:56] | TRAIN(088): [ 600/2211] Batch: 0.0880 (0.0984) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0347)
[2023/01/16 04:56] | TRAIN(088): [ 650/2211] Batch: 0.1225 (0.0979) Data: 0.0021 (0.0035) Loss: 0.0163 (0.0348)
[2023/01/16 04:57] | TRAIN(088): [ 700/2211] Batch: 0.0881 (0.0980) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0349)
[2023/01/16 04:57] | TRAIN(088): [ 750/2211] Batch: 0.0910 (0.0978) Data: 0.0017 (0.0033) Loss: 0.0897 (0.0353)
[2023/01/16 04:57] | TRAIN(088): [ 800/2211] Batch: 0.1187 (0.0978) Data: 0.0024 (0.0032) Loss: 0.0000 (0.0353)
[2023/01/16 04:57] | TRAIN(088): [ 850/2211] Batch: 0.0920 (0.0978) Data: 0.0016 (0.0031) Loss: 0.1080 (0.0359)
[2023/01/16 04:57] | TRAIN(088): [ 900/2211] Batch: 0.0876 (0.0979) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0354)
[2023/01/16 04:57] | TRAIN(088): [ 950/2211] Batch: 0.0972 (0.0975) Data: 0.0016 (0.0030) Loss: 0.0000 (0.0359)
[2023/01/16 04:57] | TRAIN(088): [1000/2211] Batch: 0.0877 (0.0973) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0357)
[2023/01/16 04:57] | TRAIN(088): [1050/2211] Batch: 0.0990 (0.0973) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0363)
[2023/01/16 04:57] | TRAIN(088): [1100/2211] Batch: 0.1100 (0.0976) Data: 0.0017 (0.0028) Loss: 0.1189 (0.0363)
[2023/01/16 04:57] | TRAIN(088): [1150/2211] Batch: 0.0932 (0.0978) Data: 0.0017 (0.0028) Loss: 0.0489 (0.0371)
[2023/01/16 04:57] | TRAIN(088): [1200/2211] Batch: 0.0878 (0.0976) Data: 0.0017 (0.0027) Loss: 0.0081 (0.0375)
[2023/01/16 04:57] | TRAIN(088): [1250/2211] Batch: 0.0878 (0.0974) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0375)
[2023/01/16 04:58] | TRAIN(088): [1300/2211] Batch: 0.1059 (0.0972) Data: 0.0017 (0.0026) Loss: 0.0259 (0.0374)
[2023/01/16 04:58] | TRAIN(088): [1350/2211] Batch: 0.1223 (0.0976) Data: 0.0018 (0.0026) Loss: 0.0161 (0.0381)
[2023/01/16 04:58] | TRAIN(088): [1400/2211] Batch: 0.0960 (0.0977) Data: 0.0017 (0.0026) Loss: 0.0081 (0.0378)
[2023/01/16 04:58] | TRAIN(088): [1450/2211] Batch: 0.0977 (0.0976) Data: 0.0017 (0.0025) Loss: 0.0054 (0.0381)
[2023/01/16 04:58] | TRAIN(088): [1500/2211] Batch: 0.0976 (0.0977) Data: 0.0018 (0.0025) Loss: 0.0022 (0.0376)
[2023/01/16 04:58] | TRAIN(088): [1550/2211] Batch: 0.1023 (0.0978) Data: 0.0017 (0.0025) Loss: 0.0915 (0.0380)
[2023/01/16 04:58] | TRAIN(088): [1600/2211] Batch: 0.1068 (0.0978) Data: 0.0017 (0.0025) Loss: 0.1205 (0.0376)
[2023/01/16 04:58] | TRAIN(088): [1650/2211] Batch: 0.0996 (0.0978) Data: 0.0021 (0.0024) Loss: 0.0000 (0.0372)
[2023/01/16 04:58] | TRAIN(088): [1700/2211] Batch: 0.0964 (0.0980) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0369)
[2023/01/16 04:58] | TRAIN(088): [1750/2211] Batch: 0.1013 (0.0980) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0371)
[2023/01/16 04:58] | TRAIN(088): [1800/2211] Batch: 0.0976 (0.0980) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0371)
[2023/01/16 04:58] | TRAIN(088): [1850/2211] Batch: 0.0988 (0.0980) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0374)
[2023/01/16 04:59] | TRAIN(088): [1900/2211] Batch: 0.0907 (0.0979) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0371)
[2023/01/16 04:59] | TRAIN(088): [1950/2211] Batch: 0.0883 (0.0979) Data: 0.0017 (0.0023) Loss: 0.0062 (0.0368)
[2023/01/16 04:59] | TRAIN(088): [2000/2211] Batch: 0.0914 (0.0978) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0368)
[2023/01/16 04:59] | TRAIN(088): [2050/2211] Batch: 0.0878 (0.0978) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0365)
[2023/01/16 04:59] | TRAIN(088): [2100/2211] Batch: 0.0954 (0.0977) Data: 0.0018 (0.0023) Loss: 0.0031 (0.0365)
[2023/01/16 04:59] | TRAIN(088): [2150/2211] Batch: 0.0878 (0.0978) Data: 0.0018 (0.0023) Loss: 0.0259 (0.0367)
[2023/01/16 04:59] | TRAIN(088): [2200/2211] Batch: 0.0816 (0.0976) Data: 0.0016 (0.0023) Loss: 0.0644 (0.0365)
[2023/01/16 04:59] | ------------------------------------------------------------
[2023/01/16 04:59] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 04:59] | ------------------------------------------------------------
[2023/01/16 04:59] |    TRAIN(88)     0:03:35     0:00:05     0:03:30      0.0366
[2023/01/16 04:59] | ------------------------------------------------------------
[2023/01/16 04:59] | **************************************************
[2023/01/16 04:59] | TRAIN(089): [  50/2211] Batch: 0.1199 (0.1297) Data: 0.0021 (0.0246) Loss: 0.0664 (0.0380)
[2023/01/16 04:59] | TRAIN(089): [ 100/2211] Batch: 0.1212 (0.1245) Data: 0.0022 (0.0135) Loss: 0.0000 (0.0404)
[2023/01/16 04:59] | TRAIN(089): [ 150/2211] Batch: 0.0893 (0.1193) Data: 0.0017 (0.0097) Loss: 0.0681 (0.0365)
[2023/01/16 04:59] | TRAIN(089): [ 200/2211] Batch: 0.0923 (0.1128) Data: 0.0016 (0.0077) Loss: 0.0229 (0.0360)
[2023/01/16 04:59] | TRAIN(089): [ 250/2211] Batch: 0.1384 (0.1122) Data: 0.0019 (0.0066) Loss: 0.0000 (0.0376)
[2023/01/16 05:00] | TRAIN(089): [ 300/2211] Batch: 0.1319 (0.1129) Data: 0.0022 (0.0058) Loss: 0.0000 (0.0368)
[2023/01/16 05:00] | TRAIN(089): [ 350/2211] Batch: 0.1183 (0.1140) Data: 0.0023 (0.0053) Loss: 0.0000 (0.0362)
[2023/01/16 05:00] | TRAIN(089): [ 400/2211] Batch: 0.0909 (0.1135) Data: 0.0018 (0.0049) Loss: 0.1561 (0.0346)
[2023/01/16 05:00] | TRAIN(089): [ 450/2211] Batch: 0.0969 (0.1114) Data: 0.0017 (0.0045) Loss: 0.0000 (0.0354)
[2023/01/16 05:00] | TRAIN(089): [ 500/2211] Batch: 0.1162 (0.1099) Data: 0.0022 (0.0042) Loss: 0.0000 (0.0355)
[2023/01/16 05:00] | TRAIN(089): [ 550/2211] Batch: 0.0901 (0.1093) Data: 0.0019 (0.0040) Loss: 0.0349 (0.0342)
[2023/01/16 05:00] | TRAIN(089): [ 600/2211] Batch: 0.0861 (0.1079) Data: 0.0017 (0.0038) Loss: 0.0066 (0.0344)
[2023/01/16 05:00] | TRAIN(089): [ 650/2211] Batch: 0.0868 (0.1063) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0339)
[2023/01/16 05:00] | TRAIN(089): [ 700/2211] Batch: 0.0883 (0.1058) Data: 0.0018 (0.0035) Loss: 0.1159 (0.0342)
[2023/01/16 05:00] | TRAIN(089): [ 750/2211] Batch: 0.1173 (0.1053) Data: 0.0023 (0.0034) Loss: 0.0000 (0.0337)
[2023/01/16 05:00] | TRAIN(089): [ 800/2211] Batch: 0.1175 (0.1053) Data: 0.0023 (0.0033) Loss: 0.0000 (0.0340)
[2023/01/16 05:01] | TRAIN(089): [ 850/2211] Batch: 0.0906 (0.1049) Data: 0.0017 (0.0032) Loss: 0.1237 (0.0350)
[2023/01/16 05:01] | TRAIN(089): [ 900/2211] Batch: 0.1207 (0.1049) Data: 0.0019 (0.0032) Loss: 0.0244 (0.0345)
[2023/01/16 05:01] | TRAIN(089): [ 950/2211] Batch: 0.0903 (0.1048) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0340)
[2023/01/16 05:01] | TRAIN(089): [1000/2211] Batch: 0.0891 (0.1041) Data: 0.0017 (0.0030) Loss: 0.0099 (0.0336)
[2023/01/16 05:01] | TRAIN(089): [1050/2211] Batch: 0.1034 (0.1035) Data: 0.0017 (0.0030) Loss: 0.1004 (0.0332)
[2023/01/16 05:01] | TRAIN(089): [1100/2211] Batch: 0.1232 (0.1039) Data: 0.0022 (0.0029) Loss: 0.0000 (0.0338)
[2023/01/16 05:01] | TRAIN(089): [1150/2211] Batch: 0.0885 (0.1042) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0337)
[2023/01/16 05:01] | TRAIN(089): [1200/2211] Batch: 0.0885 (0.1037) Data: 0.0017 (0.0028) Loss: 0.0136 (0.0335)
[2023/01/16 05:01] | TRAIN(089): [1250/2211] Batch: 0.0910 (0.1033) Data: 0.0017 (0.0028) Loss: 0.0590 (0.0335)
[2023/01/16 05:01] | TRAIN(089): [1300/2211] Batch: 0.0881 (0.1029) Data: 0.0016 (0.0028) Loss: 0.0002 (0.0337)
[2023/01/16 05:01] | TRAIN(089): [1350/2211] Batch: 0.0878 (0.1024) Data: 0.0017 (0.0027) Loss: 0.0353 (0.0336)
[2023/01/16 05:01] | TRAIN(089): [1400/2211] Batch: 0.0991 (0.1021) Data: 0.0018 (0.0027) Loss: 0.0558 (0.0334)
[2023/01/16 05:01] | TRAIN(089): [1450/2211] Batch: 0.1053 (0.1017) Data: 0.0020 (0.0026) Loss: 0.2568 (0.0338)
[2023/01/16 05:02] | TRAIN(089): [1500/2211] Batch: 0.0892 (0.1014) Data: 0.0018 (0.0026) Loss: 0.0164 (0.0334)
[2023/01/16 05:02] | TRAIN(089): [1550/2211] Batch: 0.0931 (0.1011) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0334)
[2023/01/16 05:02] | TRAIN(089): [1600/2211] Batch: 0.0888 (0.1008) Data: 0.0016 (0.0026) Loss: 0.0019 (0.0331)
[2023/01/16 05:02] | TRAIN(089): [1650/2211] Batch: 0.0963 (0.1005) Data: 0.0017 (0.0025) Loss: 0.0671 (0.0335)
[2023/01/16 05:02] | TRAIN(089): [1700/2211] Batch: 0.0955 (0.1003) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0336)
[2023/01/16 05:02] | TRAIN(089): [1750/2211] Batch: 0.0926 (0.1001) Data: 0.0018 (0.0025) Loss: 0.0335 (0.0336)
[2023/01/16 05:02] | TRAIN(089): [1800/2211] Batch: 0.0884 (0.1001) Data: 0.0020 (0.0025) Loss: 0.0000 (0.0339)
[2023/01/16 05:02] | TRAIN(089): [1850/2211] Batch: 0.0922 (0.1000) Data: 0.0016 (0.0024) Loss: 0.1548 (0.0340)
[2023/01/16 05:02] | TRAIN(089): [1900/2211] Batch: 0.1204 (0.0999) Data: 0.0021 (0.0024) Loss: 0.0000 (0.0338)
[2023/01/16 05:02] | TRAIN(089): [1950/2211] Batch: 0.0896 (0.0998) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0340)
[2023/01/16 05:02] | TRAIN(089): [2000/2211] Batch: 0.1000 (0.0996) Data: 0.0016 (0.0024) Loss: 0.0026 (0.0339)
[2023/01/16 05:02] | TRAIN(089): [2050/2211] Batch: 0.1018 (0.0994) Data: 0.0017 (0.0024) Loss: 0.0018 (0.0340)
[2023/01/16 05:03] | TRAIN(089): [2100/2211] Batch: 0.0918 (0.0994) Data: 0.0015 (0.0023) Loss: 0.1456 (0.0342)
[2023/01/16 05:03] | TRAIN(089): [2150/2211] Batch: 0.0945 (0.0992) Data: 0.0017 (0.0023) Loss: 0.0181 (0.0340)
[2023/01/16 05:03] | TRAIN(089): [2200/2211] Batch: 0.0902 (0.0991) Data: 0.0016 (0.0023) Loss: 0.0599 (0.0340)
[2023/01/16 05:03] | ------------------------------------------------------------
[2023/01/16 05:03] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:03] | ------------------------------------------------------------
[2023/01/16 05:03] |    TRAIN(89)     0:03:39     0:00:05     0:03:33      0.0340
[2023/01/16 05:03] | ------------------------------------------------------------
[2023/01/16 05:03] | **************************************************
[2023/01/16 05:03] | TRAIN(090): [  50/2211] Batch: 0.0879 (0.1258) Data: 0.0017 (0.0247) Loss: 0.0137 (0.0385)
[2023/01/16 05:03] | TRAIN(090): [ 100/2211] Batch: 0.0894 (0.1094) Data: 0.0018 (0.0134) Loss: 0.1100 (0.0440)
[2023/01/16 05:03] | TRAIN(090): [ 150/2211] Batch: 0.1251 (0.1058) Data: 0.0025 (0.0095) Loss: 0.0000 (0.0385)
[2023/01/16 05:03] | TRAIN(090): [ 200/2211] Batch: 0.0884 (0.1035) Data: 0.0018 (0.0076) Loss: 0.0000 (0.0353)
[2023/01/16 05:03] | TRAIN(090): [ 250/2211] Batch: 0.1257 (0.1057) Data: 0.0019 (0.0065) Loss: 0.0515 (0.0350)
[2023/01/16 05:03] | TRAIN(090): [ 300/2211] Batch: 0.0959 (0.1074) Data: 0.0018 (0.0058) Loss: 0.0792 (0.0358)
[2023/01/16 05:03] | TRAIN(090): [ 350/2211] Batch: 0.0895 (0.1064) Data: 0.0018 (0.0052) Loss: 0.0040 (0.0360)
[2023/01/16 05:03] | TRAIN(090): [ 400/2211] Batch: 0.1189 (0.1064) Data: 0.0022 (0.0048) Loss: 0.0000 (0.0369)
[2023/01/16 05:03] | TRAIN(090): [ 450/2211] Batch: 0.0880 (0.1048) Data: 0.0017 (0.0044) Loss: 0.0000 (0.0369)
[2023/01/16 05:04] | TRAIN(090): [ 500/2211] Batch: 0.0951 (0.1034) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0365)
[2023/01/16 05:04] | TRAIN(090): [ 550/2211] Batch: 0.0945 (0.1026) Data: 0.0017 (0.0040) Loss: 0.0000 (0.0365)
[2023/01/16 05:04] | TRAIN(090): [ 600/2211] Batch: 0.0884 (0.1019) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0357)
[2023/01/16 05:04] | TRAIN(090): [ 650/2211] Batch: 0.1108 (0.1014) Data: 0.0019 (0.0036) Loss: 0.1204 (0.0359)
[2023/01/16 05:04] | TRAIN(090): [ 700/2211] Batch: 0.0918 (0.1010) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0357)
[2023/01/16 05:04] | TRAIN(090): [ 750/2211] Batch: 0.0985 (0.1003) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0353)
[2023/01/16 05:04] | TRAIN(090): [ 800/2211] Batch: 0.1026 (0.0998) Data: 0.0017 (0.0033) Loss: 0.1562 (0.0353)
[2023/01/16 05:04] | TRAIN(090): [ 850/2211] Batch: 0.1114 (0.0996) Data: 0.0017 (0.0032) Loss: 0.0320 (0.0353)
[2023/01/16 05:04] | TRAIN(090): [ 900/2211] Batch: 0.1014 (0.0993) Data: 0.0018 (0.0031) Loss: 0.0594 (0.0353)
[2023/01/16 05:04] | TRAIN(090): [ 950/2211] Batch: 0.0892 (0.0990) Data: 0.0017 (0.0030) Loss: 0.1525 (0.0354)
[2023/01/16 05:04] | TRAIN(090): [1000/2211] Batch: 0.0877 (0.0986) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0359)
[2023/01/16 05:04] | TRAIN(090): [1050/2211] Batch: 0.0877 (0.0982) Data: 0.0017 (0.0029) Loss: 0.0222 (0.0359)
[2023/01/16 05:04] | TRAIN(090): [1100/2211] Batch: 0.0968 (0.0979) Data: 0.0018 (0.0029) Loss: 0.0013 (0.0359)
[2023/01/16 05:05] | TRAIN(090): [1150/2211] Batch: 0.0880 (0.0978) Data: 0.0018 (0.0028) Loss: 0.0042 (0.0357)
[2023/01/16 05:05] | TRAIN(090): [1200/2211] Batch: 0.1179 (0.0976) Data: 0.0020 (0.0028) Loss: 0.0011 (0.0360)
[2023/01/16 05:05] | TRAIN(090): [1250/2211] Batch: 0.0868 (0.0977) Data: 0.0019 (0.0027) Loss: 0.0808 (0.0355)
[2023/01/16 05:05] | TRAIN(090): [1300/2211] Batch: 0.0955 (0.0975) Data: 0.0017 (0.0027) Loss: 0.0313 (0.0351)
[2023/01/16 05:05] | TRAIN(090): [1350/2211] Batch: 0.0954 (0.0974) Data: 0.0016 (0.0026) Loss: 0.1232 (0.0353)
[2023/01/16 05:05] | TRAIN(090): [1400/2211] Batch: 0.0972 (0.0973) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0355)
[2023/01/16 05:05] | TRAIN(090): [1450/2211] Batch: 0.0957 (0.0971) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0356)
[2023/01/16 05:05] | TRAIN(090): [1500/2211] Batch: 0.1104 (0.0970) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0355)
[2023/01/16 05:05] | TRAIN(090): [1550/2211] Batch: 0.1028 (0.0971) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0351)
[2023/01/16 05:05] | TRAIN(090): [1600/2211] Batch: 0.0912 (0.0972) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0356)
[2023/01/16 05:05] | TRAIN(090): [1650/2211] Batch: 0.0880 (0.0970) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0351)
[2023/01/16 05:05] | TRAIN(090): [1700/2211] Batch: 0.0882 (0.0968) Data: 0.0015 (0.0024) Loss: 0.0370 (0.0349)
[2023/01/16 05:06] | TRAIN(090): [1750/2211] Batch: 0.0918 (0.0967) Data: 0.0016 (0.0024) Loss: 0.1101 (0.0348)
[2023/01/16 05:06] | TRAIN(090): [1800/2211] Batch: 0.1186 (0.0971) Data: 0.0021 (0.0024) Loss: 0.0581 (0.0345)
[2023/01/16 05:06] | TRAIN(090): [1850/2211] Batch: 0.0949 (0.0973) Data: 0.0016 (0.0024) Loss: 0.1032 (0.0345)
[2023/01/16 05:06] | TRAIN(090): [1900/2211] Batch: 0.0917 (0.0973) Data: 0.0017 (0.0024) Loss: 0.0075 (0.0345)
[2023/01/16 05:06] | TRAIN(090): [1950/2211] Batch: 0.0880 (0.0971) Data: 0.0017 (0.0024) Loss: 0.0030 (0.0341)
[2023/01/16 05:06] | TRAIN(090): [2000/2211] Batch: 0.1190 (0.0972) Data: 0.0021 (0.0024) Loss: 0.0348 (0.0340)
[2023/01/16 05:06] | TRAIN(090): [2050/2211] Batch: 0.0985 (0.0973) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0341)
[2023/01/16 05:06] | TRAIN(090): [2100/2211] Batch: 0.0974 (0.0973) Data: 0.0016 (0.0023) Loss: 0.0000 (0.0340)
[2023/01/16 05:06] | TRAIN(090): [2150/2211] Batch: 0.0960 (0.0972) Data: 0.0017 (0.0023) Loss: 0.1429 (0.0338)
[2023/01/16 05:06] | TRAIN(090): [2200/2211] Batch: 0.0819 (0.0969) Data: 0.0016 (0.0023) Loss: 0.0000 (0.0338)
[2023/01/16 05:06] | ------------------------------------------------------------
[2023/01/16 05:06] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:06] | ------------------------------------------------------------
[2023/01/16 05:06] |    TRAIN(90)     0:03:34     0:00:05     0:03:29      0.0338
[2023/01/16 05:06] | ------------------------------------------------------------
[2023/01/16 05:06] | **************************************************
[2023/01/16 05:06] | TRAIN(091): [  50/2211] Batch: 0.0893 (0.1173) Data: 0.0015 (0.0232) Loss: 0.0000 (0.0231)
[2023/01/16 05:06] | TRAIN(091): [ 100/2211] Batch: 0.0876 (0.1050) Data: 0.0017 (0.0125) Loss: 0.1000 (0.0308)
[2023/01/16 05:07] | TRAIN(091): [ 150/2211] Batch: 0.0897 (0.1035) Data: 0.0018 (0.0089) Loss: 0.0000 (0.0296)
[2023/01/16 05:07] | TRAIN(091): [ 200/2211] Batch: 0.0877 (0.1003) Data: 0.0017 (0.0071) Loss: 0.0056 (0.0294)
[2023/01/16 05:07] | TRAIN(091): [ 250/2211] Batch: 0.0999 (0.0990) Data: 0.0018 (0.0061) Loss: 0.0000 (0.0320)
[2023/01/16 05:07] | TRAIN(091): [ 300/2211] Batch: 0.0908 (0.0981) Data: 0.0014 (0.0054) Loss: 0.0259 (0.0321)
[2023/01/16 05:07] | TRAIN(091): [ 350/2211] Batch: 0.0913 (0.0976) Data: 0.0017 (0.0048) Loss: 0.0000 (0.0317)
[2023/01/16 05:07] | TRAIN(091): [ 400/2211] Batch: 0.0915 (0.0967) Data: 0.0017 (0.0044) Loss: 0.1198 (0.0321)
[2023/01/16 05:07] | TRAIN(091): [ 450/2211] Batch: 0.0874 (0.0961) Data: 0.0017 (0.0041) Loss: 0.0263 (0.0327)
[2023/01/16 05:07] | TRAIN(091): [ 500/2211] Batch: 0.0914 (0.0957) Data: 0.0017 (0.0039) Loss: 0.0000 (0.0334)
[2023/01/16 05:07] | TRAIN(091): [ 550/2211] Batch: 0.0879 (0.0954) Data: 0.0017 (0.0037) Loss: 0.1168 (0.0349)
[2023/01/16 05:07] | TRAIN(091): [ 600/2211] Batch: 0.0966 (0.0954) Data: 0.0015 (0.0035) Loss: 0.0000 (0.0347)
[2023/01/16 05:07] | TRAIN(091): [ 650/2211] Batch: 0.1046 (0.0953) Data: 0.0019 (0.0034) Loss: 0.1933 (0.0344)
[2023/01/16 05:07] | TRAIN(091): [ 700/2211] Batch: 0.0957 (0.0950) Data: 0.0017 (0.0033) Loss: 0.0045 (0.0341)
[2023/01/16 05:07] | TRAIN(091): [ 750/2211] Batch: 0.0866 (0.0946) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0340)
[2023/01/16 05:08] | TRAIN(091): [ 800/2211] Batch: 0.0891 (0.0947) Data: 0.0016 (0.0031) Loss: 0.0122 (0.0339)
[2023/01/16 05:08] | TRAIN(091): [ 850/2211] Batch: 0.0873 (0.0944) Data: 0.0014 (0.0030) Loss: 0.0000 (0.0336)
[2023/01/16 05:08] | TRAIN(091): [ 900/2211] Batch: 0.0923 (0.0948) Data: 0.0017 (0.0029) Loss: 0.0068 (0.0336)
[2023/01/16 05:08] | TRAIN(091): [ 950/2211] Batch: 0.0954 (0.0947) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0331)
[2023/01/16 05:08] | TRAIN(091): [1000/2211] Batch: 0.0929 (0.0949) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0332)
[2023/01/16 05:08] | TRAIN(091): [1050/2211] Batch: 0.0881 (0.0949) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0336)
[2023/01/16 05:08] | TRAIN(091): [1100/2211] Batch: 0.0879 (0.0951) Data: 0.0016 (0.0027) Loss: 0.1415 (0.0339)
[2023/01/16 05:08] | TRAIN(091): [1150/2211] Batch: 0.1112 (0.0951) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0332)
[2023/01/16 05:08] | TRAIN(091): [1200/2211] Batch: 0.0952 (0.0954) Data: 0.0015 (0.0026) Loss: 0.0147 (0.0342)
[2023/01/16 05:08] | TRAIN(091): [1250/2211] Batch: 0.0929 (0.0953) Data: 0.0018 (0.0026) Loss: 0.0358 (0.0343)
[2023/01/16 05:08] | TRAIN(091): [1300/2211] Batch: 0.0874 (0.0952) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0353)
[2023/01/16 05:08] | TRAIN(091): [1350/2211] Batch: 0.0868 (0.0950) Data: 0.0017 (0.0025) Loss: 0.0380 (0.0355)
[2023/01/16 05:08] | TRAIN(091): [1400/2211] Batch: 0.0887 (0.0948) Data: 0.0016 (0.0025) Loss: 0.1147 (0.0357)
[2023/01/16 05:09] | TRAIN(091): [1450/2211] Batch: 0.1024 (0.0947) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0351)
[2023/01/16 05:09] | TRAIN(091): [1500/2211] Batch: 0.0983 (0.0948) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0346)
[2023/01/16 05:09] | TRAIN(091): [1550/2211] Batch: 0.1187 (0.0951) Data: 0.0021 (0.0024) Loss: 0.0247 (0.0349)
[2023/01/16 05:09] | TRAIN(091): [1600/2211] Batch: 0.0893 (0.0953) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0352)
[2023/01/16 05:09] | TRAIN(091): [1650/2211] Batch: 0.0871 (0.0952) Data: 0.0018 (0.0024) Loss: 0.0036 (0.0350)
[2023/01/16 05:09] | TRAIN(091): [1700/2211] Batch: 0.0921 (0.0952) Data: 0.0018 (0.0024) Loss: 0.0687 (0.0351)
[2023/01/16 05:09] | TRAIN(091): [1750/2211] Batch: 0.0871 (0.0955) Data: 0.0017 (0.0024) Loss: 0.1632 (0.0351)
[2023/01/16 05:09] | TRAIN(091): [1800/2211] Batch: 0.0957 (0.0953) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0351)
[2023/01/16 05:09] | TRAIN(091): [1850/2211] Batch: 0.0878 (0.0954) Data: 0.0019 (0.0023) Loss: 0.0642 (0.0350)
[2023/01/16 05:09] | TRAIN(091): [1900/2211] Batch: 0.0870 (0.0957) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0350)
[2023/01/16 05:09] | TRAIN(091): [1950/2211] Batch: 0.0971 (0.0956) Data: 0.0019 (0.0023) Loss: 0.0477 (0.0351)
[2023/01/16 05:09] | TRAIN(091): [2000/2211] Batch: 0.0918 (0.0955) Data: 0.0017 (0.0023) Loss: 0.0021 (0.0351)
[2023/01/16 05:10] | TRAIN(091): [2050/2211] Batch: 0.0841 (0.0954) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0351)
[2023/01/16 05:10] | TRAIN(091): [2100/2211] Batch: 0.0877 (0.0954) Data: 0.0017 (0.0023) Loss: 0.0202 (0.0349)
[2023/01/16 05:10] | TRAIN(091): [2150/2211] Batch: 0.0885 (0.0953) Data: 0.0017 (0.0023) Loss: 0.1439 (0.0350)
[2023/01/16 05:10] | TRAIN(091): [2200/2211] Batch: 0.0910 (0.0952) Data: 0.0015 (0.0022) Loss: 0.0131 (0.0351)
[2023/01/16 05:10] | ------------------------------------------------------------
[2023/01/16 05:10] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:10] | ------------------------------------------------------------
[2023/01/16 05:10] |    TRAIN(91)     0:03:30     0:00:04     0:03:25      0.0350
[2023/01/16 05:10] | ------------------------------------------------------------
[2023/01/16 05:10] | **************************************************
[2023/01/16 05:10] | TRAIN(092): [  50/2211] Batch: 0.0880 (0.1202) Data: 0.0018 (0.0259) Loss: 0.0000 (0.0354)
[2023/01/16 05:10] | TRAIN(092): [ 100/2211] Batch: 0.0875 (0.1046) Data: 0.0017 (0.0139) Loss: 0.0836 (0.0320)
[2023/01/16 05:10] | TRAIN(092): [ 150/2211] Batch: 0.0869 (0.1018) Data: 0.0017 (0.0099) Loss: 0.0134 (0.0339)
[2023/01/16 05:10] | TRAIN(092): [ 200/2211] Batch: 0.0867 (0.0987) Data: 0.0017 (0.0079) Loss: 0.3045 (0.0341)
[2023/01/16 05:10] | TRAIN(092): [ 250/2211] Batch: 0.0865 (0.0978) Data: 0.0016 (0.0067) Loss: 0.0000 (0.0340)
[2023/01/16 05:10] | TRAIN(092): [ 300/2211] Batch: 0.0912 (0.0975) Data: 0.0018 (0.0058) Loss: 0.1028 (0.0330)
[2023/01/16 05:10] | TRAIN(092): [ 350/2211] Batch: 0.0818 (0.0967) Data: 0.0016 (0.0053) Loss: 0.0000 (0.0324)
[2023/01/16 05:10] | TRAIN(092): [ 400/2211] Batch: 0.0955 (0.0958) Data: 0.0018 (0.0048) Loss: 0.0182 (0.0309)
[2023/01/16 05:10] | TRAIN(092): [ 450/2211] Batch: 0.0915 (0.0955) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0320)
[2023/01/16 05:11] | TRAIN(092): [ 500/2211] Batch: 0.1168 (0.0954) Data: 0.0022 (0.0042) Loss: 0.0000 (0.0329)
[2023/01/16 05:11] | TRAIN(092): [ 550/2211] Batch: 0.0955 (0.0956) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0338)
[2023/01/16 05:11] | TRAIN(092): [ 600/2211] Batch: 0.1021 (0.0966) Data: 0.0017 (0.0038) Loss: 0.0012 (0.0345)
[2023/01/16 05:11] | TRAIN(092): [ 650/2211] Batch: 0.0937 (0.0960) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0341)
[2023/01/16 05:11] | TRAIN(092): [ 700/2211] Batch: 0.0872 (0.0956) Data: 0.0017 (0.0035) Loss: 0.0941 (0.0349)
[2023/01/16 05:11] | TRAIN(092): [ 750/2211] Batch: 0.0876 (0.0951) Data: 0.0017 (0.0034) Loss: 0.0396 (0.0347)
[2023/01/16 05:11] | TRAIN(092): [ 800/2211] Batch: 0.0942 (0.0949) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0341)
[2023/01/16 05:11] | TRAIN(092): [ 850/2211] Batch: 0.1158 (0.0948) Data: 0.0017 (0.0032) Loss: 0.0161 (0.0338)
[2023/01/16 05:11] | TRAIN(092): [ 900/2211] Batch: 0.1181 (0.0950) Data: 0.0017 (0.0031) Loss: 0.1236 (0.0339)
[2023/01/16 05:11] | TRAIN(092): [ 950/2211] Batch: 0.0957 (0.0948) Data: 0.0018 (0.0031) Loss: 0.0124 (0.0338)
[2023/01/16 05:11] | TRAIN(092): [1000/2211] Batch: 0.0945 (0.0947) Data: 0.0017 (0.0030) Loss: 0.0808 (0.0338)
[2023/01/16 05:11] | TRAIN(092): [1050/2211] Batch: 0.0968 (0.0948) Data: 0.0022 (0.0029) Loss: 0.0000 (0.0334)
[2023/01/16 05:12] | TRAIN(092): [1100/2211] Batch: 0.0976 (0.0948) Data: 0.0030 (0.0029) Loss: 0.0000 (0.0332)
[2023/01/16 05:12] | TRAIN(092): [1150/2211] Batch: 0.0878 (0.0949) Data: 0.0018 (0.0028) Loss: 0.0048 (0.0332)
[2023/01/16 05:12] | TRAIN(092): [1200/2211] Batch: 0.1040 (0.0947) Data: 0.0017 (0.0028) Loss: 0.0600 (0.0332)
[2023/01/16 05:12] | TRAIN(092): [1250/2211] Batch: 0.0872 (0.0947) Data: 0.0017 (0.0028) Loss: 0.0729 (0.0333)
[2023/01/16 05:12] | TRAIN(092): [1300/2211] Batch: 0.0949 (0.0945) Data: 0.0018 (0.0027) Loss: 0.0271 (0.0328)
[2023/01/16 05:12] | TRAIN(092): [1350/2211] Batch: 0.0954 (0.0948) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0329)
[2023/01/16 05:12] | TRAIN(092): [1400/2211] Batch: 0.0904 (0.0950) Data: 0.0017 (0.0027) Loss: 0.0794 (0.0328)
[2023/01/16 05:12] | TRAIN(092): [1450/2211] Batch: 0.0892 (0.0948) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0331)
[2023/01/16 05:12] | TRAIN(092): [1500/2211] Batch: 0.0867 (0.0946) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0332)
[2023/01/16 05:12] | TRAIN(092): [1550/2211] Batch: 0.0870 (0.0945) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0335)
[2023/01/16 05:12] | TRAIN(092): [1600/2211] Batch: 0.0871 (0.0944) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0337)
[2023/01/16 05:12] | TRAIN(092): [1650/2211] Batch: 0.0962 (0.0944) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0336)
[2023/01/16 05:12] | TRAIN(092): [1700/2211] Batch: 0.0990 (0.0944) Data: 0.0018 (0.0025) Loss: 0.0092 (0.0339)
[2023/01/16 05:13] | TRAIN(092): [1750/2211] Batch: 0.0921 (0.0943) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0339)
[2023/01/16 05:13] | TRAIN(092): [1800/2211] Batch: 0.1016 (0.0945) Data: 0.0018 (0.0025) Loss: 0.0271 (0.0339)
[2023/01/16 05:13] | TRAIN(092): [1850/2211] Batch: 0.0872 (0.0944) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0338)
[2023/01/16 05:13] | TRAIN(092): [1900/2211] Batch: 0.0955 (0.0944) Data: 0.0018 (0.0024) Loss: 0.0486 (0.0339)
[2023/01/16 05:13] | TRAIN(092): [1950/2211] Batch: 0.0921 (0.0944) Data: 0.0019 (0.0024) Loss: 0.1013 (0.0336)
[2023/01/16 05:13] | TRAIN(092): [2000/2211] Batch: 0.0868 (0.0944) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0338)
[2023/01/16 05:13] | TRAIN(092): [2050/2211] Batch: 0.0913 (0.0943) Data: 0.0017 (0.0024) Loss: 0.0562 (0.0339)
[2023/01/16 05:13] | TRAIN(092): [2100/2211] Batch: 0.0979 (0.0942) Data: 0.0018 (0.0024) Loss: 0.0192 (0.0338)
[2023/01/16 05:13] | TRAIN(092): [2150/2211] Batch: 0.1003 (0.0942) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0339)
[2023/01/16 05:13] | TRAIN(092): [2200/2211] Batch: 0.0894 (0.0942) Data: 0.0016 (0.0023) Loss: 0.0000 (0.0336)
[2023/01/16 05:13] | ------------------------------------------------------------
[2023/01/16 05:13] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:13] | ------------------------------------------------------------
[2023/01/16 05:13] |    TRAIN(92)     0:03:28     0:00:05     0:03:23      0.0338
[2023/01/16 05:13] | ------------------------------------------------------------
[2023/01/16 05:13] | **************************************************
[2023/01/16 05:13] | TRAIN(093): [  50/2211] Batch: 0.0938 (0.1269) Data: 0.0022 (0.0247) Loss: 0.0040 (0.0265)
[2023/01/16 05:13] | TRAIN(093): [ 100/2211] Batch: 0.1196 (0.1242) Data: 0.0024 (0.0136) Loss: 0.1812 (0.0274)
[2023/01/16 05:14] | TRAIN(093): [ 150/2211] Batch: 0.1230 (0.1234) Data: 0.0025 (0.0099) Loss: 0.0400 (0.0271)
[2023/01/16 05:14] | TRAIN(093): [ 200/2211] Batch: 0.0869 (0.1231) Data: 0.0018 (0.0080) Loss: 0.0000 (0.0248)
[2023/01/16 05:14] | TRAIN(093): [ 250/2211] Batch: 0.1267 (0.1222) Data: 0.0022 (0.0068) Loss: 0.0000 (0.0252)
[2023/01/16 05:14] | TRAIN(093): [ 300/2211] Batch: 0.0916 (0.1213) Data: 0.0018 (0.0060) Loss: 0.1494 (0.0269)
[2023/01/16 05:14] | TRAIN(093): [ 350/2211] Batch: 0.0957 (0.1178) Data: 0.0022 (0.0054) Loss: 0.0112 (0.0270)
[2023/01/16 05:14] | TRAIN(093): [ 400/2211] Batch: 0.1198 (0.1151) Data: 0.0021 (0.0050) Loss: 0.0000 (0.0283)
[2023/01/16 05:14] | TRAIN(093): [ 450/2211] Batch: 0.0888 (0.1127) Data: 0.0018 (0.0046) Loss: 0.0000 (0.0274)
[2023/01/16 05:14] | TRAIN(093): [ 500/2211] Batch: 0.0870 (0.1105) Data: 0.0017 (0.0043) Loss: 0.0104 (0.0266)
[2023/01/16 05:14] | TRAIN(093): [ 550/2211] Batch: 0.1279 (0.1100) Data: 0.0024 (0.0041) Loss: 0.0082 (0.0267)
[2023/01/16 05:14] | TRAIN(093): [ 600/2211] Batch: 0.1208 (0.1112) Data: 0.0015 (0.0040) Loss: 0.0251 (0.0267)
[2023/01/16 05:14] | TRAIN(093): [ 650/2211] Batch: 0.1209 (0.1122) Data: 0.0020 (0.0038) Loss: 0.1083 (0.0270)
[2023/01/16 05:15] | TRAIN(093): [ 700/2211] Batch: 0.0916 (0.1117) Data: 0.0017 (0.0036) Loss: 0.0966 (0.0275)
[2023/01/16 05:15] | TRAIN(093): [ 750/2211] Batch: 0.0974 (0.1117) Data: 0.0014 (0.0035) Loss: 0.0000 (0.0279)
[2023/01/16 05:15] | TRAIN(093): [ 800/2211] Batch: 0.0943 (0.1110) Data: 0.0014 (0.0034) Loss: 0.0064 (0.0281)
[2023/01/16 05:15] | TRAIN(093): [ 850/2211] Batch: 0.0887 (0.1103) Data: 0.0018 (0.0033) Loss: 0.0292 (0.0284)
[2023/01/16 05:15] | TRAIN(093): [ 900/2211] Batch: 0.0959 (0.1092) Data: 0.0014 (0.0032) Loss: 0.0000 (0.0285)
[2023/01/16 05:15] | TRAIN(093): [ 950/2211] Batch: 0.0972 (0.1083) Data: 0.0019 (0.0031) Loss: 0.1226 (0.0299)
[2023/01/16 05:15] | TRAIN(093): [1000/2211] Batch: 0.1274 (0.1079) Data: 0.0018 (0.0031) Loss: 0.0092 (0.0306)
[2023/01/16 05:15] | TRAIN(093): [1050/2211] Batch: 0.0923 (0.1073) Data: 0.0018 (0.0030) Loss: 0.0516 (0.0318)
[2023/01/16 05:15] | TRAIN(093): [1100/2211] Batch: 0.0888 (0.1067) Data: 0.0018 (0.0029) Loss: 0.0419 (0.0324)
[2023/01/16 05:15] | TRAIN(093): [1150/2211] Batch: 0.1108 (0.1062) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0330)
[2023/01/16 05:15] | TRAIN(093): [1200/2211] Batch: 0.0874 (0.1057) Data: 0.0014 (0.0029) Loss: 0.0000 (0.0331)
[2023/01/16 05:15] | TRAIN(093): [1250/2211] Batch: 0.0929 (0.1052) Data: 0.0019 (0.0028) Loss: 0.0595 (0.0334)
[2023/01/16 05:16] | TRAIN(093): [1300/2211] Batch: 0.0878 (0.1050) Data: 0.0018 (0.0028) Loss: 0.0108 (0.0333)
[2023/01/16 05:16] | TRAIN(093): [1350/2211] Batch: 0.0953 (0.1048) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0336)
[2023/01/16 05:16] | TRAIN(093): [1400/2211] Batch: 0.0914 (0.1045) Data: 0.0014 (0.0027) Loss: 0.0000 (0.0331)
[2023/01/16 05:16] | TRAIN(093): [1450/2211] Batch: 0.1011 (0.1042) Data: 0.0018 (0.0026) Loss: 0.0032 (0.0333)
[2023/01/16 05:16] | TRAIN(093): [1500/2211] Batch: 0.1030 (0.1040) Data: 0.0018 (0.0026) Loss: 0.0628 (0.0333)
[2023/01/16 05:16] | TRAIN(093): [1550/2211] Batch: 0.0945 (0.1038) Data: 0.0015 (0.0026) Loss: 0.1053 (0.0332)
[2023/01/16 05:16] | TRAIN(093): [1600/2211] Batch: 0.0915 (0.1035) Data: 0.0020 (0.0026) Loss: 0.0322 (0.0329)
[2023/01/16 05:16] | TRAIN(093): [1650/2211] Batch: 0.0890 (0.1032) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0331)
[2023/01/16 05:16] | TRAIN(093): [1700/2211] Batch: 0.0928 (0.1029) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0330)
[2023/01/16 05:16] | TRAIN(093): [1750/2211] Batch: 0.0919 (0.1027) Data: 0.0018 (0.0025) Loss: 0.0784 (0.0331)
[2023/01/16 05:16] | TRAIN(093): [1800/2211] Batch: 0.0878 (0.1025) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0332)
[2023/01/16 05:16] | TRAIN(093): [1850/2211] Batch: 0.0965 (0.1025) Data: 0.0018 (0.0025) Loss: 0.1802 (0.0338)
[2023/01/16 05:16] | TRAIN(093): [1900/2211] Batch: 0.0919 (0.1024) Data: 0.0017 (0.0024) Loss: 0.0315 (0.0336)
[2023/01/16 05:17] | TRAIN(093): [1950/2211] Batch: 0.1097 (0.1023) Data: 0.0017 (0.0024) Loss: 0.0269 (0.0336)
[2023/01/16 05:17] | TRAIN(093): [2000/2211] Batch: 0.0937 (0.1023) Data: 0.0018 (0.0024) Loss: 0.0523 (0.0338)
[2023/01/16 05:17] | TRAIN(093): [2050/2211] Batch: 0.0990 (0.1020) Data: 0.0018 (0.0024) Loss: 0.0847 (0.0339)
[2023/01/16 05:17] | TRAIN(093): [2100/2211] Batch: 0.0928 (0.1019) Data: 0.0017 (0.0024) Loss: 0.0308 (0.0337)
[2023/01/16 05:17] | TRAIN(093): [2150/2211] Batch: 0.0979 (0.1017) Data: 0.0018 (0.0024) Loss: 0.0068 (0.0337)
[2023/01/16 05:17] | TRAIN(093): [2200/2211] Batch: 0.0884 (0.1016) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0336)
[2023/01/16 05:17] | ------------------------------------------------------------
[2023/01/16 05:17] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:17] | ------------------------------------------------------------
[2023/01/16 05:17] |    TRAIN(93)     0:03:44     0:00:05     0:03:39      0.0337
[2023/01/16 05:17] | ------------------------------------------------------------
[2023/01/16 05:17] | **************************************************
[2023/01/16 05:17] | TRAIN(094): [  50/2211] Batch: 0.0934 (0.1264) Data: 0.0018 (0.0244) Loss: 0.0056 (0.0318)
[2023/01/16 05:17] | TRAIN(094): [ 100/2211] Batch: 0.0934 (0.1135) Data: 0.0018 (0.0132) Loss: 0.0000 (0.0321)
[2023/01/16 05:17] | TRAIN(094): [ 150/2211] Batch: 0.0890 (0.1074) Data: 0.0016 (0.0094) Loss: 0.0000 (0.0339)
[2023/01/16 05:17] | TRAIN(094): [ 200/2211] Batch: 0.0897 (0.1025) Data: 0.0018 (0.0075) Loss: 0.0000 (0.0327)
[2023/01/16 05:17] | TRAIN(094): [ 250/2211] Batch: 0.1216 (0.1034) Data: 0.0021 (0.0064) Loss: 0.0000 (0.0319)
[2023/01/16 05:18] | TRAIN(094): [ 300/2211] Batch: 0.0919 (0.1055) Data: 0.0017 (0.0056) Loss: 0.0167 (0.0329)
[2023/01/16 05:18] | TRAIN(094): [ 350/2211] Batch: 0.0915 (0.1039) Data: 0.0018 (0.0051) Loss: 0.0000 (0.0330)
[2023/01/16 05:18] | TRAIN(094): [ 400/2211] Batch: 0.1048 (0.1032) Data: 0.0015 (0.0047) Loss: 0.0000 (0.0347)
[2023/01/16 05:18] | TRAIN(094): [ 450/2211] Batch: 0.0922 (0.1020) Data: 0.0019 (0.0044) Loss: 0.0000 (0.0372)
[2023/01/16 05:18] | TRAIN(094): [ 500/2211] Batch: 0.0989 (0.1016) Data: 0.0018 (0.0041) Loss: 0.0000 (0.0368)
[2023/01/16 05:18] | TRAIN(094): [ 550/2211] Batch: 0.0900 (0.1009) Data: 0.0018 (0.0039) Loss: 0.0093 (0.0364)
[2023/01/16 05:18] | TRAIN(094): [ 600/2211] Batch: 0.0991 (0.1006) Data: 0.0015 (0.0037) Loss: 0.0000 (0.0371)
[2023/01/16 05:18] | TRAIN(094): [ 650/2211] Batch: 0.0869 (0.0999) Data: 0.0017 (0.0035) Loss: 0.0088 (0.0364)
[2023/01/16 05:18] | TRAIN(094): [ 700/2211] Batch: 0.0975 (0.0998) Data: 0.0021 (0.0034) Loss: 0.0000 (0.0356)
[2023/01/16 05:18] | TRAIN(094): [ 750/2211] Batch: 0.0927 (0.0998) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0350)
[2023/01/16 05:18] | TRAIN(094): [ 800/2211] Batch: 0.0924 (0.0996) Data: 0.0015 (0.0032) Loss: 0.0034 (0.0351)
[2023/01/16 05:18] | TRAIN(094): [ 850/2211] Batch: 0.0910 (0.0996) Data: 0.0016 (0.0031) Loss: 0.0314 (0.0345)
[2023/01/16 05:18] | TRAIN(094): [ 900/2211] Batch: 0.0925 (0.0995) Data: 0.0019 (0.0030) Loss: 0.0058 (0.0347)
[2023/01/16 05:19] | TRAIN(094): [ 950/2211] Batch: 0.0944 (0.0992) Data: 0.0016 (0.0030) Loss: 0.0000 (0.0342)
[2023/01/16 05:19] | TRAIN(094): [1000/2211] Batch: 0.0905 (0.0990) Data: 0.0016 (0.0029) Loss: 0.0051 (0.0339)
[2023/01/16 05:19] | TRAIN(094): [1050/2211] Batch: 0.0874 (0.0986) Data: 0.0017 (0.0028) Loss: 0.0043 (0.0338)
[2023/01/16 05:19] | TRAIN(094): [1100/2211] Batch: 0.0864 (0.0983) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0340)
[2023/01/16 05:19] | TRAIN(094): [1150/2211] Batch: 0.0902 (0.0982) Data: 0.0016 (0.0027) Loss: 0.0936 (0.0341)
[2023/01/16 05:19] | TRAIN(094): [1200/2211] Batch: 0.1161 (0.0990) Data: 0.0022 (0.0027) Loss: 0.0000 (0.0347)
[2023/01/16 05:19] | TRAIN(094): [1250/2211] Batch: 0.0956 (0.0990) Data: 0.0018 (0.0027) Loss: 0.0787 (0.0349)
[2023/01/16 05:19] | TRAIN(094): [1300/2211] Batch: 0.0917 (0.0990) Data: 0.0013 (0.0026) Loss: 0.0000 (0.0350)
[2023/01/16 05:19] | TRAIN(094): [1350/2211] Batch: 0.0934 (0.0988) Data: 0.0019 (0.0026) Loss: 0.1239 (0.0352)
[2023/01/16 05:19] | TRAIN(094): [1400/2211] Batch: 0.0986 (0.0992) Data: 0.0018 (0.0026) Loss: 0.0286 (0.0354)
[2023/01/16 05:19] | TRAIN(094): [1450/2211] Batch: 0.1163 (0.0996) Data: 0.0015 (0.0026) Loss: 0.0027 (0.0350)
[2023/01/16 05:19] | TRAIN(094): [1500/2211] Batch: 0.0878 (0.1000) Data: 0.0017 (0.0026) Loss: 0.0352 (0.0350)
[2023/01/16 05:20] | TRAIN(094): [1550/2211] Batch: 0.0993 (0.1001) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0351)
[2023/01/16 05:20] | TRAIN(094): [1600/2211] Batch: 0.0943 (0.0998) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0353)
[2023/01/16 05:20] | TRAIN(094): [1650/2211] Batch: 0.0915 (0.0998) Data: 0.0016 (0.0025) Loss: 0.0648 (0.0353)
[2023/01/16 05:20] | TRAIN(094): [1700/2211] Batch: 0.1068 (0.0998) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0352)
[2023/01/16 05:20] | TRAIN(094): [1750/2211] Batch: 0.1065 (0.0996) Data: 0.0013 (0.0024) Loss: 0.0001 (0.0351)
[2023/01/16 05:20] | TRAIN(094): [1800/2211] Batch: 0.0888 (0.0995) Data: 0.0017 (0.0024) Loss: 0.0201 (0.0349)
[2023/01/16 05:20] | TRAIN(094): [1850/2211] Batch: 0.0872 (0.0994) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0347)
[2023/01/16 05:20] | TRAIN(094): [1900/2211] Batch: 0.0992 (0.0992) Data: 0.0018 (0.0024) Loss: 0.0655 (0.0346)
[2023/01/16 05:20] | TRAIN(094): [1950/2211] Batch: 0.0945 (0.0991) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0342)
[2023/01/16 05:20] | TRAIN(094): [2000/2211] Batch: 0.0907 (0.0990) Data: 0.0017 (0.0023) Loss: 0.0307 (0.0344)
[2023/01/16 05:20] | TRAIN(094): [2050/2211] Batch: 0.1084 (0.0989) Data: 0.0018 (0.0023) Loss: 0.0048 (0.0344)
[2023/01/16 05:20] | TRAIN(094): [2100/2211] Batch: 0.0907 (0.0989) Data: 0.0019 (0.0023) Loss: 0.0000 (0.0345)
[2023/01/16 05:21] | TRAIN(094): [2150/2211] Batch: 0.1061 (0.0988) Data: 0.0017 (0.0023) Loss: 0.0261 (0.0345)
[2023/01/16 05:21] | TRAIN(094): [2200/2211] Batch: 0.0919 (0.0988) Data: 0.0017 (0.0023) Loss: 0.0466 (0.0345)
[2023/01/16 05:21] | ------------------------------------------------------------
[2023/01/16 05:21] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:21] | ------------------------------------------------------------
[2023/01/16 05:21] |    TRAIN(94)     0:03:38     0:00:05     0:03:33      0.0344
[2023/01/16 05:21] | ------------------------------------------------------------
[2023/01/16 05:21] | **************************************************
[2023/01/16 05:21] | TRAIN(095): [  50/2211] Batch: 0.1068 (0.1250) Data: 0.0019 (0.0245) Loss: 0.0436 (0.0368)
[2023/01/16 05:21] | TRAIN(095): [ 100/2211] Batch: 0.1015 (0.1141) Data: 0.0017 (0.0134) Loss: 0.0085 (0.0398)
[2023/01/16 05:21] | TRAIN(095): [ 150/2211] Batch: 0.0915 (0.1088) Data: 0.0018 (0.0095) Loss: 0.0233 (0.0376)
[2023/01/16 05:21] | TRAIN(095): [ 200/2211] Batch: 0.1116 (0.1062) Data: 0.0020 (0.0076) Loss: 0.0061 (0.0390)
[2023/01/16 05:21] | TRAIN(095): [ 250/2211] Batch: 0.0929 (0.1039) Data: 0.0019 (0.0064) Loss: 0.0386 (0.0397)
[2023/01/16 05:21] | TRAIN(095): [ 300/2211] Batch: 0.0935 (0.1032) Data: 0.0018 (0.0057) Loss: 0.0025 (0.0388)
[2023/01/16 05:21] | TRAIN(095): [ 350/2211] Batch: 0.0915 (0.1030) Data: 0.0018 (0.0051) Loss: 0.0809 (0.0364)
[2023/01/16 05:21] | TRAIN(095): [ 400/2211] Batch: 0.0891 (0.1017) Data: 0.0023 (0.0047) Loss: 0.0000 (0.0373)
[2023/01/16 05:21] | TRAIN(095): [ 450/2211] Batch: 0.0983 (0.1017) Data: 0.0018 (0.0044) Loss: 0.1380 (0.0368)
[2023/01/16 05:21] | TRAIN(095): [ 500/2211] Batch: 0.0917 (0.1011) Data: 0.0018 (0.0041) Loss: 0.0101 (0.0377)
[2023/01/16 05:22] | TRAIN(095): [ 550/2211] Batch: 0.0969 (0.1016) Data: 0.0018 (0.0039) Loss: 0.0000 (0.0360)
[2023/01/16 05:22] | TRAIN(095): [ 600/2211] Batch: 0.0969 (0.1012) Data: 0.0018 (0.0038) Loss: 0.0621 (0.0359)
[2023/01/16 05:22] | TRAIN(095): [ 650/2211] Batch: 0.1086 (0.1014) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0361)
[2023/01/16 05:22] | TRAIN(095): [ 700/2211] Batch: 0.0942 (0.1013) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0358)
[2023/01/16 05:22] | TRAIN(095): [ 750/2211] Batch: 0.1067 (0.1009) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0357)
[2023/01/16 05:22] | TRAIN(095): [ 800/2211] Batch: 0.0960 (0.1006) Data: 0.0019 (0.0033) Loss: 0.0000 (0.0350)
[2023/01/16 05:22] | TRAIN(095): [ 850/2211] Batch: 0.0923 (0.1015) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0353)
[2023/01/16 05:22] | TRAIN(095): [ 900/2211] Batch: 0.0926 (0.1015) Data: 0.0016 (0.0031) Loss: 0.0000 (0.0353)
[2023/01/16 05:22] | TRAIN(095): [ 950/2211] Batch: 0.1218 (0.1015) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0351)
[2023/01/16 05:22] | TRAIN(095): [1000/2211] Batch: 0.1212 (0.1025) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0347)
[2023/01/16 05:22] | TRAIN(095): [1050/2211] Batch: 0.1008 (0.1028) Data: 0.0017 (0.0029) Loss: 0.0075 (0.0344)
[2023/01/16 05:23] | TRAIN(095): [1100/2211] Batch: 0.0897 (0.1029) Data: 0.0018 (0.0029) Loss: 0.0024 (0.0341)
[2023/01/16 05:23] | TRAIN(095): [1150/2211] Batch: 0.1123 (0.1029) Data: 0.0018 (0.0029) Loss: 0.0466 (0.0338)
[2023/01/16 05:23] | TRAIN(095): [1200/2211] Batch: 0.0900 (0.1030) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0336)
[2023/01/16 05:23] | TRAIN(095): [1250/2211] Batch: 0.0956 (0.1030) Data: 0.0016 (0.0028) Loss: 0.0022 (0.0335)
[2023/01/16 05:23] | TRAIN(095): [1300/2211] Batch: 0.1095 (0.1031) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0329)
[2023/01/16 05:23] | TRAIN(095): [1350/2211] Batch: 0.1242 (0.1035) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0328)
[2023/01/16 05:23] | TRAIN(095): [1400/2211] Batch: 0.1091 (0.1039) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0325)
[2023/01/16 05:23] | TRAIN(095): [1450/2211] Batch: 0.1031 (0.1040) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0326)
[2023/01/16 05:23] | TRAIN(095): [1500/2211] Batch: 0.1062 (0.1043) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0326)
[2023/01/16 05:23] | TRAIN(095): [1550/2211] Batch: 0.0959 (0.1041) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0330)
[2023/01/16 05:23] | TRAIN(095): [1600/2211] Batch: 0.0914 (0.1041) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0332)
[2023/01/16 05:23] | TRAIN(095): [1650/2211] Batch: 0.1000 (0.1039) Data: 0.0017 (0.0025) Loss: 0.0926 (0.0335)
[2023/01/16 05:24] | TRAIN(095): [1700/2211] Batch: 0.0986 (0.1038) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0332)
[2023/01/16 05:24] | TRAIN(095): [1750/2211] Batch: 0.0968 (0.1036) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0334)
[2023/01/16 05:24] | TRAIN(095): [1800/2211] Batch: 0.0948 (0.1034) Data: 0.0019 (0.0025) Loss: 0.0203 (0.0335)
[2023/01/16 05:24] | TRAIN(095): [1850/2211] Batch: 0.0867 (0.1032) Data: 0.0019 (0.0025) Loss: 0.0868 (0.0336)
[2023/01/16 05:24] | TRAIN(095): [1900/2211] Batch: 0.1004 (0.1032) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0336)
[2023/01/16 05:24] | TRAIN(095): [1950/2211] Batch: 0.0925 (0.1031) Data: 0.0018 (0.0024) Loss: 0.0430 (0.0334)
[2023/01/16 05:24] | TRAIN(095): [2000/2211] Batch: 0.0905 (0.1032) Data: 0.0019 (0.0024) Loss: 0.1228 (0.0332)
[2023/01/16 05:24] | TRAIN(095): [2050/2211] Batch: 0.0951 (0.1030) Data: 0.0018 (0.0024) Loss: 0.0021 (0.0330)
[2023/01/16 05:24] | TRAIN(095): [2100/2211] Batch: 0.0998 (0.1029) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0329)
[2023/01/16 05:24] | TRAIN(095): [2150/2211] Batch: 0.0914 (0.1029) Data: 0.0019 (0.0024) Loss: 0.0636 (0.0329)
[2023/01/16 05:24] | TRAIN(095): [2200/2211] Batch: 0.1025 (0.1029) Data: 0.0018 (0.0024) Loss: 0.0129 (0.0328)
[2023/01/16 05:24] | ------------------------------------------------------------
[2023/01/16 05:24] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:24] | ------------------------------------------------------------
[2023/01/16 05:24] |    TRAIN(95)     0:03:47     0:00:05     0:03:42      0.0330
[2023/01/16 05:24] | ------------------------------------------------------------
[2023/01/16 05:24] | **************************************************
[2023/01/16 05:25] | TRAIN(096): [  50/2211] Batch: 0.0967 (0.1340) Data: 0.0018 (0.0246) Loss: 0.0000 (0.0235)
[2023/01/16 05:25] | TRAIN(096): [ 100/2211] Batch: 0.0899 (0.1199) Data: 0.0014 (0.0134) Loss: 0.0051 (0.0326)
[2023/01/16 05:25] | TRAIN(096): [ 150/2211] Batch: 0.1058 (0.1216) Data: 0.0018 (0.0096) Loss: 0.0000 (0.0325)
[2023/01/16 05:25] | TRAIN(096): [ 200/2211] Batch: 0.1304 (0.1220) Data: 0.0019 (0.0077) Loss: 0.0000 (0.0318)
[2023/01/16 05:25] | TRAIN(096): [ 250/2211] Batch: 0.0954 (0.1239) Data: 0.0017 (0.0066) Loss: 0.0000 (0.0309)
[2023/01/16 05:25] | TRAIN(096): [ 300/2211] Batch: 0.1027 (0.1203) Data: 0.0019 (0.0058) Loss: 0.0000 (0.0316)
[2023/01/16 05:25] | TRAIN(096): [ 350/2211] Batch: 0.1206 (0.1175) Data: 0.0022 (0.0052) Loss: 0.0661 (0.0318)
[2023/01/16 05:25] | TRAIN(096): [ 400/2211] Batch: 0.0907 (0.1147) Data: 0.0022 (0.0048) Loss: 0.0427 (0.0321)
[2023/01/16 05:25] | TRAIN(096): [ 450/2211] Batch: 0.0951 (0.1128) Data: 0.0018 (0.0045) Loss: 0.1468 (0.0327)
[2023/01/16 05:25] | TRAIN(096): [ 500/2211] Batch: 0.0962 (0.1113) Data: 0.0020 (0.0042) Loss: 0.0000 (0.0326)
[2023/01/16 05:25] | TRAIN(096): [ 550/2211] Batch: 0.0935 (0.1100) Data: 0.0018 (0.0040) Loss: 0.0082 (0.0320)
[2023/01/16 05:26] | TRAIN(096): [ 600/2211] Batch: 0.0988 (0.1089) Data: 0.0018 (0.0038) Loss: 0.0158 (0.0318)
[2023/01/16 05:26] | TRAIN(096): [ 650/2211] Batch: 0.0984 (0.1078) Data: 0.0017 (0.0037) Loss: 0.0040 (0.0316)
[2023/01/16 05:26] | TRAIN(096): [ 700/2211] Batch: 0.0948 (0.1070) Data: 0.0015 (0.0035) Loss: 0.0000 (0.0321)
[2023/01/16 05:26] | TRAIN(096): [ 750/2211] Batch: 0.1109 (0.1067) Data: 0.0019 (0.0034) Loss: 0.0604 (0.0323)
[2023/01/16 05:26] | TRAIN(096): [ 800/2211] Batch: 0.0917 (0.1062) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0326)
[2023/01/16 05:26] | TRAIN(096): [ 850/2211] Batch: 0.1090 (0.1057) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0327)
[2023/01/16 05:26] | TRAIN(096): [ 900/2211] Batch: 0.0974 (0.1051) Data: 0.0014 (0.0031) Loss: 0.0000 (0.0330)
[2023/01/16 05:26] | TRAIN(096): [ 950/2211] Batch: 0.1007 (0.1047) Data: 0.0020 (0.0031) Loss: 0.0176 (0.0326)
[2023/01/16 05:26] | TRAIN(096): [1000/2211] Batch: 0.0915 (0.1045) Data: 0.0018 (0.0030) Loss: 0.0015 (0.0330)
[2023/01/16 05:26] | TRAIN(096): [1050/2211] Batch: 0.0960 (0.1043) Data: 0.0017 (0.0030) Loss: 0.0111 (0.0329)
[2023/01/16 05:26] | TRAIN(096): [1100/2211] Batch: 0.0895 (0.1038) Data: 0.0018 (0.0029) Loss: 0.0594 (0.0328)
[2023/01/16 05:26] | TRAIN(096): [1150/2211] Batch: 0.0917 (0.1035) Data: 0.0019 (0.0029) Loss: 0.1055 (0.0331)
[2023/01/16 05:26] | TRAIN(096): [1200/2211] Batch: 0.1021 (0.1032) Data: 0.0017 (0.0028) Loss: 0.0076 (0.0332)
[2023/01/16 05:27] | TRAIN(096): [1250/2211] Batch: 0.0921 (0.1030) Data: 0.0018 (0.0028) Loss: 0.0521 (0.0332)
[2023/01/16 05:27] | TRAIN(096): [1300/2211] Batch: 0.1026 (0.1028) Data: 0.0019 (0.0027) Loss: 0.0576 (0.0332)
[2023/01/16 05:27] | TRAIN(096): [1350/2211] Batch: 0.1108 (0.1027) Data: 0.0018 (0.0027) Loss: 0.1268 (0.0330)
[2023/01/16 05:27] | TRAIN(096): [1400/2211] Batch: 0.1013 (0.1026) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0332)
[2023/01/16 05:27] | TRAIN(096): [1450/2211] Batch: 0.1084 (0.1025) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0334)
[2023/01/16 05:27] | TRAIN(096): [1500/2211] Batch: 0.0942 (0.1022) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0333)
[2023/01/16 05:27] | TRAIN(096): [1550/2211] Batch: 0.0994 (0.1020) Data: 0.0019 (0.0026) Loss: 0.0439 (0.0331)
[2023/01/16 05:27] | TRAIN(096): [1600/2211] Batch: 0.0916 (0.1018) Data: 0.0017 (0.0026) Loss: 0.0103 (0.0331)
[2023/01/16 05:27] | TRAIN(096): [1650/2211] Batch: 0.0955 (0.1017) Data: 0.0018 (0.0026) Loss: 0.0499 (0.0330)
[2023/01/16 05:27] | TRAIN(096): [1700/2211] Batch: 0.0916 (0.1014) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0331)
[2023/01/16 05:27] | TRAIN(096): [1750/2211] Batch: 0.0927 (0.1012) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0332)
[2023/01/16 05:27] | TRAIN(096): [1800/2211] Batch: 0.0986 (0.1010) Data: 0.0017 (0.0025) Loss: 0.0186 (0.0333)
[2023/01/16 05:28] | TRAIN(096): [1850/2211] Batch: 0.0883 (0.1013) Data: 0.0018 (0.0025) Loss: 0.0149 (0.0334)
[2023/01/16 05:28] | TRAIN(096): [1900/2211] Batch: 0.0939 (0.1010) Data: 0.0020 (0.0025) Loss: 0.1309 (0.0335)
[2023/01/16 05:28] | TRAIN(096): [1950/2211] Batch: 0.0994 (0.1008) Data: 0.0019 (0.0024) Loss: 0.0610 (0.0336)
[2023/01/16 05:28] | TRAIN(096): [2000/2211] Batch: 0.0981 (0.1006) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0339)
[2023/01/16 05:28] | TRAIN(096): [2050/2211] Batch: 0.0905 (0.1004) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0342)
[2023/01/16 05:28] | TRAIN(096): [2100/2211] Batch: 0.0927 (0.1003) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0340)
[2023/01/16 05:28] | TRAIN(096): [2150/2211] Batch: 0.0928 (0.1002) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0339)
[2023/01/16 05:28] | TRAIN(096): [2200/2211] Batch: 0.0927 (0.1001) Data: 0.0016 (0.0024) Loss: 0.0038 (0.0338)
[2023/01/16 05:28] | ------------------------------------------------------------
[2023/01/16 05:28] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:28] | ------------------------------------------------------------
[2023/01/16 05:28] |    TRAIN(96)     0:03:41     0:00:05     0:03:36      0.0337
[2023/01/16 05:28] | ------------------------------------------------------------
[2023/01/16 05:28] | **************************************************
[2023/01/16 05:28] | TRAIN(097): [  50/2211] Batch: 0.0988 (0.1178) Data: 0.0018 (0.0232) Loss: 0.1002 (0.0360)
[2023/01/16 05:28] | TRAIN(097): [ 100/2211] Batch: 0.1105 (0.1097) Data: 0.0017 (0.0126) Loss: 0.0850 (0.0372)
[2023/01/16 05:28] | TRAIN(097): [ 150/2211] Batch: 0.1000 (0.1072) Data: 0.0018 (0.0091) Loss: 0.0000 (0.0357)
[2023/01/16 05:28] | TRAIN(097): [ 200/2211] Batch: 0.1016 (0.1038) Data: 0.0018 (0.0073) Loss: 0.0276 (0.0346)
[2023/01/16 05:29] | TRAIN(097): [ 250/2211] Batch: 0.0877 (0.1014) Data: 0.0017 (0.0062) Loss: 0.0212 (0.0338)
[2023/01/16 05:29] | TRAIN(097): [ 300/2211] Batch: 0.0923 (0.1001) Data: 0.0017 (0.0054) Loss: 0.0000 (0.0311)
[2023/01/16 05:29] | TRAIN(097): [ 350/2211] Batch: 0.0943 (0.0994) Data: 0.0018 (0.0049) Loss: 0.0000 (0.0320)
[2023/01/16 05:29] | TRAIN(097): [ 400/2211] Batch: 0.1196 (0.0992) Data: 0.0022 (0.0045) Loss: 0.0000 (0.0320)
[2023/01/16 05:29] | TRAIN(097): [ 450/2211] Batch: 0.0948 (0.0995) Data: 0.0018 (0.0043) Loss: 0.0000 (0.0329)
[2023/01/16 05:29] | TRAIN(097): [ 500/2211] Batch: 0.0956 (0.0993) Data: 0.0017 (0.0040) Loss: 0.0078 (0.0320)
[2023/01/16 05:29] | TRAIN(097): [ 550/2211] Batch: 0.0927 (0.0989) Data: 0.0016 (0.0038) Loss: 0.0266 (0.0321)
[2023/01/16 05:29] | TRAIN(097): [ 600/2211] Batch: 0.0925 (0.0986) Data: 0.0022 (0.0036) Loss: 0.0358 (0.0321)
[2023/01/16 05:29] | TRAIN(097): [ 650/2211] Batch: 0.1228 (0.0986) Data: 0.0022 (0.0035) Loss: 0.0000 (0.0306)
[2023/01/16 05:29] | TRAIN(097): [ 700/2211] Batch: 0.0893 (0.0987) Data: 0.0018 (0.0034) Loss: 0.0347 (0.0311)
[2023/01/16 05:29] | TRAIN(097): [ 750/2211] Batch: 0.1054 (0.0987) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0315)
[2023/01/16 05:29] | TRAIN(097): [ 800/2211] Batch: 0.0949 (0.0986) Data: 0.0022 (0.0032) Loss: 0.0000 (0.0321)
[2023/01/16 05:30] | TRAIN(097): [ 850/2211] Batch: 0.1003 (0.0986) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0321)
[2023/01/16 05:30] | TRAIN(097): [ 900/2211] Batch: 0.0981 (0.0986) Data: 0.0018 (0.0030) Loss: 0.0097 (0.0318)
[2023/01/16 05:30] | TRAIN(097): [ 950/2211] Batch: 0.1087 (0.0988) Data: 0.0019 (0.0030) Loss: 0.0039 (0.0317)
[2023/01/16 05:30] | TRAIN(097): [1000/2211] Batch: 0.0916 (0.0986) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0316)
[2023/01/16 05:30] | TRAIN(097): [1050/2211] Batch: 0.0960 (0.0986) Data: 0.0017 (0.0029) Loss: 0.0455 (0.0319)
[2023/01/16 05:30] | TRAIN(097): [1100/2211] Batch: 0.0920 (0.0984) Data: 0.0014 (0.0028) Loss: 0.0753 (0.0324)
[2023/01/16 05:30] | TRAIN(097): [1150/2211] Batch: 0.0891 (0.0982) Data: 0.0018 (0.0028) Loss: 0.0501 (0.0322)
[2023/01/16 05:30] | TRAIN(097): [1200/2211] Batch: 0.0974 (0.0983) Data: 0.0018 (0.0027) Loss: 0.1019 (0.0325)
[2023/01/16 05:30] | TRAIN(097): [1250/2211] Batch: 0.1018 (0.0988) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0323)
[2023/01/16 05:30] | TRAIN(097): [1300/2211] Batch: 0.0971 (0.0989) Data: 0.0017 (0.0027) Loss: 0.0495 (0.0320)
[2023/01/16 05:30] | TRAIN(097): [1350/2211] Batch: 0.0957 (0.0986) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0318)
[2023/01/16 05:30] | TRAIN(097): [1400/2211] Batch: 0.0873 (0.0983) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0323)
[2023/01/16 05:30] | TRAIN(097): [1450/2211] Batch: 0.0978 (0.0981) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0324)
[2023/01/16 05:31] | TRAIN(097): [1500/2211] Batch: 0.1013 (0.0980) Data: 0.0018 (0.0025) Loss: 0.2062 (0.0326)
[2023/01/16 05:31] | TRAIN(097): [1550/2211] Batch: 0.0879 (0.0978) Data: 0.0017 (0.0025) Loss: 0.0197 (0.0326)
[2023/01/16 05:31] | TRAIN(097): [1600/2211] Batch: 0.0908 (0.0977) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0329)
[2023/01/16 05:31] | TRAIN(097): [1650/2211] Batch: 0.1102 (0.0976) Data: 0.0018 (0.0025) Loss: 0.0547 (0.0331)
[2023/01/16 05:31] | TRAIN(097): [1700/2211] Batch: 0.0928 (0.0975) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0330)
[2023/01/16 05:31] | TRAIN(097): [1750/2211] Batch: 0.0949 (0.0974) Data: 0.0018 (0.0024) Loss: 0.1625 (0.0331)
[2023/01/16 05:31] | TRAIN(097): [1800/2211] Batch: 0.1017 (0.0973) Data: 0.0018 (0.0024) Loss: 0.0193 (0.0329)
[2023/01/16 05:31] | TRAIN(097): [1850/2211] Batch: 0.0905 (0.0972) Data: 0.0017 (0.0024) Loss: 0.1335 (0.0330)
[2023/01/16 05:31] | TRAIN(097): [1900/2211] Batch: 0.0908 (0.0971) Data: 0.0017 (0.0024) Loss: 0.0442 (0.0328)
[2023/01/16 05:31] | TRAIN(097): [1950/2211] Batch: 0.0908 (0.0970) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0328)
[2023/01/16 05:31] | TRAIN(097): [2000/2211] Batch: 0.0958 (0.0969) Data: 0.0018 (0.0023) Loss: 0.0800 (0.0327)
[2023/01/16 05:31] | TRAIN(097): [2050/2211] Batch: 0.0942 (0.0969) Data: 0.0018 (0.0023) Loss: 0.0174 (0.0326)
[2023/01/16 05:32] | TRAIN(097): [2100/2211] Batch: 0.0907 (0.0969) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0323)
[2023/01/16 05:32] | TRAIN(097): [2150/2211] Batch: 0.0914 (0.0969) Data: 0.0018 (0.0023) Loss: 0.0734 (0.0322)
[2023/01/16 05:32] | TRAIN(097): [2200/2211] Batch: 0.0966 (0.0969) Data: 0.0021 (0.0023) Loss: 0.0770 (0.0324)
[2023/01/16 05:32] | ------------------------------------------------------------
[2023/01/16 05:32] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:32] | ------------------------------------------------------------
[2023/01/16 05:32] |    TRAIN(97)     0:03:34     0:00:05     0:03:29      0.0325
[2023/01/16 05:32] | ------------------------------------------------------------
[2023/01/16 05:32] | **************************************************
[2023/01/16 05:32] | TRAIN(098): [  50/2211] Batch: 0.1423 (0.1421) Data: 0.0019 (0.0257) Loss: 0.0395 (0.0459)
[2023/01/16 05:32] | TRAIN(098): [ 100/2211] Batch: 0.0872 (0.1223) Data: 0.0019 (0.0139) Loss: 0.0415 (0.0407)
[2023/01/16 05:32] | TRAIN(098): [ 150/2211] Batch: 0.0909 (0.1180) Data: 0.0017 (0.0100) Loss: 0.1772 (0.0372)
[2023/01/16 05:32] | TRAIN(098): [ 200/2211] Batch: 0.1177 (0.1152) Data: 0.0019 (0.0080) Loss: 0.1138 (0.0328)
[2023/01/16 05:32] | TRAIN(098): [ 250/2211] Batch: 0.1109 (0.1151) Data: 0.0017 (0.0067) Loss: 0.0426 (0.0319)
[2023/01/16 05:32] | TRAIN(098): [ 300/2211] Batch: 0.0826 (0.1124) Data: 0.0018 (0.0060) Loss: 0.0000 (0.0328)
[2023/01/16 05:32] | TRAIN(098): [ 350/2211] Batch: 0.1175 (0.1108) Data: 0.0024 (0.0054) Loss: 0.0963 (0.0325)
[2023/01/16 05:32] | TRAIN(098): [ 400/2211] Batch: 0.0879 (0.1103) Data: 0.0018 (0.0050) Loss: 0.1041 (0.0307)
[2023/01/16 05:33] | TRAIN(098): [ 450/2211] Batch: 0.1018 (0.1090) Data: 0.0018 (0.0046) Loss: 0.0000 (0.0314)
[2023/01/16 05:33] | TRAIN(098): [ 500/2211] Batch: 0.0963 (0.1074) Data: 0.0017 (0.0043) Loss: 0.0000 (0.0302)
[2023/01/16 05:33] | TRAIN(098): [ 550/2211] Batch: 0.0909 (0.1063) Data: 0.0018 (0.0041) Loss: 0.0000 (0.0306)
[2023/01/16 05:33] | TRAIN(098): [ 600/2211] Batch: 0.0874 (0.1050) Data: 0.0018 (0.0039) Loss: 0.0000 (0.0301)
[2023/01/16 05:33] | TRAIN(098): [ 650/2211] Batch: 0.0879 (0.1044) Data: 0.0017 (0.0038) Loss: 0.1531 (0.0310)
[2023/01/16 05:33] | TRAIN(098): [ 700/2211] Batch: 0.0968 (0.1039) Data: 0.0018 (0.0036) Loss: 0.1479 (0.0310)
[2023/01/16 05:33] | TRAIN(098): [ 750/2211] Batch: 0.0957 (0.1039) Data: 0.0019 (0.0035) Loss: 0.0000 (0.0306)
[2023/01/16 05:33] | TRAIN(098): [ 800/2211] Batch: 0.1113 (0.1041) Data: 0.0019 (0.0034) Loss: 0.0020 (0.0303)
[2023/01/16 05:33] | TRAIN(098): [ 850/2211] Batch: 0.0921 (0.1045) Data: 0.0018 (0.0033) Loss: 0.0161 (0.0305)
[2023/01/16 05:33] | TRAIN(098): [ 900/2211] Batch: 0.0885 (0.1041) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0302)
[2023/01/16 05:33] | TRAIN(098): [ 950/2211] Batch: 0.0918 (0.1035) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0300)
[2023/01/16 05:33] | TRAIN(098): [1000/2211] Batch: 0.0871 (0.1029) Data: 0.0018 (0.0031) Loss: 0.0089 (0.0306)
[2023/01/16 05:33] | TRAIN(098): [1050/2211] Batch: 0.0849 (0.1027) Data: 0.0017 (0.0030) Loss: 0.0626 (0.0304)
[2023/01/16 05:34] | TRAIN(098): [1100/2211] Batch: 0.1013 (0.1025) Data: 0.0019 (0.0029) Loss: 0.0643 (0.0302)
[2023/01/16 05:34] | TRAIN(098): [1150/2211] Batch: 0.0870 (0.1022) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0303)
[2023/01/16 05:34] | TRAIN(098): [1200/2211] Batch: 0.1004 (0.1019) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0304)
[2023/01/16 05:34] | TRAIN(098): [1250/2211] Batch: 0.1085 (0.1017) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0305)
[2023/01/16 05:34] | TRAIN(098): [1300/2211] Batch: 0.0920 (0.1018) Data: 0.0018 (0.0028) Loss: 0.0047 (0.0303)
[2023/01/16 05:34] | TRAIN(098): [1350/2211] Batch: 0.0970 (0.1017) Data: 0.0020 (0.0027) Loss: 0.1136 (0.0305)
[2023/01/16 05:34] | TRAIN(098): [1400/2211] Batch: 0.1010 (0.1015) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0305)
[2023/01/16 05:34] | TRAIN(098): [1450/2211] Batch: 0.1055 (0.1014) Data: 0.0020 (0.0027) Loss: 0.0349 (0.0307)
[2023/01/16 05:34] | TRAIN(098): [1500/2211] Batch: 0.0954 (0.1014) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0308)
[2023/01/16 05:34] | TRAIN(098): [1550/2211] Batch: 0.0936 (0.1012) Data: 0.0019 (0.0026) Loss: 0.0235 (0.0310)
[2023/01/16 05:34] | TRAIN(098): [1600/2211] Batch: 0.0949 (0.1010) Data: 0.0019 (0.0026) Loss: 0.0373 (0.0312)
[2023/01/16 05:34] | TRAIN(098): [1650/2211] Batch: 0.0965 (0.1009) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0314)
[2023/01/16 05:35] | TRAIN(098): [1700/2211] Batch: 0.0903 (0.1007) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0315)
[2023/01/16 05:35] | TRAIN(098): [1750/2211] Batch: 0.0947 (0.1005) Data: 0.0018 (0.0025) Loss: 0.0768 (0.0314)
[2023/01/16 05:35] | TRAIN(098): [1800/2211] Batch: 0.0910 (0.1004) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0314)
[2023/01/16 05:35] | TRAIN(098): [1850/2211] Batch: 0.1065 (0.1003) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0314)
[2023/01/16 05:35] | TRAIN(098): [1900/2211] Batch: 0.1227 (0.1007) Data: 0.0023 (0.0025) Loss: 0.0218 (0.0317)
[2023/01/16 05:35] | TRAIN(098): [1950/2211] Batch: 0.0906 (0.1009) Data: 0.0019 (0.0025) Loss: 0.0015 (0.0317)
[2023/01/16 05:35] | TRAIN(098): [2000/2211] Batch: 0.1017 (0.1010) Data: 0.0019 (0.0025) Loss: 0.1209 (0.0316)
[2023/01/16 05:35] | TRAIN(098): [2050/2211] Batch: 0.1303 (0.1013) Data: 0.0022 (0.0024) Loss: 0.0001 (0.0315)
[2023/01/16 05:35] | TRAIN(098): [2100/2211] Batch: 0.0946 (0.1016) Data: 0.0014 (0.0024) Loss: 0.0000 (0.0318)
[2023/01/16 05:35] | TRAIN(098): [2150/2211] Batch: 0.0924 (0.1015) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0318)
[2023/01/16 05:35] | TRAIN(098): [2200/2211] Batch: 0.1076 (0.1016) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0319)
[2023/01/16 05:35] | ------------------------------------------------------------
[2023/01/16 05:35] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:35] | ------------------------------------------------------------
[2023/01/16 05:35] |    TRAIN(98)     0:03:44     0:00:05     0:03:39      0.0318
[2023/01/16 05:35] | ------------------------------------------------------------
[2023/01/16 05:35] | **************************************************
[2023/01/16 05:36] | TRAIN(099): [  50/2211] Batch: 0.1220 (0.1321) Data: 0.0023 (0.0243) Loss: 0.0000 (0.0337)
[2023/01/16 05:36] | TRAIN(099): [ 100/2211] Batch: 0.1243 (0.1271) Data: 0.0015 (0.0132) Loss: 0.0056 (0.0297)
[2023/01/16 05:36] | TRAIN(099): [ 150/2211] Batch: 0.0983 (0.1267) Data: 0.0014 (0.0095) Loss: 0.0000 (0.0290)
[2023/01/16 05:36] | TRAIN(099): [ 200/2211] Batch: 0.1271 (0.1239) Data: 0.0022 (0.0076) Loss: 0.0000 (0.0264)
[2023/01/16 05:36] | TRAIN(099): [ 250/2211] Batch: 0.1223 (0.1243) Data: 0.0019 (0.0065) Loss: 0.0511 (0.0263)
[2023/01/16 05:36] | TRAIN(099): [ 300/2211] Batch: 0.1330 (0.1239) Data: 0.0023 (0.0058) Loss: 0.0241 (0.0282)
[2023/01/16 05:36] | TRAIN(099): [ 350/2211] Batch: 0.1212 (0.1222) Data: 0.0021 (0.0053) Loss: 0.0000 (0.0286)
[2023/01/16 05:36] | TRAIN(099): [ 400/2211] Batch: 0.0919 (0.1205) Data: 0.0019 (0.0049) Loss: 0.0233 (0.0286)
[2023/01/16 05:36] | TRAIN(099): [ 450/2211] Batch: 0.0915 (0.1176) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0276)
[2023/01/16 05:36] | TRAIN(099): [ 500/2211] Batch: 0.0919 (0.1153) Data: 0.0017 (0.0043) Loss: 0.1653 (0.0279)
[2023/01/16 05:36] | TRAIN(099): [ 550/2211] Batch: 0.1015 (0.1141) Data: 0.0018 (0.0040) Loss: 0.0625 (0.0285)
[2023/01/16 05:37] | TRAIN(099): [ 600/2211] Batch: 0.0955 (0.1127) Data: 0.0019 (0.0039) Loss: 0.0000 (0.0290)
[2023/01/16 05:37] | TRAIN(099): [ 650/2211] Batch: 0.0903 (0.1114) Data: 0.0018 (0.0037) Loss: 0.0885 (0.0290)
[2023/01/16 05:37] | TRAIN(099): [ 700/2211] Batch: 0.0916 (0.1101) Data: 0.0019 (0.0036) Loss: 0.0000 (0.0291)
[2023/01/16 05:37] | TRAIN(099): [ 750/2211] Batch: 0.0955 (0.1094) Data: 0.0019 (0.0035) Loss: 0.0679 (0.0290)
[2023/01/16 05:37] | TRAIN(099): [ 800/2211] Batch: 0.1215 (0.1085) Data: 0.0022 (0.0034) Loss: 0.0186 (0.0297)
[2023/01/16 05:37] | TRAIN(099): [ 850/2211] Batch: 0.1013 (0.1076) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0314)
[2023/01/16 05:37] | TRAIN(099): [ 900/2211] Batch: 0.0883 (0.1069) Data: 0.0018 (0.0032) Loss: 0.0597 (0.0312)
[2023/01/16 05:37] | TRAIN(099): [ 950/2211] Batch: 0.0913 (0.1063) Data: 0.0018 (0.0031) Loss: 0.0790 (0.0313)
[2023/01/16 05:37] | TRAIN(099): [1000/2211] Batch: 0.1048 (0.1058) Data: 0.0016 (0.0030) Loss: 0.0456 (0.0311)
[2023/01/16 05:37] | TRAIN(099): [1050/2211] Batch: 0.0955 (0.1055) Data: 0.0018 (0.0030) Loss: 0.0404 (0.0308)
[2023/01/16 05:37] | TRAIN(099): [1100/2211] Batch: 0.0987 (0.1051) Data: 0.0018 (0.0029) Loss: 0.0044 (0.0307)
[2023/01/16 05:37] | TRAIN(099): [1150/2211] Batch: 0.0884 (0.1046) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0306)
[2023/01/16 05:38] | TRAIN(099): [1200/2211] Batch: 0.1009 (0.1042) Data: 0.0019 (0.0028) Loss: 0.0029 (0.0305)
[2023/01/16 05:38] | TRAIN(099): [1250/2211] Batch: 0.0994 (0.1044) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0302)
[2023/01/16 05:38] | TRAIN(099): [1300/2211] Batch: 0.0917 (0.1041) Data: 0.0018 (0.0028) Loss: 0.0777 (0.0302)
[2023/01/16 05:38] | TRAIN(099): [1350/2211] Batch: 0.0877 (0.1037) Data: 0.0017 (0.0027) Loss: 0.0338 (0.0302)
[2023/01/16 05:38] | TRAIN(099): [1400/2211] Batch: 0.0889 (0.1035) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0301)
[2023/01/16 05:38] | TRAIN(099): [1450/2211] Batch: 0.0924 (0.1033) Data: 0.0018 (0.0027) Loss: 0.0095 (0.0299)
[2023/01/16 05:38] | TRAIN(099): [1500/2211] Batch: 0.1197 (0.1031) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0303)
[2023/01/16 05:38] | TRAIN(099): [1550/2211] Batch: 0.0942 (0.1034) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0305)
[2023/01/16 05:38] | TRAIN(099): [1600/2211] Batch: 0.0930 (0.1031) Data: 0.0018 (0.0026) Loss: 0.0806 (0.0306)
[2023/01/16 05:38] | TRAIN(099): [1650/2211] Batch: 0.0903 (0.1030) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0308)
[2023/01/16 05:38] | TRAIN(099): [1700/2211] Batch: 0.0975 (0.1029) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0308)
[2023/01/16 05:38] | TRAIN(099): [1750/2211] Batch: 0.0960 (0.1026) Data: 0.0018 (0.0025) Loss: 0.0739 (0.0315)
[2023/01/16 05:39] | TRAIN(099): [1800/2211] Batch: 0.0989 (0.1025) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0316)
[2023/01/16 05:39] | TRAIN(099): [1850/2211] Batch: 0.0875 (0.1023) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0317)
[2023/01/16 05:39] | TRAIN(099): [1900/2211] Batch: 0.1196 (0.1021) Data: 0.0022 (0.0025) Loss: 0.0204 (0.0318)
[2023/01/16 05:39] | TRAIN(099): [1950/2211] Batch: 0.1034 (0.1022) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0317)
[2023/01/16 05:39] | TRAIN(099): [2000/2211] Batch: 0.0932 (0.1022) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0319)
[2023/01/16 05:39] | TRAIN(099): [2050/2211] Batch: 0.0957 (0.1020) Data: 0.0018 (0.0024) Loss: 0.0196 (0.0318)
[2023/01/16 05:39] | TRAIN(099): [2100/2211] Batch: 0.0941 (0.1020) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0318)
[2023/01/16 05:39] | TRAIN(099): [2150/2211] Batch: 0.0993 (0.1018) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0323)
[2023/01/16 05:39] | TRAIN(099): [2200/2211] Batch: 0.0955 (0.1018) Data: 0.0017 (0.0024) Loss: 0.0070 (0.0326)
[2023/01/16 05:39] | ------------------------------------------------------------
[2023/01/16 05:39] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:39] | ------------------------------------------------------------
[2023/01/16 05:39] |    TRAIN(99)     0:03:44     0:00:05     0:03:39      0.0326
[2023/01/16 05:39] | ------------------------------------------------------------
[2023/01/16 05:39] | **************************************************
[2023/01/16 05:39] | TRAIN(100): [  50/2211] Batch: 0.0954 (0.1288) Data: 0.0018 (0.0258) Loss: 0.0000 (0.0275)
[2023/01/16 05:39] | TRAIN(100): [ 100/2211] Batch: 0.0904 (0.1119) Data: 0.0018 (0.0139) Loss: 0.0179 (0.0321)
[2023/01/16 05:39] | TRAIN(100): [ 150/2211] Batch: 0.0944 (0.1059) Data: 0.0018 (0.0099) Loss: 0.0503 (0.0306)
[2023/01/16 05:40] | TRAIN(100): [ 200/2211] Batch: 0.1176 (0.1049) Data: 0.0024 (0.0079) Loss: 0.0000 (0.0359)
[2023/01/16 05:40] | TRAIN(100): [ 250/2211] Batch: 0.1238 (0.1060) Data: 0.0022 (0.0068) Loss: 0.1368 (0.0381)
[2023/01/16 05:40] | TRAIN(100): [ 300/2211] Batch: 0.0914 (0.1044) Data: 0.0019 (0.0060) Loss: 0.0110 (0.0406)
[2023/01/16 05:40] | TRAIN(100): [ 350/2211] Batch: 0.1074 (0.1034) Data: 0.0018 (0.0054) Loss: 0.0976 (0.0395)
[2023/01/16 05:40] | TRAIN(100): [ 400/2211] Batch: 0.1297 (0.1043) Data: 0.0026 (0.0050) Loss: 0.0824 (0.0408)
[2023/01/16 05:40] | TRAIN(100): [ 450/2211] Batch: 0.0974 (0.1038) Data: 0.0017 (0.0046) Loss: 0.0691 (0.0406)
[2023/01/16 05:40] | TRAIN(100): [ 500/2211] Batch: 0.0989 (0.1029) Data: 0.0017 (0.0043) Loss: 0.0000 (0.0391)
[2023/01/16 05:40] | TRAIN(100): [ 550/2211] Batch: 0.0908 (0.1022) Data: 0.0018 (0.0041) Loss: 0.0000 (0.0387)
[2023/01/16 05:40] | TRAIN(100): [ 600/2211] Batch: 0.1009 (0.1017) Data: 0.0018 (0.0039) Loss: 0.0035 (0.0378)
[2023/01/16 05:40] | TRAIN(100): [ 650/2211] Batch: 0.1099 (0.1016) Data: 0.0015 (0.0038) Loss: 0.0000 (0.0365)
[2023/01/16 05:40] | TRAIN(100): [ 700/2211] Batch: 0.0831 (0.1010) Data: 0.0018 (0.0036) Loss: 0.1083 (0.0352)
[2023/01/16 05:40] | TRAIN(100): [ 750/2211] Batch: 0.0987 (0.1006) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0352)
[2023/01/16 05:41] | TRAIN(100): [ 800/2211] Batch: 0.0902 (0.1003) Data: 0.0013 (0.0034) Loss: 0.0255 (0.0356)
[2023/01/16 05:41] | TRAIN(100): [ 850/2211] Batch: 0.0958 (0.1001) Data: 0.0015 (0.0033) Loss: 0.0182 (0.0346)
[2023/01/16 05:41] | TRAIN(100): [ 900/2211] Batch: 0.0908 (0.0998) Data: 0.0017 (0.0032) Loss: 0.0576 (0.0342)
[2023/01/16 05:41] | TRAIN(100): [ 950/2211] Batch: 0.0903 (0.0995) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0338)
[2023/01/16 05:41] | TRAIN(100): [1000/2211] Batch: 0.0978 (0.0997) Data: 0.0015 (0.0031) Loss: 0.1498 (0.0337)
[2023/01/16 05:41] | TRAIN(100): [1050/2211] Batch: 0.1045 (0.0995) Data: 0.0014 (0.0030) Loss: 0.0446 (0.0336)
[2023/01/16 05:41] | TRAIN(100): [1100/2211] Batch: 0.0889 (0.0992) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0334)
[2023/01/16 05:41] | TRAIN(100): [1150/2211] Batch: 0.1001 (0.0993) Data: 0.0021 (0.0029) Loss: 0.0072 (0.0332)
[2023/01/16 05:41] | TRAIN(100): [1200/2211] Batch: 0.1020 (0.0993) Data: 0.0019 (0.0028) Loss: 0.0149 (0.0333)
[2023/01/16 05:41] | TRAIN(100): [1250/2211] Batch: 0.0915 (0.0992) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0332)
[2023/01/16 05:41] | TRAIN(100): [1300/2211] Batch: 0.0996 (0.0992) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0328)
[2023/01/16 05:41] | TRAIN(100): [1350/2211] Batch: 0.0943 (0.0992) Data: 0.0018 (0.0027) Loss: 0.0029 (0.0326)
[2023/01/16 05:41] | TRAIN(100): [1400/2211] Batch: 0.0867 (0.0990) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0326)
[2023/01/16 05:42] | TRAIN(100): [1450/2211] Batch: 0.0963 (0.0987) Data: 0.0018 (0.0027) Loss: 0.0990 (0.0329)
[2023/01/16 05:42] | TRAIN(100): [1500/2211] Batch: 0.0918 (0.0987) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0329)
[2023/01/16 05:42] | TRAIN(100): [1550/2211] Batch: 0.1020 (0.0987) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0327)
[2023/01/16 05:42] | TRAIN(100): [1600/2211] Batch: 0.0897 (0.0987) Data: 0.0018 (0.0026) Loss: 0.1033 (0.0325)
[2023/01/16 05:42] | TRAIN(100): [1650/2211] Batch: 0.0872 (0.0986) Data: 0.0017 (0.0026) Loss: 0.1042 (0.0322)
[2023/01/16 05:42] | TRAIN(100): [1700/2211] Batch: 0.0945 (0.0984) Data: 0.0019 (0.0025) Loss: 0.1478 (0.0328)
[2023/01/16 05:42] | TRAIN(100): [1750/2211] Batch: 0.0970 (0.0982) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0326)
[2023/01/16 05:42] | TRAIN(100): [1800/2211] Batch: 0.0920 (0.0982) Data: 0.0022 (0.0025) Loss: 0.0103 (0.0325)
[2023/01/16 05:42] | TRAIN(100): [1850/2211] Batch: 0.0875 (0.0983) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0325)
[2023/01/16 05:42] | TRAIN(100): [1900/2211] Batch: 0.0830 (0.0983) Data: 0.0017 (0.0025) Loss: 0.0831 (0.0323)
[2023/01/16 05:42] | TRAIN(100): [1950/2211] Batch: 0.0937 (0.0983) Data: 0.0020 (0.0024) Loss: 0.1349 (0.0323)
[2023/01/16 05:42] | TRAIN(100): [2000/2211] Batch: 0.0913 (0.0983) Data: 0.0017 (0.0024) Loss: 0.1166 (0.0325)
[2023/01/16 05:43] | TRAIN(100): [2050/2211] Batch: 0.0904 (0.0981) Data: 0.0016 (0.0024) Loss: 0.0366 (0.0329)
[2023/01/16 05:43] | TRAIN(100): [2100/2211] Batch: 0.1024 (0.0982) Data: 0.0020 (0.0024) Loss: 0.0953 (0.0331)
[2023/01/16 05:43] | TRAIN(100): [2150/2211] Batch: 0.0892 (0.0981) Data: 0.0018 (0.0024) Loss: 0.0716 (0.0332)
[2023/01/16 05:43] | TRAIN(100): [2200/2211] Batch: 0.0879 (0.0980) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0332)
[2023/01/16 05:43] | ------------------------------------------------------------
[2023/01/16 05:43] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:43] | ------------------------------------------------------------
[2023/01/16 05:43] |   TRAIN(100)     0:03:36     0:00:05     0:03:31      0.0332
[2023/01/16 05:43] | ------------------------------------------------------------
[2023/01/16 05:43] | **************************************************
[2023/01/16 05:43] | TRAIN(101): [  50/2211] Batch: 0.0952 (0.1218) Data: 0.0017 (0.0234) Loss: 0.0582 (0.0212)
[2023/01/16 05:43] | TRAIN(101): [ 100/2211] Batch: 0.0890 (0.1090) Data: 0.0018 (0.0127) Loss: 0.0040 (0.0281)
[2023/01/16 05:43] | TRAIN(101): [ 150/2211] Batch: 0.0933 (0.1053) Data: 0.0019 (0.0091) Loss: 0.0277 (0.0290)
[2023/01/16 05:43] | TRAIN(101): [ 200/2211] Batch: 0.0864 (0.1042) Data: 0.0016 (0.0073) Loss: 0.0143 (0.0286)
[2023/01/16 05:43] | TRAIN(101): [ 250/2211] Batch: 0.0970 (0.1025) Data: 0.0018 (0.0062) Loss: 0.0000 (0.0292)
[2023/01/16 05:43] | TRAIN(101): [ 300/2211] Batch: 0.0952 (0.1022) Data: 0.0016 (0.0055) Loss: 0.0000 (0.0283)
[2023/01/16 05:43] | TRAIN(101): [ 350/2211] Batch: 0.0914 (0.1014) Data: 0.0018 (0.0050) Loss: 0.0000 (0.0278)
[2023/01/16 05:43] | TRAIN(101): [ 400/2211] Batch: 0.1046 (0.1006) Data: 0.0018 (0.0046) Loss: 0.0162 (0.0281)
[2023/01/16 05:44] | TRAIN(101): [ 450/2211] Batch: 0.0906 (0.1000) Data: 0.0017 (0.0042) Loss: 0.0000 (0.0282)
[2023/01/16 05:44] | TRAIN(101): [ 500/2211] Batch: 0.0965 (0.0998) Data: 0.0016 (0.0040) Loss: 0.0000 (0.0275)
[2023/01/16 05:44] | TRAIN(101): [ 550/2211] Batch: 0.1198 (0.1007) Data: 0.0022 (0.0038) Loss: 0.0377 (0.0283)
[2023/01/16 05:44] | TRAIN(101): [ 600/2211] Batch: 0.0977 (0.1013) Data: 0.0022 (0.0037) Loss: 0.0693 (0.0280)
[2023/01/16 05:44] | TRAIN(101): [ 650/2211] Batch: 0.0907 (0.1007) Data: 0.0018 (0.0035) Loss: 0.1603 (0.0293)
[2023/01/16 05:44] | TRAIN(101): [ 700/2211] Batch: 0.0970 (0.1004) Data: 0.0017 (0.0034) Loss: 0.0030 (0.0299)
[2023/01/16 05:44] | TRAIN(101): [ 750/2211] Batch: 0.0895 (0.1000) Data: 0.0018 (0.0033) Loss: 0.0412 (0.0301)
[2023/01/16 05:44] | TRAIN(101): [ 800/2211] Batch: 0.0882 (0.1004) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0296)
[2023/01/16 05:44] | TRAIN(101): [ 850/2211] Batch: 0.0973 (0.1010) Data: 0.0017 (0.0031) Loss: 0.0622 (0.0291)
[2023/01/16 05:44] | TRAIN(101): [ 900/2211] Batch: 0.0950 (0.1011) Data: 0.0017 (0.0031) Loss: 0.0053 (0.0290)
[2023/01/16 05:44] | TRAIN(101): [ 950/2211] Batch: 0.0914 (0.1009) Data: 0.0020 (0.0030) Loss: 0.0000 (0.0285)
[2023/01/16 05:44] | TRAIN(101): [1000/2211] Batch: 0.0919 (0.1006) Data: 0.0017 (0.0029) Loss: 0.0049 (0.0287)
[2023/01/16 05:45] | TRAIN(101): [1050/2211] Batch: 0.0951 (0.1004) Data: 0.0018 (0.0029) Loss: 0.0027 (0.0285)
[2023/01/16 05:45] | TRAIN(101): [1100/2211] Batch: 0.0955 (0.1001) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0285)
[2023/01/16 05:45] | TRAIN(101): [1150/2211] Batch: 0.0965 (0.0998) Data: 0.0017 (0.0028) Loss: 0.0035 (0.0285)
[2023/01/16 05:45] | TRAIN(101): [1200/2211] Batch: 0.0862 (0.0996) Data: 0.0018 (0.0027) Loss: 0.0103 (0.0285)
[2023/01/16 05:45] | TRAIN(101): [1250/2211] Batch: 0.0963 (0.0996) Data: 0.0017 (0.0027) Loss: 0.0078 (0.0286)
[2023/01/16 05:45] | TRAIN(101): [1300/2211] Batch: 0.0906 (0.0995) Data: 0.0017 (0.0027) Loss: 0.0082 (0.0286)
[2023/01/16 05:45] | TRAIN(101): [1350/2211] Batch: 0.1014 (0.0998) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0285)
[2023/01/16 05:45] | TRAIN(101): [1400/2211] Batch: 0.1023 (0.0997) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0288)
[2023/01/16 05:45] | TRAIN(101): [1450/2211] Batch: 0.1029 (0.0997) Data: 0.0017 (0.0026) Loss: 0.0307 (0.0290)
[2023/01/16 05:45] | TRAIN(101): [1500/2211] Batch: 0.0907 (0.0997) Data: 0.0018 (0.0026) Loss: 0.1470 (0.0292)
[2023/01/16 05:45] | TRAIN(101): [1550/2211] Batch: 0.1074 (0.0996) Data: 0.0019 (0.0025) Loss: 0.0403 (0.0293)
[2023/01/16 05:45] | TRAIN(101): [1600/2211] Batch: 0.0914 (0.0995) Data: 0.0018 (0.0025) Loss: 0.0280 (0.0296)
[2023/01/16 05:46] | TRAIN(101): [1650/2211] Batch: 0.0897 (0.0997) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0296)
[2023/01/16 05:46] | TRAIN(101): [1700/2211] Batch: 0.0979 (0.0995) Data: 0.0018 (0.0025) Loss: 0.0001 (0.0301)
[2023/01/16 05:46] | TRAIN(101): [1750/2211] Batch: 0.1219 (0.0997) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0305)
[2023/01/16 05:46] | TRAIN(101): [1800/2211] Batch: 0.1083 (0.0999) Data: 0.0020 (0.0025) Loss: 0.0000 (0.0305)
[2023/01/16 05:46] | TRAIN(101): [1850/2211] Batch: 0.0977 (0.0999) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0305)
[2023/01/16 05:46] | TRAIN(101): [1900/2211] Batch: 0.0969 (0.0998) Data: 0.0019 (0.0024) Loss: 0.1057 (0.0307)
[2023/01/16 05:46] | TRAIN(101): [1950/2211] Batch: 0.0950 (0.0997) Data: 0.0017 (0.0024) Loss: 0.0018 (0.0307)
[2023/01/16 05:46] | TRAIN(101): [2000/2211] Batch: 0.0973 (0.0997) Data: 0.0018 (0.0024) Loss: 0.0296 (0.0303)
[2023/01/16 05:46] | TRAIN(101): [2050/2211] Batch: 0.0939 (0.0996) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0302)
[2023/01/16 05:46] | TRAIN(101): [2100/2211] Batch: 0.0969 (0.0996) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0300)
[2023/01/16 05:46] | TRAIN(101): [2150/2211] Batch: 0.0904 (0.0998) Data: 0.0017 (0.0024) Loss: 0.0492 (0.0301)
[2023/01/16 05:46] | TRAIN(101): [2200/2211] Batch: 0.1002 (0.0997) Data: 0.0017 (0.0023) Loss: 0.0740 (0.0301)
[2023/01/16 05:46] | ------------------------------------------------------------
[2023/01/16 05:46] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:46] | ------------------------------------------------------------
[2023/01/16 05:46] |   TRAIN(101)     0:03:40     0:00:05     0:03:35      0.0300
[2023/01/16 05:46] | ------------------------------------------------------------
[2023/01/16 05:46] | **************************************************
[2023/01/16 05:47] | TRAIN(102): [  50/2211] Batch: 0.0943 (0.1206) Data: 0.0020 (0.0245) Loss: 0.0000 (0.0351)
[2023/01/16 05:47] | TRAIN(102): [ 100/2211] Batch: 0.0980 (0.1114) Data: 0.0017 (0.0134) Loss: 0.0000 (0.0338)
[2023/01/16 05:47] | TRAIN(102): [ 150/2211] Batch: 0.0937 (0.1068) Data: 0.0023 (0.0095) Loss: 0.0507 (0.0359)
[2023/01/16 05:47] | TRAIN(102): [ 200/2211] Batch: 0.1255 (0.1062) Data: 0.0018 (0.0076) Loss: 0.0000 (0.0325)
[2023/01/16 05:47] | TRAIN(102): [ 250/2211] Batch: 0.1432 (0.1109) Data: 0.0025 (0.0065) Loss: 0.0027 (0.0330)
[2023/01/16 05:47] | TRAIN(102): [ 300/2211] Batch: 0.0969 (0.1106) Data: 0.0018 (0.0058) Loss: 0.0888 (0.0313)
[2023/01/16 05:47] | TRAIN(102): [ 350/2211] Batch: 0.0945 (0.1117) Data: 0.0019 (0.0053) Loss: 0.0713 (0.0306)
[2023/01/16 05:47] | TRAIN(102): [ 400/2211] Batch: 0.1009 (0.1098) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0300)
[2023/01/16 05:47] | TRAIN(102): [ 450/2211] Batch: 0.0943 (0.1084) Data: 0.0020 (0.0045) Loss: 0.0046 (0.0293)
[2023/01/16 05:47] | TRAIN(102): [ 500/2211] Batch: 0.0955 (0.1075) Data: 0.0018 (0.0042) Loss: 0.0362 (0.0295)
[2023/01/16 05:47] | TRAIN(102): [ 550/2211] Batch: 0.0983 (0.1065) Data: 0.0019 (0.0040) Loss: 0.0134 (0.0293)
[2023/01/16 05:48] | TRAIN(102): [ 600/2211] Batch: 0.1074 (0.1068) Data: 0.0019 (0.0038) Loss: 0.0297 (0.0289)
[2023/01/16 05:48] | TRAIN(102): [ 650/2211] Batch: 0.0885 (0.1065) Data: 0.0018 (0.0037) Loss: 0.0243 (0.0298)
[2023/01/16 05:48] | TRAIN(102): [ 700/2211] Batch: 0.0878 (0.1058) Data: 0.0017 (0.0035) Loss: 0.0037 (0.0294)
[2023/01/16 05:48] | TRAIN(102): [ 750/2211] Batch: 0.0935 (0.1057) Data: 0.0022 (0.0034) Loss: 0.0000 (0.0288)
[2023/01/16 05:48] | TRAIN(102): [ 800/2211] Batch: 0.0946 (0.1050) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0287)
[2023/01/16 05:48] | TRAIN(102): [ 850/2211] Batch: 0.0920 (0.1047) Data: 0.0017 (0.0032) Loss: 0.0218 (0.0290)
[2023/01/16 05:48] | TRAIN(102): [ 900/2211] Batch: 0.0876 (0.1041) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0289)
[2023/01/16 05:48] | TRAIN(102): [ 950/2211] Batch: 0.1302 (0.1037) Data: 0.0023 (0.0031) Loss: 0.0000 (0.0289)
[2023/01/16 05:48] | TRAIN(102): [1000/2211] Batch: 0.0889 (0.1033) Data: 0.0018 (0.0030) Loss: 0.0786 (0.0284)
[2023/01/16 05:48] | TRAIN(102): [1050/2211] Batch: 0.0906 (0.1029) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0281)
[2023/01/16 05:48] | TRAIN(102): [1100/2211] Batch: 0.0945 (0.1027) Data: 0.0018 (0.0029) Loss: 0.0028 (0.0280)
[2023/01/16 05:48] | TRAIN(102): [1150/2211] Batch: 0.0910 (0.1025) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0283)
[2023/01/16 05:49] | TRAIN(102): [1200/2211] Batch: 0.0881 (0.1021) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0281)
[2023/01/16 05:49] | TRAIN(102): [1250/2211] Batch: 0.0924 (0.1017) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0280)
[2023/01/16 05:49] | TRAIN(102): [1300/2211] Batch: 0.1015 (0.1016) Data: 0.0018 (0.0028) Loss: 0.0693 (0.0278)
[2023/01/16 05:49] | TRAIN(102): [1350/2211] Batch: 0.1213 (0.1015) Data: 0.0023 (0.0027) Loss: 0.0000 (0.0275)
[2023/01/16 05:49] | TRAIN(102): [1400/2211] Batch: 0.0918 (0.1016) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0279)
[2023/01/16 05:49] | TRAIN(102): [1450/2211] Batch: 0.0915 (0.1014) Data: 0.0017 (0.0027) Loss: 0.0083 (0.0279)
[2023/01/16 05:49] | TRAIN(102): [1500/2211] Batch: 0.0919 (0.1011) Data: 0.0017 (0.0026) Loss: 0.0193 (0.0282)
[2023/01/16 05:49] | TRAIN(102): [1550/2211] Batch: 0.0938 (0.1009) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0287)
[2023/01/16 05:49] | TRAIN(102): [1600/2211] Batch: 0.1014 (0.1008) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0290)
[2023/01/16 05:49] | TRAIN(102): [1650/2211] Batch: 0.0988 (0.1010) Data: 0.0018 (0.0026) Loss: 0.0252 (0.0294)
[2023/01/16 05:49] | TRAIN(102): [1700/2211] Batch: 0.0883 (0.1008) Data: 0.0019 (0.0025) Loss: 0.0206 (0.0298)
[2023/01/16 05:49] | TRAIN(102): [1750/2211] Batch: 0.0980 (0.1005) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0297)
[2023/01/16 05:49] | TRAIN(102): [1800/2211] Batch: 0.1104 (0.1004) Data: 0.0018 (0.0025) Loss: 0.0039 (0.0297)
[2023/01/16 05:50] | TRAIN(102): [1850/2211] Batch: 0.0888 (0.1002) Data: 0.0017 (0.0025) Loss: 0.0331 (0.0297)
[2023/01/16 05:50] | TRAIN(102): [1900/2211] Batch: 0.0912 (0.1002) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0299)
[2023/01/16 05:50] | TRAIN(102): [1950/2211] Batch: 0.0995 (0.1001) Data: 0.0019 (0.0024) Loss: 0.0379 (0.0300)
[2023/01/16 05:50] | TRAIN(102): [2000/2211] Batch: 0.0926 (0.1001) Data: 0.0018 (0.0024) Loss: 0.0119 (0.0303)
[2023/01/16 05:50] | TRAIN(102): [2050/2211] Batch: 0.0934 (0.1000) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0303)
[2023/01/16 05:50] | TRAIN(102): [2100/2211] Batch: 0.0994 (0.0999) Data: 0.0017 (0.0024) Loss: 0.0379 (0.0302)
[2023/01/16 05:50] | TRAIN(102): [2150/2211] Batch: 0.0972 (0.0998) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0305)
[2023/01/16 05:50] | TRAIN(102): [2200/2211] Batch: 0.1021 (0.0997) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0306)
[2023/01/16 05:50] | ------------------------------------------------------------
[2023/01/16 05:50] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:50] | ------------------------------------------------------------
[2023/01/16 05:50] |   TRAIN(102)     0:03:40     0:00:05     0:03:35      0.0306
[2023/01/16 05:50] | ------------------------------------------------------------
[2023/01/16 05:50] | **************************************************
[2023/01/16 05:50] | TRAIN(103): [  50/2211] Batch: 0.1186 (0.1390) Data: 0.0023 (0.0254) Loss: 0.0000 (0.0203)
[2023/01/16 05:50] | TRAIN(103): [ 100/2211] Batch: 0.0944 (0.1291) Data: 0.0028 (0.0140) Loss: 0.0855 (0.0309)
[2023/01/16 05:50] | TRAIN(103): [ 150/2211] Batch: 0.1103 (0.1256) Data: 0.0018 (0.0101) Loss: 0.0222 (0.0265)
[2023/01/16 05:51] | TRAIN(103): [ 200/2211] Batch: 0.0947 (0.1197) Data: 0.0017 (0.0080) Loss: 0.0000 (0.0256)
[2023/01/16 05:51] | TRAIN(103): [ 250/2211] Batch: 0.1019 (0.1154) Data: 0.0018 (0.0068) Loss: 0.0000 (0.0264)
[2023/01/16 05:51] | TRAIN(103): [ 300/2211] Batch: 0.0867 (0.1123) Data: 0.0016 (0.0059) Loss: 0.0000 (0.0257)
[2023/01/16 05:51] | TRAIN(103): [ 350/2211] Batch: 0.1265 (0.1101) Data: 0.0020 (0.0053) Loss: 0.0199 (0.0245)
[2023/01/16 05:51] | TRAIN(103): [ 400/2211] Batch: 0.1034 (0.1089) Data: 0.0018 (0.0049) Loss: 0.0000 (0.0246)
[2023/01/16 05:51] | TRAIN(103): [ 450/2211] Batch: 0.1084 (0.1073) Data: 0.0018 (0.0046) Loss: 0.0336 (0.0261)
[2023/01/16 05:51] | TRAIN(103): [ 500/2211] Batch: 0.0982 (0.1063) Data: 0.0016 (0.0043) Loss: 0.0000 (0.0269)
[2023/01/16 05:51] | TRAIN(103): [ 550/2211] Batch: 0.1086 (0.1057) Data: 0.0017 (0.0041) Loss: 0.1000 (0.0272)
[2023/01/16 05:51] | TRAIN(103): [ 600/2211] Batch: 0.1239 (0.1068) Data: 0.0022 (0.0039) Loss: 0.0000 (0.0274)
[2023/01/16 05:51] | TRAIN(103): [ 650/2211] Batch: 0.1182 (0.1078) Data: 0.0023 (0.0038) Loss: 0.1163 (0.0282)
[2023/01/16 05:51] | TRAIN(103): [ 700/2211] Batch: 0.1180 (0.1089) Data: 0.0022 (0.0037) Loss: 0.0000 (0.0284)
[2023/01/16 05:52] | TRAIN(103): [ 750/2211] Batch: 0.1135 (0.1089) Data: 0.0020 (0.0035) Loss: 0.0344 (0.0291)
[2023/01/16 05:52] | TRAIN(103): [ 800/2211] Batch: 0.1040 (0.1086) Data: 0.0017 (0.0034) Loss: 0.0825 (0.0292)
[2023/01/16 05:52] | TRAIN(103): [ 850/2211] Batch: 0.0919 (0.1082) Data: 0.0016 (0.0033) Loss: 0.0686 (0.0297)
[2023/01/16 05:52] | TRAIN(103): [ 900/2211] Batch: 0.0910 (0.1073) Data: 0.0018 (0.0032) Loss: 0.0076 (0.0302)
[2023/01/16 05:52] | TRAIN(103): [ 950/2211] Batch: 0.1016 (0.1072) Data: 0.0019 (0.0032) Loss: 0.0186 (0.0303)
[2023/01/16 05:52] | TRAIN(103): [1000/2211] Batch: 0.0984 (0.1067) Data: 0.0016 (0.0031) Loss: 0.1887 (0.0310)
[2023/01/16 05:52] | TRAIN(103): [1050/2211] Batch: 0.0960 (0.1064) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0311)
[2023/01/16 05:52] | TRAIN(103): [1100/2211] Batch: 0.0985 (0.1059) Data: 0.0015 (0.0030) Loss: 0.0159 (0.0313)
[2023/01/16 05:52] | TRAIN(103): [1150/2211] Batch: 0.0872 (0.1054) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0320)
[2023/01/16 05:52] | TRAIN(103): [1200/2211] Batch: 0.1028 (0.1050) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0319)
[2023/01/16 05:52] | TRAIN(103): [1250/2211] Batch: 0.0899 (0.1046) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0317)
[2023/01/16 05:52] | TRAIN(103): [1300/2211] Batch: 0.1160 (0.1044) Data: 0.0022 (0.0028) Loss: 0.0067 (0.0319)
[2023/01/16 05:52] | TRAIN(103): [1350/2211] Batch: 0.0939 (0.1043) Data: 0.0019 (0.0028) Loss: 0.0051 (0.0316)
[2023/01/16 05:53] | TRAIN(103): [1400/2211] Batch: 0.1011 (0.1040) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0316)
[2023/01/16 05:53] | TRAIN(103): [1450/2211] Batch: 0.1087 (0.1038) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0314)
[2023/01/16 05:53] | TRAIN(103): [1500/2211] Batch: 0.0913 (0.1037) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0312)
[2023/01/16 05:53] | TRAIN(103): [1550/2211] Batch: 0.0946 (0.1034) Data: 0.0017 (0.0026) Loss: 0.0528 (0.0310)
[2023/01/16 05:53] | TRAIN(103): [1600/2211] Batch: 0.0871 (0.1032) Data: 0.0018 (0.0026) Loss: 0.1064 (0.0311)
[2023/01/16 05:53] | TRAIN(103): [1650/2211] Batch: 0.0965 (0.1028) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0310)
[2023/01/16 05:53] | TRAIN(103): [1700/2211] Batch: 0.1057 (0.1027) Data: 0.0020 (0.0026) Loss: 0.1182 (0.0306)
[2023/01/16 05:53] | TRAIN(103): [1750/2211] Batch: 0.0905 (0.1026) Data: 0.0018 (0.0026) Loss: 0.0357 (0.0307)
[2023/01/16 05:53] | TRAIN(103): [1800/2211] Batch: 0.0944 (0.1024) Data: 0.0017 (0.0025) Loss: 0.0524 (0.0310)
[2023/01/16 05:53] | TRAIN(103): [1850/2211] Batch: 0.0914 (0.1023) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0313)
[2023/01/16 05:53] | TRAIN(103): [1900/2211] Batch: 0.0996 (0.1021) Data: 0.0021 (0.0025) Loss: 0.0079 (0.0312)
[2023/01/16 05:53] | TRAIN(103): [1950/2211] Batch: 0.0910 (0.1019) Data: 0.0018 (0.0025) Loss: 0.0068 (0.0315)
[2023/01/16 05:54] | TRAIN(103): [2000/2211] Batch: 0.0928 (0.1018) Data: 0.0017 (0.0025) Loss: 0.0295 (0.0313)
[2023/01/16 05:54] | TRAIN(103): [2050/2211] Batch: 0.0989 (0.1016) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0313)
[2023/01/16 05:54] | TRAIN(103): [2100/2211] Batch: 0.0893 (0.1016) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0313)
[2023/01/16 05:54] | TRAIN(103): [2150/2211] Batch: 0.0981 (0.1014) Data: 0.0017 (0.0024) Loss: 0.0029 (0.0312)
[2023/01/16 05:54] | TRAIN(103): [2200/2211] Batch: 0.0908 (0.1013) Data: 0.0016 (0.0024) Loss: 0.0395 (0.0310)
[2023/01/16 05:54] | ------------------------------------------------------------
[2023/01/16 05:54] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:54] | ------------------------------------------------------------
[2023/01/16 05:54] |   TRAIN(103)     0:03:43     0:00:05     0:03:38      0.0310
[2023/01/16 05:54] | ------------------------------------------------------------
[2023/01/16 05:54] | **************************************************
[2023/01/16 05:54] | TRAIN(104): [  50/2211] Batch: 0.1224 (0.1327) Data: 0.0021 (0.0247) Loss: 0.0023 (0.0273)
[2023/01/16 05:54] | TRAIN(104): [ 100/2211] Batch: 0.0917 (0.1182) Data: 0.0018 (0.0134) Loss: 0.0064 (0.0316)
[2023/01/16 05:54] | TRAIN(104): [ 150/2211] Batch: 0.1243 (0.1101) Data: 0.0025 (0.0096) Loss: 0.0387 (0.0301)
[2023/01/16 05:54] | TRAIN(104): [ 200/2211] Batch: 0.0904 (0.1067) Data: 0.0017 (0.0077) Loss: 0.0189 (0.0325)
[2023/01/16 05:54] | TRAIN(104): [ 250/2211] Batch: 0.1196 (0.1080) Data: 0.0022 (0.0066) Loss: 0.0180 (0.0315)
[2023/01/16 05:54] | TRAIN(104): [ 300/2211] Batch: 0.0924 (0.1085) Data: 0.0017 (0.0058) Loss: 0.0000 (0.0316)
[2023/01/16 05:55] | TRAIN(104): [ 350/2211] Batch: 0.0880 (0.1065) Data: 0.0017 (0.0052) Loss: 0.0475 (0.0302)
[2023/01/16 05:55] | TRAIN(104): [ 400/2211] Batch: 0.0950 (0.1059) Data: 0.0016 (0.0048) Loss: 0.0000 (0.0297)
[2023/01/16 05:55] | TRAIN(104): [ 450/2211] Batch: 0.1023 (0.1049) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0291)
[2023/01/16 05:55] | TRAIN(104): [ 500/2211] Batch: 0.0947 (0.1037) Data: 0.0016 (0.0042) Loss: 0.0678 (0.0296)
[2023/01/16 05:55] | TRAIN(104): [ 550/2211] Batch: 0.0951 (0.1031) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0287)
[2023/01/16 05:55] | TRAIN(104): [ 600/2211] Batch: 0.0910 (0.1024) Data: 0.0018 (0.0038) Loss: 0.0764 (0.0295)
[2023/01/16 05:55] | TRAIN(104): [ 650/2211] Batch: 0.1005 (0.1017) Data: 0.0017 (0.0037) Loss: 0.0120 (0.0290)
[2023/01/16 05:55] | TRAIN(104): [ 700/2211] Batch: 0.0880 (0.1010) Data: 0.0018 (0.0035) Loss: 0.0225 (0.0286)
[2023/01/16 05:55] | TRAIN(104): [ 750/2211] Batch: 0.0902 (0.1007) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0285)
[2023/01/16 05:55] | TRAIN(104): [ 800/2211] Batch: 0.1064 (0.1003) Data: 0.0016 (0.0033) Loss: 0.0000 (0.0292)
[2023/01/16 05:55] | TRAIN(104): [ 850/2211] Batch: 0.0921 (0.1002) Data: 0.0018 (0.0032) Loss: 0.1137 (0.0302)
[2023/01/16 05:55] | TRAIN(104): [ 900/2211] Batch: 0.0965 (0.1000) Data: 0.0019 (0.0031) Loss: 0.0097 (0.0306)
[2023/01/16 05:55] | TRAIN(104): [ 950/2211] Batch: 0.1136 (0.1003) Data: 0.0017 (0.0031) Loss: 0.0305 (0.0305)
[2023/01/16 05:56] | TRAIN(104): [1000/2211] Batch: 0.1008 (0.1002) Data: 0.0018 (0.0030) Loss: 0.0120 (0.0310)
[2023/01/16 05:56] | TRAIN(104): [1050/2211] Batch: 0.0903 (0.1001) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0310)
[2023/01/16 05:56] | TRAIN(104): [1100/2211] Batch: 0.1188 (0.0998) Data: 0.0022 (0.0029) Loss: 0.0000 (0.0313)
[2023/01/16 05:56] | TRAIN(104): [1150/2211] Batch: 0.0948 (0.1001) Data: 0.0018 (0.0029) Loss: 0.0943 (0.0330)
[2023/01/16 05:56] | TRAIN(104): [1200/2211] Batch: 0.1025 (0.0998) Data: 0.0017 (0.0028) Loss: 0.0566 (0.0327)
[2023/01/16 05:56] | TRAIN(104): [1250/2211] Batch: 0.0876 (0.0995) Data: 0.0018 (0.0028) Loss: 0.1122 (0.0327)
[2023/01/16 05:56] | TRAIN(104): [1300/2211] Batch: 0.0922 (0.0992) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0327)
[2023/01/16 05:56] | TRAIN(104): [1350/2211] Batch: 0.0941 (0.0992) Data: 0.0016 (0.0027) Loss: 0.0689 (0.0325)
[2023/01/16 05:56] | TRAIN(104): [1400/2211] Batch: 0.0908 (0.0990) Data: 0.0017 (0.0027) Loss: 0.1033 (0.0322)
[2023/01/16 05:56] | TRAIN(104): [1450/2211] Batch: 0.0905 (0.0989) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0321)
[2023/01/16 05:56] | TRAIN(104): [1500/2211] Batch: 0.0954 (0.0989) Data: 0.0018 (0.0026) Loss: 0.0984 (0.0326)
[2023/01/16 05:56] | TRAIN(104): [1550/2211] Batch: 0.0904 (0.0988) Data: 0.0018 (0.0026) Loss: 0.0415 (0.0326)
[2023/01/16 05:57] | TRAIN(104): [1600/2211] Batch: 0.0960 (0.0987) Data: 0.0018 (0.0026) Loss: 0.0693 (0.0326)
[2023/01/16 05:57] | TRAIN(104): [1650/2211] Batch: 0.1020 (0.0986) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0324)
[2023/01/16 05:57] | TRAIN(104): [1700/2211] Batch: 0.0953 (0.0985) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0322)
[2023/01/16 05:57] | TRAIN(104): [1750/2211] Batch: 0.0871 (0.0984) Data: 0.0018 (0.0025) Loss: 0.0671 (0.0324)
[2023/01/16 05:57] | TRAIN(104): [1800/2211] Batch: 0.0845 (0.0983) Data: 0.0018 (0.0025) Loss: 0.1169 (0.0324)
[2023/01/16 05:57] | TRAIN(104): [1850/2211] Batch: 0.0957 (0.0981) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0322)
[2023/01/16 05:57] | TRAIN(104): [1900/2211] Batch: 0.0925 (0.0980) Data: 0.0019 (0.0024) Loss: 0.0832 (0.0322)
[2023/01/16 05:57] | TRAIN(104): [1950/2211] Batch: 0.0915 (0.0979) Data: 0.0017 (0.0024) Loss: 0.0267 (0.0321)
[2023/01/16 05:57] | TRAIN(104): [2000/2211] Batch: 0.1030 (0.0979) Data: 0.0020 (0.0024) Loss: 0.0000 (0.0317)
[2023/01/16 05:57] | TRAIN(104): [2050/2211] Batch: 0.0883 (0.0978) Data: 0.0018 (0.0024) Loss: 0.0835 (0.0319)
[2023/01/16 05:57] | TRAIN(104): [2100/2211] Batch: 0.1093 (0.0977) Data: 0.0019 (0.0024) Loss: 0.0553 (0.0317)
[2023/01/16 05:57] | TRAIN(104): [2150/2211] Batch: 0.1071 (0.0976) Data: 0.0017 (0.0024) Loss: 0.0253 (0.0316)
[2023/01/16 05:57] | TRAIN(104): [2200/2211] Batch: 0.0926 (0.0976) Data: 0.0016 (0.0024) Loss: 0.0115 (0.0317)
[2023/01/16 05:57] | ------------------------------------------------------------
[2023/01/16 05:57] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 05:57] | ------------------------------------------------------------
[2023/01/16 05:57] |   TRAIN(104)     0:03:35     0:00:05     0:03:30      0.0318
[2023/01/16 05:57] | ------------------------------------------------------------
[2023/01/16 05:57] | **************************************************
[2023/01/16 05:58] | TRAIN(105): [  50/2211] Batch: 0.0980 (0.1402) Data: 0.0020 (0.0257) Loss: 0.0216 (0.0308)
[2023/01/16 05:58] | TRAIN(105): [ 100/2211] Batch: 0.0977 (0.1217) Data: 0.0018 (0.0139) Loss: 0.0000 (0.0249)
[2023/01/16 05:58] | TRAIN(105): [ 150/2211] Batch: 0.0926 (0.1136) Data: 0.0016 (0.0099) Loss: 0.0145 (0.0311)
[2023/01/16 05:58] | TRAIN(105): [ 200/2211] Batch: 0.0959 (0.1089) Data: 0.0016 (0.0078) Loss: 0.0813 (0.0303)
[2023/01/16 05:58] | TRAIN(105): [ 250/2211] Batch: 0.0890 (0.1056) Data: 0.0017 (0.0066) Loss: 0.0092 (0.0301)
[2023/01/16 05:58] | TRAIN(105): [ 300/2211] Batch: 0.1266 (0.1082) Data: 0.0022 (0.0059) Loss: 0.0000 (0.0289)
[2023/01/16 05:58] | TRAIN(105): [ 350/2211] Batch: 0.0925 (0.1081) Data: 0.0016 (0.0053) Loss: 0.0054 (0.0297)
[2023/01/16 05:58] | TRAIN(105): [ 400/2211] Batch: 0.1016 (0.1072) Data: 0.0017 (0.0049) Loss: 0.0271 (0.0301)
[2023/01/16 05:58] | TRAIN(105): [ 450/2211] Batch: 0.0953 (0.1063) Data: 0.0017 (0.0046) Loss: 0.0000 (0.0292)
[2023/01/16 05:58] | TRAIN(105): [ 500/2211] Batch: 0.0885 (0.1048) Data: 0.0017 (0.0043) Loss: 0.0854 (0.0309)
[2023/01/16 05:58] | TRAIN(105): [ 550/2211] Batch: 0.1058 (0.1039) Data: 0.0017 (0.0040) Loss: 0.1021 (0.0307)
[2023/01/16 05:59] | TRAIN(105): [ 600/2211] Batch: 0.0899 (0.1034) Data: 0.0017 (0.0039) Loss: 0.0806 (0.0309)
[2023/01/16 05:59] | TRAIN(105): [ 650/2211] Batch: 0.0916 (0.1026) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0313)
[2023/01/16 05:59] | TRAIN(105): [ 700/2211] Batch: 0.0880 (0.1020) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0307)
[2023/01/16 05:59] | TRAIN(105): [ 750/2211] Batch: 0.1064 (0.1016) Data: 0.0017 (0.0034) Loss: 0.0231 (0.0306)
[2023/01/16 05:59] | TRAIN(105): [ 800/2211] Batch: 0.0982 (0.1013) Data: 0.0018 (0.0033) Loss: 0.0518 (0.0308)
[2023/01/16 05:59] | TRAIN(105): [ 850/2211] Batch: 0.1097 (0.1010) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0310)
[2023/01/16 05:59] | TRAIN(105): [ 900/2211] Batch: 0.1177 (0.1014) Data: 0.0022 (0.0032) Loss: 0.0788 (0.0316)
[2023/01/16 05:59] | TRAIN(105): [ 950/2211] Batch: 0.0899 (0.1011) Data: 0.0022 (0.0031) Loss: 0.1663 (0.0323)
[2023/01/16 05:59] | TRAIN(105): [1000/2211] Batch: 0.1005 (0.1008) Data: 0.0015 (0.0030) Loss: 0.1275 (0.0333)
[2023/01/16 05:59] | TRAIN(105): [1050/2211] Batch: 0.0897 (0.1006) Data: 0.0018 (0.0030) Loss: 0.1718 (0.0330)
[2023/01/16 05:59] | TRAIN(105): [1100/2211] Batch: 0.1048 (0.1006) Data: 0.0017 (0.0029) Loss: 0.1140 (0.0335)
[2023/01/16 05:59] | TRAIN(105): [1150/2211] Batch: 0.0926 (0.1005) Data: 0.0017 (0.0029) Loss: 0.0088 (0.0334)
[2023/01/16 05:59] | TRAIN(105): [1200/2211] Batch: 0.0888 (0.1001) Data: 0.0018 (0.0028) Loss: 0.0404 (0.0329)
[2023/01/16 06:00] | TRAIN(105): [1250/2211] Batch: 0.0976 (0.0999) Data: 0.0022 (0.0028) Loss: 0.0000 (0.0331)
[2023/01/16 06:00] | TRAIN(105): [1300/2211] Batch: 0.0925 (0.0997) Data: 0.0018 (0.0028) Loss: 0.0558 (0.0329)
[2023/01/16 06:00] | TRAIN(105): [1350/2211] Batch: 0.0983 (0.0995) Data: 0.0016 (0.0027) Loss: 0.0269 (0.0327)
[2023/01/16 06:00] | TRAIN(105): [1400/2211] Batch: 0.0998 (0.0994) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0323)
[2023/01/16 06:00] | TRAIN(105): [1450/2211] Batch: 0.0991 (0.0992) Data: 0.0018 (0.0027) Loss: 0.0030 (0.0318)
[2023/01/16 06:00] | TRAIN(105): [1500/2211] Batch: 0.0964 (0.0992) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0316)
[2023/01/16 06:00] | TRAIN(105): [1550/2211] Batch: 0.0880 (0.0995) Data: 0.0017 (0.0026) Loss: 0.1053 (0.0316)
[2023/01/16 06:00] | TRAIN(105): [1600/2211] Batch: 0.0922 (0.0994) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0317)
[2023/01/16 06:00] | TRAIN(105): [1650/2211] Batch: 0.1024 (0.0994) Data: 0.0018 (0.0026) Loss: 0.0026 (0.0316)
[2023/01/16 06:00] | TRAIN(105): [1700/2211] Batch: 0.0994 (0.0993) Data: 0.0018 (0.0025) Loss: 0.0880 (0.0315)
[2023/01/16 06:00] | TRAIN(105): [1750/2211] Batch: 0.0923 (0.0993) Data: 0.0017 (0.0025) Loss: 0.0967 (0.0312)
[2023/01/16 06:00] | TRAIN(105): [1800/2211] Batch: 0.0929 (0.0991) Data: 0.0018 (0.0025) Loss: 0.0188 (0.0312)
[2023/01/16 06:01] | TRAIN(105): [1850/2211] Batch: 0.1177 (0.0992) Data: 0.0021 (0.0025) Loss: 0.0000 (0.0312)
[2023/01/16 06:01] | TRAIN(105): [1900/2211] Batch: 0.0977 (0.0999) Data: 0.0018 (0.0025) Loss: 0.0018 (0.0315)
[2023/01/16 06:01] | TRAIN(105): [1950/2211] Batch: 0.1159 (0.1003) Data: 0.0018 (0.0025) Loss: 0.1053 (0.0315)
[2023/01/16 06:01] | TRAIN(105): [2000/2211] Batch: 0.0874 (0.1003) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0315)
[2023/01/16 06:01] | TRAIN(105): [2050/2211] Batch: 0.0949 (0.1002) Data: 0.0018 (0.0024) Loss: 0.0251 (0.0316)
[2023/01/16 06:01] | TRAIN(105): [2100/2211] Batch: 0.0930 (0.1001) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0313)
[2023/01/16 06:01] | TRAIN(105): [2150/2211] Batch: 0.0966 (0.1001) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0314)
[2023/01/16 06:01] | TRAIN(105): [2200/2211] Batch: 0.1216 (0.1001) Data: 0.0021 (0.0024) Loss: 0.0242 (0.0311)
[2023/01/16 06:01] | ------------------------------------------------------------
[2023/01/16 06:01] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:01] | ------------------------------------------------------------
[2023/01/16 06:01] |   TRAIN(105)     0:03:41     0:00:05     0:03:36      0.0312
[2023/01/16 06:01] | ------------------------------------------------------------
[2023/01/16 06:01] | **************************************************
[2023/01/16 06:01] | TRAIN(106): [  50/2211] Batch: 0.0941 (0.1415) Data: 0.0019 (0.0231) Loss: 0.0000 (0.0345)
[2023/01/16 06:01] | TRAIN(106): [ 100/2211] Batch: 0.1380 (0.1267) Data: 0.0024 (0.0127) Loss: 0.0000 (0.0313)
[2023/01/16 06:01] | TRAIN(106): [ 150/2211] Batch: 0.1226 (0.1269) Data: 0.0022 (0.0092) Loss: 0.0000 (0.0321)
[2023/01/16 06:02] | TRAIN(106): [ 200/2211] Batch: 0.0879 (0.1239) Data: 0.0018 (0.0074) Loss: 0.0152 (0.0334)
[2023/01/16 06:02] | TRAIN(106): [ 250/2211] Batch: 0.0897 (0.1174) Data: 0.0018 (0.0063) Loss: 0.0012 (0.0338)
[2023/01/16 06:02] | TRAIN(106): [ 300/2211] Batch: 0.0912 (0.1137) Data: 0.0018 (0.0056) Loss: 0.0022 (0.0336)
[2023/01/16 06:02] | TRAIN(106): [ 350/2211] Batch: 0.1017 (0.1116) Data: 0.0019 (0.0051) Loss: 0.0038 (0.0361)
[2023/01/16 06:02] | TRAIN(106): [ 400/2211] Batch: 0.0961 (0.1106) Data: 0.0019 (0.0047) Loss: 0.0899 (0.0370)
[2023/01/16 06:02] | TRAIN(106): [ 450/2211] Batch: 0.1217 (0.1100) Data: 0.0023 (0.0044) Loss: 0.0000 (0.0362)
[2023/01/16 06:02] | TRAIN(106): [ 500/2211] Batch: 0.0922 (0.1101) Data: 0.0018 (0.0041) Loss: 0.0000 (0.0346)
[2023/01/16 06:02] | TRAIN(106): [ 550/2211] Batch: 0.0913 (0.1095) Data: 0.0017 (0.0039) Loss: 0.0000 (0.0332)
[2023/01/16 06:02] | TRAIN(106): [ 600/2211] Batch: 0.0932 (0.1084) Data: 0.0019 (0.0037) Loss: 0.0000 (0.0331)
[2023/01/16 06:02] | TRAIN(106): [ 650/2211] Batch: 0.1057 (0.1077) Data: 0.0018 (0.0036) Loss: 0.0784 (0.0321)
[2023/01/16 06:02] | TRAIN(106): [ 700/2211] Batch: 0.0891 (0.1066) Data: 0.0019 (0.0035) Loss: 0.0175 (0.0324)
[2023/01/16 06:02] | TRAIN(106): [ 750/2211] Batch: 0.0928 (0.1057) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0328)
[2023/01/16 06:03] | TRAIN(106): [ 800/2211] Batch: 0.0916 (0.1052) Data: 0.0019 (0.0033) Loss: 0.0243 (0.0333)
[2023/01/16 06:03] | TRAIN(106): [ 850/2211] Batch: 0.0949 (0.1047) Data: 0.0021 (0.0032) Loss: 0.0061 (0.0332)
[2023/01/16 06:03] | TRAIN(106): [ 900/2211] Batch: 0.0993 (0.1042) Data: 0.0018 (0.0031) Loss: 0.0828 (0.0333)
[2023/01/16 06:03] | TRAIN(106): [ 950/2211] Batch: 0.0924 (0.1036) Data: 0.0018 (0.0030) Loss: 0.0109 (0.0336)
[2023/01/16 06:03] | TRAIN(106): [1000/2211] Batch: 0.0954 (0.1033) Data: 0.0018 (0.0030) Loss: 0.0111 (0.0331)
[2023/01/16 06:03] | TRAIN(106): [1050/2211] Batch: 0.0875 (0.1030) Data: 0.0017 (0.0029) Loss: 0.0014 (0.0325)
[2023/01/16 06:03] | TRAIN(106): [1100/2211] Batch: 0.0974 (0.1028) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0324)
[2023/01/16 06:03] | TRAIN(106): [1150/2211] Batch: 0.0962 (0.1028) Data: 0.0018 (0.0028) Loss: 0.0390 (0.0325)
[2023/01/16 06:03] | TRAIN(106): [1200/2211] Batch: 0.0917 (0.1024) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0324)
[2023/01/16 06:03] | TRAIN(106): [1250/2211] Batch: 0.1090 (0.1023) Data: 0.0017 (0.0027) Loss: 0.1110 (0.0321)
[2023/01/16 06:03] | TRAIN(106): [1300/2211] Batch: 0.0917 (0.1021) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0316)
[2023/01/16 06:03] | TRAIN(106): [1350/2211] Batch: 0.0932 (0.1020) Data: 0.0022 (0.0027) Loss: 0.0000 (0.0313)
[2023/01/16 06:04] | TRAIN(106): [1400/2211] Batch: 0.0859 (0.1017) Data: 0.0019 (0.0027) Loss: 0.1386 (0.0310)
[2023/01/16 06:04] | TRAIN(106): [1450/2211] Batch: 0.0904 (0.1019) Data: 0.0018 (0.0026) Loss: 0.0145 (0.0308)
[2023/01/16 06:04] | TRAIN(106): [1500/2211] Batch: 0.1055 (0.1017) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0306)
[2023/01/16 06:04] | TRAIN(106): [1550/2211] Batch: 0.1015 (0.1016) Data: 0.0019 (0.0026) Loss: 0.0767 (0.0303)
[2023/01/16 06:04] | TRAIN(106): [1600/2211] Batch: 0.0972 (0.1014) Data: 0.0019 (0.0026) Loss: 0.0596 (0.0305)
[2023/01/16 06:04] | TRAIN(106): [1650/2211] Batch: 0.0933 (0.1013) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0302)
[2023/01/16 06:04] | TRAIN(106): [1700/2211] Batch: 0.0980 (0.1010) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0302)
[2023/01/16 06:04] | TRAIN(106): [1750/2211] Batch: 0.0962 (0.1008) Data: 0.0018 (0.0025) Loss: 0.0131 (0.0302)
[2023/01/16 06:04] | TRAIN(106): [1800/2211] Batch: 0.0915 (0.1006) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0302)
[2023/01/16 06:04] | TRAIN(106): [1850/2211] Batch: 0.0943 (0.1004) Data: 0.0019 (0.0025) Loss: 0.0735 (0.0301)
[2023/01/16 06:04] | TRAIN(106): [1900/2211] Batch: 0.0924 (0.1004) Data: 0.0017 (0.0024) Loss: 0.0569 (0.0300)
[2023/01/16 06:04] | TRAIN(106): [1950/2211] Batch: 0.0984 (0.1003) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0301)
[2023/01/16 06:05] | TRAIN(106): [2000/2211] Batch: 0.0858 (0.1002) Data: 0.0019 (0.0024) Loss: 0.0711 (0.0301)
[2023/01/16 06:05] | TRAIN(106): [2050/2211] Batch: 0.0949 (0.1002) Data: 0.0013 (0.0024) Loss: 0.0000 (0.0303)
[2023/01/16 06:05] | TRAIN(106): [2100/2211] Batch: 0.0895 (0.1002) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0301)
[2023/01/16 06:05] | TRAIN(106): [2150/2211] Batch: 0.1175 (0.1000) Data: 0.0021 (0.0024) Loss: 0.0451 (0.0300)
[2023/01/16 06:05] | TRAIN(106): [2200/2211] Batch: 0.0876 (0.1000) Data: 0.0013 (0.0024) Loss: 0.0000 (0.0298)
[2023/01/16 06:05] | ------------------------------------------------------------
[2023/01/16 06:05] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:05] | ------------------------------------------------------------
[2023/01/16 06:05] |   TRAIN(106)     0:03:41     0:00:05     0:03:36      0.0299
[2023/01/16 06:05] | ------------------------------------------------------------
[2023/01/16 06:05] | **************************************************
[2023/01/16 06:05] | TRAIN(107): [  50/2211] Batch: 0.0962 (0.1266) Data: 0.0018 (0.0251) Loss: 0.0071 (0.0288)
[2023/01/16 06:05] | TRAIN(107): [ 100/2211] Batch: 0.0898 (0.1089) Data: 0.0017 (0.0136) Loss: 0.0104 (0.0293)
[2023/01/16 06:05] | TRAIN(107): [ 150/2211] Batch: 0.0892 (0.1059) Data: 0.0018 (0.0097) Loss: 0.0047 (0.0274)
[2023/01/16 06:05] | TRAIN(107): [ 200/2211] Batch: 0.0883 (0.1039) Data: 0.0018 (0.0078) Loss: 0.0996 (0.0272)
[2023/01/16 06:05] | TRAIN(107): [ 250/2211] Batch: 0.0920 (0.1023) Data: 0.0018 (0.0066) Loss: 0.1643 (0.0322)
[2023/01/16 06:05] | TRAIN(107): [ 300/2211] Batch: 0.0922 (0.1010) Data: 0.0018 (0.0058) Loss: 0.0000 (0.0318)
[2023/01/16 06:05] | TRAIN(107): [ 350/2211] Batch: 0.0926 (0.1002) Data: 0.0018 (0.0052) Loss: 0.0244 (0.0321)
[2023/01/16 06:06] | TRAIN(107): [ 400/2211] Batch: 0.0934 (0.0995) Data: 0.0018 (0.0048) Loss: 0.0072 (0.0328)
[2023/01/16 06:06] | TRAIN(107): [ 450/2211] Batch: 0.0990 (0.0990) Data: 0.0018 (0.0045) Loss: 0.0099 (0.0339)
[2023/01/16 06:06] | TRAIN(107): [ 500/2211] Batch: 0.0955 (0.0997) Data: 0.0020 (0.0042) Loss: 0.0172 (0.0335)
[2023/01/16 06:06] | TRAIN(107): [ 550/2211] Batch: 0.1036 (0.1004) Data: 0.0019 (0.0040) Loss: 0.1037 (0.0337)
[2023/01/16 06:06] | TRAIN(107): [ 600/2211] Batch: 0.0892 (0.0999) Data: 0.0017 (0.0038) Loss: 0.0169 (0.0335)
[2023/01/16 06:06] | TRAIN(107): [ 650/2211] Batch: 0.0966 (0.0992) Data: 0.0017 (0.0037) Loss: 0.0000 (0.0326)
[2023/01/16 06:06] | TRAIN(107): [ 700/2211] Batch: 0.0973 (0.0990) Data: 0.0017 (0.0035) Loss: 0.0101 (0.0318)
[2023/01/16 06:06] | TRAIN(107): [ 750/2211] Batch: 0.0924 (0.0988) Data: 0.0018 (0.0034) Loss: 0.0577 (0.0315)
[2023/01/16 06:06] | TRAIN(107): [ 800/2211] Batch: 0.0985 (0.0985) Data: 0.0017 (0.0033) Loss: 0.0709 (0.0321)
[2023/01/16 06:06] | TRAIN(107): [ 850/2211] Batch: 0.0898 (0.0983) Data: 0.0019 (0.0032) Loss: 0.0823 (0.0317)
[2023/01/16 06:06] | TRAIN(107): [ 900/2211] Batch: 0.0976 (0.0980) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0312)
[2023/01/16 06:06] | TRAIN(107): [ 950/2211] Batch: 0.0980 (0.0977) Data: 0.0017 (0.0031) Loss: 0.0218 (0.0302)
[2023/01/16 06:06] | TRAIN(107): [1000/2211] Batch: 0.0890 (0.0975) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0295)
[2023/01/16 06:07] | TRAIN(107): [1050/2211] Batch: 0.0889 (0.0974) Data: 0.0018 (0.0030) Loss: 0.0239 (0.0288)
[2023/01/16 06:07] | TRAIN(107): [1100/2211] Batch: 0.0913 (0.0975) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0288)
[2023/01/16 06:07] | TRAIN(107): [1150/2211] Batch: 0.0976 (0.0974) Data: 0.0019 (0.0029) Loss: 0.0804 (0.0289)
[2023/01/16 06:07] | TRAIN(107): [1200/2211] Batch: 0.1085 (0.0974) Data: 0.0017 (0.0028) Loss: 0.0718 (0.0305)
[2023/01/16 06:07] | TRAIN(107): [1250/2211] Batch: 0.0931 (0.0973) Data: 0.0018 (0.0028) Loss: 0.0192 (0.0308)
[2023/01/16 06:07] | TRAIN(107): [1300/2211] Batch: 0.0925 (0.0972) Data: 0.0017 (0.0027) Loss: 0.0562 (0.0316)
[2023/01/16 06:07] | TRAIN(107): [1350/2211] Batch: 0.0871 (0.0970) Data: 0.0018 (0.0027) Loss: 0.1047 (0.0316)
[2023/01/16 06:07] | TRAIN(107): [1400/2211] Batch: 0.0874 (0.0968) Data: 0.0017 (0.0027) Loss: 0.0078 (0.0319)
[2023/01/16 06:07] | TRAIN(107): [1450/2211] Batch: 0.0835 (0.0972) Data: 0.0018 (0.0027) Loss: 0.0381 (0.0319)
[2023/01/16 06:07] | TRAIN(107): [1500/2211] Batch: 0.0924 (0.0972) Data: 0.0017 (0.0026) Loss: 0.0167 (0.0321)
[2023/01/16 06:07] | TRAIN(107): [1550/2211] Batch: 0.0880 (0.0971) Data: 0.0019 (0.0026) Loss: 0.1122 (0.0321)
[2023/01/16 06:07] | TRAIN(107): [1600/2211] Batch: 0.1185 (0.0969) Data: 0.0021 (0.0026) Loss: 0.0000 (0.0323)
[2023/01/16 06:08] | TRAIN(107): [1650/2211] Batch: 0.1005 (0.0972) Data: 0.0016 (0.0026) Loss: 0.0000 (0.0325)
[2023/01/16 06:08] | TRAIN(107): [1700/2211] Batch: 0.0883 (0.0971) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0324)
[2023/01/16 06:08] | TRAIN(107): [1750/2211] Batch: 0.0918 (0.0969) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0323)
[2023/01/16 06:08] | TRAIN(107): [1800/2211] Batch: 0.0997 (0.0970) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0327)
[2023/01/16 06:08] | TRAIN(107): [1850/2211] Batch: 0.0909 (0.0969) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0333)
[2023/01/16 06:08] | TRAIN(107): [1900/2211] Batch: 0.0925 (0.0969) Data: 0.0017 (0.0025) Loss: 0.0399 (0.0334)
[2023/01/16 06:08] | TRAIN(107): [1950/2211] Batch: 0.0932 (0.0969) Data: 0.0018 (0.0024) Loss: 0.0586 (0.0335)
[2023/01/16 06:08] | TRAIN(107): [2000/2211] Batch: 0.0937 (0.0968) Data: 0.0017 (0.0024) Loss: 0.0295 (0.0334)
[2023/01/16 06:08] | TRAIN(107): [2050/2211] Batch: 0.0897 (0.0968) Data: 0.0017 (0.0024) Loss: 0.0443 (0.0333)
[2023/01/16 06:08] | TRAIN(107): [2100/2211] Batch: 0.0974 (0.0968) Data: 0.0018 (0.0024) Loss: 0.0761 (0.0333)
[2023/01/16 06:08] | TRAIN(107): [2150/2211] Batch: 0.0928 (0.0967) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0332)
[2023/01/16 06:08] | TRAIN(107): [2200/2211] Batch: 0.0907 (0.0966) Data: 0.0014 (0.0024) Loss: 0.0000 (0.0334)
[2023/01/16 06:08] | ------------------------------------------------------------
[2023/01/16 06:08] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:08] | ------------------------------------------------------------
[2023/01/16 06:08] |   TRAIN(107)     0:03:33     0:00:05     0:03:28      0.0334
[2023/01/16 06:08] | ------------------------------------------------------------
[2023/01/16 06:08] | **************************************************
[2023/01/16 06:09] | TRAIN(108): [  50/2211] Batch: 0.1217 (0.1266) Data: 0.0017 (0.0252) Loss: 0.0132 (0.0299)
[2023/01/16 06:09] | TRAIN(108): [ 100/2211] Batch: 0.0915 (0.1232) Data: 0.0016 (0.0138) Loss: 0.0734 (0.0278)
[2023/01/16 06:09] | TRAIN(108): [ 150/2211] Batch: 0.1167 (0.1208) Data: 0.0020 (0.0100) Loss: 0.0000 (0.0314)
[2023/01/16 06:09] | TRAIN(108): [ 200/2211] Batch: 0.1246 (0.1191) Data: 0.0022 (0.0080) Loss: 0.0074 (0.0289)
[2023/01/16 06:09] | TRAIN(108): [ 250/2211] Batch: 0.1175 (0.1183) Data: 0.0024 (0.0068) Loss: 0.0000 (0.0313)
[2023/01/16 06:09] | TRAIN(108): [ 300/2211] Batch: 0.1253 (0.1174) Data: 0.0038 (0.0060) Loss: 0.0151 (0.0293)
[2023/01/16 06:09] | TRAIN(108): [ 350/2211] Batch: 0.0961 (0.1167) Data: 0.0019 (0.0055) Loss: 0.0698 (0.0298)
[2023/01/16 06:09] | TRAIN(108): [ 400/2211] Batch: 0.1002 (0.1160) Data: 0.0018 (0.0050) Loss: 0.0038 (0.0303)
[2023/01/16 06:09] | TRAIN(108): [ 450/2211] Batch: 0.0820 (0.1131) Data: 0.0017 (0.0047) Loss: 0.0000 (0.0300)
[2023/01/16 06:09] | TRAIN(108): [ 500/2211] Batch: 0.0927 (0.1105) Data: 0.0018 (0.0044) Loss: 0.0124 (0.0296)
[2023/01/16 06:09] | TRAIN(108): [ 550/2211] Batch: 0.0909 (0.1085) Data: 0.0019 (0.0042) Loss: 0.0061 (0.0293)
[2023/01/16 06:09] | TRAIN(108): [ 600/2211] Batch: 0.0822 (0.1068) Data: 0.0018 (0.0040) Loss: 0.0116 (0.0293)
[2023/01/16 06:10] | TRAIN(108): [ 650/2211] Batch: 0.0875 (0.1052) Data: 0.0017 (0.0038) Loss: 0.0234 (0.0289)
[2023/01/16 06:10] | TRAIN(108): [ 700/2211] Batch: 0.0851 (0.1037) Data: 0.0018 (0.0037) Loss: 0.0056 (0.0284)
[2023/01/16 06:10] | TRAIN(108): [ 750/2211] Batch: 0.0982 (0.1025) Data: 0.0021 (0.0035) Loss: 0.0717 (0.0277)
[2023/01/16 06:10] | TRAIN(108): [ 800/2211] Batch: 0.0906 (0.1016) Data: 0.0018 (0.0034) Loss: 0.0083 (0.0277)
[2023/01/16 06:10] | TRAIN(108): [ 850/2211] Batch: 0.0813 (0.1005) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0277)
[2023/01/16 06:10] | TRAIN(108): [ 900/2211] Batch: 0.0886 (0.0998) Data: 0.0020 (0.0032) Loss: 0.0000 (0.0284)
[2023/01/16 06:10] | TRAIN(108): [ 950/2211] Batch: 0.0874 (0.0994) Data: 0.0019 (0.0032) Loss: 0.0672 (0.0282)
[2023/01/16 06:10] | TRAIN(108): [1000/2211] Batch: 0.0837 (0.0987) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0282)
[2023/01/16 06:10] | TRAIN(108): [1050/2211] Batch: 0.0893 (0.0982) Data: 0.0016 (0.0030) Loss: 0.0000 (0.0280)
[2023/01/16 06:10] | TRAIN(108): [1100/2211] Batch: 0.0827 (0.0978) Data: 0.0016 (0.0030) Loss: 0.0736 (0.0281)
[2023/01/16 06:10] | TRAIN(108): [1150/2211] Batch: 0.0979 (0.0974) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0282)
[2023/01/16 06:10] | TRAIN(108): [1200/2211] Batch: 0.0885 (0.0971) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0281)
[2023/01/16 06:10] | TRAIN(108): [1250/2211] Batch: 0.0988 (0.0970) Data: 0.0016 (0.0028) Loss: 0.0000 (0.0281)
[2023/01/16 06:11] | TRAIN(108): [1300/2211] Batch: 0.0967 (0.0969) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0281)
[2023/01/16 06:11] | TRAIN(108): [1350/2211] Batch: 0.0975 (0.0966) Data: 0.0020 (0.0028) Loss: 0.0000 (0.0283)
[2023/01/16 06:11] | TRAIN(108): [1400/2211] Batch: 0.0966 (0.0966) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0286)
[2023/01/16 06:11] | TRAIN(108): [1450/2211] Batch: 0.0883 (0.0966) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0288)
[2023/01/16 06:11] | TRAIN(108): [1500/2211] Batch: 0.0880 (0.0966) Data: 0.0018 (0.0027) Loss: 0.0755 (0.0287)
[2023/01/16 06:11] | TRAIN(108): [1550/2211] Batch: 0.0960 (0.0968) Data: 0.0018 (0.0026) Loss: 0.0045 (0.0291)
[2023/01/16 06:11] | TRAIN(108): [1600/2211] Batch: 0.1296 (0.0978) Data: 0.0022 (0.0026) Loss: 0.0354 (0.0293)
[2023/01/16 06:11] | TRAIN(108): [1650/2211] Batch: 0.0888 (0.0983) Data: 0.0018 (0.0026) Loss: 0.0132 (0.0295)
[2023/01/16 06:11] | TRAIN(108): [1700/2211] Batch: 0.0965 (0.0985) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0294)
[2023/01/16 06:11] | TRAIN(108): [1750/2211] Batch: 0.1254 (0.0990) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0293)
[2023/01/16 06:11] | TRAIN(108): [1800/2211] Batch: 0.1144 (0.0996) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0292)
[2023/01/16 06:12] | TRAIN(108): [1850/2211] Batch: 0.0976 (0.0996) Data: 0.0019 (0.0025) Loss: 0.1255 (0.0292)
[2023/01/16 06:12] | TRAIN(108): [1900/2211] Batch: 0.0890 (0.0995) Data: 0.0018 (0.0025) Loss: 0.0061 (0.0289)
[2023/01/16 06:12] | TRAIN(108): [1950/2211] Batch: 0.0965 (0.0995) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0289)
[2023/01/16 06:12] | TRAIN(108): [2000/2211] Batch: 0.0956 (0.0994) Data: 0.0019 (0.0025) Loss: 0.0023 (0.0287)
[2023/01/16 06:12] | TRAIN(108): [2050/2211] Batch: 0.1063 (0.0993) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0286)
[2023/01/16 06:12] | TRAIN(108): [2100/2211] Batch: 0.0923 (0.0992) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0288)
[2023/01/16 06:12] | TRAIN(108): [2150/2211] Batch: 0.1140 (0.0993) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0290)
[2023/01/16 06:12] | TRAIN(108): [2200/2211] Batch: 0.0912 (0.0992) Data: 0.0017 (0.0024) Loss: 0.0136 (0.0295)
[2023/01/16 06:12] | ------------------------------------------------------------
[2023/01/16 06:12] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:12] | ------------------------------------------------------------
[2023/01/16 06:12] |   TRAIN(108)     0:03:39     0:00:05     0:03:33      0.0295
[2023/01/16 06:12] | ------------------------------------------------------------
[2023/01/16 06:12] | **************************************************
[2023/01/16 06:12] | TRAIN(109): [  50/2211] Batch: 0.0920 (0.1217) Data: 0.0016 (0.0248) Loss: 0.0272 (0.0487)
[2023/01/16 06:12] | TRAIN(109): [ 100/2211] Batch: 0.0942 (0.1104) Data: 0.0019 (0.0134) Loss: 0.0000 (0.0385)
[2023/01/16 06:12] | TRAIN(109): [ 150/2211] Batch: 0.0955 (0.1070) Data: 0.0018 (0.0096) Loss: 0.0419 (0.0394)
[2023/01/16 06:12] | TRAIN(109): [ 200/2211] Batch: 0.0879 (0.1055) Data: 0.0017 (0.0077) Loss: 0.0506 (0.0361)
[2023/01/16 06:13] | TRAIN(109): [ 250/2211] Batch: 0.0890 (0.1028) Data: 0.0017 (0.0065) Loss: 0.1203 (0.0357)
[2023/01/16 06:13] | TRAIN(109): [ 300/2211] Batch: 0.0993 (0.1010) Data: 0.0018 (0.0057) Loss: 0.0234 (0.0344)
[2023/01/16 06:13] | TRAIN(109): [ 350/2211] Batch: 0.0914 (0.1010) Data: 0.0017 (0.0052) Loss: 0.0073 (0.0345)
[2023/01/16 06:13] | TRAIN(109): [ 400/2211] Batch: 0.0908 (0.1003) Data: 0.0017 (0.0048) Loss: 0.0026 (0.0327)
[2023/01/16 06:13] | TRAIN(109): [ 450/2211] Batch: 0.0903 (0.0995) Data: 0.0018 (0.0044) Loss: 0.0243 (0.0318)
[2023/01/16 06:13] | TRAIN(109): [ 500/2211] Batch: 0.0878 (0.1003) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0315)
[2023/01/16 06:13] | TRAIN(109): [ 550/2211] Batch: 0.0953 (0.0999) Data: 0.0017 (0.0040) Loss: 0.0000 (0.0306)
[2023/01/16 06:13] | TRAIN(109): [ 600/2211] Batch: 0.0991 (0.0994) Data: 0.0019 (0.0038) Loss: 0.0000 (0.0305)
[2023/01/16 06:13] | TRAIN(109): [ 650/2211] Batch: 0.0931 (0.0993) Data: 0.0018 (0.0036) Loss: 0.0099 (0.0302)
[2023/01/16 06:13] | TRAIN(109): [ 700/2211] Batch: 0.1004 (0.0993) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0301)
[2023/01/16 06:13] | TRAIN(109): [ 750/2211] Batch: 0.1019 (0.0992) Data: 0.0018 (0.0034) Loss: 0.0058 (0.0301)
[2023/01/16 06:13] | TRAIN(109): [ 800/2211] Batch: 0.0929 (0.0989) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0295)
[2023/01/16 06:13] | TRAIN(109): [ 850/2211] Batch: 0.1051 (0.0988) Data: 0.0018 (0.0032) Loss: 0.0991 (0.0298)
[2023/01/16 06:14] | TRAIN(109): [ 900/2211] Batch: 0.1248 (0.0988) Data: 0.0021 (0.0031) Loss: 0.0000 (0.0296)
[2023/01/16 06:14] | TRAIN(109): [ 950/2211] Batch: 0.0903 (0.0984) Data: 0.0018 (0.0031) Loss: 0.0226 (0.0299)
[2023/01/16 06:14] | TRAIN(109): [1000/2211] Batch: 0.1180 (0.0982) Data: 0.0024 (0.0030) Loss: 0.0000 (0.0300)
[2023/01/16 06:14] | TRAIN(109): [1050/2211] Batch: 0.0902 (0.0981) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0300)
[2023/01/16 06:14] | TRAIN(109): [1100/2211] Batch: 0.0935 (0.0981) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0301)
[2023/01/16 06:14] | TRAIN(109): [1150/2211] Batch: 0.0995 (0.0982) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0302)
[2023/01/16 06:14] | TRAIN(109): [1200/2211] Batch: 0.0892 (0.0981) Data: 0.0018 (0.0028) Loss: 0.0889 (0.0303)
[2023/01/16 06:14] | TRAIN(109): [1250/2211] Batch: 0.1003 (0.0980) Data: 0.0018 (0.0028) Loss: 0.1023 (0.0309)
[2023/01/16 06:14] | TRAIN(109): [1300/2211] Batch: 0.0878 (0.0978) Data: 0.0018 (0.0027) Loss: 0.0053 (0.0310)
[2023/01/16 06:14] | TRAIN(109): [1350/2211] Batch: 0.0994 (0.0978) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0306)
[2023/01/16 06:14] | TRAIN(109): [1400/2211] Batch: 0.0990 (0.0976) Data: 0.0018 (0.0027) Loss: 0.0001 (0.0304)
[2023/01/16 06:14] | TRAIN(109): [1450/2211] Batch: 0.0985 (0.0974) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0306)
[2023/01/16 06:15] | TRAIN(109): [1500/2211] Batch: 0.0940 (0.0974) Data: 0.0018 (0.0026) Loss: 0.0152 (0.0305)
[2023/01/16 06:15] | TRAIN(109): [1550/2211] Batch: 0.0954 (0.0973) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0303)
[2023/01/16 06:15] | TRAIN(109): [1600/2211] Batch: 0.0929 (0.0972) Data: 0.0018 (0.0025) Loss: 0.1423 (0.0305)
[2023/01/16 06:15] | TRAIN(109): [1650/2211] Batch: 0.0918 (0.0971) Data: 0.0017 (0.0025) Loss: 0.0041 (0.0301)
[2023/01/16 06:15] | TRAIN(109): [1700/2211] Batch: 0.0913 (0.0970) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0300)
[2023/01/16 06:15] | TRAIN(109): [1750/2211] Batch: 0.0967 (0.0970) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0300)
[2023/01/16 06:15] | TRAIN(109): [1800/2211] Batch: 0.0928 (0.0969) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0299)
[2023/01/16 06:15] | TRAIN(109): [1850/2211] Batch: 0.0893 (0.0968) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0303)
[2023/01/16 06:15] | TRAIN(109): [1900/2211] Batch: 0.0923 (0.0968) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0306)
[2023/01/16 06:15] | TRAIN(109): [1950/2211] Batch: 0.0945 (0.0967) Data: 0.0022 (0.0024) Loss: 0.0000 (0.0303)
[2023/01/16 06:15] | TRAIN(109): [2000/2211] Batch: 0.0946 (0.0967) Data: 0.0018 (0.0024) Loss: 0.0129 (0.0302)
[2023/01/16 06:15] | TRAIN(109): [2050/2211] Batch: 0.0887 (0.0967) Data: 0.0017 (0.0024) Loss: 0.0637 (0.0299)
[2023/01/16 06:15] | TRAIN(109): [2100/2211] Batch: 0.0899 (0.0966) Data: 0.0019 (0.0024) Loss: 0.0014 (0.0299)
[2023/01/16 06:16] | TRAIN(109): [2150/2211] Batch: 0.0923 (0.0965) Data: 0.0020 (0.0024) Loss: 0.0698 (0.0302)
[2023/01/16 06:16] | TRAIN(109): [2200/2211] Batch: 0.0973 (0.0967) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0300)
[2023/01/16 06:16] | ------------------------------------------------------------
[2023/01/16 06:16] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:16] | ------------------------------------------------------------
[2023/01/16 06:16] |   TRAIN(109)     0:03:33     0:00:05     0:03:28      0.0301
[2023/01/16 06:16] | ------------------------------------------------------------
[2023/01/16 06:16] | **************************************************
[2023/01/16 06:16] | TRAIN(110): [  50/2211] Batch: 0.0992 (0.1235) Data: 0.0016 (0.0258) Loss: 0.0000 (0.0245)
[2023/01/16 06:16] | TRAIN(110): [ 100/2211] Batch: 0.1003 (0.1143) Data: 0.0018 (0.0140) Loss: 0.0000 (0.0281)
[2023/01/16 06:16] | TRAIN(110): [ 150/2211] Batch: 0.1186 (0.1181) Data: 0.0022 (0.0101) Loss: 0.0078 (0.0261)
[2023/01/16 06:16] | TRAIN(110): [ 200/2211] Batch: 0.0897 (0.1174) Data: 0.0019 (0.0081) Loss: 0.0000 (0.0246)
[2023/01/16 06:16] | TRAIN(110): [ 250/2211] Batch: 0.0956 (0.1149) Data: 0.0019 (0.0069) Loss: 0.0019 (0.0248)
[2023/01/16 06:16] | TRAIN(110): [ 300/2211] Batch: 0.0893 (0.1123) Data: 0.0017 (0.0061) Loss: 0.1385 (0.0255)
[2023/01/16 06:16] | TRAIN(110): [ 350/2211] Batch: 0.1041 (0.1098) Data: 0.0018 (0.0055) Loss: 0.0000 (0.0246)
[2023/01/16 06:16] | TRAIN(110): [ 400/2211] Batch: 0.0981 (0.1101) Data: 0.0018 (0.0050) Loss: 0.0737 (0.0243)
[2023/01/16 06:16] | TRAIN(110): [ 450/2211] Batch: 0.1112 (0.1087) Data: 0.0019 (0.0047) Loss: 0.2070 (0.0256)
[2023/01/16 06:17] | TRAIN(110): [ 500/2211] Batch: 0.1156 (0.1077) Data: 0.0017 (0.0044) Loss: 0.0693 (0.0263)
[2023/01/16 06:17] | TRAIN(110): [ 550/2211] Batch: 0.0941 (0.1077) Data: 0.0018 (0.0042) Loss: 0.0021 (0.0263)
[2023/01/16 06:17] | TRAIN(110): [ 600/2211] Batch: 0.0914 (0.1068) Data: 0.0018 (0.0040) Loss: 0.0064 (0.0269)
[2023/01/16 06:17] | TRAIN(110): [ 650/2211] Batch: 0.1013 (0.1061) Data: 0.0019 (0.0038) Loss: 0.0000 (0.0267)
[2023/01/16 06:17] | TRAIN(110): [ 700/2211] Batch: 0.1092 (0.1059) Data: 0.0019 (0.0037) Loss: 0.0000 (0.0274)
[2023/01/16 06:17] | TRAIN(110): [ 750/2211] Batch: 0.0927 (0.1054) Data: 0.0018 (0.0035) Loss: 0.1169 (0.0277)
[2023/01/16 06:17] | TRAIN(110): [ 800/2211] Batch: 0.1276 (0.1053) Data: 0.0022 (0.0034) Loss: 0.0585 (0.0281)
[2023/01/16 06:17] | TRAIN(110): [ 850/2211] Batch: 0.0900 (0.1048) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0283)
[2023/01/16 06:17] | TRAIN(110): [ 900/2211] Batch: 0.0973 (0.1044) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0287)
[2023/01/16 06:17] | TRAIN(110): [ 950/2211] Batch: 0.1100 (0.1044) Data: 0.0014 (0.0032) Loss: 0.0445 (0.0291)
[2023/01/16 06:17] | TRAIN(110): [1000/2211] Batch: 0.0992 (0.1042) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0298)
[2023/01/16 06:17] | TRAIN(110): [1050/2211] Batch: 0.1023 (0.1039) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0300)
[2023/01/16 06:18] | TRAIN(110): [1100/2211] Batch: 0.0918 (0.1035) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0300)
[2023/01/16 06:18] | TRAIN(110): [1150/2211] Batch: 0.1062 (0.1034) Data: 0.0022 (0.0029) Loss: 0.0000 (0.0305)
[2023/01/16 06:18] | TRAIN(110): [1200/2211] Batch: 0.0952 (0.1031) Data: 0.0018 (0.0029) Loss: 0.0089 (0.0304)
[2023/01/16 06:18] | TRAIN(110): [1250/2211] Batch: 0.0978 (0.1031) Data: 0.0021 (0.0028) Loss: 0.0556 (0.0305)
[2023/01/16 06:18] | TRAIN(110): [1300/2211] Batch: 0.0918 (0.1029) Data: 0.0019 (0.0028) Loss: 0.0821 (0.0307)
[2023/01/16 06:18] | TRAIN(110): [1350/2211] Batch: 0.0969 (0.1026) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0305)
[2023/01/16 06:18] | TRAIN(110): [1400/2211] Batch: 0.1174 (0.1027) Data: 0.0022 (0.0027) Loss: 0.0000 (0.0306)
[2023/01/16 06:18] | TRAIN(110): [1450/2211] Batch: 0.0955 (0.1028) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0303)
[2023/01/16 06:18] | TRAIN(110): [1500/2211] Batch: 0.0986 (0.1026) Data: 0.0014 (0.0027) Loss: 0.0115 (0.0304)
[2023/01/16 06:18] | TRAIN(110): [1550/2211] Batch: 0.0924 (0.1023) Data: 0.0021 (0.0026) Loss: 0.0000 (0.0307)
[2023/01/16 06:18] | TRAIN(110): [1600/2211] Batch: 0.0888 (0.1022) Data: 0.0017 (0.0026) Loss: 0.0105 (0.0303)
[2023/01/16 06:18] | TRAIN(110): [1650/2211] Batch: 0.0884 (0.1019) Data: 0.0018 (0.0026) Loss: 0.0457 (0.0302)
[2023/01/16 06:19] | TRAIN(110): [1700/2211] Batch: 0.1119 (0.1018) Data: 0.0019 (0.0026) Loss: 0.0048 (0.0300)
[2023/01/16 06:19] | TRAIN(110): [1750/2211] Batch: 0.0974 (0.1016) Data: 0.0017 (0.0025) Loss: 0.0051 (0.0298)
[2023/01/16 06:19] | TRAIN(110): [1800/2211] Batch: 0.0890 (0.1015) Data: 0.0018 (0.0025) Loss: 0.0809 (0.0298)
[2023/01/16 06:19] | TRAIN(110): [1850/2211] Batch: 0.1291 (0.1014) Data: 0.0023 (0.0025) Loss: 0.1996 (0.0297)
[2023/01/16 06:19] | TRAIN(110): [1900/2211] Batch: 0.1025 (0.1015) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0299)
[2023/01/16 06:19] | TRAIN(110): [1950/2211] Batch: 0.0924 (0.1013) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0298)
[2023/01/16 06:19] | TRAIN(110): [2000/2211] Batch: 0.0894 (0.1011) Data: 0.0018 (0.0025) Loss: 0.0081 (0.0301)
[2023/01/16 06:19] | TRAIN(110): [2050/2211] Batch: 0.0912 (0.1009) Data: 0.0018 (0.0024) Loss: 0.0297 (0.0297)
[2023/01/16 06:19] | TRAIN(110): [2100/2211] Batch: 0.0905 (0.1009) Data: 0.0018 (0.0024) Loss: 0.0614 (0.0296)
[2023/01/16 06:19] | TRAIN(110): [2150/2211] Batch: 0.0922 (0.1007) Data: 0.0017 (0.0024) Loss: 0.0469 (0.0295)
[2023/01/16 06:19] | TRAIN(110): [2200/2211] Batch: 0.0959 (0.1009) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0297)
[2023/01/16 06:19] | ------------------------------------------------------------
[2023/01/16 06:19] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:19] | ------------------------------------------------------------
[2023/01/16 06:19] |   TRAIN(110)     0:03:43     0:00:05     0:03:37      0.0297
[2023/01/16 06:19] | ------------------------------------------------------------
[2023/01/16 06:19] | **************************************************
[2023/01/16 06:19] | TRAIN(111): [  50/2211] Batch: 0.0927 (0.1326) Data: 0.0019 (0.0360) Loss: 0.1486 (0.0342)
[2023/01/16 06:20] | TRAIN(111): [ 100/2211] Batch: 0.0847 (0.1135) Data: 0.0018 (0.0191) Loss: 0.0000 (0.0295)
[2023/01/16 06:20] | TRAIN(111): [ 150/2211] Batch: 0.0880 (0.1057) Data: 0.0018 (0.0134) Loss: 0.0000 (0.0319)
[2023/01/16 06:20] | TRAIN(111): [ 200/2211] Batch: 0.0947 (0.1018) Data: 0.0018 (0.0105) Loss: 0.0000 (0.0310)
[2023/01/16 06:20] | TRAIN(111): [ 250/2211] Batch: 0.1267 (0.1045) Data: 0.0023 (0.0088) Loss: 0.0859 (0.0336)
[2023/01/16 06:20] | TRAIN(111): [ 300/2211] Batch: 0.0953 (0.1040) Data: 0.0017 (0.0077) Loss: 0.0000 (0.0341)
[2023/01/16 06:20] | TRAIN(111): [ 350/2211] Batch: 0.0920 (0.1032) Data: 0.0018 (0.0068) Loss: 0.0820 (0.0346)
[2023/01/16 06:20] | TRAIN(111): [ 400/2211] Batch: 0.0917 (0.1022) Data: 0.0018 (0.0062) Loss: 0.0117 (0.0340)
[2023/01/16 06:20] | TRAIN(111): [ 450/2211] Batch: 0.0883 (0.1013) Data: 0.0018 (0.0057) Loss: 0.0000 (0.0327)
[2023/01/16 06:20] | TRAIN(111): [ 500/2211] Batch: 0.1246 (0.1010) Data: 0.0022 (0.0054) Loss: 0.0580 (0.0324)
[2023/01/16 06:20] | TRAIN(111): [ 550/2211] Batch: 0.0903 (0.1013) Data: 0.0018 (0.0050) Loss: 0.0027 (0.0319)
[2023/01/16 06:20] | TRAIN(111): [ 600/2211] Batch: 0.0940 (0.1009) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0326)
[2023/01/16 06:20] | TRAIN(111): [ 650/2211] Batch: 0.0891 (0.1003) Data: 0.0018 (0.0046) Loss: 0.0615 (0.0319)
[2023/01/16 06:21] | TRAIN(111): [ 700/2211] Batch: 0.0878 (0.0997) Data: 0.0018 (0.0044) Loss: 0.0035 (0.0310)
[2023/01/16 06:21] | TRAIN(111): [ 750/2211] Batch: 0.0924 (0.0993) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0320)
[2023/01/16 06:21] | TRAIN(111): [ 800/2211] Batch: 0.0908 (0.0991) Data: 0.0019 (0.0040) Loss: 0.0000 (0.0320)
[2023/01/16 06:21] | TRAIN(111): [ 850/2211] Batch: 0.0975 (0.0995) Data: 0.0019 (0.0039) Loss: 0.0000 (0.0316)
[2023/01/16 06:21] | TRAIN(111): [ 900/2211] Batch: 0.0936 (0.0994) Data: 0.0019 (0.0038) Loss: 0.0610 (0.0315)
[2023/01/16 06:21] | TRAIN(111): [ 950/2211] Batch: 0.0937 (0.0998) Data: 0.0017 (0.0037) Loss: 0.0517 (0.0313)
[2023/01/16 06:21] | TRAIN(111): [1000/2211] Batch: 0.0895 (0.0996) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0312)
[2023/01/16 06:21] | TRAIN(111): [1050/2211] Batch: 0.0884 (0.0997) Data: 0.0019 (0.0035) Loss: 0.0000 (0.0312)
[2023/01/16 06:21] | TRAIN(111): [1100/2211] Batch: 0.0916 (0.0994) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0317)
[2023/01/16 06:21] | TRAIN(111): [1150/2211] Batch: 0.0913 (0.0991) Data: 0.0018 (0.0034) Loss: 0.0062 (0.0317)
[2023/01/16 06:21] | TRAIN(111): [1200/2211] Batch: 0.0919 (0.0989) Data: 0.0017 (0.0033) Loss: 0.0548 (0.0315)
[2023/01/16 06:21] | TRAIN(111): [1250/2211] Batch: 0.0918 (0.0988) Data: 0.0019 (0.0032) Loss: 0.0634 (0.0310)
[2023/01/16 06:22] | TRAIN(111): [1300/2211] Batch: 0.0959 (0.0987) Data: 0.0018 (0.0032) Loss: 0.0019 (0.0308)
[2023/01/16 06:22] | TRAIN(111): [1350/2211] Batch: 0.1180 (0.0991) Data: 0.0021 (0.0031) Loss: 0.0195 (0.0308)
[2023/01/16 06:22] | TRAIN(111): [1400/2211] Batch: 0.1228 (0.0991) Data: 0.0023 (0.0031) Loss: 0.0000 (0.0312)
[2023/01/16 06:22] | TRAIN(111): [1450/2211] Batch: 0.0915 (0.0990) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0313)
[2023/01/16 06:22] | TRAIN(111): [1500/2211] Batch: 0.0917 (0.0989) Data: 0.0019 (0.0030) Loss: 0.0670 (0.0314)
[2023/01/16 06:22] | TRAIN(111): [1550/2211] Batch: 0.0883 (0.0987) Data: 0.0018 (0.0030) Loss: 0.0515 (0.0316)
[2023/01/16 06:22] | TRAIN(111): [1600/2211] Batch: 0.0979 (0.0986) Data: 0.0018 (0.0029) Loss: 0.0497 (0.0314)
[2023/01/16 06:22] | TRAIN(111): [1650/2211] Batch: 0.1090 (0.0985) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0314)
[2023/01/16 06:22] | TRAIN(111): [1700/2211] Batch: 0.0880 (0.0983) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0310)
[2023/01/16 06:22] | TRAIN(111): [1750/2211] Batch: 0.0966 (0.0981) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0309)
[2023/01/16 06:22] | TRAIN(111): [1800/2211] Batch: 0.0921 (0.0980) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0308)
[2023/01/16 06:22] | TRAIN(111): [1850/2211] Batch: 0.0875 (0.0981) Data: 0.0016 (0.0028) Loss: 0.0120 (0.0305)
[2023/01/16 06:22] | TRAIN(111): [1900/2211] Batch: 0.1254 (0.0983) Data: 0.0024 (0.0028) Loss: 0.0135 (0.0306)
[2023/01/16 06:23] | TRAIN(111): [1950/2211] Batch: 0.0898 (0.0984) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0302)
[2023/01/16 06:23] | TRAIN(111): [2000/2211] Batch: 0.1093 (0.0983) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0301)
[2023/01/16 06:23] | TRAIN(111): [2050/2211] Batch: 0.0872 (0.0982) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0300)
[2023/01/16 06:23] | TRAIN(111): [2100/2211] Batch: 0.0921 (0.0986) Data: 0.0017 (0.0027) Loss: 0.0702 (0.0296)
[2023/01/16 06:23] | TRAIN(111): [2150/2211] Batch: 0.0884 (0.0985) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0298)
[2023/01/16 06:23] | TRAIN(111): [2200/2211] Batch: 0.0969 (0.0983) Data: 0.0015 (0.0026) Loss: 0.0000 (0.0297)
[2023/01/16 06:23] | ------------------------------------------------------------
[2023/01/16 06:23] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:23] | ------------------------------------------------------------
[2023/01/16 06:23] |   TRAIN(111)     0:03:37     0:00:05     0:03:31      0.0297
[2023/01/16 06:23] | ------------------------------------------------------------
[2023/01/16 06:23] | **************************************************
[2023/01/16 06:23] | TRAIN(112): [  50/2211] Batch: 0.0982 (0.1307) Data: 0.0018 (0.0262) Loss: 0.0000 (0.0249)
[2023/01/16 06:23] | TRAIN(112): [ 100/2211] Batch: 0.0908 (0.1140) Data: 0.0018 (0.0141) Loss: 0.0000 (0.0219)
[2023/01/16 06:23] | TRAIN(112): [ 150/2211] Batch: 0.0933 (0.1085) Data: 0.0019 (0.0100) Loss: 0.0502 (0.0252)
[2023/01/16 06:23] | TRAIN(112): [ 200/2211] Batch: 0.0950 (0.1075) Data: 0.0017 (0.0080) Loss: 0.1230 (0.0274)
[2023/01/16 06:23] | TRAIN(112): [ 250/2211] Batch: 0.0939 (0.1043) Data: 0.0018 (0.0067) Loss: 0.0591 (0.0276)
[2023/01/16 06:24] | TRAIN(112): [ 300/2211] Batch: 0.0889 (0.1024) Data: 0.0017 (0.0059) Loss: 0.0084 (0.0273)
[2023/01/16 06:24] | TRAIN(112): [ 350/2211] Batch: 0.0887 (0.1012) Data: 0.0013 (0.0053) Loss: 0.0821 (0.0270)
[2023/01/16 06:24] | TRAIN(112): [ 400/2211] Batch: 0.1085 (0.1002) Data: 0.0013 (0.0048) Loss: 0.0000 (0.0267)
[2023/01/16 06:24] | TRAIN(112): [ 450/2211] Batch: 0.1024 (0.0998) Data: 0.0018 (0.0045) Loss: 0.0071 (0.0275)
[2023/01/16 06:24] | TRAIN(112): [ 500/2211] Batch: 0.0913 (0.1001) Data: 0.0018 (0.0042) Loss: 0.1156 (0.0271)
[2023/01/16 06:24] | TRAIN(112): [ 550/2211] Batch: 0.0875 (0.1001) Data: 0.0017 (0.0040) Loss: 0.1118 (0.0270)
[2023/01/16 06:24] | TRAIN(112): [ 600/2211] Batch: 0.0996 (0.1008) Data: 0.0017 (0.0038) Loss: 0.0051 (0.0272)
[2023/01/16 06:24] | TRAIN(112): [ 650/2211] Batch: 0.1057 (0.1006) Data: 0.0018 (0.0037) Loss: 0.0032 (0.0280)
[2023/01/16 06:24] | TRAIN(112): [ 700/2211] Batch: 0.1017 (0.1004) Data: 0.0017 (0.0035) Loss: 0.0203 (0.0276)
[2023/01/16 06:24] | TRAIN(112): [ 750/2211] Batch: 0.0900 (0.1000) Data: 0.0017 (0.0034) Loss: 0.1244 (0.0277)
[2023/01/16 06:24] | TRAIN(112): [ 800/2211] Batch: 0.1014 (0.0999) Data: 0.0017 (0.0033) Loss: 0.0039 (0.0277)
[2023/01/16 06:24] | TRAIN(112): [ 850/2211] Batch: 0.1016 (0.1003) Data: 0.0018 (0.0032) Loss: 0.0165 (0.0273)
[2023/01/16 06:25] | TRAIN(112): [ 900/2211] Batch: 0.0924 (0.1002) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0265)
[2023/01/16 06:25] | TRAIN(112): [ 950/2211] Batch: 0.0904 (0.0999) Data: 0.0019 (0.0031) Loss: 0.0019 (0.0260)
[2023/01/16 06:25] | TRAIN(112): [1000/2211] Batch: 0.0937 (0.0996) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0260)
[2023/01/16 06:25] | TRAIN(112): [1050/2211] Batch: 0.0885 (0.0993) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0261)
[2023/01/16 06:25] | TRAIN(112): [1100/2211] Batch: 0.0874 (0.0988) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0263)
[2023/01/16 06:25] | TRAIN(112): [1150/2211] Batch: 0.0995 (0.0989) Data: 0.0018 (0.0029) Loss: 0.0562 (0.0263)
[2023/01/16 06:25] | TRAIN(112): [1200/2211] Batch: 0.0821 (0.0986) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0262)
[2023/01/16 06:25] | TRAIN(112): [1250/2211] Batch: 0.0834 (0.0980) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0263)
[2023/01/16 06:25] | TRAIN(112): [1300/2211] Batch: 0.0882 (0.0976) Data: 0.0018 (0.0027) Loss: 0.1413 (0.0263)
[2023/01/16 06:25] | TRAIN(112): [1350/2211] Batch: 0.0849 (0.0971) Data: 0.0018 (0.0027) Loss: 0.0898 (0.0264)
[2023/01/16 06:25] | TRAIN(112): [1400/2211] Batch: 0.0979 (0.0972) Data: 0.0018 (0.0027) Loss: 0.0472 (0.0266)
[2023/01/16 06:25] | TRAIN(112): [1450/2211] Batch: 0.0955 (0.0973) Data: 0.0018 (0.0026) Loss: 0.0249 (0.0267)
[2023/01/16 06:25] | TRAIN(112): [1500/2211] Batch: 0.1148 (0.0974) Data: 0.0017 (0.0026) Loss: 0.0314 (0.0267)
[2023/01/16 06:26] | TRAIN(112): [1550/2211] Batch: 0.0900 (0.0976) Data: 0.0018 (0.0026) Loss: 0.0182 (0.0266)
[2023/01/16 06:26] | TRAIN(112): [1600/2211] Batch: 0.0973 (0.0975) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0265)
[2023/01/16 06:26] | TRAIN(112): [1650/2211] Batch: 0.0944 (0.0975) Data: 0.0016 (0.0025) Loss: 0.0083 (0.0265)
[2023/01/16 06:26] | TRAIN(112): [1700/2211] Batch: 0.0957 (0.0974) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0264)
[2023/01/16 06:26] | TRAIN(112): [1750/2211] Batch: 0.0945 (0.0974) Data: 0.0017 (0.0025) Loss: 0.0824 (0.0266)
[2023/01/16 06:26] | TRAIN(112): [1800/2211] Batch: 0.0933 (0.0974) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0265)
[2023/01/16 06:26] | TRAIN(112): [1850/2211] Batch: 0.0873 (0.0972) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0266)
[2023/01/16 06:26] | TRAIN(112): [1900/2211] Batch: 0.0992 (0.0971) Data: 0.0016 (0.0024) Loss: 0.0566 (0.0268)
[2023/01/16 06:26] | TRAIN(112): [1950/2211] Batch: 0.0865 (0.0970) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0267)
[2023/01/16 06:26] | TRAIN(112): [2000/2211] Batch: 0.1170 (0.0969) Data: 0.0021 (0.0024) Loss: 0.0000 (0.0268)
[2023/01/16 06:26] | TRAIN(112): [2050/2211] Batch: 0.1010 (0.0971) Data: 0.0018 (0.0024) Loss: 0.0950 (0.0271)
[2023/01/16 06:26] | TRAIN(112): [2100/2211] Batch: 0.0871 (0.0971) Data: 0.0017 (0.0024) Loss: 0.0020 (0.0271)
[2023/01/16 06:26] | TRAIN(112): [2150/2211] Batch: 0.1181 (0.0971) Data: 0.0017 (0.0024) Loss: 0.1081 (0.0274)
[2023/01/16 06:27] | TRAIN(112): [2200/2211] Batch: 0.0988 (0.0972) Data: 0.0016 (0.0024) Loss: 0.0814 (0.0277)
[2023/01/16 06:27] | ------------------------------------------------------------
[2023/01/16 06:27] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:27] | ------------------------------------------------------------
[2023/01/16 06:27] |   TRAIN(112)     0:03:34     0:00:05     0:03:29      0.0277
[2023/01/16 06:27] | ------------------------------------------------------------
[2023/01/16 06:27] | **************************************************
[2023/01/16 06:27] | TRAIN(113): [  50/2211] Batch: 0.1240 (0.1396) Data: 0.0025 (0.0267) Loss: 0.0000 (0.0247)
[2023/01/16 06:27] | TRAIN(113): [ 100/2211] Batch: 0.0883 (0.1179) Data: 0.0017 (0.0144) Loss: 0.0306 (0.0248)
[2023/01/16 06:27] | TRAIN(113): [ 150/2211] Batch: 0.0962 (0.1096) Data: 0.0016 (0.0102) Loss: 0.0000 (0.0230)
[2023/01/16 06:27] | TRAIN(113): [ 200/2211] Batch: 0.0898 (0.1098) Data: 0.0018 (0.0082) Loss: 0.0000 (0.0252)
[2023/01/16 06:27] | TRAIN(113): [ 250/2211] Batch: 0.0981 (0.1084) Data: 0.0018 (0.0069) Loss: 0.0000 (0.0244)
[2023/01/16 06:27] | TRAIN(113): [ 300/2211] Batch: 0.1247 (0.1102) Data: 0.0024 (0.0061) Loss: 0.0000 (0.0249)
[2023/01/16 06:27] | TRAIN(113): [ 350/2211] Batch: 0.0873 (0.1090) Data: 0.0018 (0.0056) Loss: 0.1205 (0.0259)
[2023/01/16 06:27] | TRAIN(113): [ 400/2211] Batch: 0.1247 (0.1093) Data: 0.0022 (0.0051) Loss: 0.0457 (0.0280)
[2023/01/16 06:27] | TRAIN(113): [ 450/2211] Batch: 0.0930 (0.1097) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0293)
[2023/01/16 06:27] | TRAIN(113): [ 500/2211] Batch: 0.0900 (0.1099) Data: 0.0018 (0.0045) Loss: 0.0529 (0.0300)
[2023/01/16 06:28] | TRAIN(113): [ 550/2211] Batch: 0.0987 (0.1084) Data: 0.0021 (0.0043) Loss: 0.0181 (0.0301)
[2023/01/16 06:28] | TRAIN(113): [ 600/2211] Batch: 0.0968 (0.1075) Data: 0.0020 (0.0041) Loss: 0.0261 (0.0303)
[2023/01/16 06:28] | TRAIN(113): [ 650/2211] Batch: 0.1182 (0.1065) Data: 0.0022 (0.0039) Loss: 0.0000 (0.0304)
[2023/01/16 06:28] | TRAIN(113): [ 700/2211] Batch: 0.0942 (0.1060) Data: 0.0016 (0.0037) Loss: 0.0000 (0.0297)
[2023/01/16 06:28] | TRAIN(113): [ 750/2211] Batch: 0.0880 (0.1052) Data: 0.0014 (0.0036) Loss: 0.0102 (0.0291)
[2023/01/16 06:28] | TRAIN(113): [ 800/2211] Batch: 0.0882 (0.1044) Data: 0.0018 (0.0035) Loss: 0.0172 (0.0293)
[2023/01/16 06:28] | TRAIN(113): [ 850/2211] Batch: 0.0895 (0.1040) Data: 0.0018 (0.0034) Loss: 0.0346 (0.0295)
[2023/01/16 06:28] | TRAIN(113): [ 900/2211] Batch: 0.1125 (0.1033) Data: 0.0017 (0.0033) Loss: 0.0849 (0.0297)
[2023/01/16 06:28] | TRAIN(113): [ 950/2211] Batch: 0.0890 (0.1028) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0296)
[2023/01/16 06:28] | TRAIN(113): [1000/2211] Batch: 0.0864 (0.1026) Data: 0.0017 (0.0032) Loss: 0.1258 (0.0294)
[2023/01/16 06:28] | TRAIN(113): [1050/2211] Batch: 0.0893 (0.1021) Data: 0.0015 (0.0031) Loss: 0.0152 (0.0294)
[2023/01/16 06:28] | TRAIN(113): [1100/2211] Batch: 0.0935 (0.1018) Data: 0.0018 (0.0030) Loss: 0.1152 (0.0293)
[2023/01/16 06:29] | TRAIN(113): [1150/2211] Batch: 0.0993 (0.1016) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0294)
[2023/01/16 06:29] | TRAIN(113): [1200/2211] Batch: 0.0881 (0.1012) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0304)
[2023/01/16 06:29] | TRAIN(113): [1250/2211] Batch: 0.0878 (0.1008) Data: 0.0019 (0.0029) Loss: 0.0039 (0.0304)
[2023/01/16 06:29] | TRAIN(113): [1300/2211] Batch: 0.0877 (0.1004) Data: 0.0017 (0.0028) Loss: 0.0304 (0.0303)
[2023/01/16 06:29] | TRAIN(113): [1350/2211] Batch: 0.0946 (0.1000) Data: 0.0016 (0.0028) Loss: 0.1021 (0.0305)
[2023/01/16 06:29] | TRAIN(113): [1400/2211] Batch: 0.1262 (0.1002) Data: 0.0020 (0.0028) Loss: 0.0165 (0.0304)
[2023/01/16 06:29] | TRAIN(113): [1450/2211] Batch: 0.0883 (0.1001) Data: 0.0018 (0.0027) Loss: 0.0104 (0.0305)
[2023/01/16 06:29] | TRAIN(113): [1500/2211] Batch: 0.0919 (0.1001) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0304)
[2023/01/16 06:29] | TRAIN(113): [1550/2211] Batch: 0.0917 (0.1000) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0304)
[2023/01/16 06:29] | TRAIN(113): [1600/2211] Batch: 0.1069 (0.0999) Data: 0.0016 (0.0026) Loss: 0.0373 (0.0305)
[2023/01/16 06:29] | TRAIN(113): [1650/2211] Batch: 0.1212 (0.1000) Data: 0.0022 (0.0026) Loss: 0.1082 (0.0309)
[2023/01/16 06:29] | TRAIN(113): [1700/2211] Batch: 0.0991 (0.1001) Data: 0.0017 (0.0026) Loss: 0.0021 (0.0311)
[2023/01/16 06:29] | TRAIN(113): [1750/2211] Batch: 0.0971 (0.1000) Data: 0.0017 (0.0026) Loss: 0.0099 (0.0313)
[2023/01/16 06:30] | TRAIN(113): [1800/2211] Batch: 0.1027 (0.0998) Data: 0.0018 (0.0026) Loss: 0.0991 (0.0317)
[2023/01/16 06:30] | TRAIN(113): [1850/2211] Batch: 0.0920 (0.0996) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0317)
[2023/01/16 06:30] | TRAIN(113): [1900/2211] Batch: 0.0945 (0.0995) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0316)
[2023/01/16 06:30] | TRAIN(113): [1950/2211] Batch: 0.0884 (0.0995) Data: 0.0018 (0.0025) Loss: 0.0837 (0.0316)
[2023/01/16 06:30] | TRAIN(113): [2000/2211] Batch: 0.0923 (0.0998) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0316)
[2023/01/16 06:30] | TRAIN(113): [2050/2211] Batch: 0.0882 (0.0997) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0321)
[2023/01/16 06:30] | TRAIN(113): [2100/2211] Batch: 0.1060 (0.0996) Data: 0.0017 (0.0025) Loss: 0.0001 (0.0319)
[2023/01/16 06:30] | TRAIN(113): [2150/2211] Batch: 0.0917 (0.0995) Data: 0.0017 (0.0024) Loss: 0.1876 (0.0320)
[2023/01/16 06:30] | TRAIN(113): [2200/2211] Batch: 0.1221 (0.0997) Data: 0.0021 (0.0024) Loss: 0.0638 (0.0320)
[2023/01/16 06:30] | ------------------------------------------------------------
[2023/01/16 06:30] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:30] | ------------------------------------------------------------
[2023/01/16 06:30] |   TRAIN(113)     0:03:40     0:00:05     0:03:35      0.0320
[2023/01/16 06:30] | ------------------------------------------------------------
[2023/01/16 06:30] | **************************************************
[2023/01/16 06:30] | TRAIN(114): [  50/2211] Batch: 0.0971 (0.1170) Data: 0.0018 (0.0258) Loss: 0.1009 (0.0297)
[2023/01/16 06:30] | TRAIN(114): [ 100/2211] Batch: 0.0936 (0.1053) Data: 0.0019 (0.0139) Loss: 0.0043 (0.0278)
[2023/01/16 06:31] | TRAIN(114): [ 150/2211] Batch: 0.1231 (0.1027) Data: 0.0024 (0.0099) Loss: 0.0010 (0.0275)
[2023/01/16 06:31] | TRAIN(114): [ 200/2211] Batch: 0.1100 (0.1058) Data: 0.0017 (0.0080) Loss: 0.0000 (0.0282)
[2023/01/16 06:31] | TRAIN(114): [ 250/2211] Batch: 0.1228 (0.1057) Data: 0.0025 (0.0068) Loss: 0.0000 (0.0273)
[2023/01/16 06:31] | TRAIN(114): [ 300/2211] Batch: 0.1189 (0.1075) Data: 0.0023 (0.0060) Loss: 0.0364 (0.0290)
[2023/01/16 06:31] | TRAIN(114): [ 350/2211] Batch: 0.1250 (0.1071) Data: 0.0022 (0.0055) Loss: 0.0000 (0.0285)
[2023/01/16 06:31] | TRAIN(114): [ 400/2211] Batch: 0.1018 (0.1068) Data: 0.0018 (0.0050) Loss: 0.0002 (0.0292)
[2023/01/16 06:31] | TRAIN(114): [ 450/2211] Batch: 0.0987 (0.1071) Data: 0.0017 (0.0047) Loss: 0.0000 (0.0296)
[2023/01/16 06:31] | TRAIN(114): [ 500/2211] Batch: 0.0941 (0.1056) Data: 0.0019 (0.0044) Loss: 0.0755 (0.0295)
[2023/01/16 06:31] | TRAIN(114): [ 550/2211] Batch: 0.0908 (0.1049) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0293)
[2023/01/16 06:31] | TRAIN(114): [ 600/2211] Batch: 0.0927 (0.1041) Data: 0.0019 (0.0040) Loss: 0.0243 (0.0288)
[2023/01/16 06:31] | TRAIN(114): [ 650/2211] Batch: 0.0880 (0.1034) Data: 0.0017 (0.0038) Loss: 0.0000 (0.0294)
[2023/01/16 06:31] | TRAIN(114): [ 700/2211] Batch: 0.0998 (0.1026) Data: 0.0020 (0.0037) Loss: 0.0093 (0.0289)
[2023/01/16 06:32] | TRAIN(114): [ 750/2211] Batch: 0.1183 (0.1028) Data: 0.0022 (0.0036) Loss: 0.0857 (0.0290)
[2023/01/16 06:32] | TRAIN(114): [ 800/2211] Batch: 0.1083 (0.1038) Data: 0.0016 (0.0035) Loss: 0.0232 (0.0286)
[2023/01/16 06:32] | TRAIN(114): [ 850/2211] Batch: 0.1136 (0.1043) Data: 0.0014 (0.0034) Loss: 0.0000 (0.0284)
[2023/01/16 06:32] | TRAIN(114): [ 900/2211] Batch: 0.1239 (0.1062) Data: 0.0024 (0.0033) Loss: 0.0000 (0.0283)
[2023/01/16 06:32] | TRAIN(114): [ 950/2211] Batch: 0.1271 (0.1073) Data: 0.0023 (0.0032) Loss: 0.0293 (0.0281)
[2023/01/16 06:32] | TRAIN(114): [1000/2211] Batch: 0.1094 (0.1082) Data: 0.0016 (0.0032) Loss: 0.0051 (0.0283)
[2023/01/16 06:32] | TRAIN(114): [1050/2211] Batch: 0.1092 (0.1087) Data: 0.0021 (0.0031) Loss: 0.0032 (0.0283)
[2023/01/16 06:32] | TRAIN(114): [1100/2211] Batch: 0.1022 (0.1083) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0286)
[2023/01/16 06:32] | TRAIN(114): [1150/2211] Batch: 0.1026 (0.1084) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0282)
[2023/01/16 06:32] | TRAIN(114): [1200/2211] Batch: 0.1218 (0.1085) Data: 0.0018 (0.0029) Loss: 0.0335 (0.0280)
[2023/01/16 06:33] | TRAIN(114): [1250/2211] Batch: 0.1217 (0.1089) Data: 0.0015 (0.0029) Loss: 0.0000 (0.0275)
[2023/01/16 06:33] | TRAIN(114): [1300/2211] Batch: 0.1163 (0.1092) Data: 0.0016 (0.0028) Loss: 0.0367 (0.0274)
[2023/01/16 06:33] | TRAIN(114): [1350/2211] Batch: 0.0871 (0.1092) Data: 0.0014 (0.0028) Loss: 0.0031 (0.0273)
[2023/01/16 06:33] | TRAIN(114): [1400/2211] Batch: 0.0936 (0.1086) Data: 0.0019 (0.0028) Loss: 0.0825 (0.0274)
[2023/01/16 06:33] | TRAIN(114): [1450/2211] Batch: 0.0911 (0.1083) Data: 0.0018 (0.0027) Loss: 0.0446 (0.0271)
[2023/01/16 06:33] | TRAIN(114): [1500/2211] Batch: 0.1009 (0.1080) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0271)
[2023/01/16 06:33] | TRAIN(114): [1550/2211] Batch: 0.1095 (0.1078) Data: 0.0016 (0.0027) Loss: 0.0593 (0.0272)
[2023/01/16 06:33] | TRAIN(114): [1600/2211] Batch: 0.1195 (0.1079) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0271)
[2023/01/16 06:33] | TRAIN(114): [1650/2211] Batch: 0.0919 (0.1080) Data: 0.0017 (0.0026) Loss: 0.0185 (0.0273)
[2023/01/16 06:33] | TRAIN(114): [1700/2211] Batch: 0.0883 (0.1075) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0282)
[2023/01/16 06:33] | TRAIN(114): [1750/2211] Batch: 0.0950 (0.1071) Data: 0.0019 (0.0026) Loss: 0.0927 (0.0282)
[2023/01/16 06:33] | TRAIN(114): [1800/2211] Batch: 0.1106 (0.1069) Data: 0.0018 (0.0025) Loss: 0.0178 (0.0285)
[2023/01/16 06:34] | TRAIN(114): [1850/2211] Batch: 0.0954 (0.1067) Data: 0.0018 (0.0025) Loss: 0.0662 (0.0285)
[2023/01/16 06:34] | TRAIN(114): [1900/2211] Batch: 0.1001 (0.1065) Data: 0.0014 (0.0025) Loss: 0.0571 (0.0285)
[2023/01/16 06:34] | TRAIN(114): [1950/2211] Batch: 0.0894 (0.1062) Data: 0.0018 (0.0025) Loss: 0.0069 (0.0284)
[2023/01/16 06:34] | TRAIN(114): [2000/2211] Batch: 0.0979 (0.1060) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0283)
[2023/01/16 06:34] | TRAIN(114): [2050/2211] Batch: 0.0932 (0.1058) Data: 0.0021 (0.0024) Loss: 0.0209 (0.0284)
[2023/01/16 06:34] | TRAIN(114): [2100/2211] Batch: 0.0969 (0.1055) Data: 0.0019 (0.0024) Loss: 0.1561 (0.0281)
[2023/01/16 06:34] | TRAIN(114): [2150/2211] Batch: 0.0902 (0.1052) Data: 0.0014 (0.0024) Loss: 0.0000 (0.0282)
[2023/01/16 06:34] | TRAIN(114): [2200/2211] Batch: 0.0993 (0.1050) Data: 0.0016 (0.0024) Loss: 0.0244 (0.0282)
[2023/01/16 06:34] | ------------------------------------------------------------
[2023/01/16 06:34] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:34] | ------------------------------------------------------------
[2023/01/16 06:34] |   TRAIN(114)     0:03:52     0:00:05     0:03:46      0.0281
[2023/01/16 06:34] | ------------------------------------------------------------
[2023/01/16 06:34] | **************************************************
[2023/01/16 06:34] | TRAIN(115): [  50/2211] Batch: 0.0953 (0.1230) Data: 0.0017 (0.0263) Loss: 0.0922 (0.0219)
[2023/01/16 06:34] | TRAIN(115): [ 100/2211] Batch: 0.0878 (0.1073) Data: 0.0017 (0.0142) Loss: 0.1005 (0.0221)
[2023/01/16 06:34] | TRAIN(115): [ 150/2211] Batch: 0.0873 (0.1020) Data: 0.0017 (0.0101) Loss: 0.0221 (0.0230)
[2023/01/16 06:34] | TRAIN(115): [ 200/2211] Batch: 0.0881 (0.0993) Data: 0.0019 (0.0080) Loss: 0.0678 (0.0238)
[2023/01/16 06:35] | TRAIN(115): [ 250/2211] Batch: 0.0982 (0.0983) Data: 0.0021 (0.0068) Loss: 0.0109 (0.0244)
[2023/01/16 06:35] | TRAIN(115): [ 300/2211] Batch: 0.0976 (0.0977) Data: 0.0017 (0.0060) Loss: 0.0416 (0.0260)
[2023/01/16 06:35] | TRAIN(115): [ 350/2211] Batch: 0.0915 (0.0970) Data: 0.0018 (0.0054) Loss: 0.0056 (0.0242)
[2023/01/16 06:35] | TRAIN(115): [ 400/2211] Batch: 0.0856 (0.0974) Data: 0.0020 (0.0049) Loss: 0.0559 (0.0259)
[2023/01/16 06:35] | TRAIN(115): [ 450/2211] Batch: 0.0895 (0.0978) Data: 0.0018 (0.0046) Loss: 0.0564 (0.0262)
[2023/01/16 06:35] | TRAIN(115): [ 500/2211] Batch: 0.0955 (0.0976) Data: 0.0018 (0.0043) Loss: 0.1729 (0.0267)
[2023/01/16 06:35] | TRAIN(115): [ 550/2211] Batch: 0.0897 (0.0978) Data: 0.0017 (0.0041) Loss: 0.0494 (0.0269)
[2023/01/16 06:35] | TRAIN(115): [ 600/2211] Batch: 0.1015 (0.0980) Data: 0.0019 (0.0039) Loss: 0.0277 (0.0265)
[2023/01/16 06:35] | TRAIN(115): [ 650/2211] Batch: 0.0932 (0.0978) Data: 0.0018 (0.0038) Loss: 0.0031 (0.0267)
[2023/01/16 06:35] | TRAIN(115): [ 700/2211] Batch: 0.0951 (0.0978) Data: 0.0015 (0.0036) Loss: 0.0216 (0.0272)
[2023/01/16 06:35] | TRAIN(115): [ 750/2211] Batch: 0.0989 (0.0979) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0271)
[2023/01/16 06:35] | TRAIN(115): [ 800/2211] Batch: 0.0881 (0.0974) Data: 0.0018 (0.0034) Loss: 0.2528 (0.0274)
[2023/01/16 06:36] | TRAIN(115): [ 850/2211] Batch: 0.1189 (0.0972) Data: 0.0022 (0.0033) Loss: 0.0000 (0.0279)
[2023/01/16 06:36] | TRAIN(115): [ 900/2211] Batch: 0.0944 (0.0972) Data: 0.0017 (0.0032) Loss: 0.0033 (0.0279)
[2023/01/16 06:36] | TRAIN(115): [ 950/2211] Batch: 0.0928 (0.0971) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0278)
[2023/01/16 06:36] | TRAIN(115): [1000/2211] Batch: 0.0876 (0.0968) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0281)
[2023/01/16 06:36] | TRAIN(115): [1050/2211] Batch: 0.0930 (0.0965) Data: 0.0018 (0.0030) Loss: 0.0048 (0.0285)
[2023/01/16 06:36] | TRAIN(115): [1100/2211] Batch: 0.0945 (0.0969) Data: 0.0016 (0.0030) Loss: 0.0556 (0.0285)
[2023/01/16 06:36] | TRAIN(115): [1150/2211] Batch: 0.0950 (0.0968) Data: 0.0019 (0.0029) Loss: 0.0684 (0.0289)
[2023/01/16 06:36] | TRAIN(115): [1200/2211] Batch: 0.1024 (0.0968) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0288)
[2023/01/16 06:36] | TRAIN(115): [1250/2211] Batch: 0.1182 (0.0974) Data: 0.0022 (0.0028) Loss: 0.0043 (0.0289)
[2023/01/16 06:36] | TRAIN(115): [1300/2211] Batch: 0.0994 (0.0974) Data: 0.0020 (0.0028) Loss: 0.0000 (0.0288)
[2023/01/16 06:36] | TRAIN(115): [1350/2211] Batch: 0.0957 (0.0972) Data: 0.0018 (0.0028) Loss: 0.0239 (0.0291)
[2023/01/16 06:36] | TRAIN(115): [1400/2211] Batch: 0.0936 (0.0971) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0293)
[2023/01/16 06:36] | TRAIN(115): [1450/2211] Batch: 0.0976 (0.0970) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0291)
[2023/01/16 06:37] | TRAIN(115): [1500/2211] Batch: 0.0850 (0.0968) Data: 0.0018 (0.0027) Loss: 0.0444 (0.0297)
[2023/01/16 06:37] | TRAIN(115): [1550/2211] Batch: 0.1525 (0.0970) Data: 0.0026 (0.0026) Loss: 0.0537 (0.0294)
[2023/01/16 06:37] | TRAIN(115): [1600/2211] Batch: 0.0895 (0.0971) Data: 0.0018 (0.0026) Loss: 0.0806 (0.0296)
[2023/01/16 06:37] | TRAIN(115): [1650/2211] Batch: 0.0977 (0.0970) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0292)
[2023/01/16 06:37] | TRAIN(115): [1700/2211] Batch: 0.1171 (0.0969) Data: 0.0018 (0.0026) Loss: 0.0014 (0.0293)
[2023/01/16 06:37] | TRAIN(115): [1750/2211] Batch: 0.0979 (0.0974) Data: 0.0017 (0.0026) Loss: 0.0339 (0.0290)
[2023/01/16 06:37] | TRAIN(115): [1800/2211] Batch: 0.0931 (0.0974) Data: 0.0018 (0.0025) Loss: 0.0928 (0.0292)
[2023/01/16 06:37] | TRAIN(115): [1850/2211] Batch: 0.0906 (0.0974) Data: 0.0019 (0.0025) Loss: 0.0948 (0.0292)
[2023/01/16 06:37] | TRAIN(115): [1900/2211] Batch: 0.0910 (0.0973) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0291)
[2023/01/16 06:37] | TRAIN(115): [1950/2211] Batch: 0.0900 (0.0971) Data: 0.0018 (0.0025) Loss: 0.0276 (0.0291)
[2023/01/16 06:37] | TRAIN(115): [2000/2211] Batch: 0.0879 (0.0971) Data: 0.0019 (0.0025) Loss: 0.0935 (0.0289)
[2023/01/16 06:37] | TRAIN(115): [2050/2211] Batch: 0.0964 (0.0970) Data: 0.0020 (0.0025) Loss: 0.0823 (0.0288)
[2023/01/16 06:38] | TRAIN(115): [2100/2211] Batch: 0.0926 (0.0971) Data: 0.0017 (0.0024) Loss: 0.0497 (0.0288)
[2023/01/16 06:38] | TRAIN(115): [2150/2211] Batch: 0.0894 (0.0970) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0288)
[2023/01/16 06:38] | TRAIN(115): [2200/2211] Batch: 0.0867 (0.0968) Data: 0.0016 (0.0024) Loss: 0.0586 (0.0288)
[2023/01/16 06:38] | ------------------------------------------------------------
[2023/01/16 06:38] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:38] | ------------------------------------------------------------
[2023/01/16 06:38] |   TRAIN(115)     0:03:33     0:00:05     0:03:28      0.0289
[2023/01/16 06:38] | ------------------------------------------------------------
[2023/01/16 06:38] | **************************************************
[2023/01/16 06:38] | TRAIN(116): [  50/2211] Batch: 0.0883 (0.1185) Data: 0.0018 (0.0243) Loss: 0.0362 (0.0222)
[2023/01/16 06:38] | TRAIN(116): [ 100/2211] Batch: 0.0905 (0.1046) Data: 0.0019 (0.0132) Loss: 0.0020 (0.0251)
[2023/01/16 06:38] | TRAIN(116): [ 150/2211] Batch: 0.0891 (0.0999) Data: 0.0018 (0.0094) Loss: 0.0000 (0.0264)
[2023/01/16 06:38] | TRAIN(116): [ 200/2211] Batch: 0.0999 (0.1005) Data: 0.0016 (0.0076) Loss: 0.0942 (0.0323)
[2023/01/16 06:38] | TRAIN(116): [ 250/2211] Batch: 0.0875 (0.1011) Data: 0.0018 (0.0065) Loss: 0.0000 (0.0316)
[2023/01/16 06:38] | TRAIN(116): [ 300/2211] Batch: 0.1199 (0.1038) Data: 0.0023 (0.0058) Loss: 0.0041 (0.0320)
[2023/01/16 06:38] | TRAIN(116): [ 350/2211] Batch: 0.1215 (0.1047) Data: 0.0022 (0.0052) Loss: 0.0234 (0.0323)
[2023/01/16 06:38] | TRAIN(116): [ 400/2211] Batch: 0.0889 (0.1039) Data: 0.0018 (0.0048) Loss: 0.0461 (0.0299)
[2023/01/16 06:38] | TRAIN(116): [ 450/2211] Batch: 0.0887 (0.1021) Data: 0.0019 (0.0045) Loss: 0.0000 (0.0296)
[2023/01/16 06:39] | TRAIN(116): [ 500/2211] Batch: 0.0945 (0.1013) Data: 0.0017 (0.0042) Loss: 0.0000 (0.0292)
[2023/01/16 06:39] | TRAIN(116): [ 550/2211] Batch: 0.0969 (0.1006) Data: 0.0019 (0.0040) Loss: 0.0000 (0.0283)
[2023/01/16 06:39] | TRAIN(116): [ 600/2211] Batch: 0.0850 (0.1001) Data: 0.0018 (0.0038) Loss: 0.0705 (0.0279)
[2023/01/16 06:39] | TRAIN(116): [ 650/2211] Batch: 0.0881 (0.0999) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0279)
[2023/01/16 06:39] | TRAIN(116): [ 700/2211] Batch: 0.0909 (0.0999) Data: 0.0019 (0.0036) Loss: 0.0116 (0.0287)
[2023/01/16 06:39] | TRAIN(116): [ 750/2211] Batch: 0.0883 (0.0994) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0295)
[2023/01/16 06:39] | TRAIN(116): [ 800/2211] Batch: 0.0992 (0.0991) Data: 0.0017 (0.0033) Loss: 0.0000 (0.0313)
[2023/01/16 06:39] | TRAIN(116): [ 850/2211] Batch: 0.0885 (0.0989) Data: 0.0019 (0.0033) Loss: 0.0714 (0.0315)
[2023/01/16 06:39] | TRAIN(116): [ 900/2211] Batch: 0.0891 (0.0989) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0312)
[2023/01/16 06:39] | TRAIN(116): [ 950/2211] Batch: 0.1188 (0.0990) Data: 0.0023 (0.0031) Loss: 0.0176 (0.0313)
[2023/01/16 06:39] | TRAIN(116): [1000/2211] Batch: 0.0837 (0.0991) Data: 0.0018 (0.0031) Loss: 0.0076 (0.0320)
[2023/01/16 06:39] | TRAIN(116): [1050/2211] Batch: 0.0989 (0.0989) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0320)
[2023/01/16 06:40] | TRAIN(116): [1100/2211] Batch: 0.0978 (0.0991) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0316)
[2023/01/16 06:40] | TRAIN(116): [1150/2211] Batch: 0.0911 (0.0989) Data: 0.0019 (0.0029) Loss: 0.0456 (0.0317)
[2023/01/16 06:40] | TRAIN(116): [1200/2211] Batch: 0.0909 (0.0987) Data: 0.0015 (0.0029) Loss: 0.0270 (0.0319)
[2023/01/16 06:40] | TRAIN(116): [1250/2211] Batch: 0.0871 (0.0988) Data: 0.0018 (0.0028) Loss: 0.0031 (0.0316)
[2023/01/16 06:40] | TRAIN(116): [1300/2211] Batch: 0.0889 (0.0987) Data: 0.0018 (0.0028) Loss: 0.0538 (0.0313)
[2023/01/16 06:40] | TRAIN(116): [1350/2211] Batch: 0.0945 (0.0986) Data: 0.0019 (0.0028) Loss: 0.0199 (0.0314)
[2023/01/16 06:40] | TRAIN(116): [1400/2211] Batch: 0.0867 (0.0983) Data: 0.0017 (0.0027) Loss: 0.0030 (0.0315)
[2023/01/16 06:40] | TRAIN(116): [1450/2211] Batch: 0.1011 (0.0984) Data: 0.0017 (0.0027) Loss: 0.0084 (0.0313)
[2023/01/16 06:40] | TRAIN(116): [1500/2211] Batch: 0.0934 (0.0984) Data: 0.0018 (0.0027) Loss: 0.0349 (0.0310)
[2023/01/16 06:40] | TRAIN(116): [1550/2211] Batch: 0.0950 (0.0983) Data: 0.0018 (0.0026) Loss: 0.0786 (0.0310)
[2023/01/16 06:40] | TRAIN(116): [1600/2211] Batch: 0.1076 (0.0982) Data: 0.0017 (0.0026) Loss: 0.0931 (0.0311)
[2023/01/16 06:40] | TRAIN(116): [1650/2211] Batch: 0.0895 (0.0980) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0312)
[2023/01/16 06:40] | TRAIN(116): [1700/2211] Batch: 0.0930 (0.0980) Data: 0.0019 (0.0026) Loss: 0.0242 (0.0311)
[2023/01/16 06:41] | TRAIN(116): [1750/2211] Batch: 0.0925 (0.0979) Data: 0.0018 (0.0025) Loss: 0.0045 (0.0310)
[2023/01/16 06:41] | TRAIN(116): [1800/2211] Batch: 0.0881 (0.0978) Data: 0.0018 (0.0025) Loss: 0.0737 (0.0312)
[2023/01/16 06:41] | TRAIN(116): [1850/2211] Batch: 0.1260 (0.0976) Data: 0.0022 (0.0025) Loss: 0.0358 (0.0311)
[2023/01/16 06:41] | TRAIN(116): [1900/2211] Batch: 0.0927 (0.0977) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0307)
[2023/01/16 06:41] | TRAIN(116): [1950/2211] Batch: 0.0914 (0.0976) Data: 0.0018 (0.0025) Loss: 0.0974 (0.0307)
[2023/01/16 06:41] | TRAIN(116): [2000/2211] Batch: 0.0938 (0.0975) Data: 0.0017 (0.0025) Loss: 0.0074 (0.0306)
[2023/01/16 06:41] | TRAIN(116): [2050/2211] Batch: 0.0892 (0.0974) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0303)
[2023/01/16 06:41] | TRAIN(116): [2100/2211] Batch: 0.1176 (0.0972) Data: 0.0022 (0.0024) Loss: 0.0000 (0.0301)
[2023/01/16 06:41] | TRAIN(116): [2150/2211] Batch: 0.0925 (0.0972) Data: 0.0023 (0.0024) Loss: 0.0000 (0.0299)
[2023/01/16 06:41] | TRAIN(116): [2200/2211] Batch: 0.0997 (0.0971) Data: 0.0017 (0.0024) Loss: 0.0040 (0.0295)
[2023/01/16 06:41] | ------------------------------------------------------------
[2023/01/16 06:41] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:41] | ------------------------------------------------------------
[2023/01/16 06:41] |   TRAIN(116)     0:03:34     0:00:05     0:03:29      0.0295
[2023/01/16 06:41] | ------------------------------------------------------------
[2023/01/16 06:41] | **************************************************
[2023/01/16 06:41] | TRAIN(117): [  50/2211] Batch: 0.1186 (0.1296) Data: 0.0024 (0.0267) Loss: 0.1225 (0.0313)
[2023/01/16 06:41] | TRAIN(117): [ 100/2211] Batch: 0.1108 (0.1216) Data: 0.0044 (0.0145) Loss: 0.0000 (0.0304)
[2023/01/16 06:42] | TRAIN(117): [ 150/2211] Batch: 0.0874 (0.1173) Data: 0.0018 (0.0104) Loss: 0.0293 (0.0293)
[2023/01/16 06:42] | TRAIN(117): [ 200/2211] Batch: 0.0862 (0.1115) Data: 0.0017 (0.0083) Loss: 0.0052 (0.0265)
[2023/01/16 06:42] | TRAIN(117): [ 250/2211] Batch: 0.1189 (0.1134) Data: 0.0021 (0.0071) Loss: 0.0628 (0.0256)
[2023/01/16 06:42] | TRAIN(117): [ 300/2211] Batch: 0.0874 (0.1120) Data: 0.0018 (0.0063) Loss: 0.0000 (0.0296)
[2023/01/16 06:42] | TRAIN(117): [ 350/2211] Batch: 0.1002 (0.1098) Data: 0.0018 (0.0056) Loss: 0.0000 (0.0303)
[2023/01/16 06:42] | TRAIN(117): [ 400/2211] Batch: 0.0937 (0.1081) Data: 0.0018 (0.0052) Loss: 0.0000 (0.0294)
[2023/01/16 06:42] | TRAIN(117): [ 450/2211] Batch: 0.0956 (0.1077) Data: 0.0017 (0.0048) Loss: 0.0247 (0.0299)
[2023/01/16 06:42] | TRAIN(117): [ 500/2211] Batch: 0.0956 (0.1064) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0286)
[2023/01/16 06:42] | TRAIN(117): [ 550/2211] Batch: 0.0947 (0.1057) Data: 0.0019 (0.0043) Loss: 0.0000 (0.0291)
[2023/01/16 06:42] | TRAIN(117): [ 600/2211] Batch: 0.0873 (0.1044) Data: 0.0018 (0.0041) Loss: 0.0000 (0.0288)
[2023/01/16 06:42] | TRAIN(117): [ 650/2211] Batch: 0.0949 (0.1039) Data: 0.0017 (0.0039) Loss: 0.0648 (0.0283)
[2023/01/16 06:42] | TRAIN(117): [ 700/2211] Batch: 0.0936 (0.1032) Data: 0.0018 (0.0037) Loss: 0.0515 (0.0285)
[2023/01/16 06:43] | TRAIN(117): [ 750/2211] Batch: 0.0889 (0.1025) Data: 0.0019 (0.0036) Loss: 0.1121 (0.0287)
[2023/01/16 06:43] | TRAIN(117): [ 800/2211] Batch: 0.0968 (0.1023) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0290)
[2023/01/16 06:43] | TRAIN(117): [ 850/2211] Batch: 0.0893 (0.1026) Data: 0.0017 (0.0034) Loss: 0.1777 (0.0305)
[2023/01/16 06:43] | TRAIN(117): [ 900/2211] Batch: 0.1174 (0.1024) Data: 0.0022 (0.0033) Loss: 0.0656 (0.0318)
[2023/01/16 06:43] | TRAIN(117): [ 950/2211] Batch: 0.0902 (0.1019) Data: 0.0016 (0.0033) Loss: 0.0288 (0.0318)
[2023/01/16 06:43] | TRAIN(117): [1000/2211] Batch: 0.0960 (0.1017) Data: 0.0019 (0.0032) Loss: 0.0307 (0.0318)
[2023/01/16 06:43] | TRAIN(117): [1050/2211] Batch: 0.0978 (0.1017) Data: 0.0018 (0.0031) Loss: 0.0107 (0.0323)
[2023/01/16 06:43] | TRAIN(117): [1100/2211] Batch: 0.0883 (0.1013) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0326)
[2023/01/16 06:43] | TRAIN(117): [1150/2211] Batch: 0.0928 (0.1009) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0323)
[2023/01/16 06:43] | TRAIN(117): [1200/2211] Batch: 0.0881 (0.1006) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0324)
[2023/01/16 06:43] | TRAIN(117): [1250/2211] Batch: 0.0875 (0.1003) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0323)
[2023/01/16 06:43] | TRAIN(117): [1300/2211] Batch: 0.0873 (0.1005) Data: 0.0017 (0.0029) Loss: 0.0021 (0.0320)
[2023/01/16 06:44] | TRAIN(117): [1350/2211] Batch: 0.0900 (0.1006) Data: 0.0017 (0.0029) Loss: 0.0019 (0.0317)
[2023/01/16 06:44] | TRAIN(117): [1400/2211] Batch: 0.0875 (0.1002) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0316)
[2023/01/16 06:44] | TRAIN(117): [1450/2211] Batch: 0.0899 (0.1000) Data: 0.0018 (0.0028) Loss: 0.0025 (0.0314)
[2023/01/16 06:44] | TRAIN(117): [1500/2211] Batch: 0.0871 (0.0998) Data: 0.0018 (0.0028) Loss: 0.1557 (0.0315)
[2023/01/16 06:44] | TRAIN(117): [1550/2211] Batch: 0.0855 (0.0995) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0312)
[2023/01/16 06:44] | TRAIN(117): [1600/2211] Batch: 0.0874 (0.0993) Data: 0.0017 (0.0027) Loss: 0.0645 (0.0311)
[2023/01/16 06:44] | TRAIN(117): [1650/2211] Batch: 0.0997 (0.0990) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0310)
[2023/01/16 06:44] | TRAIN(117): [1700/2211] Batch: 0.0909 (0.0989) Data: 0.0018 (0.0026) Loss: 0.0023 (0.0311)
[2023/01/16 06:44] | TRAIN(117): [1750/2211] Batch: 0.0947 (0.0988) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0310)
[2023/01/16 06:44] | TRAIN(117): [1800/2211] Batch: 0.0881 (0.0986) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0307)
[2023/01/16 06:44] | TRAIN(117): [1850/2211] Batch: 0.0883 (0.0984) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0304)
[2023/01/16 06:44] | TRAIN(117): [1900/2211] Batch: 0.1007 (0.0982) Data: 0.0020 (0.0026) Loss: 0.0203 (0.0303)
[2023/01/16 06:44] | TRAIN(117): [1950/2211] Batch: 0.0968 (0.0981) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0299)
[2023/01/16 06:45] | TRAIN(117): [2000/2211] Batch: 0.1019 (0.0981) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0298)
[2023/01/16 06:45] | TRAIN(117): [2050/2211] Batch: 0.0900 (0.0979) Data: 0.0016 (0.0025) Loss: 0.1156 (0.0299)
[2023/01/16 06:45] | TRAIN(117): [2100/2211] Batch: 0.0917 (0.0978) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0299)
[2023/01/16 06:45] | TRAIN(117): [2150/2211] Batch: 0.0974 (0.0977) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0298)
[2023/01/16 06:45] | TRAIN(117): [2200/2211] Batch: 0.0901 (0.0977) Data: 0.0017 (0.0025) Loss: 0.1285 (0.0298)
[2023/01/16 06:45] | ------------------------------------------------------------
[2023/01/16 06:45] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:45] | ------------------------------------------------------------
[2023/01/16 06:45] |   TRAIN(117)     0:03:36     0:00:05     0:03:30      0.0298
[2023/01/16 06:45] | ------------------------------------------------------------
[2023/01/16 06:45] | **************************************************
[2023/01/16 06:45] | TRAIN(118): [  50/2211] Batch: 0.0986 (0.1256) Data: 0.0016 (0.0257) Loss: 0.0000 (0.0201)
[2023/01/16 06:45] | TRAIN(118): [ 100/2211] Batch: 0.0902 (0.1153) Data: 0.0018 (0.0139) Loss: 0.0792 (0.0243)
[2023/01/16 06:45] | TRAIN(118): [ 150/2211] Batch: 0.1029 (0.1095) Data: 0.0016 (0.0099) Loss: 0.0000 (0.0231)
[2023/01/16 06:45] | TRAIN(118): [ 200/2211] Batch: 0.1240 (0.1116) Data: 0.0022 (0.0080) Loss: 0.0000 (0.0238)
[2023/01/16 06:45] | TRAIN(118): [ 250/2211] Batch: 0.1232 (0.1123) Data: 0.0019 (0.0068) Loss: 0.0077 (0.0237)
[2023/01/16 06:45] | TRAIN(118): [ 300/2211] Batch: 0.1073 (0.1114) Data: 0.0018 (0.0059) Loss: 0.1209 (0.0277)
[2023/01/16 06:46] | TRAIN(118): [ 350/2211] Batch: 0.1135 (0.1106) Data: 0.0018 (0.0054) Loss: 0.0000 (0.0273)
[2023/01/16 06:46] | TRAIN(118): [ 400/2211] Batch: 0.0997 (0.1087) Data: 0.0018 (0.0049) Loss: 0.0693 (0.0284)
[2023/01/16 06:46] | TRAIN(118): [ 450/2211] Batch: 0.0912 (0.1076) Data: 0.0019 (0.0046) Loss: 0.0719 (0.0274)
[2023/01/16 06:46] | TRAIN(118): [ 500/2211] Batch: 0.0877 (0.1061) Data: 0.0017 (0.0043) Loss: 0.0061 (0.0272)
[2023/01/16 06:46] | TRAIN(118): [ 550/2211] Batch: 0.0879 (0.1047) Data: 0.0017 (0.0041) Loss: 0.0165 (0.0275)
[2023/01/16 06:46] | TRAIN(118): [ 600/2211] Batch: 0.0980 (0.1036) Data: 0.0017 (0.0039) Loss: 0.0507 (0.0275)
[2023/01/16 06:46] | TRAIN(118): [ 650/2211] Batch: 0.0957 (0.1030) Data: 0.0014 (0.0037) Loss: 0.0018 (0.0272)
[2023/01/16 06:46] | TRAIN(118): [ 700/2211] Batch: 0.1002 (0.1024) Data: 0.0018 (0.0036) Loss: 0.0039 (0.0276)
[2023/01/16 06:46] | TRAIN(118): [ 750/2211] Batch: 0.0907 (0.1018) Data: 0.0018 (0.0035) Loss: 0.0234 (0.0284)
[2023/01/16 06:46] | TRAIN(118): [ 800/2211] Batch: 0.0828 (0.1013) Data: 0.0017 (0.0034) Loss: 0.0941 (0.0281)
[2023/01/16 06:46] | TRAIN(118): [ 850/2211] Batch: 0.0880 (0.1007) Data: 0.0013 (0.0033) Loss: 0.0000 (0.0276)
[2023/01/16 06:46] | TRAIN(118): [ 900/2211] Batch: 0.0998 (0.1004) Data: 0.0020 (0.0032) Loss: 0.0000 (0.0272)
[2023/01/16 06:46] | TRAIN(118): [ 950/2211] Batch: 0.0905 (0.1002) Data: 0.0018 (0.0031) Loss: 0.0188 (0.0270)
[2023/01/16 06:47] | TRAIN(118): [1000/2211] Batch: 0.0972 (0.0999) Data: 0.0018 (0.0030) Loss: 0.0985 (0.0271)
[2023/01/16 06:47] | TRAIN(118): [1050/2211] Batch: 0.0874 (0.0994) Data: 0.0017 (0.0030) Loss: 0.0843 (0.0272)
[2023/01/16 06:47] | TRAIN(118): [1100/2211] Batch: 0.0879 (0.0991) Data: 0.0018 (0.0029) Loss: 0.0684 (0.0275)
[2023/01/16 06:47] | TRAIN(118): [1150/2211] Batch: 0.0933 (0.0989) Data: 0.0017 (0.0029) Loss: 0.0720 (0.0275)
[2023/01/16 06:47] | TRAIN(118): [1200/2211] Batch: 0.0929 (0.0988) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0275)
[2023/01/16 06:47] | TRAIN(118): [1250/2211] Batch: 0.0880 (0.0985) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0275)
[2023/01/16 06:47] | TRAIN(118): [1300/2211] Batch: 0.1041 (0.0982) Data: 0.0021 (0.0028) Loss: 0.0646 (0.0275)
[2023/01/16 06:47] | TRAIN(118): [1350/2211] Batch: 0.0979 (0.0981) Data: 0.0018 (0.0027) Loss: 0.0079 (0.0275)
[2023/01/16 06:47] | TRAIN(118): [1400/2211] Batch: 0.1182 (0.0981) Data: 0.0022 (0.0027) Loss: 0.0475 (0.0275)
[2023/01/16 06:47] | TRAIN(118): [1450/2211] Batch: 0.1000 (0.0979) Data: 0.0017 (0.0027) Loss: 0.0086 (0.0274)
[2023/01/16 06:47] | TRAIN(118): [1500/2211] Batch: 0.1015 (0.0979) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0275)
[2023/01/16 06:47] | TRAIN(118): [1550/2211] Batch: 0.0911 (0.0979) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0273)
[2023/01/16 06:47] | TRAIN(118): [1600/2211] Batch: 0.1001 (0.0978) Data: 0.0017 (0.0026) Loss: 0.0787 (0.0272)
[2023/01/16 06:48] | TRAIN(118): [1650/2211] Batch: 0.0911 (0.0977) Data: 0.0017 (0.0026) Loss: 0.0411 (0.0273)
[2023/01/16 06:48] | TRAIN(118): [1700/2211] Batch: 0.0944 (0.0977) Data: 0.0018 (0.0025) Loss: 0.0326 (0.0272)
[2023/01/16 06:48] | TRAIN(118): [1750/2211] Batch: 0.0962 (0.0976) Data: 0.0017 (0.0025) Loss: 0.0155 (0.0275)
[2023/01/16 06:48] | TRAIN(118): [1800/2211] Batch: 0.1062 (0.0977) Data: 0.0018 (0.0025) Loss: 0.0651 (0.0275)
[2023/01/16 06:48] | TRAIN(118): [1850/2211] Batch: 0.0945 (0.0978) Data: 0.0016 (0.0025) Loss: 0.0368 (0.0277)
[2023/01/16 06:48] | TRAIN(118): [1900/2211] Batch: 0.0974 (0.0977) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0277)
[2023/01/16 06:48] | TRAIN(118): [1950/2211] Batch: 0.0907 (0.0975) Data: 0.0018 (0.0024) Loss: 0.0775 (0.0277)
[2023/01/16 06:48] | TRAIN(118): [2000/2211] Batch: 0.0932 (0.0975) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0275)
[2023/01/16 06:48] | TRAIN(118): [2050/2211] Batch: 0.0879 (0.0974) Data: 0.0025 (0.0024) Loss: 0.0000 (0.0276)
[2023/01/16 06:48] | TRAIN(118): [2100/2211] Batch: 0.0858 (0.0971) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0275)
[2023/01/16 06:48] | TRAIN(118): [2150/2211] Batch: 0.0869 (0.0968) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0277)
[2023/01/16 06:48] | TRAIN(118): [2200/2211] Batch: 0.0830 (0.0965) Data: 0.0016 (0.0024) Loss: 0.0349 (0.0279)
[2023/01/16 06:48] | ------------------------------------------------------------
[2023/01/16 06:48] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:48] | ------------------------------------------------------------
[2023/01/16 06:48] |   TRAIN(118)     0:03:33     0:00:05     0:03:28      0.0281
[2023/01/16 06:48] | ------------------------------------------------------------
[2023/01/16 06:48] | **************************************************
[2023/01/16 06:49] | TRAIN(119): [  50/2211] Batch: 0.1247 (0.1509) Data: 0.0018 (0.0254) Loss: 0.0000 (0.0220)
[2023/01/16 06:49] | TRAIN(119): [ 100/2211] Batch: 0.0927 (0.1246) Data: 0.0017 (0.0137) Loss: 0.0165 (0.0231)
[2023/01/16 06:49] | TRAIN(119): [ 150/2211] Batch: 0.0998 (0.1147) Data: 0.0017 (0.0098) Loss: 0.0000 (0.0214)
[2023/01/16 06:49] | TRAIN(119): [ 200/2211] Batch: 0.0916 (0.1101) Data: 0.0018 (0.0078) Loss: 0.0037 (0.0212)
[2023/01/16 06:49] | TRAIN(119): [ 250/2211] Batch: 0.1199 (0.1081) Data: 0.0021 (0.0066) Loss: 0.0021 (0.0212)
[2023/01/16 06:49] | TRAIN(119): [ 300/2211] Batch: 0.0988 (0.1073) Data: 0.0017 (0.0058) Loss: 0.0029 (0.0223)
[2023/01/16 06:49] | TRAIN(119): [ 350/2211] Batch: 0.0991 (0.1080) Data: 0.0018 (0.0053) Loss: 0.0000 (0.0228)
[2023/01/16 06:49] | TRAIN(119): [ 400/2211] Batch: 0.1016 (0.1067) Data: 0.0019 (0.0048) Loss: 0.0053 (0.0238)
[2023/01/16 06:49] | TRAIN(119): [ 450/2211] Batch: 0.0896 (0.1055) Data: 0.0017 (0.0045) Loss: 0.0000 (0.0242)
[2023/01/16 06:49] | TRAIN(119): [ 500/2211] Batch: 0.0928 (0.1046) Data: 0.0018 (0.0042) Loss: 0.0656 (0.0233)
[2023/01/16 06:49] | TRAIN(119): [ 550/2211] Batch: 0.0897 (0.1041) Data: 0.0017 (0.0040) Loss: 0.0092 (0.0240)
[2023/01/16 06:49] | TRAIN(119): [ 600/2211] Batch: 0.0972 (0.1037) Data: 0.0017 (0.0038) Loss: 0.0000 (0.0247)
[2023/01/16 06:50] | TRAIN(119): [ 650/2211] Batch: 0.0894 (0.1030) Data: 0.0017 (0.0037) Loss: 0.0229 (0.0251)
[2023/01/16 06:50] | TRAIN(119): [ 700/2211] Batch: 0.0960 (0.1022) Data: 0.0018 (0.0035) Loss: 0.0251 (0.0247)
[2023/01/16 06:50] | TRAIN(119): [ 750/2211] Batch: 0.0912 (0.1017) Data: 0.0017 (0.0034) Loss: 0.1305 (0.0256)
[2023/01/16 06:50] | TRAIN(119): [ 800/2211] Batch: 0.0899 (0.1012) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0264)
[2023/01/16 06:50] | TRAIN(119): [ 850/2211] Batch: 0.0882 (0.1006) Data: 0.0017 (0.0032) Loss: 0.0273 (0.0261)
[2023/01/16 06:50] | TRAIN(119): [ 900/2211] Batch: 0.0949 (0.1003) Data: 0.0017 (0.0031) Loss: 0.0700 (0.0257)
[2023/01/16 06:50] | TRAIN(119): [ 950/2211] Batch: 0.0890 (0.1000) Data: 0.0017 (0.0031) Loss: 0.0002 (0.0258)
[2023/01/16 06:50] | TRAIN(119): [1000/2211] Batch: 0.0887 (0.0995) Data: 0.0017 (0.0030) Loss: 0.0320 (0.0255)
[2023/01/16 06:50] | TRAIN(119): [1050/2211] Batch: 0.0888 (0.0991) Data: 0.0019 (0.0030) Loss: 0.0058 (0.0257)
[2023/01/16 06:50] | TRAIN(119): [1100/2211] Batch: 0.0966 (0.0988) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0257)
[2023/01/16 06:50] | TRAIN(119): [1150/2211] Batch: 0.0990 (0.0985) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0255)
[2023/01/16 06:50] | TRAIN(119): [1200/2211] Batch: 0.0914 (0.0983) Data: 0.0016 (0.0028) Loss: 0.0000 (0.0256)
[2023/01/16 06:50] | TRAIN(119): [1250/2211] Batch: 0.0931 (0.0980) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0263)
[2023/01/16 06:51] | TRAIN(119): [1300/2211] Batch: 0.0977 (0.0979) Data: 0.0017 (0.0027) Loss: 0.1218 (0.0269)
[2023/01/16 06:51] | TRAIN(119): [1350/2211] Batch: 0.1138 (0.0978) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0268)
[2023/01/16 06:51] | TRAIN(119): [1400/2211] Batch: 0.0877 (0.0975) Data: 0.0017 (0.0027) Loss: 0.0044 (0.0267)
[2023/01/16 06:51] | TRAIN(119): [1450/2211] Batch: 0.1050 (0.0973) Data: 0.0018 (0.0026) Loss: 0.0503 (0.0267)
[2023/01/16 06:51] | TRAIN(119): [1500/2211] Batch: 0.1203 (0.0974) Data: 0.0022 (0.0026) Loss: 0.0530 (0.0266)
[2023/01/16 06:51] | TRAIN(119): [1550/2211] Batch: 0.0991 (0.0975) Data: 0.0018 (0.0026) Loss: 0.0014 (0.0269)
[2023/01/16 06:51] | TRAIN(119): [1600/2211] Batch: 0.1036 (0.0974) Data: 0.0018 (0.0026) Loss: 0.1129 (0.0271)
[2023/01/16 06:51] | TRAIN(119): [1650/2211] Batch: 0.0882 (0.0973) Data: 0.0017 (0.0025) Loss: 0.0965 (0.0269)
[2023/01/16 06:51] | TRAIN(119): [1700/2211] Batch: 0.1169 (0.0974) Data: 0.0022 (0.0025) Loss: 0.0018 (0.0271)
[2023/01/16 06:51] | TRAIN(119): [1750/2211] Batch: 0.0984 (0.0974) Data: 0.0017 (0.0025) Loss: 0.0086 (0.0273)
[2023/01/16 06:51] | TRAIN(119): [1800/2211] Batch: 0.0942 (0.0975) Data: 0.0022 (0.0025) Loss: 0.0000 (0.0274)
[2023/01/16 06:51] | TRAIN(119): [1850/2211] Batch: 0.0987 (0.0973) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0273)
[2023/01/16 06:52] | TRAIN(119): [1900/2211] Batch: 0.0980 (0.0972) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0274)
[2023/01/16 06:52] | TRAIN(119): [1950/2211] Batch: 0.0960 (0.0972) Data: 0.0017 (0.0024) Loss: 0.0097 (0.0277)
[2023/01/16 06:52] | TRAIN(119): [2000/2211] Batch: 0.1450 (0.0973) Data: 0.0025 (0.0024) Loss: 0.0000 (0.0280)
[2023/01/16 06:52] | TRAIN(119): [2050/2211] Batch: 0.1089 (0.0979) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0281)
[2023/01/16 06:52] | TRAIN(119): [2100/2211] Batch: 0.0889 (0.0979) Data: 0.0013 (0.0024) Loss: 0.0000 (0.0281)
[2023/01/16 06:52] | TRAIN(119): [2150/2211] Batch: 0.0983 (0.0979) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0285)
[2023/01/16 06:52] | TRAIN(119): [2200/2211] Batch: 0.0875 (0.0978) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0284)
[2023/01/16 06:52] | ------------------------------------------------------------
[2023/01/16 06:52] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:52] | ------------------------------------------------------------
[2023/01/16 06:52] |   TRAIN(119)     0:03:36     0:00:05     0:03:31      0.0283
[2023/01/16 06:52] | ------------------------------------------------------------
[2023/01/16 06:52] | **************************************************
[2023/01/16 06:52] | TRAIN(120): [  50/2211] Batch: 0.0935 (0.1224) Data: 0.0018 (0.0219) Loss: 0.0280 (0.0156)
[2023/01/16 06:52] | TRAIN(120): [ 100/2211] Batch: 0.1000 (0.1097) Data: 0.0018 (0.0119) Loss: 0.0000 (0.0202)
[2023/01/16 06:52] | TRAIN(120): [ 150/2211] Batch: 0.1224 (0.1109) Data: 0.0022 (0.0087) Loss: 0.0000 (0.0182)
[2023/01/16 06:52] | TRAIN(120): [ 200/2211] Batch: 0.1036 (0.1110) Data: 0.0015 (0.0071) Loss: 0.0000 (0.0233)
[2023/01/16 06:53] | TRAIN(120): [ 250/2211] Batch: 0.0908 (0.1083) Data: 0.0019 (0.0060) Loss: 0.0000 (0.0234)
[2023/01/16 06:53] | TRAIN(120): [ 300/2211] Batch: 0.0991 (0.1068) Data: 0.0015 (0.0053) Loss: 0.0000 (0.0240)
[2023/01/16 06:53] | TRAIN(120): [ 350/2211] Batch: 0.0898 (0.1049) Data: 0.0016 (0.0048) Loss: 0.0887 (0.0233)
[2023/01/16 06:53] | TRAIN(120): [ 400/2211] Batch: 0.1280 (0.1040) Data: 0.0023 (0.0044) Loss: 0.0000 (0.0228)
[2023/01/16 06:53] | TRAIN(120): [ 450/2211] Batch: 0.1098 (0.1062) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0226)
[2023/01/16 06:53] | TRAIN(120): [ 500/2211] Batch: 0.0995 (0.1057) Data: 0.0019 (0.0039) Loss: 0.0368 (0.0226)
[2023/01/16 06:53] | TRAIN(120): [ 550/2211] Batch: 0.0897 (0.1054) Data: 0.0018 (0.0037) Loss: 0.0187 (0.0228)
[2023/01/16 06:53] | TRAIN(120): [ 600/2211] Batch: 0.0900 (0.1054) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0224)
[2023/01/16 06:53] | TRAIN(120): [ 650/2211] Batch: 0.0887 (0.1045) Data: 0.0017 (0.0035) Loss: 0.1153 (0.0226)
[2023/01/16 06:53] | TRAIN(120): [ 700/2211] Batch: 0.0884 (0.1036) Data: 0.0018 (0.0033) Loss: 0.0932 (0.0230)
[2023/01/16 06:53] | TRAIN(120): [ 750/2211] Batch: 0.0825 (0.1026) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0232)
[2023/01/16 06:53] | TRAIN(120): [ 800/2211] Batch: 0.0883 (0.1019) Data: 0.0018 (0.0031) Loss: 0.0243 (0.0233)
[2023/01/16 06:53] | TRAIN(120): [ 850/2211] Batch: 0.0889 (0.1013) Data: 0.0017 (0.0031) Loss: 0.2101 (0.0238)
[2023/01/16 06:54] | TRAIN(120): [ 900/2211] Batch: 0.0968 (0.1012) Data: 0.0017 (0.0030) Loss: 0.0043 (0.0240)
[2023/01/16 06:54] | TRAIN(120): [ 950/2211] Batch: 0.0923 (0.1010) Data: 0.0017 (0.0029) Loss: 0.0382 (0.0247)
[2023/01/16 06:54] | TRAIN(120): [1000/2211] Batch: 0.0886 (0.1007) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0248)
[2023/01/16 06:54] | TRAIN(120): [1050/2211] Batch: 0.0941 (0.1007) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0252)
[2023/01/16 06:54] | TRAIN(120): [1100/2211] Batch: 0.0962 (0.1005) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0256)
[2023/01/16 06:54] | TRAIN(120): [1150/2211] Batch: 0.0919 (0.1003) Data: 0.0018 (0.0028) Loss: 0.0073 (0.0255)
[2023/01/16 06:54] | TRAIN(120): [1200/2211] Batch: 0.0932 (0.1000) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0259)
[2023/01/16 06:54] | TRAIN(120): [1250/2211] Batch: 0.0901 (0.0997) Data: 0.0018 (0.0027) Loss: 0.0349 (0.0261)
[2023/01/16 06:54] | TRAIN(120): [1300/2211] Batch: 0.0938 (0.0999) Data: 0.0018 (0.0026) Loss: 0.0080 (0.0260)
[2023/01/16 06:54] | TRAIN(120): [1350/2211] Batch: 0.0974 (0.0999) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0261)
[2023/01/16 06:54] | TRAIN(120): [1400/2211] Batch: 0.0896 (0.0997) Data: 0.0019 (0.0026) Loss: 0.0245 (0.0258)
[2023/01/16 06:54] | TRAIN(120): [1450/2211] Batch: 0.0931 (0.0996) Data: 0.0019 (0.0026) Loss: 0.0189 (0.0259)
[2023/01/16 06:55] | TRAIN(120): [1500/2211] Batch: 0.0990 (0.0995) Data: 0.0018 (0.0025) Loss: 0.0383 (0.0262)
[2023/01/16 06:55] | TRAIN(120): [1550/2211] Batch: 0.0871 (0.0994) Data: 0.0018 (0.0025) Loss: 0.0368 (0.0266)
[2023/01/16 06:55] | TRAIN(120): [1600/2211] Batch: 0.1071 (0.0992) Data: 0.0019 (0.0025) Loss: 0.0096 (0.0268)
[2023/01/16 06:55] | TRAIN(120): [1650/2211] Batch: 0.1173 (0.0992) Data: 0.0022 (0.0025) Loss: 0.0708 (0.0265)
[2023/01/16 06:55] | TRAIN(120): [1700/2211] Batch: 0.0883 (0.0991) Data: 0.0018 (0.0025) Loss: 0.0138 (0.0267)
[2023/01/16 06:55] | TRAIN(120): [1750/2211] Batch: 0.1181 (0.0990) Data: 0.0022 (0.0024) Loss: 0.0218 (0.0269)
[2023/01/16 06:55] | TRAIN(120): [1800/2211] Batch: 0.1099 (0.0990) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0270)
[2023/01/16 06:55] | TRAIN(120): [1850/2211] Batch: 0.1017 (0.0989) Data: 0.0018 (0.0024) Loss: 0.0485 (0.0272)
[2023/01/16 06:55] | TRAIN(120): [1900/2211] Batch: 0.0992 (0.0988) Data: 0.0016 (0.0024) Loss: 0.1023 (0.0274)
[2023/01/16 06:55] | TRAIN(120): [1950/2211] Batch: 0.1008 (0.0987) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0275)
[2023/01/16 06:55] | TRAIN(120): [2000/2211] Batch: 0.1018 (0.0986) Data: 0.0020 (0.0024) Loss: 0.0974 (0.0276)
[2023/01/16 06:55] | TRAIN(120): [2050/2211] Batch: 0.0926 (0.0987) Data: 0.0018 (0.0023) Loss: 0.0175 (0.0277)
[2023/01/16 06:56] | TRAIN(120): [2100/2211] Batch: 0.0895 (0.0986) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0277)
[2023/01/16 06:56] | TRAIN(120): [2150/2211] Batch: 0.0900 (0.0986) Data: 0.0021 (0.0023) Loss: 0.0300 (0.0278)
[2023/01/16 06:56] | TRAIN(120): [2200/2211] Batch: 0.0886 (0.0985) Data: 0.0015 (0.0023) Loss: 0.0000 (0.0278)
[2023/01/16 06:56] | ------------------------------------------------------------
[2023/01/16 06:56] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:56] | ------------------------------------------------------------
[2023/01/16 06:56] |   TRAIN(120)     0:03:37     0:00:05     0:03:32      0.0279
[2023/01/16 06:56] | ------------------------------------------------------------
[2023/01/16 06:56] | **************************************************
[2023/01/16 06:56] | TRAIN(121): [  50/2211] Batch: 0.0887 (0.1226) Data: 0.0017 (0.0266) Loss: 0.0322 (0.0289)
[2023/01/16 06:56] | TRAIN(121): [ 100/2211] Batch: 0.1192 (0.1163) Data: 0.0021 (0.0144) Loss: 0.0000 (0.0297)
[2023/01/16 06:56] | TRAIN(121): [ 150/2211] Batch: 0.1248 (0.1173) Data: 0.0023 (0.0104) Loss: 0.0168 (0.0266)
[2023/01/16 06:56] | TRAIN(121): [ 200/2211] Batch: 0.1243 (0.1183) Data: 0.0022 (0.0084) Loss: 0.0000 (0.0278)
[2023/01/16 06:56] | TRAIN(121): [ 250/2211] Batch: 0.1185 (0.1192) Data: 0.0022 (0.0071) Loss: 0.1953 (0.0298)
[2023/01/16 06:56] | TRAIN(121): [ 300/2211] Batch: 0.1354 (0.1200) Data: 0.0026 (0.0063) Loss: 0.0000 (0.0298)
[2023/01/16 06:56] | TRAIN(121): [ 350/2211] Batch: 0.1248 (0.1206) Data: 0.0022 (0.0058) Loss: 0.0371 (0.0283)
[2023/01/16 06:56] | TRAIN(121): [ 400/2211] Batch: 0.0995 (0.1193) Data: 0.0017 (0.0053) Loss: 0.0000 (0.0283)
[2023/01/16 06:57] | TRAIN(121): [ 450/2211] Batch: 0.1025 (0.1165) Data: 0.0018 (0.0049) Loss: 0.0177 (0.0283)
[2023/01/16 06:57] | TRAIN(121): [ 500/2211] Batch: 0.1609 (0.1148) Data: 0.0019 (0.0046) Loss: 0.0353 (0.0279)
[2023/01/16 06:57] | TRAIN(121): [ 550/2211] Batch: 0.1203 (0.1160) Data: 0.0022 (0.0044) Loss: 0.0697 (0.0281)
[2023/01/16 06:57] | TRAIN(121): [ 600/2211] Batch: 0.1048 (0.1156) Data: 0.0017 (0.0042) Loss: 0.0000 (0.0281)
[2023/01/16 06:57] | TRAIN(121): [ 650/2211] Batch: 0.1088 (0.1140) Data: 0.0017 (0.0040) Loss: 0.0803 (0.0292)
[2023/01/16 06:57] | TRAIN(121): [ 700/2211] Batch: 0.0882 (0.1128) Data: 0.0017 (0.0038) Loss: 0.0554 (0.0301)
[2023/01/16 06:57] | TRAIN(121): [ 750/2211] Batch: 0.0881 (0.1118) Data: 0.0018 (0.0037) Loss: 0.0830 (0.0302)
[2023/01/16 06:57] | TRAIN(121): [ 800/2211] Batch: 0.1055 (0.1112) Data: 0.0018 (0.0036) Loss: 0.0313 (0.0300)
[2023/01/16 06:57] | TRAIN(121): [ 850/2211] Batch: 0.1075 (0.1106) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0297)
[2023/01/16 06:57] | TRAIN(121): [ 900/2211] Batch: 0.1190 (0.1100) Data: 0.0022 (0.0034) Loss: 0.0000 (0.0298)
[2023/01/16 06:57] | TRAIN(121): [ 950/2211] Batch: 0.0989 (0.1099) Data: 0.0019 (0.0033) Loss: 0.0104 (0.0297)
[2023/01/16 06:58] | TRAIN(121): [1000/2211] Batch: 0.0919 (0.1092) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0297)
[2023/01/16 06:58] | TRAIN(121): [1050/2211] Batch: 0.0953 (0.1085) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0295)
[2023/01/16 06:58] | TRAIN(121): [1100/2211] Batch: 0.0966 (0.1079) Data: 0.0017 (0.0031) Loss: 0.0121 (0.0290)
[2023/01/16 06:58] | TRAIN(121): [1150/2211] Batch: 0.0982 (0.1072) Data: 0.0019 (0.0030) Loss: 0.0022 (0.0286)
[2023/01/16 06:58] | TRAIN(121): [1200/2211] Batch: 0.0872 (0.1066) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0287)
[2023/01/16 06:58] | TRAIN(121): [1250/2211] Batch: 0.0953 (0.1060) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0290)
[2023/01/16 06:58] | TRAIN(121): [1300/2211] Batch: 0.0879 (0.1054) Data: 0.0017 (0.0029) Loss: 0.0436 (0.0290)
[2023/01/16 06:58] | TRAIN(121): [1350/2211] Batch: 0.1120 (0.1052) Data: 0.0019 (0.0028) Loss: 0.0313 (0.0291)
[2023/01/16 06:58] | TRAIN(121): [1400/2211] Batch: 0.1079 (0.1052) Data: 0.0016 (0.0028) Loss: 0.0691 (0.0292)
[2023/01/16 06:58] | TRAIN(121): [1450/2211] Batch: 0.1013 (0.1049) Data: 0.0018 (0.0028) Loss: 0.0303 (0.0293)
[2023/01/16 06:58] | TRAIN(121): [1500/2211] Batch: 0.0965 (0.1046) Data: 0.0017 (0.0027) Loss: 0.0319 (0.0292)
[2023/01/16 06:58] | TRAIN(121): [1550/2211] Batch: 0.0906 (0.1043) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0291)
[2023/01/16 06:58] | TRAIN(121): [1600/2211] Batch: 0.1040 (0.1041) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0291)
[2023/01/16 06:59] | TRAIN(121): [1650/2211] Batch: 0.0889 (0.1038) Data: 0.0017 (0.0027) Loss: 0.0024 (0.0291)
[2023/01/16 06:59] | TRAIN(121): [1700/2211] Batch: 0.0883 (0.1035) Data: 0.0018 (0.0026) Loss: 0.0249 (0.0287)
[2023/01/16 06:59] | TRAIN(121): [1750/2211] Batch: 0.0991 (0.1031) Data: 0.0019 (0.0026) Loss: 0.0033 (0.0284)
[2023/01/16 06:59] | TRAIN(121): [1800/2211] Batch: 0.0883 (0.1028) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0281)
[2023/01/16 06:59] | TRAIN(121): [1850/2211] Batch: 0.0933 (0.1026) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0284)
[2023/01/16 06:59] | TRAIN(121): [1900/2211] Batch: 0.0939 (0.1026) Data: 0.0018 (0.0025) Loss: 0.0541 (0.0287)
[2023/01/16 06:59] | TRAIN(121): [1950/2211] Batch: 0.0845 (0.1024) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0288)
[2023/01/16 06:59] | TRAIN(121): [2000/2211] Batch: 0.0972 (0.1022) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0290)
[2023/01/16 06:59] | TRAIN(121): [2050/2211] Batch: 0.0827 (0.1020) Data: 0.0017 (0.0025) Loss: 0.0701 (0.0289)
[2023/01/16 06:59] | TRAIN(121): [2100/2211] Batch: 0.0860 (0.1016) Data: 0.0017 (0.0025) Loss: 0.0040 (0.0289)
[2023/01/16 06:59] | TRAIN(121): [2150/2211] Batch: 0.0983 (0.1014) Data: 0.0020 (0.0025) Loss: 0.0000 (0.0286)
[2023/01/16 06:59] | TRAIN(121): [2200/2211] Batch: 0.1139 (0.1012) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0284)
[2023/01/16 06:59] | ------------------------------------------------------------
[2023/01/16 06:59] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 06:59] | ------------------------------------------------------------
[2023/01/16 06:59] |   TRAIN(121)     0:03:43     0:00:05     0:03:38      0.0284
[2023/01/16 06:59] | ------------------------------------------------------------
[2023/01/16 06:59] | **************************************************
[2023/01/16 07:00] | TRAIN(122): [  50/2211] Batch: 0.0845 (0.1190) Data: 0.0017 (0.0262) Loss: 0.0000 (0.0280)
[2023/01/16 07:00] | TRAIN(122): [ 100/2211] Batch: 0.1224 (0.1032) Data: 0.0024 (0.0141) Loss: 0.0863 (0.0285)
[2023/01/16 07:00] | TRAIN(122): [ 150/2211] Batch: 0.0999 (0.1053) Data: 0.0017 (0.0101) Loss: 0.0583 (0.0280)
[2023/01/16 07:00] | TRAIN(122): [ 200/2211] Batch: 0.1238 (0.1081) Data: 0.0022 (0.0081) Loss: 0.0000 (0.0279)
[2023/01/16 07:00] | TRAIN(122): [ 250/2211] Batch: 0.1286 (0.1119) Data: 0.0019 (0.0069) Loss: 0.0000 (0.0262)
[2023/01/16 07:00] | TRAIN(122): [ 300/2211] Batch: 0.1229 (0.1142) Data: 0.0022 (0.0061) Loss: 0.0028 (0.0261)
[2023/01/16 07:00] | TRAIN(122): [ 350/2211] Batch: 0.1014 (0.1128) Data: 0.0017 (0.0055) Loss: 0.0000 (0.0258)
[2023/01/16 07:00] | TRAIN(122): [ 400/2211] Batch: 0.0921 (0.1106) Data: 0.0018 (0.0051) Loss: 0.0000 (0.0271)
[2023/01/16 07:00] | TRAIN(122): [ 450/2211] Batch: 0.0949 (0.1086) Data: 0.0017 (0.0047) Loss: 0.0387 (0.0281)
[2023/01/16 07:00] | TRAIN(122): [ 500/2211] Batch: 0.1174 (0.1082) Data: 0.0023 (0.0044) Loss: 0.0000 (0.0283)
[2023/01/16 07:00] | TRAIN(122): [ 550/2211] Batch: 0.1019 (0.1079) Data: 0.0019 (0.0042) Loss: 0.0000 (0.0281)
[2023/01/16 07:00] | TRAIN(122): [ 600/2211] Batch: 0.0916 (0.1068) Data: 0.0017 (0.0040) Loss: 0.0031 (0.0270)
[2023/01/16 07:01] | TRAIN(122): [ 650/2211] Batch: 0.0991 (0.1059) Data: 0.0015 (0.0038) Loss: 0.0000 (0.0274)
[2023/01/16 07:01] | TRAIN(122): [ 700/2211] Batch: 0.0873 (0.1052) Data: 0.0017 (0.0037) Loss: 0.0139 (0.0269)
[2023/01/16 07:01] | TRAIN(122): [ 750/2211] Batch: 0.0904 (0.1049) Data: 0.0017 (0.0035) Loss: 0.0358 (0.0269)
[2023/01/16 07:01] | TRAIN(122): [ 800/2211] Batch: 0.0886 (0.1041) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0264)
[2023/01/16 07:01] | TRAIN(122): [ 850/2211] Batch: 0.0887 (0.1033) Data: 0.0018 (0.0033) Loss: 0.0280 (0.0262)
[2023/01/16 07:01] | TRAIN(122): [ 900/2211] Batch: 0.0988 (0.1027) Data: 0.0017 (0.0032) Loss: 0.0039 (0.0265)
[2023/01/16 07:01] | TRAIN(122): [ 950/2211] Batch: 0.0972 (0.1023) Data: 0.0018 (0.0032) Loss: 0.0241 (0.0267)
[2023/01/16 07:01] | TRAIN(122): [1000/2211] Batch: 0.0938 (0.1019) Data: 0.0018 (0.0031) Loss: 0.0049 (0.0264)
[2023/01/16 07:01] | TRAIN(122): [1050/2211] Batch: 0.0977 (0.1016) Data: 0.0015 (0.0030) Loss: 0.0000 (0.0262)
[2023/01/16 07:01] | TRAIN(122): [1100/2211] Batch: 0.0874 (0.1011) Data: 0.0017 (0.0030) Loss: 0.0469 (0.0260)
[2023/01/16 07:01] | TRAIN(122): [1150/2211] Batch: 0.0973 (0.1008) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0262)
[2023/01/16 07:01] | TRAIN(122): [1200/2211] Batch: 0.0891 (0.1006) Data: 0.0018 (0.0029) Loss: 0.0190 (0.0260)
[2023/01/16 07:02] | TRAIN(122): [1250/2211] Batch: 0.0909 (0.1003) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0261)
[2023/01/16 07:02] | TRAIN(122): [1300/2211] Batch: 0.0910 (0.1001) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0260)
[2023/01/16 07:02] | TRAIN(122): [1350/2211] Batch: 0.1030 (0.1000) Data: 0.0018 (0.0027) Loss: 0.0239 (0.0265)
[2023/01/16 07:02] | TRAIN(122): [1400/2211] Batch: 0.0969 (0.0999) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0264)
[2023/01/16 07:02] | TRAIN(122): [1450/2211] Batch: 0.1156 (0.1005) Data: 0.0015 (0.0027) Loss: 0.1250 (0.0263)
[2023/01/16 07:02] | TRAIN(122): [1500/2211] Batch: 0.0878 (0.1003) Data: 0.0014 (0.0027) Loss: 0.0297 (0.0260)
[2023/01/16 07:02] | TRAIN(122): [1550/2211] Batch: 0.0980 (0.1001) Data: 0.0018 (0.0026) Loss: 0.0723 (0.0263)
[2023/01/16 07:02] | TRAIN(122): [1600/2211] Batch: 0.0905 (0.0999) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0265)
[2023/01/16 07:02] | TRAIN(122): [1650/2211] Batch: 0.1230 (0.0998) Data: 0.0024 (0.0026) Loss: 0.0097 (0.0266)
[2023/01/16 07:02] | TRAIN(122): [1700/2211] Batch: 0.1177 (0.1006) Data: 0.0022 (0.0026) Loss: 0.0235 (0.0267)
[2023/01/16 07:02] | TRAIN(122): [1750/2211] Batch: 0.0966 (0.1005) Data: 0.0017 (0.0025) Loss: 0.0140 (0.0267)
[2023/01/16 07:02] | TRAIN(122): [1800/2211] Batch: 0.0967 (0.1002) Data: 0.0017 (0.0025) Loss: 0.0060 (0.0267)
[2023/01/16 07:02] | TRAIN(122): [1850/2211] Batch: 0.0880 (0.0999) Data: 0.0017 (0.0025) Loss: 0.0597 (0.0267)
[2023/01/16 07:03] | TRAIN(122): [1900/2211] Batch: 0.0963 (0.1002) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0264)
[2023/01/16 07:03] | TRAIN(122): [1950/2211] Batch: 0.0893 (0.1001) Data: 0.0017 (0.0025) Loss: 0.0656 (0.0264)
[2023/01/16 07:03] | TRAIN(122): [2000/2211] Batch: 0.0888 (0.1001) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0261)
[2023/01/16 07:03] | TRAIN(122): [2050/2211] Batch: 0.0877 (0.0999) Data: 0.0017 (0.0024) Loss: 0.1329 (0.0264)
[2023/01/16 07:03] | TRAIN(122): [2100/2211] Batch: 0.1188 (0.0999) Data: 0.0023 (0.0024) Loss: 0.0000 (0.0265)
[2023/01/16 07:03] | TRAIN(122): [2150/2211] Batch: 0.0924 (0.1001) Data: 0.0020 (0.0024) Loss: 0.0431 (0.0266)
[2023/01/16 07:03] | TRAIN(122): [2200/2211] Batch: 0.0896 (0.1001) Data: 0.0016 (0.0024) Loss: 0.0389 (0.0269)
[2023/01/16 07:03] | ------------------------------------------------------------
[2023/01/16 07:03] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:03] | ------------------------------------------------------------
[2023/01/16 07:03] |   TRAIN(122)     0:03:41     0:00:05     0:03:35      0.0269
[2023/01/16 07:03] | ------------------------------------------------------------
[2023/01/16 07:03] | **************************************************
[2023/01/16 07:03] | TRAIN(123): [  50/2211] Batch: 0.1309 (0.1348) Data: 0.0023 (0.0264) Loss: 0.0017 (0.0249)
[2023/01/16 07:03] | TRAIN(123): [ 100/2211] Batch: 0.0885 (0.1148) Data: 0.0017 (0.0142) Loss: 0.0000 (0.0352)
[2023/01/16 07:03] | TRAIN(123): [ 150/2211] Batch: 0.0892 (0.1084) Data: 0.0018 (0.0101) Loss: 0.0063 (0.0329)
[2023/01/16 07:03] | TRAIN(123): [ 200/2211] Batch: 0.0886 (0.1064) Data: 0.0016 (0.0080) Loss: 0.0000 (0.0312)
[2023/01/16 07:04] | TRAIN(123): [ 250/2211] Batch: 0.0878 (0.1066) Data: 0.0013 (0.0068) Loss: 0.0000 (0.0312)
[2023/01/16 07:04] | TRAIN(123): [ 300/2211] Batch: 0.1250 (0.1076) Data: 0.0024 (0.0060) Loss: 0.0073 (0.0307)
[2023/01/16 07:04] | TRAIN(123): [ 350/2211] Batch: 0.0904 (0.1080) Data: 0.0017 (0.0054) Loss: 0.0000 (0.0299)
[2023/01/16 07:04] | TRAIN(123): [ 400/2211] Batch: 0.0951 (0.1083) Data: 0.0017 (0.0050) Loss: 0.0000 (0.0281)
[2023/01/16 07:04] | TRAIN(123): [ 450/2211] Batch: 0.0925 (0.1070) Data: 0.0018 (0.0047) Loss: 0.0000 (0.0288)
[2023/01/16 07:04] | TRAIN(123): [ 500/2211] Batch: 0.0897 (0.1057) Data: 0.0017 (0.0044) Loss: 0.0077 (0.0271)
[2023/01/16 07:04] | TRAIN(123): [ 550/2211] Batch: 0.0883 (0.1044) Data: 0.0017 (0.0041) Loss: 0.0000 (0.0272)
[2023/01/16 07:04] | TRAIN(123): [ 600/2211] Batch: 0.0964 (0.1036) Data: 0.0017 (0.0039) Loss: 0.0237 (0.0273)
[2023/01/16 07:04] | TRAIN(123): [ 650/2211] Batch: 0.0888 (0.1029) Data: 0.0017 (0.0038) Loss: 0.0000 (0.0281)
[2023/01/16 07:04] | TRAIN(123): [ 700/2211] Batch: 0.0944 (0.1023) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0291)
[2023/01/16 07:04] | TRAIN(123): [ 750/2211] Batch: 0.0927 (0.1018) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0290)
[2023/01/16 07:04] | TRAIN(123): [ 800/2211] Batch: 0.0978 (0.1013) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0296)
[2023/01/16 07:05] | TRAIN(123): [ 850/2211] Batch: 0.1032 (0.1021) Data: 0.0019 (0.0033) Loss: 0.0070 (0.0300)
[2023/01/16 07:05] | TRAIN(123): [ 900/2211] Batch: 0.0996 (0.1016) Data: 0.0018 (0.0032) Loss: 0.1662 (0.0301)
[2023/01/16 07:05] | TRAIN(123): [ 950/2211] Batch: 0.0880 (0.1012) Data: 0.0017 (0.0031) Loss: 0.0775 (0.0305)
[2023/01/16 07:05] | TRAIN(123): [1000/2211] Batch: 0.0869 (0.1008) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0305)
[2023/01/16 07:05] | TRAIN(123): [1050/2211] Batch: 0.0873 (0.1003) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0313)
[2023/01/16 07:05] | TRAIN(123): [1100/2211] Batch: 0.0952 (0.1000) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0312)
[2023/01/16 07:05] | TRAIN(123): [1150/2211] Batch: 0.0963 (0.0997) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0318)
[2023/01/16 07:05] | TRAIN(123): [1200/2211] Batch: 0.0880 (0.0996) Data: 0.0018 (0.0029) Loss: 0.0182 (0.0314)
[2023/01/16 07:05] | TRAIN(123): [1250/2211] Batch: 0.0920 (0.0994) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0312)
[2023/01/16 07:05] | TRAIN(123): [1300/2211] Batch: 0.0872 (0.0991) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0312)
[2023/01/16 07:05] | TRAIN(123): [1350/2211] Batch: 0.0878 (0.0987) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0312)
[2023/01/16 07:05] | TRAIN(123): [1400/2211] Batch: 0.0862 (0.0985) Data: 0.0017 (0.0027) Loss: 0.0040 (0.0311)
[2023/01/16 07:05] | TRAIN(123): [1450/2211] Batch: 0.0903 (0.0985) Data: 0.0017 (0.0027) Loss: 0.0235 (0.0310)
[2023/01/16 07:06] | TRAIN(123): [1500/2211] Batch: 0.0876 (0.0983) Data: 0.0017 (0.0027) Loss: 0.1203 (0.0309)
[2023/01/16 07:06] | TRAIN(123): [1550/2211] Batch: 0.0981 (0.0983) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0309)
[2023/01/16 07:06] | TRAIN(123): [1600/2211] Batch: 0.0886 (0.0980) Data: 0.0017 (0.0026) Loss: 0.1386 (0.0307)
[2023/01/16 07:06] | TRAIN(123): [1650/2211] Batch: 0.1005 (0.0978) Data: 0.0017 (0.0026) Loss: 0.0022 (0.0304)
[2023/01/16 07:06] | TRAIN(123): [1700/2211] Batch: 0.1019 (0.0978) Data: 0.0017 (0.0026) Loss: 0.0623 (0.0306)
[2023/01/16 07:06] | TRAIN(123): [1750/2211] Batch: 0.0967 (0.0976) Data: 0.0015 (0.0025) Loss: 0.0170 (0.0306)
[2023/01/16 07:06] | TRAIN(123): [1800/2211] Batch: 0.0868 (0.0975) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0303)
[2023/01/16 07:06] | TRAIN(123): [1850/2211] Batch: 0.1179 (0.0977) Data: 0.0021 (0.0025) Loss: 0.1224 (0.0301)
[2023/01/16 07:06] | TRAIN(123): [1900/2211] Batch: 0.0962 (0.0977) Data: 0.0017 (0.0025) Loss: 0.0037 (0.0300)
[2023/01/16 07:06] | TRAIN(123): [1950/2211] Batch: 0.0874 (0.0975) Data: 0.0015 (0.0025) Loss: 0.0000 (0.0299)
[2023/01/16 07:06] | TRAIN(123): [2000/2211] Batch: 0.0873 (0.0974) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0297)
[2023/01/16 07:06] | TRAIN(123): [2050/2211] Batch: 0.0951 (0.0972) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0296)
[2023/01/16 07:07] | TRAIN(123): [2100/2211] Batch: 0.0900 (0.0971) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0297)
[2023/01/16 07:07] | TRAIN(123): [2150/2211] Batch: 0.0915 (0.0970) Data: 0.0017 (0.0024) Loss: 0.0102 (0.0294)
[2023/01/16 07:07] | TRAIN(123): [2200/2211] Batch: 0.0889 (0.0969) Data: 0.0016 (0.0024) Loss: 0.0022 (0.0293)
[2023/01/16 07:07] | ------------------------------------------------------------
[2023/01/16 07:07] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:07] | ------------------------------------------------------------
[2023/01/16 07:07] |   TRAIN(123)     0:03:34     0:00:05     0:03:28      0.0292
[2023/01/16 07:07] | ------------------------------------------------------------
[2023/01/16 07:07] | **************************************************
[2023/01/16 07:07] | TRAIN(124): [  50/2211] Batch: 0.1067 (0.1257) Data: 0.0017 (0.0258) Loss: 0.0930 (0.0314)
[2023/01/16 07:07] | TRAIN(124): [ 100/2211] Batch: 0.0865 (0.1218) Data: 0.0019 (0.0141) Loss: 0.0000 (0.0254)
[2023/01/16 07:07] | TRAIN(124): [ 150/2211] Batch: 0.0947 (0.1124) Data: 0.0017 (0.0100) Loss: 0.0502 (0.0231)
[2023/01/16 07:07] | TRAIN(124): [ 200/2211] Batch: 0.1324 (0.1099) Data: 0.0022 (0.0080) Loss: 0.0000 (0.0269)
[2023/01/16 07:07] | TRAIN(124): [ 250/2211] Batch: 0.0890 (0.1088) Data: 0.0016 (0.0068) Loss: 0.0000 (0.0269)
[2023/01/16 07:07] | TRAIN(124): [ 300/2211] Batch: 0.0922 (0.1061) Data: 0.0018 (0.0059) Loss: 0.0000 (0.0271)
[2023/01/16 07:07] | TRAIN(124): [ 350/2211] Batch: 0.0961 (0.1047) Data: 0.0019 (0.0053) Loss: 0.0000 (0.0279)
[2023/01/16 07:07] | TRAIN(124): [ 400/2211] Batch: 0.0887 (0.1036) Data: 0.0018 (0.0049) Loss: 0.0000 (0.0268)
[2023/01/16 07:07] | TRAIN(124): [ 450/2211] Batch: 0.0897 (0.1027) Data: 0.0020 (0.0046) Loss: 0.0456 (0.0261)
[2023/01/16 07:08] | TRAIN(124): [ 500/2211] Batch: 0.0990 (0.1029) Data: 0.0021 (0.0043) Loss: 0.0070 (0.0265)
[2023/01/16 07:08] | TRAIN(124): [ 550/2211] Batch: 0.0925 (0.1020) Data: 0.0024 (0.0041) Loss: 0.0000 (0.0266)
[2023/01/16 07:08] | TRAIN(124): [ 600/2211] Batch: 0.0922 (0.1011) Data: 0.0020 (0.0039) Loss: 0.0052 (0.0272)
[2023/01/16 07:08] | TRAIN(124): [ 650/2211] Batch: 0.0920 (0.1009) Data: 0.0017 (0.0037) Loss: 0.0000 (0.0277)
[2023/01/16 07:08] | TRAIN(124): [ 700/2211] Batch: 0.0933 (0.1003) Data: 0.0018 (0.0036) Loss: 0.1394 (0.0279)
[2023/01/16 07:08] | TRAIN(124): [ 750/2211] Batch: 0.0950 (0.0998) Data: 0.0023 (0.0035) Loss: 0.0894 (0.0280)
[2023/01/16 07:08] | TRAIN(124): [ 800/2211] Batch: 0.1041 (0.1002) Data: 0.0019 (0.0034) Loss: 0.0095 (0.0277)
[2023/01/16 07:08] | TRAIN(124): [ 850/2211] Batch: 0.0941 (0.1001) Data: 0.0018 (0.0033) Loss: 0.0217 (0.0277)
[2023/01/16 07:08] | TRAIN(124): [ 900/2211] Batch: 0.0887 (0.0996) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0280)
[2023/01/16 07:08] | TRAIN(124): [ 950/2211] Batch: 0.0877 (0.0991) Data: 0.0018 (0.0032) Loss: 0.0039 (0.0275)
[2023/01/16 07:08] | TRAIN(124): [1000/2211] Batch: 0.0918 (0.0988) Data: 0.0019 (0.0031) Loss: 0.1137 (0.0272)
[2023/01/16 07:08] | TRAIN(124): [1050/2211] Batch: 0.0884 (0.0990) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0271)
[2023/01/16 07:08] | TRAIN(124): [1100/2211] Batch: 0.0922 (0.0987) Data: 0.0019 (0.0030) Loss: 0.0288 (0.0272)
[2023/01/16 07:09] | TRAIN(124): [1150/2211] Batch: 0.0882 (0.0985) Data: 0.0018 (0.0029) Loss: 0.0649 (0.0272)
[2023/01/16 07:09] | TRAIN(124): [1200/2211] Batch: 0.0878 (0.0982) Data: 0.0018 (0.0029) Loss: 0.0619 (0.0271)
[2023/01/16 07:09] | TRAIN(124): [1250/2211] Batch: 0.0882 (0.0979) Data: 0.0018 (0.0028) Loss: 0.0040 (0.0271)
[2023/01/16 07:09] | TRAIN(124): [1300/2211] Batch: 0.0918 (0.0982) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0270)
[2023/01/16 07:09] | TRAIN(124): [1350/2211] Batch: 0.0973 (0.0979) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0268)
[2023/01/16 07:09] | TRAIN(124): [1400/2211] Batch: 0.0973 (0.0979) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0266)
[2023/01/16 07:09] | TRAIN(124): [1450/2211] Batch: 0.0914 (0.0979) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0269)
[2023/01/16 07:09] | TRAIN(124): [1500/2211] Batch: 0.0922 (0.0979) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0267)
[2023/01/16 07:09] | TRAIN(124): [1550/2211] Batch: 0.1019 (0.0979) Data: 0.0018 (0.0027) Loss: 0.0584 (0.0267)
[2023/01/16 07:09] | TRAIN(124): [1600/2211] Batch: 0.0935 (0.0977) Data: 0.0018 (0.0026) Loss: 0.0894 (0.0271)
[2023/01/16 07:09] | TRAIN(124): [1650/2211] Batch: 0.0994 (0.0975) Data: 0.0018 (0.0026) Loss: 0.0318 (0.0275)
[2023/01/16 07:09] | TRAIN(124): [1700/2211] Batch: 0.0888 (0.0973) Data: 0.0018 (0.0026) Loss: 0.0228 (0.0272)
[2023/01/16 07:10] | TRAIN(124): [1750/2211] Batch: 0.0880 (0.0972) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0271)
[2023/01/16 07:10] | TRAIN(124): [1800/2211] Batch: 0.0841 (0.0971) Data: 0.0018 (0.0026) Loss: 0.0494 (0.0274)
[2023/01/16 07:10] | TRAIN(124): [1850/2211] Batch: 0.0896 (0.0969) Data: 0.0018 (0.0025) Loss: 0.0054 (0.0273)
[2023/01/16 07:10] | TRAIN(124): [1900/2211] Batch: 0.0978 (0.0968) Data: 0.0018 (0.0025) Loss: 0.0266 (0.0274)
[2023/01/16 07:10] | TRAIN(124): [1950/2211] Batch: 0.1173 (0.0969) Data: 0.0023 (0.0025) Loss: 0.0000 (0.0273)
[2023/01/16 07:10] | TRAIN(124): [2000/2211] Batch: 0.0882 (0.0968) Data: 0.0019 (0.0025) Loss: 0.1205 (0.0274)
[2023/01/16 07:10] | TRAIN(124): [2050/2211] Batch: 0.0958 (0.0968) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0273)
[2023/01/16 07:10] | TRAIN(124): [2100/2211] Batch: 0.0880 (0.0967) Data: 0.0017 (0.0025) Loss: 0.0286 (0.0273)
[2023/01/16 07:10] | TRAIN(124): [2150/2211] Batch: 0.1213 (0.0965) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0272)
[2023/01/16 07:10] | TRAIN(124): [2200/2211] Batch: 0.0876 (0.0964) Data: 0.0015 (0.0024) Loss: 0.0000 (0.0272)
[2023/01/16 07:10] | ------------------------------------------------------------
[2023/01/16 07:10] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:10] | ------------------------------------------------------------
[2023/01/16 07:10] |   TRAIN(124)     0:03:33     0:00:05     0:03:27      0.0271
[2023/01/16 07:10] | ------------------------------------------------------------
[2023/01/16 07:10] | **************************************************
[2023/01/16 07:10] | TRAIN(125): [  50/2211] Batch: 0.1061 (0.1260) Data: 0.0020 (0.0243) Loss: 0.0036 (0.0293)
[2023/01/16 07:10] | TRAIN(125): [ 100/2211] Batch: 0.0900 (0.1098) Data: 0.0018 (0.0132) Loss: 0.0000 (0.0270)
[2023/01/16 07:10] | TRAIN(125): [ 150/2211] Batch: 0.0861 (0.1024) Data: 0.0018 (0.0094) Loss: 0.0340 (0.0266)
[2023/01/16 07:11] | TRAIN(125): [ 200/2211] Batch: 0.0869 (0.0983) Data: 0.0019 (0.0075) Loss: 0.0057 (0.0259)
[2023/01/16 07:11] | TRAIN(125): [ 250/2211] Batch: 0.0912 (0.0978) Data: 0.0018 (0.0064) Loss: 0.0000 (0.0270)
[2023/01/16 07:11] | TRAIN(125): [ 300/2211] Batch: 0.0887 (0.0969) Data: 0.0017 (0.0057) Loss: 0.0865 (0.0261)
[2023/01/16 07:11] | TRAIN(125): [ 350/2211] Batch: 0.0909 (0.0967) Data: 0.0019 (0.0051) Loss: 0.0000 (0.0254)
[2023/01/16 07:11] | TRAIN(125): [ 400/2211] Batch: 0.0875 (0.0960) Data: 0.0017 (0.0047) Loss: 0.0000 (0.0245)
[2023/01/16 07:11] | TRAIN(125): [ 450/2211] Batch: 0.0878 (0.0959) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0248)
[2023/01/16 07:11] | TRAIN(125): [ 500/2211] Batch: 0.1020 (0.0960) Data: 0.0019 (0.0041) Loss: 0.0062 (0.0248)
[2023/01/16 07:11] | TRAIN(125): [ 550/2211] Batch: 0.0880 (0.0956) Data: 0.0019 (0.0039) Loss: 0.0048 (0.0251)
[2023/01/16 07:11] | TRAIN(125): [ 600/2211] Batch: 0.0884 (0.0952) Data: 0.0019 (0.0038) Loss: 0.0000 (0.0250)
[2023/01/16 07:11] | TRAIN(125): [ 650/2211] Batch: 0.0915 (0.0952) Data: 0.0019 (0.0036) Loss: 0.0041 (0.0250)
[2023/01/16 07:11] | TRAIN(125): [ 700/2211] Batch: 0.0882 (0.0950) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0253)
[2023/01/16 07:11] | TRAIN(125): [ 750/2211] Batch: 0.0878 (0.0947) Data: 0.0018 (0.0034) Loss: 0.0205 (0.0251)
[2023/01/16 07:11] | TRAIN(125): [ 800/2211] Batch: 0.0983 (0.0946) Data: 0.0019 (0.0033) Loss: 0.0094 (0.0253)
[2023/01/16 07:12] | TRAIN(125): [ 850/2211] Batch: 0.1191 (0.0958) Data: 0.0022 (0.0032) Loss: 0.0218 (0.0253)
[2023/01/16 07:12] | TRAIN(125): [ 900/2211] Batch: 0.0883 (0.0960) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0253)
[2023/01/16 07:12] | TRAIN(125): [ 950/2211] Batch: 0.0912 (0.0959) Data: 0.0019 (0.0031) Loss: 0.1109 (0.0252)
[2023/01/16 07:12] | TRAIN(125): [1000/2211] Batch: 0.1182 (0.0962) Data: 0.0023 (0.0030) Loss: 0.0000 (0.0252)
[2023/01/16 07:12] | TRAIN(125): [1050/2211] Batch: 0.0956 (0.0966) Data: 0.0017 (0.0030) Loss: 0.0302 (0.0251)
[2023/01/16 07:12] | TRAIN(125): [1100/2211] Batch: 0.1212 (0.0973) Data: 0.0021 (0.0029) Loss: 0.1118 (0.0257)
[2023/01/16 07:12] | TRAIN(125): [1150/2211] Batch: 0.0900 (0.0972) Data: 0.0017 (0.0029) Loss: 0.0098 (0.0258)
[2023/01/16 07:12] | TRAIN(125): [1200/2211] Batch: 0.0873 (0.0971) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0258)
[2023/01/16 07:12] | TRAIN(125): [1250/2211] Batch: 0.0877 (0.0969) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0263)
[2023/01/16 07:12] | TRAIN(125): [1300/2211] Batch: 0.1294 (0.0974) Data: 0.0022 (0.0028) Loss: 0.0000 (0.0265)
[2023/01/16 07:12] | TRAIN(125): [1350/2211] Batch: 0.0927 (0.0978) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0264)
[2023/01/16 07:13] | TRAIN(125): [1400/2211] Batch: 0.0917 (0.0975) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0265)
[2023/01/16 07:13] | TRAIN(125): [1450/2211] Batch: 0.0940 (0.0974) Data: 0.0018 (0.0027) Loss: 0.0089 (0.0262)
[2023/01/16 07:13] | TRAIN(125): [1500/2211] Batch: 0.0901 (0.0973) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0259)
[2023/01/16 07:13] | TRAIN(125): [1550/2211] Batch: 0.0836 (0.0971) Data: 0.0018 (0.0026) Loss: 0.0123 (0.0258)
[2023/01/16 07:13] | TRAIN(125): [1600/2211] Batch: 0.0859 (0.0966) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0258)
[2023/01/16 07:13] | TRAIN(125): [1650/2211] Batch: 0.0879 (0.0964) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0261)
[2023/01/16 07:13] | TRAIN(125): [1700/2211] Batch: 0.0885 (0.0968) Data: 0.0018 (0.0026) Loss: 0.0137 (0.0260)
[2023/01/16 07:13] | TRAIN(125): [1750/2211] Batch: 0.1181 (0.0967) Data: 0.0023 (0.0025) Loss: 0.0033 (0.0260)
[2023/01/16 07:13] | TRAIN(125): [1800/2211] Batch: 0.0913 (0.0972) Data: 0.0018 (0.0025) Loss: 0.0679 (0.0260)
[2023/01/16 07:13] | TRAIN(125): [1850/2211] Batch: 0.1018 (0.0976) Data: 0.0019 (0.0025) Loss: 0.0780 (0.0261)
[2023/01/16 07:13] | TRAIN(125): [1900/2211] Batch: 0.1011 (0.0976) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0261)
[2023/01/16 07:13] | TRAIN(125): [1950/2211] Batch: 0.0929 (0.0976) Data: 0.0019 (0.0025) Loss: 0.0064 (0.0262)
[2023/01/16 07:13] | TRAIN(125): [2000/2211] Batch: 0.0885 (0.0975) Data: 0.0018 (0.0025) Loss: 0.0105 (0.0266)
[2023/01/16 07:14] | TRAIN(125): [2050/2211] Batch: 0.0894 (0.0974) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0268)
[2023/01/16 07:14] | TRAIN(125): [2100/2211] Batch: 0.0881 (0.0972) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0268)
[2023/01/16 07:14] | TRAIN(125): [2150/2211] Batch: 0.0883 (0.0971) Data: 0.0018 (0.0024) Loss: 0.0457 (0.0268)
[2023/01/16 07:14] | TRAIN(125): [2200/2211] Batch: 0.0918 (0.0970) Data: 0.0017 (0.0024) Loss: 0.0589 (0.0266)
[2023/01/16 07:14] | ------------------------------------------------------------
[2023/01/16 07:14] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:14] | ------------------------------------------------------------
[2023/01/16 07:14] |   TRAIN(125)     0:03:34     0:00:05     0:03:29      0.0267
[2023/01/16 07:14] | ------------------------------------------------------------
[2023/01/16 07:14] | **************************************************
[2023/01/16 07:14] | TRAIN(126): [  50/2211] Batch: 0.0917 (0.1201) Data: 0.0017 (0.0262) Loss: 0.0000 (0.0270)
[2023/01/16 07:14] | TRAIN(126): [ 100/2211] Batch: 0.0874 (0.1058) Data: 0.0018 (0.0141) Loss: 0.0190 (0.0260)
[2023/01/16 07:14] | TRAIN(126): [ 150/2211] Batch: 0.0877 (0.1011) Data: 0.0018 (0.0101) Loss: 0.0000 (0.0276)
[2023/01/16 07:14] | TRAIN(126): [ 200/2211] Batch: 0.1176 (0.0997) Data: 0.0018 (0.0080) Loss: 0.0000 (0.0244)
[2023/01/16 07:14] | TRAIN(126): [ 250/2211] Batch: 0.1008 (0.0992) Data: 0.0017 (0.0068) Loss: 0.0000 (0.0235)
[2023/01/16 07:14] | TRAIN(126): [ 300/2211] Batch: 0.0999 (0.0997) Data: 0.0018 (0.0059) Loss: 0.0337 (0.0255)
[2023/01/16 07:14] | TRAIN(126): [ 350/2211] Batch: 0.0923 (0.0999) Data: 0.0020 (0.0054) Loss: 0.0760 (0.0250)
[2023/01/16 07:14] | TRAIN(126): [ 400/2211] Batch: 0.0969 (0.0995) Data: 0.0018 (0.0049) Loss: 0.0015 (0.0249)
[2023/01/16 07:15] | TRAIN(126): [ 450/2211] Batch: 0.1027 (0.0989) Data: 0.0017 (0.0046) Loss: 0.1875 (0.0249)
[2023/01/16 07:15] | TRAIN(126): [ 500/2211] Batch: 0.0907 (0.0984) Data: 0.0017 (0.0043) Loss: 0.0000 (0.0254)
[2023/01/16 07:15] | TRAIN(126): [ 550/2211] Batch: 0.1247 (0.0984) Data: 0.0023 (0.0041) Loss: 0.0190 (0.0259)
[2023/01/16 07:15] | TRAIN(126): [ 600/2211] Batch: 0.0905 (0.0987) Data: 0.0017 (0.0039) Loss: 0.0188 (0.0257)
[2023/01/16 07:15] | TRAIN(126): [ 650/2211] Batch: 0.1186 (0.0996) Data: 0.0022 (0.0038) Loss: 0.0000 (0.0260)
[2023/01/16 07:15] | TRAIN(126): [ 700/2211] Batch: 0.0919 (0.0998) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0269)
[2023/01/16 07:15] | TRAIN(126): [ 750/2211] Batch: 0.0915 (0.1000) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0273)
[2023/01/16 07:15] | TRAIN(126): [ 800/2211] Batch: 0.0872 (0.0996) Data: 0.0018 (0.0034) Loss: 0.0725 (0.0274)
[2023/01/16 07:15] | TRAIN(126): [ 850/2211] Batch: 0.0947 (0.0993) Data: 0.0018 (0.0033) Loss: 0.0151 (0.0278)
[2023/01/16 07:15] | TRAIN(126): [ 900/2211] Batch: 0.0880 (0.0990) Data: 0.0017 (0.0032) Loss: 0.0217 (0.0281)
[2023/01/16 07:15] | TRAIN(126): [ 950/2211] Batch: 0.0874 (0.0990) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0284)
[2023/01/16 07:15] | TRAIN(126): [1000/2211] Batch: 0.0947 (0.0988) Data: 0.0016 (0.0031) Loss: 0.0548 (0.0293)
[2023/01/16 07:16] | TRAIN(126): [1050/2211] Batch: 0.0922 (0.0987) Data: 0.0018 (0.0030) Loss: 0.0282 (0.0292)
[2023/01/16 07:16] | TRAIN(126): [1100/2211] Batch: 0.1168 (0.0993) Data: 0.0022 (0.0030) Loss: 0.0746 (0.0290)
[2023/01/16 07:16] | TRAIN(126): [1150/2211] Batch: 0.1489 (0.0992) Data: 0.0022 (0.0029) Loss: 0.1246 (0.0287)
[2023/01/16 07:16] | TRAIN(126): [1200/2211] Batch: 0.0960 (0.0991) Data: 0.0018 (0.0029) Loss: 0.0579 (0.0289)
[2023/01/16 07:16] | TRAIN(126): [1250/2211] Batch: 0.0880 (0.0989) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0284)
[2023/01/16 07:16] | TRAIN(126): [1300/2211] Batch: 0.0914 (0.0987) Data: 0.0017 (0.0028) Loss: 0.0396 (0.0284)
[2023/01/16 07:16] | TRAIN(126): [1350/2211] Batch: 0.1015 (0.0986) Data: 0.0022 (0.0028) Loss: 0.0000 (0.0291)
[2023/01/16 07:16] | TRAIN(126): [1400/2211] Batch: 0.1200 (0.0989) Data: 0.0023 (0.0028) Loss: 0.0893 (0.0292)
[2023/01/16 07:16] | TRAIN(126): [1450/2211] Batch: 0.0869 (0.0992) Data: 0.0017 (0.0027) Loss: 0.0019 (0.0293)
[2023/01/16 07:16] | TRAIN(126): [1500/2211] Batch: 0.1271 (0.0991) Data: 0.0021 (0.0027) Loss: 0.0473 (0.0291)
[2023/01/16 07:16] | TRAIN(126): [1550/2211] Batch: 0.0995 (0.0992) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0293)
[2023/01/16 07:16] | TRAIN(126): [1600/2211] Batch: 0.0978 (0.0991) Data: 0.0017 (0.0026) Loss: 0.0933 (0.0291)
[2023/01/16 07:17] | TRAIN(126): [1650/2211] Batch: 0.1088 (0.0990) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0288)
[2023/01/16 07:17] | TRAIN(126): [1700/2211] Batch: 0.1185 (0.0992) Data: 0.0022 (0.0026) Loss: 0.0024 (0.0289)
[2023/01/16 07:17] | TRAIN(126): [1750/2211] Batch: 0.1140 (0.0997) Data: 0.0017 (0.0026) Loss: 0.2020 (0.0291)
[2023/01/16 07:17] | TRAIN(126): [1800/2211] Batch: 0.0918 (0.0996) Data: 0.0016 (0.0026) Loss: 0.0101 (0.0290)
[2023/01/16 07:17] | TRAIN(126): [1850/2211] Batch: 0.1024 (0.0994) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0290)
[2023/01/16 07:17] | TRAIN(126): [1900/2211] Batch: 0.1300 (0.0994) Data: 0.0025 (0.0025) Loss: 0.0000 (0.0288)
[2023/01/16 07:17] | TRAIN(126): [1950/2211] Batch: 0.1095 (0.0994) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0288)
[2023/01/16 07:17] | TRAIN(126): [2000/2211] Batch: 0.0914 (0.0994) Data: 0.0018 (0.0025) Loss: 0.0449 (0.0289)
[2023/01/16 07:17] | TRAIN(126): [2050/2211] Batch: 0.0875 (0.0992) Data: 0.0018 (0.0025) Loss: 0.0868 (0.0289)
[2023/01/16 07:17] | TRAIN(126): [2100/2211] Batch: 0.0984 (0.0990) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0288)
[2023/01/16 07:17] | TRAIN(126): [2150/2211] Batch: 0.0873 (0.0988) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0286)
[2023/01/16 07:17] | TRAIN(126): [2200/2211] Batch: 0.0866 (0.0989) Data: 0.0015 (0.0024) Loss: 0.0000 (0.0285)
[2023/01/16 07:17] | ------------------------------------------------------------
[2023/01/16 07:17] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:17] | ------------------------------------------------------------
[2023/01/16 07:17] |   TRAIN(126)     0:03:38     0:00:05     0:03:33      0.0284
[2023/01/16 07:17] | ------------------------------------------------------------
[2023/01/16 07:17] | **************************************************
[2023/01/16 07:18] | TRAIN(127): [  50/2211] Batch: 0.0957 (0.1201) Data: 0.0019 (0.0260) Loss: 0.0000 (0.0148)
[2023/01/16 07:18] | TRAIN(127): [ 100/2211] Batch: 0.0979 (0.1097) Data: 0.0019 (0.0140) Loss: 0.0576 (0.0395)
[2023/01/16 07:18] | TRAIN(127): [ 150/2211] Batch: 0.0924 (0.1060) Data: 0.0019 (0.0100) Loss: 0.1542 (0.0395)
[2023/01/16 07:18] | TRAIN(127): [ 200/2211] Batch: 0.1300 (0.1095) Data: 0.0021 (0.0081) Loss: 0.0624 (0.0375)
[2023/01/16 07:18] | TRAIN(127): [ 250/2211] Batch: 0.1003 (0.1121) Data: 0.0019 (0.0069) Loss: 0.0000 (0.0381)
[2023/01/16 07:18] | TRAIN(127): [ 300/2211] Batch: 0.1254 (0.1117) Data: 0.0022 (0.0061) Loss: 0.0000 (0.0355)
[2023/01/16 07:18] | TRAIN(127): [ 350/2211] Batch: 0.1188 (0.1132) Data: 0.0022 (0.0056) Loss: 0.0000 (0.0344)
[2023/01/16 07:18] | TRAIN(127): [ 400/2211] Batch: 0.1183 (0.1134) Data: 0.0022 (0.0052) Loss: 0.0363 (0.0334)
[2023/01/16 07:18] | TRAIN(127): [ 450/2211] Batch: 0.0880 (0.1116) Data: 0.0018 (0.0048) Loss: 0.1113 (0.0335)
[2023/01/16 07:18] | TRAIN(127): [ 500/2211] Batch: 0.0879 (0.1095) Data: 0.0018 (0.0045) Loss: 0.0128 (0.0338)
[2023/01/16 07:18] | TRAIN(127): [ 550/2211] Batch: 0.0919 (0.1080) Data: 0.0019 (0.0042) Loss: 0.0157 (0.0321)
[2023/01/16 07:19] | TRAIN(127): [ 600/2211] Batch: 0.0869 (0.1070) Data: 0.0017 (0.0040) Loss: 0.0249 (0.0314)
[2023/01/16 07:19] | TRAIN(127): [ 650/2211] Batch: 0.0978 (0.1057) Data: 0.0018 (0.0039) Loss: 0.0021 (0.0306)
[2023/01/16 07:19] | TRAIN(127): [ 700/2211] Batch: 0.0878 (0.1045) Data: 0.0018 (0.0037) Loss: 0.0467 (0.0300)
[2023/01/16 07:19] | TRAIN(127): [ 750/2211] Batch: 0.1296 (0.1038) Data: 0.0025 (0.0036) Loss: 0.0000 (0.0295)
[2023/01/16 07:19] | TRAIN(127): [ 800/2211] Batch: 0.1177 (0.1045) Data: 0.0022 (0.0035) Loss: 0.0822 (0.0297)
[2023/01/16 07:19] | TRAIN(127): [ 850/2211] Batch: 0.0882 (0.1049) Data: 0.0017 (0.0034) Loss: 0.0468 (0.0292)
[2023/01/16 07:19] | TRAIN(127): [ 900/2211] Batch: 0.0875 (0.1044) Data: 0.0018 (0.0033) Loss: 0.0225 (0.0294)
[2023/01/16 07:19] | TRAIN(127): [ 950/2211] Batch: 0.0919 (0.1044) Data: 0.0018 (0.0033) Loss: 0.0152 (0.0297)
[2023/01/16 07:19] | TRAIN(127): [1000/2211] Batch: 0.1040 (0.1038) Data: 0.0017 (0.0032) Loss: 0.0000 (0.0294)
[2023/01/16 07:19] | TRAIN(127): [1050/2211] Batch: 0.0913 (0.1032) Data: 0.0018 (0.0031) Loss: 0.0968 (0.0293)
[2023/01/16 07:19] | TRAIN(127): [1100/2211] Batch: 0.0920 (0.1030) Data: 0.0018 (0.0031) Loss: 0.0363 (0.0292)
[2023/01/16 07:19] | TRAIN(127): [1150/2211] Batch: 0.0969 (0.1025) Data: 0.0019 (0.0030) Loss: 0.0000 (0.0289)
[2023/01/16 07:20] | TRAIN(127): [1200/2211] Batch: 0.1038 (0.1023) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0290)
[2023/01/16 07:20] | TRAIN(127): [1250/2211] Batch: 0.0973 (0.1021) Data: 0.0018 (0.0029) Loss: 0.0047 (0.0291)
[2023/01/16 07:20] | TRAIN(127): [1300/2211] Batch: 0.0916 (0.1019) Data: 0.0018 (0.0029) Loss: 0.0298 (0.0293)
[2023/01/16 07:20] | TRAIN(127): [1350/2211] Batch: 0.0975 (0.1016) Data: 0.0021 (0.0028) Loss: 0.0000 (0.0294)
[2023/01/16 07:20] | TRAIN(127): [1400/2211] Batch: 0.0895 (0.1013) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0293)
[2023/01/16 07:20] | TRAIN(127): [1450/2211] Batch: 0.0969 (0.1010) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0290)
[2023/01/16 07:20] | TRAIN(127): [1500/2211] Batch: 0.0899 (0.1009) Data: 0.0017 (0.0027) Loss: 0.1141 (0.0291)
[2023/01/16 07:20] | TRAIN(127): [1550/2211] Batch: 0.0880 (0.1007) Data: 0.0018 (0.0027) Loss: 0.0451 (0.0294)
[2023/01/16 07:20] | TRAIN(127): [1600/2211] Batch: 0.0933 (0.1005) Data: 0.0018 (0.0027) Loss: 0.0555 (0.0292)
[2023/01/16 07:20] | TRAIN(127): [1650/2211] Batch: 0.0907 (0.1003) Data: 0.0018 (0.0027) Loss: 0.0515 (0.0291)
[2023/01/16 07:20] | TRAIN(127): [1700/2211] Batch: 0.0900 (0.1001) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0289)
[2023/01/16 07:20] | TRAIN(127): [1750/2211] Batch: 0.0882 (0.0999) Data: 0.0018 (0.0026) Loss: 0.1050 (0.0291)
[2023/01/16 07:20] | TRAIN(127): [1800/2211] Batch: 0.0876 (0.0997) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0288)
[2023/01/16 07:21] | TRAIN(127): [1850/2211] Batch: 0.0908 (0.0995) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0285)
[2023/01/16 07:21] | TRAIN(127): [1900/2211] Batch: 0.0980 (0.0994) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0282)
[2023/01/16 07:21] | TRAIN(127): [1950/2211] Batch: 0.0918 (0.0993) Data: 0.0017 (0.0025) Loss: 0.1894 (0.0280)
[2023/01/16 07:21] | TRAIN(127): [2000/2211] Batch: 0.0909 (0.0991) Data: 0.0018 (0.0025) Loss: 0.1395 (0.0278)
[2023/01/16 07:21] | TRAIN(127): [2050/2211] Batch: 0.0900 (0.0989) Data: 0.0018 (0.0025) Loss: 0.0992 (0.0278)
[2023/01/16 07:21] | TRAIN(127): [2100/2211] Batch: 0.0952 (0.0987) Data: 0.0017 (0.0025) Loss: 0.0088 (0.0277)
[2023/01/16 07:21] | TRAIN(127): [2150/2211] Batch: 0.0890 (0.0987) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0279)
[2023/01/16 07:21] | TRAIN(127): [2200/2211] Batch: 0.0892 (0.0986) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0280)
[2023/01/16 07:21] | ------------------------------------------------------------
[2023/01/16 07:21] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:21] | ------------------------------------------------------------
[2023/01/16 07:21] |   TRAIN(127)     0:03:37     0:00:05     0:03:32      0.0281
[2023/01/16 07:21] | ------------------------------------------------------------
[2023/01/16 07:21] | **************************************************
[2023/01/16 07:21] | TRAIN(128): [  50/2211] Batch: 0.0909 (0.1255) Data: 0.0019 (0.0246) Loss: 0.0254 (0.0351)
[2023/01/16 07:21] | TRAIN(128): [ 100/2211] Batch: 0.0877 (0.1078) Data: 0.0017 (0.0133) Loss: 0.0000 (0.0328)
[2023/01/16 07:21] | TRAIN(128): [ 150/2211] Batch: 0.0967 (0.1033) Data: 0.0017 (0.0095) Loss: 0.0000 (0.0328)
[2023/01/16 07:21] | TRAIN(128): [ 200/2211] Batch: 0.1193 (0.1028) Data: 0.0021 (0.0076) Loss: 0.0770 (0.0312)
[2023/01/16 07:22] | TRAIN(128): [ 250/2211] Batch: 0.0872 (0.1014) Data: 0.0018 (0.0065) Loss: 0.0983 (0.0292)
[2023/01/16 07:22] | TRAIN(128): [ 300/2211] Batch: 0.0868 (0.0998) Data: 0.0017 (0.0057) Loss: 0.1811 (0.0294)
[2023/01/16 07:22] | TRAIN(128): [ 350/2211] Batch: 0.0988 (0.0996) Data: 0.0017 (0.0052) Loss: 0.0398 (0.0295)
[2023/01/16 07:22] | TRAIN(128): [ 400/2211] Batch: 0.0869 (0.0984) Data: 0.0017 (0.0047) Loss: 0.1436 (0.0296)
[2023/01/16 07:22] | TRAIN(128): [ 450/2211] Batch: 0.0947 (0.0975) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0289)
[2023/01/16 07:22] | TRAIN(128): [ 500/2211] Batch: 0.0911 (0.0981) Data: 0.0018 (0.0042) Loss: 0.0000 (0.0292)
[2023/01/16 07:22] | TRAIN(128): [ 550/2211] Batch: 0.0886 (0.0979) Data: 0.0017 (0.0039) Loss: 0.0000 (0.0299)
[2023/01/16 07:22] | TRAIN(128): [ 600/2211] Batch: 0.0890 (0.0976) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0307)
[2023/01/16 07:22] | TRAIN(128): [ 650/2211] Batch: 0.0885 (0.0972) Data: 0.0019 (0.0036) Loss: 0.0348 (0.0300)
[2023/01/16 07:22] | TRAIN(128): [ 700/2211] Batch: 0.1017 (0.0972) Data: 0.0018 (0.0035) Loss: 0.0210 (0.0296)
[2023/01/16 07:22] | TRAIN(128): [ 750/2211] Batch: 0.0830 (0.0977) Data: 0.0017 (0.0034) Loss: 0.1005 (0.0296)
[2023/01/16 07:22] | TRAIN(128): [ 800/2211] Batch: 0.1190 (0.0975) Data: 0.0022 (0.0033) Loss: 0.0000 (0.0295)
[2023/01/16 07:22] | TRAIN(128): [ 850/2211] Batch: 0.0914 (0.0975) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0299)
[2023/01/16 07:23] | TRAIN(128): [ 900/2211] Batch: 0.0985 (0.0971) Data: 0.0017 (0.0031) Loss: 0.0311 (0.0298)
[2023/01/16 07:23] | TRAIN(128): [ 950/2211] Batch: 0.0975 (0.0969) Data: 0.0017 (0.0031) Loss: 0.0088 (0.0297)
[2023/01/16 07:23] | TRAIN(128): [1000/2211] Batch: 0.0879 (0.0967) Data: 0.0017 (0.0030) Loss: 0.0052 (0.0296)
[2023/01/16 07:23] | TRAIN(128): [1050/2211] Batch: 0.0921 (0.0966) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0289)
[2023/01/16 07:23] | TRAIN(128): [1100/2211] Batch: 0.0867 (0.0964) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0289)
[2023/01/16 07:23] | TRAIN(128): [1150/2211] Batch: 0.0887 (0.0963) Data: 0.0018 (0.0028) Loss: 0.2107 (0.0298)
[2023/01/16 07:23] | TRAIN(128): [1200/2211] Batch: 0.1000 (0.0964) Data: 0.0023 (0.0028) Loss: 0.0000 (0.0298)
[2023/01/16 07:23] | TRAIN(128): [1250/2211] Batch: 0.0877 (0.0964) Data: 0.0018 (0.0028) Loss: 0.0092 (0.0298)
[2023/01/16 07:23] | TRAIN(128): [1300/2211] Batch: 0.1194 (0.0964) Data: 0.0021 (0.0027) Loss: 0.0000 (0.0296)
[2023/01/16 07:23] | TRAIN(128): [1350/2211] Batch: 0.0926 (0.0965) Data: 0.0015 (0.0027) Loss: 0.0000 (0.0296)
[2023/01/16 07:23] | TRAIN(128): [1400/2211] Batch: 0.0870 (0.0964) Data: 0.0018 (0.0027) Loss: 0.0091 (0.0296)
[2023/01/16 07:23] | TRAIN(128): [1450/2211] Batch: 0.1113 (0.0960) Data: 0.0023 (0.0026) Loss: 0.0584 (0.0292)
[2023/01/16 07:23] | TRAIN(128): [1500/2211] Batch: 0.0894 (0.0962) Data: 0.0017 (0.0026) Loss: 0.0278 (0.0291)
[2023/01/16 07:24] | TRAIN(128): [1550/2211] Batch: 0.0883 (0.0961) Data: 0.0017 (0.0026) Loss: 0.0327 (0.0287)
[2023/01/16 07:24] | TRAIN(128): [1600/2211] Batch: 0.0959 (0.0960) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0286)
[2023/01/16 07:24] | TRAIN(128): [1650/2211] Batch: 0.1209 (0.0960) Data: 0.0023 (0.0025) Loss: 0.0000 (0.0285)
[2023/01/16 07:24] | TRAIN(128): [1700/2211] Batch: 0.1094 (0.0963) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0282)
[2023/01/16 07:24] | TRAIN(128): [1750/2211] Batch: 0.0902 (0.0962) Data: 0.0017 (0.0025) Loss: 0.1800 (0.0282)
[2023/01/16 07:24] | TRAIN(128): [1800/2211] Batch: 0.0900 (0.0962) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0281)
[2023/01/16 07:24] | TRAIN(128): [1850/2211] Batch: 0.0877 (0.0960) Data: 0.0018 (0.0025) Loss: 0.0588 (0.0282)
[2023/01/16 07:24] | TRAIN(128): [1900/2211] Batch: 0.0886 (0.0959) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0282)
[2023/01/16 07:24] | TRAIN(128): [1950/2211] Batch: 0.0894 (0.0959) Data: 0.0018 (0.0024) Loss: 0.0041 (0.0281)
[2023/01/16 07:24] | TRAIN(128): [2000/2211] Batch: 0.0881 (0.0958) Data: 0.0018 (0.0024) Loss: 0.0740 (0.0283)
[2023/01/16 07:24] | TRAIN(128): [2050/2211] Batch: 0.0944 (0.0957) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0281)
[2023/01/16 07:24] | TRAIN(128): [2100/2211] Batch: 0.1159 (0.0957) Data: 0.0018 (0.0024) Loss: 0.0241 (0.0279)
[2023/01/16 07:25] | TRAIN(128): [2150/2211] Batch: 0.0972 (0.0956) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0279)
[2023/01/16 07:25] | TRAIN(128): [2200/2211] Batch: 0.0822 (0.0955) Data: 0.0016 (0.0024) Loss: 0.1062 (0.0279)
[2023/01/16 07:25] | ------------------------------------------------------------
[2023/01/16 07:25] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:25] | ------------------------------------------------------------
[2023/01/16 07:25] |   TRAIN(128)     0:03:30     0:00:05     0:03:25      0.0281
[2023/01/16 07:25] | ------------------------------------------------------------
[2023/01/16 07:25] | **************************************************
[2023/01/16 07:25] | TRAIN(129): [  50/2211] Batch: 0.0981 (0.1270) Data: 0.0018 (0.0246) Loss: 0.1360 (0.0250)
[2023/01/16 07:25] | TRAIN(129): [ 100/2211] Batch: 0.0891 (0.1091) Data: 0.0017 (0.0133) Loss: 0.0000 (0.0252)
[2023/01/16 07:25] | TRAIN(129): [ 150/2211] Batch: 0.1176 (0.1110) Data: 0.0022 (0.0096) Loss: 0.0000 (0.0269)
[2023/01/16 07:25] | TRAIN(129): [ 200/2211] Batch: 0.0900 (0.1085) Data: 0.0019 (0.0077) Loss: 0.0180 (0.0302)
[2023/01/16 07:25] | TRAIN(129): [ 250/2211] Batch: 0.0869 (0.1049) Data: 0.0018 (0.0065) Loss: 0.0131 (0.0278)
[2023/01/16 07:25] | TRAIN(129): [ 300/2211] Batch: 0.0909 (0.1035) Data: 0.0018 (0.0057) Loss: 0.1907 (0.0274)
[2023/01/16 07:25] | TRAIN(129): [ 350/2211] Batch: 0.0949 (0.1018) Data: 0.0019 (0.0052) Loss: 0.0000 (0.0267)
[2023/01/16 07:25] | TRAIN(129): [ 400/2211] Batch: 0.0924 (0.1008) Data: 0.0018 (0.0048) Loss: 0.0174 (0.0268)
[2023/01/16 07:25] | TRAIN(129): [ 450/2211] Batch: 0.0908 (0.1011) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0265)
[2023/01/16 07:25] | TRAIN(129): [ 500/2211] Batch: 0.1088 (0.1013) Data: 0.0017 (0.0042) Loss: 0.0704 (0.0261)
[2023/01/16 07:26] | TRAIN(129): [ 550/2211] Batch: 0.0932 (0.1007) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0257)
[2023/01/16 07:26] | TRAIN(129): [ 600/2211] Batch: 0.0891 (0.1001) Data: 0.0019 (0.0038) Loss: 0.0000 (0.0248)
[2023/01/16 07:26] | TRAIN(129): [ 650/2211] Batch: 0.0941 (0.1003) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0250)
[2023/01/16 07:26] | TRAIN(129): [ 700/2211] Batch: 0.0879 (0.0998) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0245)
[2023/01/16 07:26] | TRAIN(129): [ 750/2211] Batch: 0.0940 (0.0995) Data: 0.0018 (0.0034) Loss: 0.1020 (0.0251)
[2023/01/16 07:26] | TRAIN(129): [ 800/2211] Batch: 0.0917 (0.0989) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0254)
[2023/01/16 07:26] | TRAIN(129): [ 850/2211] Batch: 0.1181 (0.0985) Data: 0.0022 (0.0032) Loss: 0.2118 (0.0258)
[2023/01/16 07:26] | TRAIN(129): [ 900/2211] Batch: 0.1180 (0.0982) Data: 0.0022 (0.0031) Loss: 0.0000 (0.0255)
[2023/01/16 07:26] | TRAIN(129): [ 950/2211] Batch: 0.0968 (0.0979) Data: 0.0017 (0.0031) Loss: 0.0633 (0.0255)
[2023/01/16 07:26] | TRAIN(129): [1000/2211] Batch: 0.0937 (0.0977) Data: 0.0017 (0.0030) Loss: 0.0044 (0.0252)
[2023/01/16 07:26] | TRAIN(129): [1050/2211] Batch: 0.0977 (0.0977) Data: 0.0018 (0.0029) Loss: 0.0286 (0.0251)
[2023/01/16 07:26] | TRAIN(129): [1100/2211] Batch: 0.0878 (0.0975) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0252)
[2023/01/16 07:26] | TRAIN(129): [1150/2211] Batch: 0.1182 (0.0973) Data: 0.0022 (0.0028) Loss: 0.0000 (0.0251)
[2023/01/16 07:27] | TRAIN(129): [1200/2211] Batch: 0.0949 (0.0973) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0248)
[2023/01/16 07:27] | TRAIN(129): [1250/2211] Batch: 0.0921 (0.0971) Data: 0.0018 (0.0027) Loss: 0.0709 (0.0247)
[2023/01/16 07:27] | TRAIN(129): [1300/2211] Batch: 0.0912 (0.0968) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0251)
[2023/01/16 07:27] | TRAIN(129): [1350/2211] Batch: 0.0982 (0.0968) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0254)
[2023/01/16 07:27] | TRAIN(129): [1400/2211] Batch: 0.1184 (0.0968) Data: 0.0022 (0.0026) Loss: 0.0926 (0.0257)
[2023/01/16 07:27] | TRAIN(129): [1450/2211] Batch: 0.1019 (0.0970) Data: 0.0018 (0.0026) Loss: 0.0109 (0.0257)
[2023/01/16 07:27] | TRAIN(129): [1500/2211] Batch: 0.0984 (0.0970) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0259)
[2023/01/16 07:27] | TRAIN(129): [1550/2211] Batch: 0.0909 (0.0971) Data: 0.0016 (0.0026) Loss: 0.0084 (0.0258)
[2023/01/16 07:27] | TRAIN(129): [1600/2211] Batch: 0.0896 (0.0969) Data: 0.0019 (0.0025) Loss: 0.0732 (0.0258)
[2023/01/16 07:27] | TRAIN(129): [1650/2211] Batch: 0.0969 (0.0970) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0260)
[2023/01/16 07:27] | TRAIN(129): [1700/2211] Batch: 0.1038 (0.0969) Data: 0.0013 (0.0025) Loss: 0.0103 (0.0260)
[2023/01/16 07:27] | TRAIN(129): [1750/2211] Batch: 0.0967 (0.0968) Data: 0.0015 (0.0025) Loss: 0.0000 (0.0261)
[2023/01/16 07:28] | TRAIN(129): [1800/2211] Batch: 0.0920 (0.0967) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0259)
[2023/01/16 07:28] | TRAIN(129): [1850/2211] Batch: 0.0909 (0.0966) Data: 0.0022 (0.0024) Loss: 0.0000 (0.0260)
[2023/01/16 07:28] | TRAIN(129): [1900/2211] Batch: 0.0887 (0.0965) Data: 0.0014 (0.0024) Loss: 0.0000 (0.0258)
[2023/01/16 07:28] | TRAIN(129): [1950/2211] Batch: 0.0870 (0.0964) Data: 0.0017 (0.0024) Loss: 0.1064 (0.0257)
[2023/01/16 07:28] | TRAIN(129): [2000/2211] Batch: 0.0985 (0.0963) Data: 0.0019 (0.0024) Loss: 0.0046 (0.0255)
[2023/01/16 07:28] | TRAIN(129): [2050/2211] Batch: 0.0884 (0.0962) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0254)
[2023/01/16 07:28] | TRAIN(129): [2100/2211] Batch: 0.0955 (0.0961) Data: 0.0018 (0.0024) Loss: 0.0022 (0.0252)
[2023/01/16 07:28] | TRAIN(129): [2150/2211] Batch: 0.0923 (0.0960) Data: 0.0022 (0.0023) Loss: 0.0776 (0.0253)
[2023/01/16 07:28] | TRAIN(129): [2200/2211] Batch: 0.0896 (0.0958) Data: 0.0016 (0.0023) Loss: 0.0000 (0.0252)
[2023/01/16 07:28] | ------------------------------------------------------------
[2023/01/16 07:28] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:28] | ------------------------------------------------------------
[2023/01/16 07:28] |   TRAIN(129)     0:03:31     0:00:05     0:03:26      0.0253
[2023/01/16 07:28] | ------------------------------------------------------------
[2023/01/16 07:28] | **************************************************
[2023/01/16 07:28] | TRAIN(130): [  50/2211] Batch: 0.1244 (0.1517) Data: 0.0022 (0.0255) Loss: 0.0000 (0.0421)
[2023/01/16 07:28] | TRAIN(130): [ 100/2211] Batch: 0.0886 (0.1301) Data: 0.0018 (0.0139) Loss: 0.0062 (0.0384)
[2023/01/16 07:28] | TRAIN(130): [ 150/2211] Batch: 0.1189 (0.1199) Data: 0.0021 (0.0099) Loss: 0.0159 (0.0352)
[2023/01/16 07:29] | TRAIN(130): [ 200/2211] Batch: 0.0905 (0.1155) Data: 0.0019 (0.0079) Loss: 0.0054 (0.0317)
[2023/01/16 07:29] | TRAIN(130): [ 250/2211] Batch: 0.1223 (0.1111) Data: 0.0021 (0.0067) Loss: 0.0015 (0.0313)
[2023/01/16 07:29] | TRAIN(130): [ 300/2211] Batch: 0.1173 (0.1122) Data: 0.0022 (0.0060) Loss: 0.0000 (0.0307)
[2023/01/16 07:29] | TRAIN(130): [ 350/2211] Batch: 0.0888 (0.1090) Data: 0.0018 (0.0054) Loss: 0.0428 (0.0308)
[2023/01/16 07:29] | TRAIN(130): [ 400/2211] Batch: 0.0980 (0.1071) Data: 0.0018 (0.0049) Loss: 0.0725 (0.0308)
[2023/01/16 07:29] | TRAIN(130): [ 450/2211] Batch: 0.0918 (0.1059) Data: 0.0018 (0.0046) Loss: 0.0198 (0.0299)
[2023/01/16 07:29] | TRAIN(130): [ 500/2211] Batch: 0.0882 (0.1044) Data: 0.0018 (0.0043) Loss: 0.0437 (0.0293)
[2023/01/16 07:29] | TRAIN(130): [ 550/2211] Batch: 0.0874 (0.1034) Data: 0.0017 (0.0041) Loss: 0.0475 (0.0282)
[2023/01/16 07:29] | TRAIN(130): [ 600/2211] Batch: 0.0960 (0.1025) Data: 0.0018 (0.0039) Loss: 0.0096 (0.0278)
[2023/01/16 07:29] | TRAIN(130): [ 650/2211] Batch: 0.0950 (0.1016) Data: 0.0017 (0.0037) Loss: 0.1891 (0.0275)
[2023/01/16 07:29] | TRAIN(130): [ 700/2211] Batch: 0.0890 (0.1011) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0266)
[2023/01/16 07:29] | TRAIN(130): [ 750/2211] Batch: 0.0972 (0.1004) Data: 0.0017 (0.0035) Loss: 0.0098 (0.0266)
[2023/01/16 07:29] | TRAIN(130): [ 800/2211] Batch: 0.1010 (0.1000) Data: 0.0018 (0.0034) Loss: 0.0134 (0.0269)
[2023/01/16 07:30] | TRAIN(130): [ 850/2211] Batch: 0.0932 (0.1003) Data: 0.0018 (0.0033) Loss: 0.1223 (0.0273)
[2023/01/16 07:30] | TRAIN(130): [ 900/2211] Batch: 0.0975 (0.1000) Data: 0.0018 (0.0032) Loss: 0.0384 (0.0271)
[2023/01/16 07:30] | TRAIN(130): [ 950/2211] Batch: 0.0953 (0.0998) Data: 0.0018 (0.0031) Loss: 0.0023 (0.0275)
[2023/01/16 07:30] | TRAIN(130): [1000/2211] Batch: 0.0907 (0.0993) Data: 0.0017 (0.0031) Loss: 0.0024 (0.0275)
[2023/01/16 07:30] | TRAIN(130): [1050/2211] Batch: 0.0871 (0.0989) Data: 0.0018 (0.0030) Loss: 0.0729 (0.0272)
[2023/01/16 07:30] | TRAIN(130): [1100/2211] Batch: 0.0904 (0.0985) Data: 0.0019 (0.0030) Loss: 0.1267 (0.0277)
[2023/01/16 07:30] | TRAIN(130): [1150/2211] Batch: 0.0868 (0.0983) Data: 0.0018 (0.0029) Loss: 0.0084 (0.0283)
[2023/01/16 07:30] | TRAIN(130): [1200/2211] Batch: 0.0964 (0.0980) Data: 0.0018 (0.0029) Loss: 0.1269 (0.0288)
[2023/01/16 07:30] | TRAIN(130): [1250/2211] Batch: 0.0871 (0.0978) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0286)
[2023/01/16 07:30] | TRAIN(130): [1300/2211] Batch: 0.1037 (0.0977) Data: 0.0018 (0.0028) Loss: 0.1547 (0.0286)
[2023/01/16 07:30] | TRAIN(130): [1350/2211] Batch: 0.0870 (0.0974) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0295)
[2023/01/16 07:30] | TRAIN(130): [1400/2211] Batch: 0.0881 (0.0971) Data: 0.0017 (0.0027) Loss: 0.1179 (0.0296)
[2023/01/16 07:30] | TRAIN(130): [1450/2211] Batch: 0.1048 (0.0970) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0295)
[2023/01/16 07:31] | TRAIN(130): [1500/2211] Batch: 0.0889 (0.0968) Data: 0.0018 (0.0026) Loss: 0.0172 (0.0291)
[2023/01/16 07:31] | TRAIN(130): [1550/2211] Batch: 0.1005 (0.0966) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0290)
[2023/01/16 07:31] | TRAIN(130): [1600/2211] Batch: 0.0872 (0.0965) Data: 0.0016 (0.0026) Loss: 0.1359 (0.0292)
[2023/01/16 07:31] | TRAIN(130): [1650/2211] Batch: 0.0926 (0.0965) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0291)
[2023/01/16 07:31] | TRAIN(130): [1700/2211] Batch: 0.0885 (0.0963) Data: 0.0018 (0.0025) Loss: 0.0594 (0.0290)
[2023/01/16 07:31] | TRAIN(130): [1750/2211] Batch: 0.1204 (0.0964) Data: 0.0021 (0.0025) Loss: 0.0000 (0.0290)
[2023/01/16 07:31] | TRAIN(130): [1800/2211] Batch: 0.0890 (0.0967) Data: 0.0014 (0.0025) Loss: 0.1140 (0.0293)
[2023/01/16 07:31] | TRAIN(130): [1850/2211] Batch: 0.1017 (0.0966) Data: 0.0020 (0.0025) Loss: 0.0018 (0.0289)
[2023/01/16 07:31] | TRAIN(130): [1900/2211] Batch: 0.0908 (0.0968) Data: 0.0018 (0.0025) Loss: 0.0623 (0.0288)
[2023/01/16 07:31] | TRAIN(130): [1950/2211] Batch: 0.0959 (0.0968) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0288)
[2023/01/16 07:31] | TRAIN(130): [2000/2211] Batch: 0.0890 (0.0966) Data: 0.0015 (0.0024) Loss: 0.0000 (0.0289)
[2023/01/16 07:31] | TRAIN(130): [2050/2211] Batch: 0.0918 (0.0966) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0288)
[2023/01/16 07:32] | TRAIN(130): [2100/2211] Batch: 0.0979 (0.0965) Data: 0.0018 (0.0024) Loss: 0.0473 (0.0288)
[2023/01/16 07:32] | TRAIN(130): [2150/2211] Batch: 0.0871 (0.0964) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0287)
[2023/01/16 07:32] | TRAIN(130): [2200/2211] Batch: 0.0864 (0.0963) Data: 0.0015 (0.0024) Loss: 0.0000 (0.0286)
[2023/01/16 07:32] | ------------------------------------------------------------
[2023/01/16 07:32] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:32] | ------------------------------------------------------------
[2023/01/16 07:32] |   TRAIN(130)     0:03:32     0:00:05     0:03:27      0.0287
[2023/01/16 07:32] | ------------------------------------------------------------
[2023/01/16 07:32] | **************************************************
[2023/01/16 07:32] | TRAIN(131): [  50/2211] Batch: 0.1178 (0.1247) Data: 0.0021 (0.0258) Loss: 0.0697 (0.0304)
[2023/01/16 07:32] | TRAIN(131): [ 100/2211] Batch: 0.1085 (0.1142) Data: 0.0018 (0.0140) Loss: 0.0000 (0.0260)
[2023/01/16 07:32] | TRAIN(131): [ 150/2211] Batch: 0.0876 (0.1064) Data: 0.0018 (0.0099) Loss: 0.0729 (0.0254)
[2023/01/16 07:32] | TRAIN(131): [ 200/2211] Batch: 0.0919 (0.1041) Data: 0.0017 (0.0079) Loss: 0.0000 (0.0248)
[2023/01/16 07:32] | TRAIN(131): [ 250/2211] Batch: 0.1125 (0.1044) Data: 0.0017 (0.0067) Loss: 0.0000 (0.0247)
[2023/01/16 07:32] | TRAIN(131): [ 300/2211] Batch: 0.1121 (0.1062) Data: 0.0017 (0.0059) Loss: 0.0055 (0.0246)
[2023/01/16 07:32] | TRAIN(131): [ 350/2211] Batch: 0.1181 (0.1084) Data: 0.0022 (0.0053) Loss: 0.0000 (0.0248)
[2023/01/16 07:32] | TRAIN(131): [ 400/2211] Batch: 0.1050 (0.1067) Data: 0.0018 (0.0049) Loss: 0.0535 (0.0267)
[2023/01/16 07:32] | TRAIN(131): [ 450/2211] Batch: 0.0990 (0.1057) Data: 0.0015 (0.0045) Loss: 0.0000 (0.0268)
[2023/01/16 07:33] | TRAIN(131): [ 500/2211] Batch: 0.0830 (0.1047) Data: 0.0017 (0.0043) Loss: 0.0235 (0.0267)
[2023/01/16 07:33] | TRAIN(131): [ 550/2211] Batch: 0.1001 (0.1039) Data: 0.0019 (0.0040) Loss: 0.0496 (0.0263)
[2023/01/16 07:33] | TRAIN(131): [ 600/2211] Batch: 0.0899 (0.1038) Data: 0.0019 (0.0038) Loss: 0.0045 (0.0264)
[2023/01/16 07:33] | TRAIN(131): [ 650/2211] Batch: 0.1023 (0.1029) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0270)
[2023/01/16 07:33] | TRAIN(131): [ 700/2211] Batch: 0.0895 (0.1021) Data: 0.0017 (0.0035) Loss: 0.0580 (0.0265)
[2023/01/16 07:33] | TRAIN(131): [ 750/2211] Batch: 0.1175 (0.1031) Data: 0.0022 (0.0035) Loss: 0.0530 (0.0264)
[2023/01/16 07:33] | TRAIN(131): [ 800/2211] Batch: 0.1235 (0.1041) Data: 0.0022 (0.0034) Loss: 0.0000 (0.0267)
[2023/01/16 07:33] | TRAIN(131): [ 850/2211] Batch: 0.0921 (0.1045) Data: 0.0023 (0.0033) Loss: 0.0614 (0.0268)
[2023/01/16 07:33] | TRAIN(131): [ 900/2211] Batch: 0.0979 (0.1039) Data: 0.0013 (0.0032) Loss: 0.0000 (0.0269)
[2023/01/16 07:33] | TRAIN(131): [ 950/2211] Batch: 0.0911 (0.1034) Data: 0.0014 (0.0031) Loss: 0.0973 (0.0266)
[2023/01/16 07:33] | TRAIN(131): [1000/2211] Batch: 0.0872 (0.1028) Data: 0.0018 (0.0031) Loss: 0.1917 (0.0263)
[2023/01/16 07:33] | TRAIN(131): [1050/2211] Batch: 0.0876 (0.1025) Data: 0.0018 (0.0030) Loss: 0.0088 (0.0262)
[2023/01/16 07:34] | TRAIN(131): [1100/2211] Batch: 0.1013 (0.1023) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0261)
[2023/01/16 07:34] | TRAIN(131): [1150/2211] Batch: 0.0931 (0.1019) Data: 0.0014 (0.0029) Loss: 0.0472 (0.0263)
[2023/01/16 07:34] | TRAIN(131): [1200/2211] Batch: 0.1005 (0.1017) Data: 0.0018 (0.0028) Loss: 0.1203 (0.0259)
[2023/01/16 07:34] | TRAIN(131): [1250/2211] Batch: 0.0990 (0.1016) Data: 0.0023 (0.0028) Loss: 0.0560 (0.0257)
[2023/01/16 07:34] | TRAIN(131): [1300/2211] Batch: 0.0942 (0.1012) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0255)
[2023/01/16 07:34] | TRAIN(131): [1350/2211] Batch: 0.0926 (0.1011) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0255)
[2023/01/16 07:34] | TRAIN(131): [1400/2211] Batch: 0.0883 (0.1007) Data: 0.0018 (0.0027) Loss: 0.0754 (0.0257)
[2023/01/16 07:34] | TRAIN(131): [1450/2211] Batch: 0.0979 (0.1005) Data: 0.0019 (0.0027) Loss: 0.0021 (0.0257)
[2023/01/16 07:34] | TRAIN(131): [1500/2211] Batch: 0.0889 (0.1003) Data: 0.0017 (0.0026) Loss: 0.0028 (0.0256)
[2023/01/16 07:34] | TRAIN(131): [1550/2211] Batch: 0.1163 (0.1004) Data: 0.0021 (0.0026) Loss: 0.0020 (0.0257)
[2023/01/16 07:34] | TRAIN(131): [1600/2211] Batch: 0.0963 (0.1002) Data: 0.0018 (0.0026) Loss: 0.0219 (0.0255)
[2023/01/16 07:34] | TRAIN(131): [1650/2211] Batch: 0.0875 (0.0999) Data: 0.0017 (0.0026) Loss: 0.1175 (0.0256)
[2023/01/16 07:35] | TRAIN(131): [1700/2211] Batch: 0.1004 (0.0996) Data: 0.0018 (0.0025) Loss: 0.0884 (0.0260)
[2023/01/16 07:35] | TRAIN(131): [1750/2211] Batch: 0.0923 (0.0997) Data: 0.0018 (0.0025) Loss: 0.0075 (0.0259)
[2023/01/16 07:35] | TRAIN(131): [1800/2211] Batch: 0.0878 (0.0996) Data: 0.0018 (0.0025) Loss: 0.0858 (0.0259)
[2023/01/16 07:35] | TRAIN(131): [1850/2211] Batch: 0.0958 (0.0993) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0259)
[2023/01/16 07:35] | TRAIN(131): [1900/2211] Batch: 0.0977 (0.0991) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0260)
[2023/01/16 07:35] | TRAIN(131): [1950/2211] Batch: 0.0943 (0.0989) Data: 0.0018 (0.0024) Loss: 0.0433 (0.0259)
[2023/01/16 07:35] | TRAIN(131): [2000/2211] Batch: 0.0948 (0.0989) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0258)
[2023/01/16 07:35] | TRAIN(131): [2050/2211] Batch: 0.1172 (0.0990) Data: 0.0022 (0.0024) Loss: 0.0190 (0.0256)
[2023/01/16 07:35] | TRAIN(131): [2100/2211] Batch: 0.0871 (0.0994) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0256)
[2023/01/16 07:35] | TRAIN(131): [2150/2211] Batch: 0.1181 (0.0993) Data: 0.0021 (0.0024) Loss: 0.0193 (0.0255)
[2023/01/16 07:35] | TRAIN(131): [2200/2211] Batch: 0.1010 (0.0997) Data: 0.0017 (0.0024) Loss: 0.0044 (0.0255)
[2023/01/16 07:35] | ------------------------------------------------------------
[2023/01/16 07:35] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:35] | ------------------------------------------------------------
[2023/01/16 07:35] |   TRAIN(131)     0:03:40     0:00:05     0:03:35      0.0255
[2023/01/16 07:35] | ------------------------------------------------------------
[2023/01/16 07:35] | **************************************************
[2023/01/16 07:35] | TRAIN(132): [  50/2211] Batch: 0.0885 (0.1283) Data: 0.0016 (0.0249) Loss: 0.0601 (0.0363)
[2023/01/16 07:36] | TRAIN(132): [ 100/2211] Batch: 0.1001 (0.1118) Data: 0.0018 (0.0135) Loss: 0.0038 (0.0353)
[2023/01/16 07:36] | TRAIN(132): [ 150/2211] Batch: 0.0893 (0.1057) Data: 0.0017 (0.0096) Loss: 0.0600 (0.0299)
[2023/01/16 07:36] | TRAIN(132): [ 200/2211] Batch: 0.0876 (0.1050) Data: 0.0018 (0.0077) Loss: 0.0000 (0.0256)
[2023/01/16 07:36] | TRAIN(132): [ 250/2211] Batch: 0.0916 (0.1025) Data: 0.0022 (0.0066) Loss: 0.0000 (0.0249)
[2023/01/16 07:36] | TRAIN(132): [ 300/2211] Batch: 0.0972 (0.1017) Data: 0.0018 (0.0058) Loss: 0.0072 (0.0256)
[2023/01/16 07:36] | TRAIN(132): [ 350/2211] Batch: 0.0918 (0.1006) Data: 0.0015 (0.0052) Loss: 0.0726 (0.0256)
[2023/01/16 07:36] | TRAIN(132): [ 400/2211] Batch: 0.0970 (0.0994) Data: 0.0019 (0.0048) Loss: 0.0483 (0.0256)
[2023/01/16 07:36] | TRAIN(132): [ 450/2211] Batch: 0.0950 (0.0984) Data: 0.0018 (0.0044) Loss: 0.0080 (0.0261)
[2023/01/16 07:36] | TRAIN(132): [ 500/2211] Batch: 0.0851 (0.0978) Data: 0.0017 (0.0042) Loss: 0.0418 (0.0262)
[2023/01/16 07:36] | TRAIN(132): [ 550/2211] Batch: 0.0971 (0.0972) Data: 0.0017 (0.0040) Loss: 0.0103 (0.0260)
[2023/01/16 07:36] | TRAIN(132): [ 600/2211] Batch: 0.0879 (0.0966) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0261)
[2023/01/16 07:36] | TRAIN(132): [ 650/2211] Batch: 0.0868 (0.0961) Data: 0.0019 (0.0036) Loss: 0.0018 (0.0258)
[2023/01/16 07:36] | TRAIN(132): [ 700/2211] Batch: 0.0899 (0.0959) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0253)
[2023/01/16 07:37] | TRAIN(132): [ 750/2211] Batch: 0.0879 (0.0957) Data: 0.0018 (0.0034) Loss: 0.0174 (0.0253)
[2023/01/16 07:37] | TRAIN(132): [ 800/2211] Batch: 0.0878 (0.0953) Data: 0.0019 (0.0033) Loss: 0.0036 (0.0246)
[2023/01/16 07:37] | TRAIN(132): [ 850/2211] Batch: 0.0945 (0.0950) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0247)
[2023/01/16 07:37] | TRAIN(132): [ 900/2211] Batch: 0.0931 (0.0949) Data: 0.0017 (0.0031) Loss: 0.0230 (0.0251)
[2023/01/16 07:37] | TRAIN(132): [ 950/2211] Batch: 0.0915 (0.0951) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0251)
[2023/01/16 07:37] | TRAIN(132): [1000/2211] Batch: 0.1042 (0.0950) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0249)
[2023/01/16 07:37] | TRAIN(132): [1050/2211] Batch: 0.1173 (0.0950) Data: 0.0022 (0.0029) Loss: 0.0337 (0.0252)
[2023/01/16 07:37] | TRAIN(132): [1100/2211] Batch: 0.0850 (0.0949) Data: 0.0019 (0.0029) Loss: 0.0141 (0.0261)
[2023/01/16 07:37] | TRAIN(132): [1150/2211] Batch: 0.0924 (0.0949) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0258)
[2023/01/16 07:37] | TRAIN(132): [1200/2211] Batch: 0.0920 (0.0946) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0258)
[2023/01/16 07:37] | TRAIN(132): [1250/2211] Batch: 0.1507 (0.0950) Data: 0.0023 (0.0028) Loss: 0.0000 (0.0257)
[2023/01/16 07:37] | TRAIN(132): [1300/2211] Batch: 0.0848 (0.0951) Data: 0.0018 (0.0027) Loss: 0.2191 (0.0265)
[2023/01/16 07:38] | TRAIN(132): [1350/2211] Batch: 0.0864 (0.0952) Data: 0.0018 (0.0027) Loss: 0.1207 (0.0265)
[2023/01/16 07:38] | TRAIN(132): [1400/2211] Batch: 0.1130 (0.0953) Data: 0.0020 (0.0027) Loss: 0.0000 (0.0267)
[2023/01/16 07:38] | TRAIN(132): [1450/2211] Batch: 0.1043 (0.0954) Data: 0.0018 (0.0026) Loss: 0.0361 (0.0269)
[2023/01/16 07:38] | TRAIN(132): [1500/2211] Batch: 0.0879 (0.0955) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0266)
[2023/01/16 07:38] | TRAIN(132): [1550/2211] Batch: 0.0989 (0.0954) Data: 0.0018 (0.0026) Loss: 0.0043 (0.0266)
[2023/01/16 07:38] | TRAIN(132): [1600/2211] Batch: 0.1228 (0.0956) Data: 0.0022 (0.0026) Loss: 0.0000 (0.0267)
[2023/01/16 07:38] | TRAIN(132): [1650/2211] Batch: 0.0900 (0.0957) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0265)
[2023/01/16 07:38] | TRAIN(132): [1700/2211] Batch: 0.1022 (0.0957) Data: 0.0016 (0.0025) Loss: 0.0933 (0.0267)
[2023/01/16 07:38] | TRAIN(132): [1750/2211] Batch: 0.0911 (0.0955) Data: 0.0013 (0.0025) Loss: 0.0286 (0.0267)
[2023/01/16 07:38] | TRAIN(132): [1800/2211] Batch: 0.0964 (0.0957) Data: 0.0018 (0.0025) Loss: 0.0629 (0.0265)
[2023/01/16 07:38] | TRAIN(132): [1850/2211] Batch: 0.1047 (0.0958) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0267)
[2023/01/16 07:38] | TRAIN(132): [1900/2211] Batch: 0.1009 (0.0957) Data: 0.0016 (0.0024) Loss: 0.0665 (0.0272)
[2023/01/16 07:38] | TRAIN(132): [1950/2211] Batch: 0.0930 (0.0958) Data: 0.0018 (0.0024) Loss: 0.0089 (0.0275)
[2023/01/16 07:39] | TRAIN(132): [2000/2211] Batch: 0.0971 (0.0959) Data: 0.0016 (0.0024) Loss: 0.0281 (0.0276)
[2023/01/16 07:39] | TRAIN(132): [2050/2211] Batch: 0.0972 (0.0961) Data: 0.0018 (0.0024) Loss: 0.0030 (0.0277)
[2023/01/16 07:39] | TRAIN(132): [2100/2211] Batch: 0.1064 (0.0963) Data: 0.0018 (0.0024) Loss: 0.0032 (0.0275)
[2023/01/16 07:39] | TRAIN(132): [2150/2211] Batch: 0.0967 (0.0963) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0276)
[2023/01/16 07:39] | TRAIN(132): [2200/2211] Batch: 0.0881 (0.0962) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0277)
[2023/01/16 07:39] | ------------------------------------------------------------
[2023/01/16 07:39] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:39] | ------------------------------------------------------------
[2023/01/16 07:39] |   TRAIN(132)     0:03:32     0:00:05     0:03:27      0.0277
[2023/01/16 07:39] | ------------------------------------------------------------
[2023/01/16 07:39] | **************************************************
[2023/01/16 07:39] | TRAIN(133): [  50/2211] Batch: 0.1348 (0.1331) Data: 0.0022 (0.0252) Loss: 0.1213 (0.0240)
[2023/01/16 07:39] | TRAIN(133): [ 100/2211] Batch: 0.0881 (0.1168) Data: 0.0018 (0.0136) Loss: 0.0249 (0.0237)
[2023/01/16 07:39] | TRAIN(133): [ 150/2211] Batch: 0.1245 (0.1120) Data: 0.0022 (0.0097) Loss: 0.0148 (0.0242)
[2023/01/16 07:39] | TRAIN(133): [ 200/2211] Batch: 0.0947 (0.1119) Data: 0.0021 (0.0078) Loss: 0.0000 (0.0249)
[2023/01/16 07:39] | TRAIN(133): [ 250/2211] Batch: 0.1430 (0.1151) Data: 0.0022 (0.0066) Loss: 0.1571 (0.0266)
[2023/01/16 07:39] | TRAIN(133): [ 300/2211] Batch: 0.1542 (0.1158) Data: 0.0022 (0.0058) Loss: 0.0000 (0.0259)
[2023/01/16 07:40] | TRAIN(133): [ 350/2211] Batch: 0.1228 (0.1159) Data: 0.0022 (0.0053) Loss: 0.0720 (0.0305)
[2023/01/16 07:40] | TRAIN(133): [ 400/2211] Batch: 0.0875 (0.1141) Data: 0.0017 (0.0049) Loss: 0.1611 (0.0307)
[2023/01/16 07:40] | TRAIN(133): [ 450/2211] Batch: 0.1052 (0.1135) Data: 0.0018 (0.0045) Loss: 0.0000 (0.0300)
[2023/01/16 07:40] | TRAIN(133): [ 500/2211] Batch: 0.1002 (0.1123) Data: 0.0018 (0.0043) Loss: 0.1137 (0.0299)
[2023/01/16 07:40] | TRAIN(133): [ 550/2211] Batch: 0.0995 (0.1109) Data: 0.0018 (0.0040) Loss: 0.0084 (0.0293)
[2023/01/16 07:40] | TRAIN(133): [ 600/2211] Batch: 0.0881 (0.1098) Data: 0.0016 (0.0038) Loss: 0.0085 (0.0291)
[2023/01/16 07:40] | TRAIN(133): [ 650/2211] Batch: 0.0983 (0.1085) Data: 0.0017 (0.0037) Loss: 0.0022 (0.0282)
[2023/01/16 07:40] | TRAIN(133): [ 700/2211] Batch: 0.0890 (0.1072) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0280)
[2023/01/16 07:40] | TRAIN(133): [ 750/2211] Batch: 0.0955 (0.1062) Data: 0.0017 (0.0034) Loss: 0.0130 (0.0282)
[2023/01/16 07:40] | TRAIN(133): [ 800/2211] Batch: 0.0956 (0.1054) Data: 0.0018 (0.0033) Loss: 0.1016 (0.0282)
[2023/01/16 07:40] | TRAIN(133): [ 850/2211] Batch: 0.0976 (0.1045) Data: 0.0018 (0.0032) Loss: 0.0026 (0.0286)
[2023/01/16 07:40] | TRAIN(133): [ 900/2211] Batch: 0.0957 (0.1040) Data: 0.0019 (0.0032) Loss: 0.0778 (0.0282)
[2023/01/16 07:41] | TRAIN(133): [ 950/2211] Batch: 0.1176 (0.1041) Data: 0.0029 (0.0031) Loss: 0.0082 (0.0279)
[2023/01/16 07:41] | TRAIN(133): [1000/2211] Batch: 0.0874 (0.1042) Data: 0.0017 (0.0030) Loss: 0.0863 (0.0278)
[2023/01/16 07:41] | TRAIN(133): [1050/2211] Batch: 0.1186 (0.1042) Data: 0.0019 (0.0030) Loss: 0.0093 (0.0278)
[2023/01/16 07:41] | TRAIN(133): [1100/2211] Batch: 0.0980 (0.1041) Data: 0.0015 (0.0029) Loss: 0.1685 (0.0280)
[2023/01/16 07:41] | TRAIN(133): [1150/2211] Batch: 0.1185 (0.1036) Data: 0.0018 (0.0029) Loss: 0.0237 (0.0280)
[2023/01/16 07:41] | TRAIN(133): [1200/2211] Batch: 0.0981 (0.1035) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0278)
[2023/01/16 07:41] | TRAIN(133): [1250/2211] Batch: 0.0925 (0.1034) Data: 0.0019 (0.0028) Loss: 0.0501 (0.0279)
[2023/01/16 07:41] | TRAIN(133): [1300/2211] Batch: 0.0977 (0.1034) Data: 0.0019 (0.0028) Loss: 0.0776 (0.0281)
[2023/01/16 07:41] | TRAIN(133): [1350/2211] Batch: 0.1068 (0.1032) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0280)
[2023/01/16 07:41] | TRAIN(133): [1400/2211] Batch: 0.0955 (0.1029) Data: 0.0018 (0.0027) Loss: 0.0213 (0.0281)
[2023/01/16 07:41] | TRAIN(133): [1450/2211] Batch: 0.0909 (0.1026) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0280)
[2023/01/16 07:41] | TRAIN(133): [1500/2211] Batch: 0.0985 (0.1024) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0278)
[2023/01/16 07:42] | TRAIN(133): [1550/2211] Batch: 0.0907 (0.1023) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0279)
[2023/01/16 07:42] | TRAIN(133): [1600/2211] Batch: 0.0882 (0.1020) Data: 0.0017 (0.0026) Loss: 0.0412 (0.0278)
[2023/01/16 07:42] | TRAIN(133): [1650/2211] Batch: 0.0912 (0.1018) Data: 0.0017 (0.0026) Loss: 0.1196 (0.0278)
[2023/01/16 07:42] | TRAIN(133): [1700/2211] Batch: 0.0877 (0.1015) Data: 0.0019 (0.0025) Loss: 0.0067 (0.0276)
[2023/01/16 07:42] | TRAIN(133): [1750/2211] Batch: 0.0973 (0.1014) Data: 0.0018 (0.0025) Loss: 0.1199 (0.0276)
[2023/01/16 07:42] | TRAIN(133): [1800/2211] Batch: 0.0903 (0.1011) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0275)
[2023/01/16 07:42] | TRAIN(133): [1850/2211] Batch: 0.0960 (0.1008) Data: 0.0017 (0.0025) Loss: 0.0759 (0.0276)
[2023/01/16 07:42] | TRAIN(133): [1900/2211] Batch: 0.0841 (0.1006) Data: 0.0018 (0.0025) Loss: 0.0331 (0.0273)
[2023/01/16 07:42] | TRAIN(133): [1950/2211] Batch: 0.0899 (0.1003) Data: 0.0016 (0.0024) Loss: 0.0176 (0.0272)
[2023/01/16 07:42] | TRAIN(133): [2000/2211] Batch: 0.1015 (0.1001) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0270)
[2023/01/16 07:42] | TRAIN(133): [2050/2211] Batch: 0.0881 (0.1002) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0269)
[2023/01/16 07:42] | TRAIN(133): [2100/2211] Batch: 0.0885 (0.1001) Data: 0.0018 (0.0024) Loss: 0.0345 (0.0268)
[2023/01/16 07:42] | TRAIN(133): [2150/2211] Batch: 0.0921 (0.1000) Data: 0.0016 (0.0024) Loss: 0.0348 (0.0268)
[2023/01/16 07:43] | TRAIN(133): [2200/2211] Batch: 0.1003 (0.0999) Data: 0.0016 (0.0024) Loss: 0.0647 (0.0269)
[2023/01/16 07:43] | ------------------------------------------------------------
[2023/01/16 07:43] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:43] | ------------------------------------------------------------
[2023/01/16 07:43] |   TRAIN(133)     0:03:40     0:00:05     0:03:35      0.0269
[2023/01/16 07:43] | ------------------------------------------------------------
[2023/01/16 07:43] | **************************************************
[2023/01/16 07:43] | TRAIN(134): [  50/2211] Batch: 0.0928 (0.1273) Data: 0.0020 (0.0251) Loss: 0.0794 (0.0221)
[2023/01/16 07:43] | TRAIN(134): [ 100/2211] Batch: 0.1219 (0.1127) Data: 0.0023 (0.0136) Loss: 0.0000 (0.0239)
[2023/01/16 07:43] | TRAIN(134): [ 150/2211] Batch: 0.1282 (0.1179) Data: 0.0020 (0.0099) Loss: 0.0000 (0.0217)
[2023/01/16 07:43] | TRAIN(134): [ 200/2211] Batch: 0.1085 (0.1215) Data: 0.0015 (0.0080) Loss: 0.0000 (0.0213)
[2023/01/16 07:43] | TRAIN(134): [ 250/2211] Batch: 0.0879 (0.1164) Data: 0.0018 (0.0067) Loss: 0.0000 (0.0206)
[2023/01/16 07:43] | TRAIN(134): [ 300/2211] Batch: 0.1250 (0.1167) Data: 0.0024 (0.0060) Loss: 0.0000 (0.0231)
[2023/01/16 07:43] | TRAIN(134): [ 350/2211] Batch: 0.1182 (0.1152) Data: 0.0022 (0.0054) Loss: 0.0106 (0.0230)
[2023/01/16 07:43] | TRAIN(134): [ 400/2211] Batch: 0.0924 (0.1138) Data: 0.0018 (0.0050) Loss: 0.0000 (0.0228)
[2023/01/16 07:43] | TRAIN(134): [ 450/2211] Batch: 0.1180 (0.1131) Data: 0.0022 (0.0047) Loss: 0.0058 (0.0227)
[2023/01/16 07:44] | TRAIN(134): [ 500/2211] Batch: 0.1189 (0.1139) Data: 0.0024 (0.0044) Loss: 0.0000 (0.0238)
[2023/01/16 07:44] | TRAIN(134): [ 550/2211] Batch: 0.0976 (0.1140) Data: 0.0017 (0.0042) Loss: 0.0000 (0.0234)
[2023/01/16 07:44] | TRAIN(134): [ 600/2211] Batch: 0.0887 (0.1127) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0238)
[2023/01/16 07:44] | TRAIN(134): [ 650/2211] Batch: 0.0979 (0.1113) Data: 0.0017 (0.0039) Loss: 0.0000 (0.0239)
[2023/01/16 07:44] | TRAIN(134): [ 700/2211] Batch: 0.0919 (0.1102) Data: 0.0019 (0.0037) Loss: 0.0000 (0.0240)
[2023/01/16 07:44] | TRAIN(134): [ 750/2211] Batch: 0.0909 (0.1093) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0246)
[2023/01/16 07:44] | TRAIN(134): [ 800/2211] Batch: 0.0975 (0.1084) Data: 0.0018 (0.0035) Loss: 0.0211 (0.0245)
[2023/01/16 07:44] | TRAIN(134): [ 850/2211] Batch: 0.0834 (0.1075) Data: 0.0019 (0.0034) Loss: 0.0000 (0.0244)
[2023/01/16 07:44] | TRAIN(134): [ 900/2211] Batch: 0.1270 (0.1082) Data: 0.0023 (0.0033) Loss: 0.0832 (0.0250)
[2023/01/16 07:44] | TRAIN(134): [ 950/2211] Batch: 0.0887 (0.1086) Data: 0.0018 (0.0032) Loss: 0.0785 (0.0255)
[2023/01/16 07:44] | TRAIN(134): [1000/2211] Batch: 0.1057 (0.1079) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0261)
[2023/01/16 07:44] | TRAIN(134): [1050/2211] Batch: 0.0931 (0.1073) Data: 0.0018 (0.0031) Loss: 0.0061 (0.0262)
[2023/01/16 07:45] | TRAIN(134): [1100/2211] Batch: 0.0916 (0.1068) Data: 0.0019 (0.0031) Loss: 0.0001 (0.0259)
[2023/01/16 07:45] | TRAIN(134): [1150/2211] Batch: 0.0901 (0.1064) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0262)
[2023/01/16 07:45] | TRAIN(134): [1200/2211] Batch: 0.0886 (0.1057) Data: 0.0017 (0.0029) Loss: 0.0465 (0.0271)
[2023/01/16 07:45] | TRAIN(134): [1250/2211] Batch: 0.0950 (0.1053) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0268)
[2023/01/16 07:45] | TRAIN(134): [1300/2211] Batch: 0.0880 (0.1050) Data: 0.0018 (0.0029) Loss: 0.0046 (0.0272)
[2023/01/16 07:45] | TRAIN(134): [1350/2211] Batch: 0.0880 (0.1046) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0273)
[2023/01/16 07:45] | TRAIN(134): [1400/2211] Batch: 0.1073 (0.1049) Data: 0.0017 (0.0028) Loss: 0.0949 (0.0274)
[2023/01/16 07:45] | TRAIN(134): [1450/2211] Batch: 0.0952 (0.1047) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0273)
[2023/01/16 07:45] | TRAIN(134): [1500/2211] Batch: 0.0981 (0.1044) Data: 0.0018 (0.0027) Loss: 0.0034 (0.0268)
[2023/01/16 07:45] | TRAIN(134): [1550/2211] Batch: 0.0957 (0.1040) Data: 0.0031 (0.0027) Loss: 0.0745 (0.0266)
[2023/01/16 07:45] | TRAIN(134): [1600/2211] Batch: 0.0887 (0.1040) Data: 0.0018 (0.0027) Loss: 0.0410 (0.0265)
[2023/01/16 07:45] | TRAIN(134): [1650/2211] Batch: 0.0891 (0.1037) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0265)
[2023/01/16 07:46] | TRAIN(134): [1700/2211] Batch: 0.0954 (0.1035) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0266)
[2023/01/16 07:46] | TRAIN(134): [1750/2211] Batch: 0.0982 (0.1033) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0264)
[2023/01/16 07:46] | TRAIN(134): [1800/2211] Batch: 0.0934 (0.1034) Data: 0.0016 (0.0026) Loss: 0.0630 (0.0265)
[2023/01/16 07:46] | TRAIN(134): [1850/2211] Batch: 0.1184 (0.1033) Data: 0.0021 (0.0026) Loss: 0.0165 (0.0264)
[2023/01/16 07:46] | TRAIN(134): [1900/2211] Batch: 0.1078 (0.1031) Data: 0.0019 (0.0025) Loss: 0.0771 (0.0266)
[2023/01/16 07:46] | TRAIN(134): [1950/2211] Batch: 0.0881 (0.1029) Data: 0.0018 (0.0025) Loss: 0.0358 (0.0266)
[2023/01/16 07:46] | TRAIN(134): [2000/2211] Batch: 0.0997 (0.1027) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0266)
[2023/01/16 07:46] | TRAIN(134): [2050/2211] Batch: 0.1110 (0.1025) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0266)
[2023/01/16 07:46] | TRAIN(134): [2100/2211] Batch: 0.0979 (0.1023) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0264)
[2023/01/16 07:46] | TRAIN(134): [2150/2211] Batch: 0.0990 (0.1021) Data: 0.0017 (0.0025) Loss: 0.0620 (0.0263)
[2023/01/16 07:46] | TRAIN(134): [2200/2211] Batch: 0.0905 (0.1019) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0263)
[2023/01/16 07:46] | ------------------------------------------------------------
[2023/01/16 07:46] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:46] | ------------------------------------------------------------
[2023/01/16 07:46] |   TRAIN(134)     0:03:45     0:00:05     0:03:39      0.0263
[2023/01/16 07:46] | ------------------------------------------------------------
[2023/01/16 07:46] | **************************************************
[2023/01/16 07:46] | TRAIN(135): [  50/2211] Batch: 0.1248 (0.1341) Data: 0.0021 (0.0267) Loss: 0.0268 (0.0204)
[2023/01/16 07:47] | TRAIN(135): [ 100/2211] Batch: 0.0884 (0.1242) Data: 0.0018 (0.0144) Loss: 0.0462 (0.0253)
[2023/01/16 07:47] | TRAIN(135): [ 150/2211] Batch: 0.0987 (0.1147) Data: 0.0018 (0.0102) Loss: 0.0023 (0.0272)
[2023/01/16 07:47] | TRAIN(135): [ 200/2211] Batch: 0.0929 (0.1123) Data: 0.0018 (0.0082) Loss: 0.0000 (0.0280)
[2023/01/16 07:47] | TRAIN(135): [ 250/2211] Batch: 0.0923 (0.1091) Data: 0.0016 (0.0069) Loss: 0.0368 (0.0288)
[2023/01/16 07:47] | TRAIN(135): [ 300/2211] Batch: 0.0887 (0.1082) Data: 0.0018 (0.0061) Loss: 0.0000 (0.0271)
[2023/01/16 07:47] | TRAIN(135): [ 350/2211] Batch: 0.1005 (0.1084) Data: 0.0019 (0.0055) Loss: 0.0626 (0.0264)
[2023/01/16 07:47] | TRAIN(135): [ 400/2211] Batch: 0.0993 (0.1069) Data: 0.0017 (0.0050) Loss: 0.0327 (0.0263)
[2023/01/16 07:47] | TRAIN(135): [ 450/2211] Batch: 0.0895 (0.1058) Data: 0.0018 (0.0047) Loss: 0.0038 (0.0266)
[2023/01/16 07:47] | TRAIN(135): [ 500/2211] Batch: 0.0929 (0.1055) Data: 0.0014 (0.0044) Loss: 0.0703 (0.0271)
[2023/01/16 07:47] | TRAIN(135): [ 550/2211] Batch: 0.0911 (0.1047) Data: 0.0019 (0.0041) Loss: 0.0069 (0.0261)
[2023/01/16 07:47] | TRAIN(135): [ 600/2211] Batch: 0.0954 (0.1043) Data: 0.0019 (0.0039) Loss: 0.0000 (0.0261)
[2023/01/16 07:47] | TRAIN(135): [ 650/2211] Batch: 0.1028 (0.1049) Data: 0.0019 (0.0038) Loss: 0.0000 (0.0257)
[2023/01/16 07:48] | TRAIN(135): [ 700/2211] Batch: 0.1030 (0.1045) Data: 0.0017 (0.0036) Loss: 0.0000 (0.0259)
[2023/01/16 07:48] | TRAIN(135): [ 750/2211] Batch: 0.0924 (0.1039) Data: 0.0016 (0.0035) Loss: 0.0000 (0.0253)
[2023/01/16 07:48] | TRAIN(135): [ 800/2211] Batch: 0.0977 (0.1033) Data: 0.0018 (0.0034) Loss: 0.0817 (0.0255)
[2023/01/16 07:48] | TRAIN(135): [ 850/2211] Batch: 0.1063 (0.1031) Data: 0.0016 (0.0033) Loss: 0.0400 (0.0256)
[2023/01/16 07:48] | TRAIN(135): [ 900/2211] Batch: 0.0969 (0.1027) Data: 0.0018 (0.0032) Loss: 0.0043 (0.0255)
[2023/01/16 07:48] | TRAIN(135): [ 950/2211] Batch: 0.0877 (0.1023) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0247)
[2023/01/16 07:48] | TRAIN(135): [1000/2211] Batch: 0.0959 (0.1018) Data: 0.0018 (0.0031) Loss: 0.0199 (0.0243)
[2023/01/16 07:48] | TRAIN(135): [1050/2211] Batch: 0.0876 (0.1014) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0244)
[2023/01/16 07:48] | TRAIN(135): [1100/2211] Batch: 0.1104 (0.1013) Data: 0.0016 (0.0030) Loss: 0.0084 (0.0245)
[2023/01/16 07:48] | TRAIN(135): [1150/2211] Batch: 0.0915 (0.1010) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0250)
[2023/01/16 07:48] | TRAIN(135): [1200/2211] Batch: 0.0888 (0.1008) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0252)
[2023/01/16 07:48] | TRAIN(135): [1250/2211] Batch: 0.1073 (0.1005) Data: 0.0019 (0.0028) Loss: 0.0300 (0.0253)
[2023/01/16 07:49] | TRAIN(135): [1300/2211] Batch: 0.0919 (0.1003) Data: 0.0017 (0.0028) Loss: 0.0691 (0.0253)
[2023/01/16 07:49] | TRAIN(135): [1350/2211] Batch: 0.0917 (0.1001) Data: 0.0016 (0.0027) Loss: 0.0000 (0.0255)
[2023/01/16 07:49] | TRAIN(135): [1400/2211] Batch: 0.0925 (0.1000) Data: 0.0016 (0.0027) Loss: 0.0366 (0.0257)
[2023/01/16 07:49] | TRAIN(135): [1450/2211] Batch: 0.0925 (0.1000) Data: 0.0014 (0.0027) Loss: 0.0000 (0.0256)
[2023/01/16 07:49] | TRAIN(135): [1500/2211] Batch: 0.0914 (0.0999) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0259)
[2023/01/16 07:49] | TRAIN(135): [1550/2211] Batch: 0.0982 (0.1000) Data: 0.0014 (0.0026) Loss: 0.0305 (0.0262)
[2023/01/16 07:49] | TRAIN(135): [1600/2211] Batch: 0.1093 (0.1000) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0260)
[2023/01/16 07:49] | TRAIN(135): [1650/2211] Batch: 0.0905 (0.0999) Data: 0.0017 (0.0026) Loss: 0.0677 (0.0259)
[2023/01/16 07:49] | TRAIN(135): [1700/2211] Batch: 0.1178 (0.0998) Data: 0.0022 (0.0025) Loss: 0.0938 (0.0259)
[2023/01/16 07:49] | TRAIN(135): [1750/2211] Batch: 0.1050 (0.0998) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0261)
[2023/01/16 07:49] | TRAIN(135): [1800/2211] Batch: 0.0997 (0.0996) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0259)
[2023/01/16 07:49] | TRAIN(135): [1850/2211] Batch: 0.1040 (0.0995) Data: 0.0013 (0.0025) Loss: 0.0365 (0.0259)
[2023/01/16 07:50] | TRAIN(135): [1900/2211] Batch: 0.0897 (0.0993) Data: 0.0020 (0.0024) Loss: 0.1054 (0.0261)
[2023/01/16 07:50] | TRAIN(135): [1950/2211] Batch: 0.1041 (0.0992) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0263)
[2023/01/16 07:50] | TRAIN(135): [2000/2211] Batch: 0.0883 (0.0991) Data: 0.0018 (0.0024) Loss: 0.1487 (0.0264)
[2023/01/16 07:50] | TRAIN(135): [2050/2211] Batch: 0.0879 (0.0990) Data: 0.0013 (0.0024) Loss: 0.0000 (0.0265)
[2023/01/16 07:50] | TRAIN(135): [2100/2211] Batch: 0.0910 (0.0989) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0266)
[2023/01/16 07:50] | TRAIN(135): [2150/2211] Batch: 0.0912 (0.0989) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0266)
[2023/01/16 07:50] | TRAIN(135): [2200/2211] Batch: 0.0960 (0.0987) Data: 0.0017 (0.0023) Loss: 0.0060 (0.0268)
[2023/01/16 07:50] | ------------------------------------------------------------
[2023/01/16 07:50] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:50] | ------------------------------------------------------------
[2023/01/16 07:50] |   TRAIN(135)     0:03:38     0:00:05     0:03:33      0.0269
[2023/01/16 07:50] | ------------------------------------------------------------
[2023/01/16 07:50] | **************************************************
[2023/01/16 07:50] | TRAIN(136): [  50/2211] Batch: 0.0898 (0.1264) Data: 0.0018 (0.0254) Loss: 0.0155 (0.0319)
[2023/01/16 07:50] | TRAIN(136): [ 100/2211] Batch: 0.0898 (0.1118) Data: 0.0016 (0.0137) Loss: 0.0776 (0.0339)
[2023/01/16 07:50] | TRAIN(136): [ 150/2211] Batch: 0.0968 (0.1070) Data: 0.0018 (0.0098) Loss: 0.0000 (0.0322)
[2023/01/16 07:50] | TRAIN(136): [ 200/2211] Batch: 0.1073 (0.1038) Data: 0.0018 (0.0078) Loss: 0.0000 (0.0332)
[2023/01/16 07:50] | TRAIN(136): [ 250/2211] Batch: 0.0919 (0.1031) Data: 0.0018 (0.0066) Loss: 0.0035 (0.0321)
[2023/01/16 07:51] | TRAIN(136): [ 300/2211] Batch: 0.0873 (0.1021) Data: 0.0018 (0.0058) Loss: 0.0994 (0.0318)
[2023/01/16 07:51] | TRAIN(136): [ 350/2211] Batch: 0.0937 (0.1025) Data: 0.0017 (0.0052) Loss: 0.0000 (0.0297)
[2023/01/16 07:51] | TRAIN(136): [ 400/2211] Batch: 0.0934 (0.1015) Data: 0.0019 (0.0048) Loss: 0.0933 (0.0288)
[2023/01/16 07:51] | TRAIN(136): [ 450/2211] Batch: 0.0942 (0.1016) Data: 0.0018 (0.0045) Loss: 0.0002 (0.0286)
[2023/01/16 07:51] | TRAIN(136): [ 500/2211] Batch: 0.0907 (0.1009) Data: 0.0018 (0.0042) Loss: 0.0184 (0.0288)
[2023/01/16 07:51] | TRAIN(136): [ 550/2211] Batch: 0.0906 (0.1002) Data: 0.0022 (0.0040) Loss: 0.0000 (0.0286)
[2023/01/16 07:51] | TRAIN(136): [ 600/2211] Batch: 0.1047 (0.0998) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0281)
[2023/01/16 07:51] | TRAIN(136): [ 650/2211] Batch: 0.0989 (0.0993) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0277)
[2023/01/16 07:51] | TRAIN(136): [ 700/2211] Batch: 0.0904 (0.0991) Data: 0.0017 (0.0035) Loss: 0.0281 (0.0275)
[2023/01/16 07:51] | TRAIN(136): [ 750/2211] Batch: 0.0930 (0.0988) Data: 0.0018 (0.0034) Loss: 0.0000 (0.0273)
[2023/01/16 07:51] | TRAIN(136): [ 800/2211] Batch: 0.1087 (0.0986) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0269)
[2023/01/16 07:51] | TRAIN(136): [ 850/2211] Batch: 0.0987 (0.0985) Data: 0.0018 (0.0032) Loss: 0.0136 (0.0262)
[2023/01/16 07:51] | TRAIN(136): [ 900/2211] Batch: 0.0982 (0.0984) Data: 0.0017 (0.0031) Loss: 0.0020 (0.0263)
[2023/01/16 07:52] | TRAIN(136): [ 950/2211] Batch: 0.0991 (0.0985) Data: 0.0018 (0.0031) Loss: 0.0548 (0.0266)
[2023/01/16 07:52] | TRAIN(136): [1000/2211] Batch: 0.0891 (0.0982) Data: 0.0019 (0.0030) Loss: 0.0039 (0.0266)
[2023/01/16 07:52] | TRAIN(136): [1050/2211] Batch: 0.0984 (0.0979) Data: 0.0018 (0.0029) Loss: 0.0411 (0.0264)
[2023/01/16 07:52] | TRAIN(136): [1100/2211] Batch: 0.1002 (0.0978) Data: 0.0019 (0.0029) Loss: 0.0126 (0.0264)
[2023/01/16 07:52] | TRAIN(136): [1150/2211] Batch: 0.0917 (0.0976) Data: 0.0015 (0.0028) Loss: 0.0158 (0.0266)
[2023/01/16 07:52] | TRAIN(136): [1200/2211] Batch: 0.0952 (0.0974) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0264)
[2023/01/16 07:52] | TRAIN(136): [1250/2211] Batch: 0.0976 (0.0973) Data: 0.0018 (0.0028) Loss: 0.0011 (0.0262)
[2023/01/16 07:52] | TRAIN(136): [1300/2211] Batch: 0.0897 (0.0972) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0260)
[2023/01/16 07:52] | TRAIN(136): [1350/2211] Batch: 0.1060 (0.0972) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0260)
[2023/01/16 07:52] | TRAIN(136): [1400/2211] Batch: 0.1075 (0.0971) Data: 0.0018 (0.0026) Loss: 0.0266 (0.0259)
[2023/01/16 07:52] | TRAIN(136): [1450/2211] Batch: 0.0956 (0.0972) Data: 0.0015 (0.0026) Loss: 0.1099 (0.0259)
[2023/01/16 07:52] | TRAIN(136): [1500/2211] Batch: 0.1118 (0.0974) Data: 0.0017 (0.0026) Loss: 0.0056 (0.0260)
[2023/01/16 07:53] | TRAIN(136): [1550/2211] Batch: 0.0886 (0.0974) Data: 0.0018 (0.0026) Loss: 0.0257 (0.0258)
[2023/01/16 07:53] | TRAIN(136): [1600/2211] Batch: 0.0925 (0.0973) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0256)
[2023/01/16 07:53] | TRAIN(136): [1650/2211] Batch: 0.0982 (0.0976) Data: 0.0016 (0.0025) Loss: 0.0043 (0.0256)
[2023/01/16 07:53] | TRAIN(136): [1700/2211] Batch: 0.0923 (0.0975) Data: 0.0018 (0.0025) Loss: 0.0800 (0.0256)
[2023/01/16 07:53] | TRAIN(136): [1750/2211] Batch: 0.0908 (0.0975) Data: 0.0018 (0.0025) Loss: 0.0081 (0.0254)
[2023/01/16 07:53] | TRAIN(136): [1800/2211] Batch: 0.0944 (0.0973) Data: 0.0017 (0.0025) Loss: 0.0490 (0.0256)
[2023/01/16 07:53] | TRAIN(136): [1850/2211] Batch: 0.0965 (0.0972) Data: 0.0017 (0.0024) Loss: 0.0373 (0.0254)
[2023/01/16 07:53] | TRAIN(136): [1900/2211] Batch: 0.0910 (0.0972) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0256)
[2023/01/16 07:53] | TRAIN(136): [1950/2211] Batch: 0.1030 (0.0972) Data: 0.0018 (0.0024) Loss: 0.0530 (0.0257)
[2023/01/16 07:53] | TRAIN(136): [2000/2211] Batch: 0.1187 (0.0974) Data: 0.0022 (0.0024) Loss: 0.1018 (0.0257)
[2023/01/16 07:53] | TRAIN(136): [2050/2211] Batch: 0.1206 (0.0976) Data: 0.0023 (0.0024) Loss: 0.0000 (0.0257)
[2023/01/16 07:53] | TRAIN(136): [2100/2211] Batch: 0.0914 (0.0977) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0254)
[2023/01/16 07:53] | TRAIN(136): [2150/2211] Batch: 0.0919 (0.0976) Data: 0.0018 (0.0024) Loss: 0.0304 (0.0257)
[2023/01/16 07:54] | TRAIN(136): [2200/2211] Batch: 0.0903 (0.0976) Data: 0.0016 (0.0023) Loss: 0.0000 (0.0257)
[2023/01/16 07:54] | ------------------------------------------------------------
[2023/01/16 07:54] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:54] | ------------------------------------------------------------
[2023/01/16 07:54] |   TRAIN(136)     0:03:35     0:00:05     0:03:30      0.0258
[2023/01/16 07:54] | ------------------------------------------------------------
[2023/01/16 07:54] | **************************************************
[2023/01/16 07:54] | TRAIN(137): [  50/2211] Batch: 0.1192 (0.1455) Data: 0.0022 (0.0240) Loss: 0.0000 (0.0211)
[2023/01/16 07:54] | TRAIN(137): [ 100/2211] Batch: 0.1250 (0.1310) Data: 0.0022 (0.0132) Loss: 0.0183 (0.0218)
[2023/01/16 07:54] | TRAIN(137): [ 150/2211] Batch: 0.1258 (0.1264) Data: 0.0024 (0.0095) Loss: 0.1445 (0.0217)
[2023/01/16 07:54] | TRAIN(137): [ 200/2211] Batch: 0.0944 (0.1211) Data: 0.0021 (0.0077) Loss: 0.0000 (0.0267)
[2023/01/16 07:54] | TRAIN(137): [ 250/2211] Batch: 0.0950 (0.1212) Data: 0.0020 (0.0066) Loss: 0.0000 (0.0255)
[2023/01/16 07:54] | TRAIN(137): [ 300/2211] Batch: 0.0950 (0.1190) Data: 0.0017 (0.0058) Loss: 0.0000 (0.0275)
[2023/01/16 07:54] | TRAIN(137): [ 350/2211] Batch: 0.1038 (0.1157) Data: 0.0032 (0.0053) Loss: 0.0000 (0.0275)
[2023/01/16 07:54] | TRAIN(137): [ 400/2211] Batch: 0.0875 (0.1136) Data: 0.0018 (0.0048) Loss: 0.0988 (0.0298)
[2023/01/16 07:54] | TRAIN(137): [ 450/2211] Batch: 0.0887 (0.1114) Data: 0.0017 (0.0045) Loss: 0.0124 (0.0296)
[2023/01/16 07:55] | TRAIN(137): [ 500/2211] Batch: 0.0977 (0.1095) Data: 0.0026 (0.0042) Loss: 0.0000 (0.0299)
[2023/01/16 07:55] | TRAIN(137): [ 550/2211] Batch: 0.0962 (0.1094) Data: 0.0017 (0.0040) Loss: 0.0306 (0.0307)
[2023/01/16 07:55] | TRAIN(137): [ 600/2211] Batch: 0.0879 (0.1086) Data: 0.0015 (0.0038) Loss: 0.0136 (0.0306)
[2023/01/16 07:55] | TRAIN(137): [ 650/2211] Batch: 0.0930 (0.1075) Data: 0.0018 (0.0037) Loss: 0.0490 (0.0302)
[2023/01/16 07:55] | TRAIN(137): [ 700/2211] Batch: 0.0880 (0.1065) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0297)
[2023/01/16 07:55] | TRAIN(137): [ 750/2211] Batch: 0.1012 (0.1060) Data: 0.0017 (0.0034) Loss: 0.0629 (0.0292)
[2023/01/16 07:55] | TRAIN(137): [ 800/2211] Batch: 0.0973 (0.1052) Data: 0.0018 (0.0033) Loss: 0.1677 (0.0298)
[2023/01/16 07:55] | TRAIN(137): [ 850/2211] Batch: 0.1008 (0.1044) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0294)
[2023/01/16 07:55] | TRAIN(137): [ 900/2211] Batch: 0.1007 (0.1040) Data: 0.0022 (0.0031) Loss: 0.0025 (0.0296)
[2023/01/16 07:55] | TRAIN(137): [ 950/2211] Batch: 0.1213 (0.1038) Data: 0.0022 (0.0031) Loss: 0.0000 (0.0294)
[2023/01/16 07:55] | TRAIN(137): [1000/2211] Batch: 0.0877 (0.1035) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0289)
[2023/01/16 07:55] | TRAIN(137): [1050/2211] Batch: 0.1293 (0.1030) Data: 0.0023 (0.0030) Loss: 0.0000 (0.0286)
[2023/01/16 07:55] | TRAIN(137): [1100/2211] Batch: 0.1016 (0.1026) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0286)
[2023/01/16 07:56] | TRAIN(137): [1150/2211] Batch: 0.0946 (0.1022) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0284)
[2023/01/16 07:56] | TRAIN(137): [1200/2211] Batch: 0.0893 (0.1019) Data: 0.0018 (0.0028) Loss: 0.0538 (0.0280)
[2023/01/16 07:56] | TRAIN(137): [1250/2211] Batch: 0.0880 (0.1015) Data: 0.0018 (0.0028) Loss: 0.0376 (0.0280)
[2023/01/16 07:56] | TRAIN(137): [1300/2211] Batch: 0.0983 (0.1012) Data: 0.0017 (0.0027) Loss: 0.0217 (0.0278)
[2023/01/16 07:56] | TRAIN(137): [1350/2211] Batch: 0.1179 (0.1011) Data: 0.0022 (0.0027) Loss: 0.0333 (0.0275)
[2023/01/16 07:56] | TRAIN(137): [1400/2211] Batch: 0.1011 (0.1012) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0278)
[2023/01/16 07:56] | TRAIN(137): [1450/2211] Batch: 0.0878 (0.1009) Data: 0.0015 (0.0026) Loss: 0.0000 (0.0281)
[2023/01/16 07:56] | TRAIN(137): [1500/2211] Batch: 0.0874 (0.1007) Data: 0.0017 (0.0026) Loss: 0.1551 (0.0282)
[2023/01/16 07:56] | TRAIN(137): [1550/2211] Batch: 0.0926 (0.1004) Data: 0.0017 (0.0026) Loss: 0.0968 (0.0281)
[2023/01/16 07:56] | TRAIN(137): [1600/2211] Batch: 0.0911 (0.1002) Data: 0.0015 (0.0026) Loss: 0.0473 (0.0279)
[2023/01/16 07:56] | TRAIN(137): [1650/2211] Batch: 0.0941 (0.1001) Data: 0.0020 (0.0025) Loss: 0.0633 (0.0280)
[2023/01/16 07:56] | TRAIN(137): [1700/2211] Batch: 0.0888 (0.1000) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0280)
[2023/01/16 07:57] | TRAIN(137): [1750/2211] Batch: 0.0895 (0.0998) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0278)
[2023/01/16 07:57] | TRAIN(137): [1800/2211] Batch: 0.0930 (0.0997) Data: 0.0018 (0.0025) Loss: 0.0290 (0.0278)
[2023/01/16 07:57] | TRAIN(137): [1850/2211] Batch: 0.0875 (0.0995) Data: 0.0017 (0.0024) Loss: 0.0735 (0.0277)
[2023/01/16 07:57] | TRAIN(137): [1900/2211] Batch: 0.0936 (0.0993) Data: 0.0019 (0.0024) Loss: 0.0933 (0.0274)
[2023/01/16 07:57] | TRAIN(137): [1950/2211] Batch: 0.0875 (0.0992) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0272)
[2023/01/16 07:57] | TRAIN(137): [2000/2211] Batch: 0.0881 (0.0991) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0272)
[2023/01/16 07:57] | TRAIN(137): [2050/2211] Batch: 0.0885 (0.0988) Data: 0.0017 (0.0024) Loss: 0.0336 (0.0275)
[2023/01/16 07:57] | TRAIN(137): [2100/2211] Batch: 0.0904 (0.0988) Data: 0.0017 (0.0024) Loss: 0.0743 (0.0273)
[2023/01/16 07:57] | TRAIN(137): [2150/2211] Batch: 0.0872 (0.0986) Data: 0.0017 (0.0024) Loss: 0.0090 (0.0273)
[2023/01/16 07:57] | TRAIN(137): [2200/2211] Batch: 0.0864 (0.0984) Data: 0.0015 (0.0023) Loss: 0.0000 (0.0271)
[2023/01/16 07:57] | ------------------------------------------------------------
[2023/01/16 07:57] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 07:57] | ------------------------------------------------------------
[2023/01/16 07:57] |   TRAIN(137)     0:03:37     0:00:05     0:03:32      0.0271
[2023/01/16 07:57] | ------------------------------------------------------------
[2023/01/16 07:57] | **************************************************
[2023/01/16 07:57] | TRAIN(138): [  50/2211] Batch: 0.0881 (0.1189) Data: 0.0017 (0.0249) Loss: 0.0256 (0.0217)
[2023/01/16 07:57] | TRAIN(138): [ 100/2211] Batch: 0.0940 (0.1050) Data: 0.0018 (0.0135) Loss: 0.0432 (0.0218)
[2023/01/16 07:57] | TRAIN(138): [ 150/2211] Batch: 0.0845 (0.1008) Data: 0.0018 (0.0096) Loss: 0.1031 (0.0219)
[2023/01/16 07:58] | TRAIN(138): [ 200/2211] Batch: 0.0957 (0.0983) Data: 0.0015 (0.0076) Loss: 0.0019 (0.0233)
[2023/01/16 07:58] | TRAIN(138): [ 250/2211] Batch: 0.0977 (0.0969) Data: 0.0018 (0.0064) Loss: 0.0158 (0.0236)
[2023/01/16 07:58] | TRAIN(138): [ 300/2211] Batch: 0.0885 (0.0958) Data: 0.0017 (0.0057) Loss: 0.0471 (0.0246)
[2023/01/16 07:58] | TRAIN(138): [ 350/2211] Batch: 0.0895 (0.0980) Data: 0.0017 (0.0051) Loss: 0.0000 (0.0255)
[2023/01/16 07:58] | TRAIN(138): [ 400/2211] Batch: 0.0860 (0.0974) Data: 0.0018 (0.0047) Loss: 0.0017 (0.0246)
[2023/01/16 07:58] | TRAIN(138): [ 450/2211] Batch: 0.0901 (0.0966) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0247)
[2023/01/16 07:58] | TRAIN(138): [ 500/2211] Batch: 0.0933 (0.0961) Data: 0.0018 (0.0041) Loss: 0.0015 (0.0245)
[2023/01/16 07:58] | TRAIN(138): [ 550/2211] Batch: 0.1345 (0.0969) Data: 0.0022 (0.0039) Loss: 0.0400 (0.0240)
[2023/01/16 07:58] | TRAIN(138): [ 600/2211] Batch: 0.0933 (0.0978) Data: 0.0018 (0.0038) Loss: 0.0474 (0.0242)
[2023/01/16 07:58] | TRAIN(138): [ 650/2211] Batch: 0.0843 (0.0978) Data: 0.0019 (0.0036) Loss: 0.0000 (0.0247)
[2023/01/16 07:58] | TRAIN(138): [ 700/2211] Batch: 0.0903 (0.0976) Data: 0.0018 (0.0035) Loss: 0.0420 (0.0249)
[2023/01/16 07:58] | TRAIN(138): [ 750/2211] Batch: 0.0883 (0.0972) Data: 0.0019 (0.0034) Loss: 0.0000 (0.0244)
[2023/01/16 07:59] | TRAIN(138): [ 800/2211] Batch: 0.0916 (0.0968) Data: 0.0017 (0.0033) Loss: 0.0233 (0.0243)
[2023/01/16 07:59] | TRAIN(138): [ 850/2211] Batch: 0.1196 (0.0971) Data: 0.0021 (0.0032) Loss: 0.0175 (0.0242)
[2023/01/16 07:59] | TRAIN(138): [ 900/2211] Batch: 0.0977 (0.0969) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0242)
[2023/01/16 07:59] | TRAIN(138): [ 950/2211] Batch: 0.0884 (0.0966) Data: 0.0017 (0.0030) Loss: 0.0021 (0.0243)
[2023/01/16 07:59] | TRAIN(138): [1000/2211] Batch: 0.0987 (0.0963) Data: 0.0017 (0.0030) Loss: 0.0458 (0.0247)
[2023/01/16 07:59] | TRAIN(138): [1050/2211] Batch: 0.0876 (0.0960) Data: 0.0018 (0.0029) Loss: 0.1952 (0.0245)
[2023/01/16 07:59] | TRAIN(138): [1100/2211] Batch: 0.0939 (0.0962) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0247)
[2023/01/16 07:59] | TRAIN(138): [1150/2211] Batch: 0.0953 (0.0960) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0245)
[2023/01/16 07:59] | TRAIN(138): [1200/2211] Batch: 0.0934 (0.0960) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0245)
[2023/01/16 07:59] | TRAIN(138): [1250/2211] Batch: 0.0898 (0.0959) Data: 0.0018 (0.0027) Loss: 0.0605 (0.0246)
[2023/01/16 07:59] | TRAIN(138): [1300/2211] Batch: 0.0899 (0.0958) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0246)
[2023/01/16 07:59] | TRAIN(138): [1350/2211] Batch: 0.0950 (0.0958) Data: 0.0018 (0.0027) Loss: 0.0821 (0.0245)
[2023/01/16 07:59] | TRAIN(138): [1400/2211] Batch: 0.0973 (0.0958) Data: 0.0019 (0.0026) Loss: 0.1150 (0.0249)
[2023/01/16 08:00] | TRAIN(138): [1450/2211] Batch: 0.0958 (0.0957) Data: 0.0018 (0.0026) Loss: 0.0874 (0.0249)
[2023/01/16 08:00] | TRAIN(138): [1500/2211] Batch: 0.0930 (0.0957) Data: 0.0017 (0.0026) Loss: 0.0157 (0.0248)
[2023/01/16 08:00] | TRAIN(138): [1550/2211] Batch: 0.0988 (0.0957) Data: 0.0017 (0.0026) Loss: 0.0395 (0.0250)
[2023/01/16 08:00] | TRAIN(138): [1600/2211] Batch: 0.0877 (0.0959) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0247)
[2023/01/16 08:00] | TRAIN(138): [1650/2211] Batch: 0.0922 (0.0960) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0248)
[2023/01/16 08:00] | TRAIN(138): [1700/2211] Batch: 0.0906 (0.0960) Data: 0.0018 (0.0025) Loss: 0.0059 (0.0250)
[2023/01/16 08:00] | TRAIN(138): [1750/2211] Batch: 0.0891 (0.0959) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0249)
[2023/01/16 08:00] | TRAIN(138): [1800/2211] Batch: 0.0978 (0.0958) Data: 0.0018 (0.0025) Loss: 0.0845 (0.0252)
[2023/01/16 08:00] | TRAIN(138): [1850/2211] Batch: 0.0962 (0.0959) Data: 0.0018 (0.0025) Loss: 0.1429 (0.0252)
[2023/01/16 08:00] | TRAIN(138): [1900/2211] Batch: 0.0911 (0.0961) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0252)
[2023/01/16 08:00] | TRAIN(138): [1950/2211] Batch: 0.0892 (0.0961) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0256)
[2023/01/16 08:00] | TRAIN(138): [2000/2211] Batch: 0.0912 (0.0961) Data: 0.0018 (0.0024) Loss: 0.0647 (0.0259)
[2023/01/16 08:01] | TRAIN(138): [2050/2211] Batch: 0.0891 (0.0961) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0265)
[2023/01/16 08:01] | TRAIN(138): [2100/2211] Batch: 0.0922 (0.0961) Data: 0.0018 (0.0024) Loss: 0.0078 (0.0266)
[2023/01/16 08:01] | TRAIN(138): [2150/2211] Batch: 0.1015 (0.0960) Data: 0.0018 (0.0024) Loss: 0.0254 (0.0265)
[2023/01/16 08:01] | TRAIN(138): [2200/2211] Batch: 0.1250 (0.0961) Data: 0.0021 (0.0024) Loss: 0.0000 (0.0267)
[2023/01/16 08:01] | ------------------------------------------------------------
[2023/01/16 08:01] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:01] | ------------------------------------------------------------
[2023/01/16 08:01] |   TRAIN(138)     0:03:32     0:00:05     0:03:27      0.0266
[2023/01/16 08:01] | ------------------------------------------------------------
[2023/01/16 08:01] | **************************************************
[2023/01/16 08:01] | TRAIN(139): [  50/2211] Batch: 0.1192 (0.1392) Data: 0.0021 (0.0261) Loss: 0.0000 (0.0187)
[2023/01/16 08:01] | TRAIN(139): [ 100/2211] Batch: 0.0935 (0.1220) Data: 0.0017 (0.0142) Loss: 0.0123 (0.0248)
[2023/01/16 08:01] | TRAIN(139): [ 150/2211] Batch: 0.0946 (0.1126) Data: 0.0020 (0.0101) Loss: 0.0174 (0.0242)
[2023/01/16 08:01] | TRAIN(139): [ 200/2211] Batch: 0.0923 (0.1085) Data: 0.0018 (0.0080) Loss: 0.0000 (0.0251)
[2023/01/16 08:01] | TRAIN(139): [ 250/2211] Batch: 0.0878 (0.1055) Data: 0.0017 (0.0068) Loss: 0.0174 (0.0235)
[2023/01/16 08:01] | TRAIN(139): [ 300/2211] Batch: 0.1149 (0.1039) Data: 0.0018 (0.0059) Loss: 0.1078 (0.0239)
[2023/01/16 08:01] | TRAIN(139): [ 350/2211] Batch: 0.0993 (0.1040) Data: 0.0019 (0.0054) Loss: 0.0000 (0.0257)
[2023/01/16 08:01] | TRAIN(139): [ 400/2211] Batch: 0.0888 (0.1025) Data: 0.0018 (0.0049) Loss: 0.0044 (0.0252)
[2023/01/16 08:02] | TRAIN(139): [ 450/2211] Batch: 0.0972 (0.1013) Data: 0.0018 (0.0046) Loss: 0.0126 (0.0239)
[2023/01/16 08:02] | TRAIN(139): [ 500/2211] Batch: 0.0939 (0.1003) Data: 0.0017 (0.0043) Loss: 0.0000 (0.0253)
[2023/01/16 08:02] | TRAIN(139): [ 550/2211] Batch: 0.0886 (0.1002) Data: 0.0019 (0.0041) Loss: 0.0000 (0.0251)
[2023/01/16 08:02] | TRAIN(139): [ 600/2211] Batch: 0.0935 (0.0998) Data: 0.0019 (0.0039) Loss: 0.0680 (0.0246)
[2023/01/16 08:02] | TRAIN(139): [ 650/2211] Batch: 0.0924 (0.0998) Data: 0.0018 (0.0038) Loss: 0.0088 (0.0243)
[2023/01/16 08:02] | TRAIN(139): [ 700/2211] Batch: 0.0924 (0.0998) Data: 0.0018 (0.0036) Loss: 0.0000 (0.0243)
[2023/01/16 08:02] | TRAIN(139): [ 750/2211] Batch: 0.0881 (0.0993) Data: 0.0018 (0.0035) Loss: 0.0256 (0.0242)
[2023/01/16 08:02] | TRAIN(139): [ 800/2211] Batch: 0.0918 (0.0989) Data: 0.0017 (0.0034) Loss: 0.0541 (0.0240)
[2023/01/16 08:02] | TRAIN(139): [ 850/2211] Batch: 0.0955 (0.0985) Data: 0.0017 (0.0033) Loss: 0.0781 (0.0241)
[2023/01/16 08:02] | TRAIN(139): [ 900/2211] Batch: 0.1017 (0.0986) Data: 0.0021 (0.0032) Loss: 0.0000 (0.0242)
[2023/01/16 08:02] | TRAIN(139): [ 950/2211] Batch: 0.0949 (0.0990) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0242)
[2023/01/16 08:02] | TRAIN(139): [1000/2211] Batch: 0.0983 (0.0989) Data: 0.0019 (0.0031) Loss: 0.0000 (0.0242)
[2023/01/16 08:02] | TRAIN(139): [1050/2211] Batch: 0.1192 (0.0987) Data: 0.0023 (0.0030) Loss: 0.0610 (0.0244)
[2023/01/16 08:03] | TRAIN(139): [1100/2211] Batch: 0.1041 (0.0987) Data: 0.0017 (0.0030) Loss: 0.0076 (0.0247)
[2023/01/16 08:03] | TRAIN(139): [1150/2211] Batch: 0.0989 (0.0985) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0245)
[2023/01/16 08:03] | TRAIN(139): [1200/2211] Batch: 0.1208 (0.0983) Data: 0.0021 (0.0029) Loss: 0.0542 (0.0247)
[2023/01/16 08:03] | TRAIN(139): [1250/2211] Batch: 0.1182 (0.0992) Data: 0.0022 (0.0029) Loss: 0.0303 (0.0247)
[2023/01/16 08:03] | TRAIN(139): [1300/2211] Batch: 0.0907 (0.0993) Data: 0.0018 (0.0028) Loss: 0.0706 (0.0244)
[2023/01/16 08:03] | TRAIN(139): [1350/2211] Batch: 0.0894 (0.0991) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0241)
[2023/01/16 08:03] | TRAIN(139): [1400/2211] Batch: 0.0894 (0.0992) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0243)
[2023/01/16 08:03] | TRAIN(139): [1450/2211] Batch: 0.0914 (0.0994) Data: 0.0017 (0.0027) Loss: 0.0038 (0.0243)
[2023/01/16 08:03] | TRAIN(139): [1500/2211] Batch: 0.0914 (0.0993) Data: 0.0018 (0.0027) Loss: 0.1401 (0.0244)
[2023/01/16 08:03] | TRAIN(139): [1550/2211] Batch: 0.0888 (0.0991) Data: 0.0018 (0.0027) Loss: 0.0631 (0.0248)
[2023/01/16 08:03] | TRAIN(139): [1600/2211] Batch: 0.0895 (0.0992) Data: 0.0018 (0.0027) Loss: 0.0041 (0.0246)
[2023/01/16 08:03] | TRAIN(139): [1650/2211] Batch: 0.0885 (0.0992) Data: 0.0018 (0.0026) Loss: 0.0909 (0.0248)
[2023/01/16 08:04] | TRAIN(139): [1700/2211] Batch: 0.0893 (0.0989) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0253)
[2023/01/16 08:04] | TRAIN(139): [1750/2211] Batch: 0.0910 (0.0988) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0253)
[2023/01/16 08:04] | TRAIN(139): [1800/2211] Batch: 0.0887 (0.0986) Data: 0.0020 (0.0026) Loss: 0.0000 (0.0255)
[2023/01/16 08:04] | TRAIN(139): [1850/2211] Batch: 0.0870 (0.0984) Data: 0.0017 (0.0026) Loss: 0.0317 (0.0255)
[2023/01/16 08:04] | TRAIN(139): [1900/2211] Batch: 0.0882 (0.0982) Data: 0.0017 (0.0025) Loss: 0.0163 (0.0258)
[2023/01/16 08:04] | TRAIN(139): [1950/2211] Batch: 0.0893 (0.0981) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0257)
[2023/01/16 08:04] | TRAIN(139): [2000/2211] Batch: 0.0894 (0.0979) Data: 0.0018 (0.0025) Loss: 0.0042 (0.0257)
[2023/01/16 08:04] | TRAIN(139): [2050/2211] Batch: 0.0853 (0.0979) Data: 0.0018 (0.0025) Loss: 0.1113 (0.0261)
[2023/01/16 08:04] | TRAIN(139): [2100/2211] Batch: 0.0874 (0.0977) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0261)
[2023/01/16 08:04] | TRAIN(139): [2150/2211] Batch: 0.0824 (0.0974) Data: 0.0018 (0.0025) Loss: 0.0999 (0.0260)
[2023/01/16 08:04] | TRAIN(139): [2200/2211] Batch: 0.0816 (0.0971) Data: 0.0017 (0.0024) Loss: 0.0442 (0.0260)
[2023/01/16 08:04] | ------------------------------------------------------------
[2023/01/16 08:04] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:04] | ------------------------------------------------------------
[2023/01/16 08:04] |   TRAIN(139)     0:03:34     0:00:05     0:03:29      0.0260
[2023/01/16 08:04] | ------------------------------------------------------------
[2023/01/16 08:04] | **************************************************
[2023/01/16 08:04] | TRAIN(140): [  50/2211] Batch: 0.0973 (0.1221) Data: 0.0018 (0.0254) Loss: 0.0000 (0.0236)
[2023/01/16 08:05] | TRAIN(140): [ 100/2211] Batch: 0.0882 (0.1069) Data: 0.0018 (0.0138) Loss: 0.0000 (0.0221)
[2023/01/16 08:05] | TRAIN(140): [ 150/2211] Batch: 0.0884 (0.1017) Data: 0.0017 (0.0098) Loss: 0.0000 (0.0241)
[2023/01/16 08:05] | TRAIN(140): [ 200/2211] Batch: 0.0979 (0.0992) Data: 0.0021 (0.0078) Loss: 0.0044 (0.0256)
[2023/01/16 08:05] | TRAIN(140): [ 250/2211] Batch: 0.0953 (0.0999) Data: 0.0018 (0.0067) Loss: 0.0275 (0.0273)
[2023/01/16 08:05] | TRAIN(140): [ 300/2211] Batch: 0.0999 (0.1008) Data: 0.0019 (0.0059) Loss: 0.0140 (0.0272)
[2023/01/16 08:05] | TRAIN(140): [ 350/2211] Batch: 0.0892 (0.0999) Data: 0.0018 (0.0053) Loss: 0.0724 (0.0272)
[2023/01/16 08:05] | TRAIN(140): [ 400/2211] Batch: 0.1027 (0.1000) Data: 0.0020 (0.0049) Loss: 0.0019 (0.0280)
[2023/01/16 08:05] | TRAIN(140): [ 450/2211] Batch: 0.1199 (0.1010) Data: 0.0022 (0.0046) Loss: 0.0000 (0.0272)
[2023/01/16 08:05] | TRAIN(140): [ 500/2211] Batch: 0.0919 (0.1007) Data: 0.0018 (0.0043) Loss: 0.0304 (0.0267)
[2023/01/16 08:05] | TRAIN(140): [ 550/2211] Batch: 0.1188 (0.1004) Data: 0.0022 (0.0041) Loss: 0.0380 (0.0268)
[2023/01/16 08:05] | TRAIN(140): [ 600/2211] Batch: 0.0898 (0.1000) Data: 0.0018 (0.0039) Loss: 0.0000 (0.0261)
[2023/01/16 08:05] | TRAIN(140): [ 650/2211] Batch: 0.0904 (0.0995) Data: 0.0018 (0.0037) Loss: 0.0076 (0.0262)
[2023/01/16 08:06] | TRAIN(140): [ 700/2211] Batch: 0.0903 (0.0998) Data: 0.0019 (0.0036) Loss: 0.0729 (0.0258)
[2023/01/16 08:06] | TRAIN(140): [ 750/2211] Batch: 0.1062 (0.0995) Data: 0.0018 (0.0035) Loss: 0.0037 (0.0255)
[2023/01/16 08:06] | TRAIN(140): [ 800/2211] Batch: 0.1042 (0.0989) Data: 0.0019 (0.0034) Loss: 0.0000 (0.0253)
[2023/01/16 08:06] | TRAIN(140): [ 850/2211] Batch: 0.1189 (0.0994) Data: 0.0022 (0.0033) Loss: 0.0076 (0.0257)
[2023/01/16 08:06] | TRAIN(140): [ 900/2211] Batch: 0.0887 (0.0991) Data: 0.0019 (0.0032) Loss: 0.0076 (0.0256)
[2023/01/16 08:06] | TRAIN(140): [ 950/2211] Batch: 0.0890 (0.0994) Data: 0.0018 (0.0032) Loss: 0.0434 (0.0255)
[2023/01/16 08:06] | TRAIN(140): [1000/2211] Batch: 0.0847 (0.0993) Data: 0.0019 (0.0031) Loss: 0.0021 (0.0251)
[2023/01/16 08:06] | TRAIN(140): [1050/2211] Batch: 0.0913 (0.0988) Data: 0.0026 (0.0030) Loss: 0.0000 (0.0252)
[2023/01/16 08:06] | TRAIN(140): [1100/2211] Batch: 0.0914 (0.0986) Data: 0.0016 (0.0030) Loss: 0.0935 (0.0254)
[2023/01/16 08:06] | TRAIN(140): [1150/2211] Batch: 0.0929 (0.0982) Data: 0.0018 (0.0029) Loss: 0.0808 (0.0257)
[2023/01/16 08:06] | TRAIN(140): [1200/2211] Batch: 0.0903 (0.0981) Data: 0.0018 (0.0029) Loss: 0.0059 (0.0255)
[2023/01/16 08:06] | TRAIN(140): [1250/2211] Batch: 0.1085 (0.0981) Data: 0.0017 (0.0028) Loss: 0.0054 (0.0254)
[2023/01/16 08:06] | TRAIN(140): [1300/2211] Batch: 0.0882 (0.0982) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0255)
[2023/01/16 08:07] | TRAIN(140): [1350/2211] Batch: 0.1430 (0.0985) Data: 0.0021 (0.0027) Loss: 0.0000 (0.0255)
[2023/01/16 08:07] | TRAIN(140): [1400/2211] Batch: 0.1010 (0.0986) Data: 0.0018 (0.0027) Loss: 0.0135 (0.0258)
[2023/01/16 08:07] | TRAIN(140): [1450/2211] Batch: 0.0944 (0.0985) Data: 0.0013 (0.0027) Loss: 0.0781 (0.0260)
[2023/01/16 08:07] | TRAIN(140): [1500/2211] Batch: 0.1078 (0.0984) Data: 0.0018 (0.0026) Loss: 0.0748 (0.0260)
[2023/01/16 08:07] | TRAIN(140): [1550/2211] Batch: 0.1373 (0.0989) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0258)
[2023/01/16 08:07] | TRAIN(140): [1600/2211] Batch: 0.0923 (0.0990) Data: 0.0019 (0.0026) Loss: 0.0276 (0.0259)
[2023/01/16 08:07] | TRAIN(140): [1650/2211] Batch: 0.0955 (0.0989) Data: 0.0020 (0.0025) Loss: 0.0000 (0.0259)
[2023/01/16 08:07] | TRAIN(140): [1700/2211] Batch: 0.0965 (0.0988) Data: 0.0015 (0.0025) Loss: 0.0838 (0.0259)
[2023/01/16 08:07] | TRAIN(140): [1750/2211] Batch: 0.0913 (0.0987) Data: 0.0014 (0.0025) Loss: 0.0000 (0.0257)
[2023/01/16 08:07] | TRAIN(140): [1800/2211] Batch: 0.0854 (0.0986) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0258)
[2023/01/16 08:07] | TRAIN(140): [1850/2211] Batch: 0.0882 (0.0984) Data: 0.0018 (0.0025) Loss: 0.0614 (0.0257)
[2023/01/16 08:07] | TRAIN(140): [1900/2211] Batch: 0.0898 (0.0983) Data: 0.0015 (0.0024) Loss: 0.0061 (0.0260)
[2023/01/16 08:08] | TRAIN(140): [1950/2211] Batch: 0.0922 (0.0983) Data: 0.0017 (0.0024) Loss: 0.1275 (0.0263)
[2023/01/16 08:08] | TRAIN(140): [2000/2211] Batch: 0.0895 (0.0981) Data: 0.0017 (0.0024) Loss: 0.0029 (0.0262)
[2023/01/16 08:08] | TRAIN(140): [2050/2211] Batch: 0.0868 (0.0981) Data: 0.0018 (0.0024) Loss: 0.0299 (0.0261)
[2023/01/16 08:08] | TRAIN(140): [2100/2211] Batch: 0.0905 (0.0983) Data: 0.0019 (0.0024) Loss: 0.0400 (0.0260)
[2023/01/16 08:08] | TRAIN(140): [2150/2211] Batch: 0.0872 (0.0983) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0262)
[2023/01/16 08:08] | TRAIN(140): [2200/2211] Batch: 0.0870 (0.0981) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0263)
[2023/01/16 08:08] | ------------------------------------------------------------
[2023/01/16 08:08] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:08] | ------------------------------------------------------------
[2023/01/16 08:08] |   TRAIN(140)     0:03:36     0:00:05     0:03:31      0.0264
[2023/01/16 08:08] | ------------------------------------------------------------
[2023/01/16 08:08] | **************************************************
[2023/01/16 08:08] | TRAIN(141): [  50/2211] Batch: 0.0922 (0.1214) Data: 0.0020 (0.0255) Loss: 0.0273 (0.0264)
[2023/01/16 08:08] | TRAIN(141): [ 100/2211] Batch: 0.0831 (0.1068) Data: 0.0016 (0.0137) Loss: 0.0055 (0.0251)
[2023/01/16 08:08] | TRAIN(141): [ 150/2211] Batch: 0.0907 (0.1019) Data: 0.0018 (0.0098) Loss: 0.0000 (0.0212)
[2023/01/16 08:08] | TRAIN(141): [ 200/2211] Batch: 0.0870 (0.1015) Data: 0.0018 (0.0078) Loss: 0.0000 (0.0230)
[2023/01/16 08:08] | TRAIN(141): [ 250/2211] Batch: 0.1026 (0.1005) Data: 0.0019 (0.0066) Loss: 0.0575 (0.0230)
[2023/01/16 08:08] | TRAIN(141): [ 300/2211] Batch: 0.1230 (0.1005) Data: 0.0022 (0.0058) Loss: 0.0000 (0.0229)
[2023/01/16 08:09] | TRAIN(141): [ 350/2211] Batch: 0.0869 (0.1008) Data: 0.0017 (0.0053) Loss: 0.0000 (0.0220)
[2023/01/16 08:09] | TRAIN(141): [ 400/2211] Batch: 0.0945 (0.1002) Data: 0.0018 (0.0049) Loss: 0.0000 (0.0234)
[2023/01/16 08:09] | TRAIN(141): [ 450/2211] Batch: 0.0889 (0.0995) Data: 0.0018 (0.0045) Loss: 0.0050 (0.0241)
[2023/01/16 08:09] | TRAIN(141): [ 500/2211] Batch: 0.0924 (0.0988) Data: 0.0017 (0.0042) Loss: 0.0576 (0.0235)
[2023/01/16 08:09] | TRAIN(141): [ 550/2211] Batch: 0.0879 (0.0981) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0236)
[2023/01/16 08:09] | TRAIN(141): [ 600/2211] Batch: 0.0891 (0.0978) Data: 0.0018 (0.0038) Loss: 0.0000 (0.0245)
[2023/01/16 08:09] | TRAIN(141): [ 650/2211] Batch: 0.0939 (0.0977) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0244)
[2023/01/16 08:09] | TRAIN(141): [ 700/2211] Batch: 0.0876 (0.0973) Data: 0.0016 (0.0035) Loss: 0.0000 (0.0240)
[2023/01/16 08:09] | TRAIN(141): [ 750/2211] Batch: 0.0898 (0.0968) Data: 0.0017 (0.0034) Loss: 0.0000 (0.0238)
[2023/01/16 08:09] | TRAIN(141): [ 800/2211] Batch: 0.0903 (0.0964) Data: 0.0018 (0.0033) Loss: 0.1232 (0.0244)
[2023/01/16 08:09] | TRAIN(141): [ 850/2211] Batch: 0.0890 (0.0961) Data: 0.0018 (0.0032) Loss: 0.0032 (0.0245)
[2023/01/16 08:09] | TRAIN(141): [ 900/2211] Batch: 0.0884 (0.0959) Data: 0.0017 (0.0031) Loss: 0.1117 (0.0246)
[2023/01/16 08:09] | TRAIN(141): [ 950/2211] Batch: 0.0882 (0.0956) Data: 0.0017 (0.0031) Loss: 0.0448 (0.0243)
[2023/01/16 08:10] | TRAIN(141): [1000/2211] Batch: 0.1057 (0.0956) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0241)
[2023/01/16 08:10] | TRAIN(141): [1050/2211] Batch: 0.0887 (0.0955) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0240)
[2023/01/16 08:10] | TRAIN(141): [1100/2211] Batch: 0.0974 (0.0953) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0239)
[2023/01/16 08:10] | TRAIN(141): [1150/2211] Batch: 0.0944 (0.0953) Data: 0.0017 (0.0029) Loss: 0.0484 (0.0238)
[2023/01/16 08:10] | TRAIN(141): [1200/2211] Batch: 0.0899 (0.0953) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0237)
[2023/01/16 08:10] | TRAIN(141): [1250/2211] Batch: 0.0932 (0.0953) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0236)
[2023/01/16 08:10] | TRAIN(141): [1300/2211] Batch: 0.0905 (0.0952) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0239)
[2023/01/16 08:10] | TRAIN(141): [1350/2211] Batch: 0.0932 (0.0955) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0236)
[2023/01/16 08:10] | TRAIN(141): [1400/2211] Batch: 0.1008 (0.0955) Data: 0.0019 (0.0027) Loss: 0.0222 (0.0238)
[2023/01/16 08:10] | TRAIN(141): [1450/2211] Batch: 0.0984 (0.0955) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0241)
[2023/01/16 08:10] | TRAIN(141): [1500/2211] Batch: 0.0895 (0.0954) Data: 0.0017 (0.0026) Loss: 0.1078 (0.0243)
[2023/01/16 08:10] | TRAIN(141): [1550/2211] Batch: 0.0971 (0.0953) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0241)
[2023/01/16 08:11] | TRAIN(141): [1600/2211] Batch: 0.0975 (0.0953) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0239)
[2023/01/16 08:11] | TRAIN(141): [1650/2211] Batch: 0.0947 (0.0952) Data: 0.0018 (0.0025) Loss: 0.0039 (0.0240)
[2023/01/16 08:11] | TRAIN(141): [1700/2211] Batch: 0.0881 (0.0953) Data: 0.0017 (0.0025) Loss: 0.0150 (0.0239)
[2023/01/16 08:11] | TRAIN(141): [1750/2211] Batch: 0.1025 (0.0953) Data: 0.0018 (0.0025) Loss: 0.0725 (0.0240)
[2023/01/16 08:11] | TRAIN(141): [1800/2211] Batch: 0.0882 (0.0958) Data: 0.0018 (0.0025) Loss: 0.0041 (0.0247)
[2023/01/16 08:11] | TRAIN(141): [1850/2211] Batch: 0.0961 (0.0959) Data: 0.0016 (0.0025) Loss: 0.0003 (0.0247)
[2023/01/16 08:11] | TRAIN(141): [1900/2211] Batch: 0.0881 (0.0957) Data: 0.0017 (0.0024) Loss: 0.0311 (0.0248)
[2023/01/16 08:11] | TRAIN(141): [1950/2211] Batch: 0.0893 (0.0956) Data: 0.0018 (0.0024) Loss: 0.0212 (0.0248)
[2023/01/16 08:11] | TRAIN(141): [2000/2211] Batch: 0.0921 (0.0957) Data: 0.0018 (0.0024) Loss: 0.0063 (0.0248)
[2023/01/16 08:11] | TRAIN(141): [2050/2211] Batch: 0.0936 (0.0957) Data: 0.0018 (0.0024) Loss: 0.0754 (0.0247)
[2023/01/16 08:11] | TRAIN(141): [2100/2211] Batch: 0.0951 (0.0957) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0247)
[2023/01/16 08:11] | TRAIN(141): [2150/2211] Batch: 0.1042 (0.0956) Data: 0.0017 (0.0024) Loss: 0.0036 (0.0247)
[2023/01/16 08:11] | TRAIN(141): [2200/2211] Batch: 0.1118 (0.0956) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0246)
[2023/01/16 08:11] | ------------------------------------------------------------
[2023/01/16 08:11] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:11] | ------------------------------------------------------------
[2023/01/16 08:11] |   TRAIN(141)     0:03:31     0:00:05     0:03:26      0.0246
[2023/01/16 08:11] | ------------------------------------------------------------
[2023/01/16 08:11] | **************************************************
[2023/01/16 08:12] | TRAIN(142): [  50/2211] Batch: 0.0945 (0.1240) Data: 0.0017 (0.0255) Loss: 0.0000 (0.0180)
[2023/01/16 08:12] | TRAIN(142): [ 100/2211] Batch: 0.1027 (0.1172) Data: 0.0018 (0.0139) Loss: 0.0000 (0.0193)
[2023/01/16 08:12] | TRAIN(142): [ 150/2211] Batch: 0.0942 (0.1139) Data: 0.0017 (0.0099) Loss: 0.0277 (0.0194)
[2023/01/16 08:12] | TRAIN(142): [ 200/2211] Batch: 0.1013 (0.1094) Data: 0.0014 (0.0078) Loss: 0.0000 (0.0193)
[2023/01/16 08:12] | TRAIN(142): [ 250/2211] Batch: 0.0948 (0.1063) Data: 0.0018 (0.0066) Loss: 0.0000 (0.0206)
[2023/01/16 08:12] | TRAIN(142): [ 300/2211] Batch: 0.0938 (0.1052) Data: 0.0017 (0.0057) Loss: 0.0000 (0.0205)
[2023/01/16 08:12] | TRAIN(142): [ 350/2211] Batch: 0.1065 (0.1045) Data: 0.0018 (0.0052) Loss: 0.0034 (0.0225)
[2023/01/16 08:12] | TRAIN(142): [ 400/2211] Batch: 0.0883 (0.1042) Data: 0.0018 (0.0047) Loss: 0.0000 (0.0224)
[2023/01/16 08:12] | TRAIN(142): [ 450/2211] Batch: 0.0884 (0.1036) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0237)
[2023/01/16 08:12] | TRAIN(142): [ 500/2211] Batch: 0.0889 (0.1023) Data: 0.0018 (0.0042) Loss: 0.0054 (0.0261)
[2023/01/16 08:12] | TRAIN(142): [ 550/2211] Batch: 0.0951 (0.1016) Data: 0.0018 (0.0040) Loss: 0.0000 (0.0251)
[2023/01/16 08:13] | TRAIN(142): [ 600/2211] Batch: 0.0879 (0.1008) Data: 0.0018 (0.0038) Loss: 0.0287 (0.0256)
[2023/01/16 08:13] | TRAIN(142): [ 650/2211] Batch: 0.0961 (0.1008) Data: 0.0013 (0.0036) Loss: 0.0000 (0.0254)
[2023/01/16 08:13] | TRAIN(142): [ 700/2211] Batch: 0.0938 (0.1006) Data: 0.0013 (0.0035) Loss: 0.0031 (0.0256)
[2023/01/16 08:13] | TRAIN(142): [ 750/2211] Batch: 0.0957 (0.1007) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0253)
[2023/01/16 08:13] | TRAIN(142): [ 800/2211] Batch: 0.0950 (0.1003) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0247)
[2023/01/16 08:13] | TRAIN(142): [ 850/2211] Batch: 0.0952 (0.1000) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0249)
[2023/01/16 08:13] | TRAIN(142): [ 900/2211] Batch: 0.0951 (0.0998) Data: 0.0021 (0.0031) Loss: 0.0047 (0.0247)
[2023/01/16 08:13] | TRAIN(142): [ 950/2211] Batch: 0.1002 (0.0995) Data: 0.0018 (0.0030) Loss: 0.1320 (0.0252)
[2023/01/16 08:13] | TRAIN(142): [1000/2211] Batch: 0.0983 (0.0997) Data: 0.0018 (0.0029) Loss: 0.0974 (0.0259)
[2023/01/16 08:13] | TRAIN(142): [1050/2211] Batch: 0.1190 (0.0997) Data: 0.0021 (0.0029) Loss: 0.0297 (0.0254)
[2023/01/16 08:13] | TRAIN(142): [1100/2211] Batch: 0.0893 (0.0999) Data: 0.0018 (0.0028) Loss: 0.0358 (0.0253)
[2023/01/16 08:13] | TRAIN(142): [1150/2211] Batch: 0.0918 (0.0997) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0251)
[2023/01/16 08:13] | TRAIN(142): [1200/2211] Batch: 0.0924 (0.0995) Data: 0.0018 (0.0027) Loss: 0.0945 (0.0255)
[2023/01/16 08:14] | TRAIN(142): [1250/2211] Batch: 0.0881 (0.0993) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0254)
[2023/01/16 08:14] | TRAIN(142): [1300/2211] Batch: 0.0961 (0.0990) Data: 0.0015 (0.0027) Loss: 0.0000 (0.0252)
[2023/01/16 08:14] | TRAIN(142): [1350/2211] Batch: 0.0958 (0.0987) Data: 0.0016 (0.0026) Loss: 0.0000 (0.0249)
[2023/01/16 08:14] | TRAIN(142): [1400/2211] Batch: 0.1028 (0.0985) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0251)
[2023/01/16 08:14] | TRAIN(142): [1450/2211] Batch: 0.0911 (0.0983) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0249)
[2023/01/16 08:14] | TRAIN(142): [1500/2211] Batch: 0.0883 (0.0981) Data: 0.0015 (0.0026) Loss: 0.0000 (0.0249)
[2023/01/16 08:14] | TRAIN(142): [1550/2211] Batch: 0.0925 (0.0980) Data: 0.0017 (0.0025) Loss: 0.0017 (0.0249)
[2023/01/16 08:14] | TRAIN(142): [1600/2211] Batch: 0.0923 (0.0979) Data: 0.0018 (0.0025) Loss: 0.0013 (0.0249)
[2023/01/16 08:14] | TRAIN(142): [1650/2211] Batch: 0.0919 (0.0979) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0248)
[2023/01/16 08:14] | TRAIN(142): [1700/2211] Batch: 0.0911 (0.0980) Data: 0.0018 (0.0025) Loss: 0.0657 (0.0249)
[2023/01/16 08:14] | TRAIN(142): [1750/2211] Batch: 0.1079 (0.0980) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0247)
[2023/01/16 08:14] | TRAIN(142): [1800/2211] Batch: 0.1006 (0.0980) Data: 0.0024 (0.0024) Loss: 0.0000 (0.0249)
[2023/01/16 08:15] | TRAIN(142): [1850/2211] Batch: 0.0913 (0.0979) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0251)
[2023/01/16 08:15] | TRAIN(142): [1900/2211] Batch: 0.0949 (0.0979) Data: 0.0015 (0.0024) Loss: 0.0000 (0.0251)
[2023/01/16 08:15] | TRAIN(142): [1950/2211] Batch: 0.0916 (0.0978) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0250)
[2023/01/16 08:15] | TRAIN(142): [2000/2211] Batch: 0.0968 (0.0976) Data: 0.0018 (0.0024) Loss: 0.0408 (0.0251)
[2023/01/16 08:15] | TRAIN(142): [2050/2211] Batch: 0.0899 (0.0979) Data: 0.0018 (0.0023) Loss: 0.0028 (0.0251)
[2023/01/16 08:15] | TRAIN(142): [2100/2211] Batch: 0.0921 (0.0980) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0250)
[2023/01/16 08:15] | TRAIN(142): [2150/2211] Batch: 0.0984 (0.0978) Data: 0.0017 (0.0023) Loss: 0.0466 (0.0250)
[2023/01/16 08:15] | TRAIN(142): [2200/2211] Batch: 0.0978 (0.0978) Data: 0.0015 (0.0023) Loss: 0.0467 (0.0251)
[2023/01/16 08:15] | ------------------------------------------------------------
[2023/01/16 08:15] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:15] | ------------------------------------------------------------
[2023/01/16 08:15] |   TRAIN(142)     0:03:36     0:00:05     0:03:31      0.0251
[2023/01/16 08:15] | ------------------------------------------------------------
[2023/01/16 08:15] | **************************************************
[2023/01/16 08:15] | TRAIN(143): [  50/2211] Batch: 0.1192 (0.1295) Data: 0.0022 (0.0249) Loss: 0.0202 (0.0224)
[2023/01/16 08:15] | TRAIN(143): [ 100/2211] Batch: 0.1177 (0.1235) Data: 0.0022 (0.0136) Loss: 0.0000 (0.0217)
[2023/01/16 08:15] | TRAIN(143): [ 150/2211] Batch: 0.0911 (0.1174) Data: 0.0015 (0.0097) Loss: 0.0034 (0.0220)
[2023/01/16 08:15] | TRAIN(143): [ 200/2211] Batch: 0.1135 (0.1126) Data: 0.0017 (0.0077) Loss: 0.0000 (0.0223)
[2023/01/16 08:16] | TRAIN(143): [ 250/2211] Batch: 0.0907 (0.1101) Data: 0.0017 (0.0065) Loss: 0.0000 (0.0215)
[2023/01/16 08:16] | TRAIN(143): [ 300/2211] Batch: 0.0910 (0.1075) Data: 0.0018 (0.0058) Loss: 0.0817 (0.0240)
[2023/01/16 08:16] | TRAIN(143): [ 350/2211] Batch: 0.0930 (0.1076) Data: 0.0017 (0.0052) Loss: 0.0038 (0.0243)
[2023/01/16 08:16] | TRAIN(143): [ 400/2211] Batch: 0.1028 (0.1068) Data: 0.0014 (0.0048) Loss: 0.0000 (0.0256)
[2023/01/16 08:16] | TRAIN(143): [ 450/2211] Batch: 0.0959 (0.1052) Data: 0.0018 (0.0044) Loss: 0.0443 (0.0276)
[2023/01/16 08:16] | TRAIN(143): [ 500/2211] Batch: 0.1137 (0.1045) Data: 0.0017 (0.0042) Loss: 0.0000 (0.0280)
[2023/01/16 08:16] | TRAIN(143): [ 550/2211] Batch: 0.1098 (0.1040) Data: 0.0017 (0.0039) Loss: 0.0000 (0.0274)
[2023/01/16 08:16] | TRAIN(143): [ 600/2211] Batch: 0.1035 (0.1039) Data: 0.0016 (0.0037) Loss: 0.0000 (0.0272)
[2023/01/16 08:16] | TRAIN(143): [ 650/2211] Batch: 0.1256 (0.1041) Data: 0.0013 (0.0036) Loss: 0.0000 (0.0273)
[2023/01/16 08:16] | TRAIN(143): [ 700/2211] Batch: 0.1024 (0.1037) Data: 0.0017 (0.0035) Loss: 0.0787 (0.0272)
[2023/01/16 08:16] | TRAIN(143): [ 750/2211] Batch: 0.1316 (0.1034) Data: 0.0019 (0.0033) Loss: 0.0000 (0.0272)
[2023/01/16 08:16] | TRAIN(143): [ 800/2211] Batch: 0.0997 (0.1030) Data: 0.0018 (0.0032) Loss: 0.0942 (0.0272)
[2023/01/16 08:17] | TRAIN(143): [ 850/2211] Batch: 0.1009 (0.1026) Data: 0.0014 (0.0032) Loss: 0.0092 (0.0274)
[2023/01/16 08:17] | TRAIN(143): [ 900/2211] Batch: 0.1060 (0.1022) Data: 0.0016 (0.0031) Loss: 0.0834 (0.0266)
[2023/01/16 08:17] | TRAIN(143): [ 950/2211] Batch: 0.1171 (0.1022) Data: 0.0016 (0.0030) Loss: 0.0020 (0.0268)
[2023/01/16 08:17] | TRAIN(143): [1000/2211] Batch: 0.1033 (0.1019) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0264)
[2023/01/16 08:17] | TRAIN(143): [1050/2211] Batch: 0.0877 (0.1014) Data: 0.0016 (0.0029) Loss: 0.0000 (0.0268)
[2023/01/16 08:17] | TRAIN(143): [1100/2211] Batch: 0.0916 (0.1010) Data: 0.0017 (0.0028) Loss: 0.0000 (0.0266)
[2023/01/16 08:17] | TRAIN(143): [1150/2211] Batch: 0.0895 (0.1007) Data: 0.0018 (0.0028) Loss: 0.0565 (0.0264)
[2023/01/16 08:17] | TRAIN(143): [1200/2211] Batch: 0.0990 (0.1006) Data: 0.0018 (0.0027) Loss: 0.0635 (0.0264)
[2023/01/16 08:17] | TRAIN(143): [1250/2211] Batch: 0.0894 (0.1006) Data: 0.0017 (0.0027) Loss: 0.0702 (0.0263)
[2023/01/16 08:17] | TRAIN(143): [1300/2211] Batch: 0.1094 (0.1006) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0259)
[2023/01/16 08:17] | TRAIN(143): [1350/2211] Batch: 0.0892 (0.1005) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0260)
[2023/01/16 08:17] | TRAIN(143): [1400/2211] Batch: 0.0909 (0.1005) Data: 0.0018 (0.0026) Loss: 0.1279 (0.0258)
[2023/01/16 08:18] | TRAIN(143): [1450/2211] Batch: 0.0899 (0.1004) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0257)
[2023/01/16 08:18] | TRAIN(143): [1500/2211] Batch: 0.0910 (0.1002) Data: 0.0018 (0.0026) Loss: 0.1029 (0.0258)
[2023/01/16 08:18] | TRAIN(143): [1550/2211] Batch: 0.0947 (0.0999) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0260)
[2023/01/16 08:18] | TRAIN(143): [1600/2211] Batch: 0.0981 (0.0998) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0259)
[2023/01/16 08:18] | TRAIN(143): [1650/2211] Batch: 0.0919 (0.0997) Data: 0.0017 (0.0025) Loss: 0.0177 (0.0258)
[2023/01/16 08:18] | TRAIN(143): [1700/2211] Batch: 0.0921 (0.0995) Data: 0.0019 (0.0025) Loss: 0.1229 (0.0259)
[2023/01/16 08:18] | TRAIN(143): [1750/2211] Batch: 0.0993 (0.0996) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0259)
[2023/01/16 08:18] | TRAIN(143): [1800/2211] Batch: 0.1019 (0.0998) Data: 0.0036 (0.0024) Loss: 0.0000 (0.0259)
[2023/01/16 08:18] | TRAIN(143): [1850/2211] Batch: 0.0920 (0.0997) Data: 0.0014 (0.0024) Loss: 0.0000 (0.0258)
[2023/01/16 08:18] | TRAIN(143): [1900/2211] Batch: 0.1235 (0.0997) Data: 0.0020 (0.0024) Loss: 0.0982 (0.0258)
[2023/01/16 08:18] | TRAIN(143): [1950/2211] Batch: 0.0923 (0.0997) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0257)
[2023/01/16 08:18] | TRAIN(143): [2000/2211] Batch: 0.1117 (0.0997) Data: 0.0018 (0.0024) Loss: 0.1022 (0.0257)
[2023/01/16 08:19] | TRAIN(143): [2050/2211] Batch: 0.0899 (0.0997) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0256)
[2023/01/16 08:19] | TRAIN(143): [2100/2211] Batch: 0.0897 (0.0996) Data: 0.0019 (0.0023) Loss: 0.0000 (0.0257)
[2023/01/16 08:19] | TRAIN(143): [2150/2211] Batch: 0.1025 (0.0996) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0258)
[2023/01/16 08:19] | TRAIN(143): [2200/2211] Batch: 0.0913 (0.0995) Data: 0.0016 (0.0023) Loss: 0.0051 (0.0258)
[2023/01/16 08:19] | ------------------------------------------------------------
[2023/01/16 08:19] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:19] | ------------------------------------------------------------
[2023/01/16 08:19] |   TRAIN(143)     0:03:39     0:00:05     0:03:34      0.0259
[2023/01/16 08:19] | ------------------------------------------------------------
[2023/01/16 08:19] | **************************************************
[2023/01/16 08:19] | TRAIN(144): [  50/2211] Batch: 0.1024 (0.1307) Data: 0.0017 (0.0237) Loss: 0.0000 (0.0357)
[2023/01/16 08:19] | TRAIN(144): [ 100/2211] Batch: 0.1255 (0.1211) Data: 0.0023 (0.0130) Loss: 0.1805 (0.0324)
[2023/01/16 08:19] | TRAIN(144): [ 150/2211] Batch: 0.1297 (0.1220) Data: 0.0023 (0.0094) Loss: 0.0127 (0.0297)
[2023/01/16 08:19] | TRAIN(144): [ 200/2211] Batch: 0.1173 (0.1224) Data: 0.0022 (0.0077) Loss: 0.0000 (0.0282)
[2023/01/16 08:19] | TRAIN(144): [ 250/2211] Batch: 0.0992 (0.1202) Data: 0.0018 (0.0065) Loss: 0.0286 (0.0269)
[2023/01/16 08:19] | TRAIN(144): [ 300/2211] Batch: 0.1028 (0.1166) Data: 0.0016 (0.0057) Loss: 0.0000 (0.0260)
[2023/01/16 08:19] | TRAIN(144): [ 350/2211] Batch: 0.0955 (0.1135) Data: 0.0018 (0.0052) Loss: 0.0000 (0.0263)
[2023/01/16 08:20] | TRAIN(144): [ 400/2211] Batch: 0.0932 (0.1119) Data: 0.0017 (0.0048) Loss: 0.0169 (0.0259)
[2023/01/16 08:20] | TRAIN(144): [ 450/2211] Batch: 0.0926 (0.1098) Data: 0.0017 (0.0044) Loss: 0.0000 (0.0257)
[2023/01/16 08:20] | TRAIN(144): [ 500/2211] Batch: 0.0997 (0.1086) Data: 0.0017 (0.0042) Loss: 0.0000 (0.0252)
[2023/01/16 08:20] | TRAIN(144): [ 550/2211] Batch: 0.1079 (0.1075) Data: 0.0019 (0.0040) Loss: 0.0000 (0.0255)
[2023/01/16 08:20] | TRAIN(144): [ 600/2211] Batch: 0.0885 (0.1068) Data: 0.0017 (0.0038) Loss: 0.0598 (0.0258)
[2023/01/16 08:20] | TRAIN(144): [ 650/2211] Batch: 0.0918 (0.1064) Data: 0.0016 (0.0036) Loss: 0.0350 (0.0256)
[2023/01/16 08:20] | TRAIN(144): [ 700/2211] Batch: 0.0957 (0.1055) Data: 0.0018 (0.0035) Loss: 0.0120 (0.0257)
[2023/01/16 08:20] | TRAIN(144): [ 750/2211] Batch: 0.0917 (0.1057) Data: 0.0018 (0.0034) Loss: 0.0746 (0.0253)
[2023/01/16 08:20] | TRAIN(144): [ 800/2211] Batch: 0.1040 (0.1051) Data: 0.0016 (0.0033) Loss: 0.0000 (0.0250)
[2023/01/16 08:20] | TRAIN(144): [ 850/2211] Batch: 0.0926 (0.1051) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0250)
[2023/01/16 08:20] | TRAIN(144): [ 900/2211] Batch: 0.0986 (0.1045) Data: 0.0018 (0.0031) Loss: 0.0000 (0.0256)
[2023/01/16 08:20] | TRAIN(144): [ 950/2211] Batch: 0.1040 (0.1041) Data: 0.0017 (0.0031) Loss: 0.0013 (0.0256)
[2023/01/16 08:20] | TRAIN(144): [1000/2211] Batch: 0.0923 (0.1037) Data: 0.0018 (0.0030) Loss: 0.1061 (0.0253)
[2023/01/16 08:21] | TRAIN(144): [1050/2211] Batch: 0.0960 (0.1034) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0255)
[2023/01/16 08:21] | TRAIN(144): [1100/2211] Batch: 0.0927 (0.1030) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0256)
[2023/01/16 08:21] | TRAIN(144): [1150/2211] Batch: 0.1010 (0.1028) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0253)
[2023/01/16 08:21] | TRAIN(144): [1200/2211] Batch: 0.0948 (0.1024) Data: 0.0018 (0.0028) Loss: 0.1077 (0.0253)
[2023/01/16 08:21] | TRAIN(144): [1250/2211] Batch: 0.0905 (0.1022) Data: 0.0018 (0.0028) Loss: 0.0019 (0.0253)
[2023/01/16 08:21] | TRAIN(144): [1300/2211] Batch: 0.0932 (0.1020) Data: 0.0018 (0.0027) Loss: 0.0043 (0.0254)
[2023/01/16 08:21] | TRAIN(144): [1350/2211] Batch: 0.0971 (0.1017) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0253)
[2023/01/16 08:21] | TRAIN(144): [1400/2211] Batch: 0.0978 (0.1015) Data: 0.0018 (0.0027) Loss: 0.0012 (0.0249)
[2023/01/16 08:21] | TRAIN(144): [1450/2211] Batch: 0.0890 (0.1012) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0248)
[2023/01/16 08:21] | TRAIN(144): [1500/2211] Batch: 0.1016 (0.1011) Data: 0.0018 (0.0026) Loss: 0.1079 (0.0249)
[2023/01/16 08:21] | TRAIN(144): [1550/2211] Batch: 0.1076 (0.1009) Data: 0.0019 (0.0026) Loss: 0.0496 (0.0252)
[2023/01/16 08:21] | TRAIN(144): [1600/2211] Batch: 0.0930 (0.1012) Data: 0.0017 (0.0026) Loss: 0.0559 (0.0250)
[2023/01/16 08:22] | TRAIN(144): [1650/2211] Batch: 0.0905 (0.1010) Data: 0.0018 (0.0025) Loss: 0.0021 (0.0249)
[2023/01/16 08:22] | TRAIN(144): [1700/2211] Batch: 0.0963 (0.1008) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0250)
[2023/01/16 08:22] | TRAIN(144): [1750/2211] Batch: 0.0904 (0.1006) Data: 0.0017 (0.0025) Loss: 0.0000 (0.0251)
[2023/01/16 08:22] | TRAIN(144): [1800/2211] Batch: 0.1098 (0.1005) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0251)
[2023/01/16 08:22] | TRAIN(144): [1850/2211] Batch: 0.0921 (0.1003) Data: 0.0018 (0.0024) Loss: 0.0023 (0.0249)
[2023/01/16 08:22] | TRAIN(144): [1900/2211] Batch: 0.0914 (0.1002) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0246)
[2023/01/16 08:22] | TRAIN(144): [1950/2211] Batch: 0.0962 (0.1001) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0245)
[2023/01/16 08:22] | TRAIN(144): [2000/2211] Batch: 0.1053 (0.1002) Data: 0.0018 (0.0024) Loss: 0.0090 (0.0246)
[2023/01/16 08:22] | TRAIN(144): [2050/2211] Batch: 0.0905 (0.1002) Data: 0.0017 (0.0024) Loss: 0.0697 (0.0245)
[2023/01/16 08:22] | TRAIN(144): [2100/2211] Batch: 0.1010 (0.1000) Data: 0.0017 (0.0024) Loss: 0.0249 (0.0245)
[2023/01/16 08:22] | TRAIN(144): [2150/2211] Batch: 0.0879 (0.0998) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0244)
[2023/01/16 08:22] | TRAIN(144): [2200/2211] Batch: 0.0951 (0.0997) Data: 0.0017 (0.0023) Loss: 0.2134 (0.0244)
[2023/01/16 08:22] | ------------------------------------------------------------
[2023/01/16 08:22] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:22] | ------------------------------------------------------------
[2023/01/16 08:22] |   TRAIN(144)     0:03:40     0:00:05     0:03:35      0.0246
[2023/01/16 08:22] | ------------------------------------------------------------
[2023/01/16 08:22] | **************************************************
[2023/01/16 08:23] | TRAIN(145): [  50/2211] Batch: 0.0997 (0.1359) Data: 0.0018 (0.0255) Loss: 0.0339 (0.0365)
[2023/01/16 08:23] | TRAIN(145): [ 100/2211] Batch: 0.1191 (0.1147) Data: 0.0022 (0.0138) Loss: 0.0037 (0.0310)
[2023/01/16 08:23] | TRAIN(145): [ 150/2211] Batch: 0.0897 (0.1095) Data: 0.0018 (0.0099) Loss: 0.0000 (0.0298)
[2023/01/16 08:23] | TRAIN(145): [ 200/2211] Batch: 0.0978 (0.1109) Data: 0.0018 (0.0079) Loss: 0.0856 (0.0300)
[2023/01/16 08:23] | TRAIN(145): [ 250/2211] Batch: 0.0982 (0.1081) Data: 0.0018 (0.0067) Loss: 0.0184 (0.0296)
[2023/01/16 08:23] | TRAIN(145): [ 300/2211] Batch: 0.0880 (0.1061) Data: 0.0018 (0.0059) Loss: 0.0548 (0.0278)
[2023/01/16 08:23] | TRAIN(145): [ 350/2211] Batch: 0.0959 (0.1044) Data: 0.0017 (0.0053) Loss: 0.0190 (0.0270)
[2023/01/16 08:23] | TRAIN(145): [ 400/2211] Batch: 0.0922 (0.1050) Data: 0.0017 (0.0049) Loss: 0.0345 (0.0267)
[2023/01/16 08:23] | TRAIN(145): [ 450/2211] Batch: 0.0976 (0.1042) Data: 0.0018 (0.0046) Loss: 0.0612 (0.0272)
[2023/01/16 08:23] | TRAIN(145): [ 500/2211] Batch: 0.1000 (0.1037) Data: 0.0018 (0.0043) Loss: 0.0026 (0.0270)
[2023/01/16 08:23] | TRAIN(145): [ 550/2211] Batch: 0.0980 (0.1029) Data: 0.0016 (0.0041) Loss: 0.0052 (0.0267)
[2023/01/16 08:23] | TRAIN(145): [ 600/2211] Batch: 0.0981 (0.1022) Data: 0.0018 (0.0039) Loss: 0.1252 (0.0265)
[2023/01/16 08:24] | TRAIN(145): [ 650/2211] Batch: 0.0928 (0.1016) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0271)
[2023/01/16 08:24] | TRAIN(145): [ 700/2211] Batch: 0.1164 (0.1027) Data: 0.0019 (0.0036) Loss: 0.0844 (0.0271)
[2023/01/16 08:24] | TRAIN(145): [ 750/2211] Batch: 0.0924 (0.1030) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0266)
[2023/01/16 08:24] | TRAIN(145): [ 800/2211] Batch: 0.1209 (0.1024) Data: 0.0022 (0.0034) Loss: 0.0203 (0.0265)
[2023/01/16 08:24] | TRAIN(145): [ 850/2211] Batch: 0.1062 (0.1033) Data: 0.0017 (0.0033) Loss: 0.0392 (0.0261)
[2023/01/16 08:24] | TRAIN(145): [ 900/2211] Batch: 0.1035 (0.1033) Data: 0.0016 (0.0032) Loss: 0.0017 (0.0263)
[2023/01/16 08:24] | TRAIN(145): [ 950/2211] Batch: 0.0914 (0.1028) Data: 0.0017 (0.0031) Loss: 0.0001 (0.0259)
[2023/01/16 08:24] | TRAIN(145): [1000/2211] Batch: 0.0874 (0.1023) Data: 0.0019 (0.0031) Loss: 0.0233 (0.0256)
[2023/01/16 08:24] | TRAIN(145): [1050/2211] Batch: 0.0892 (0.1020) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0253)
[2023/01/16 08:24] | TRAIN(145): [1100/2211] Batch: 0.0872 (0.1017) Data: 0.0017 (0.0030) Loss: 0.0000 (0.0253)
[2023/01/16 08:24] | TRAIN(145): [1150/2211] Batch: 0.1018 (0.1014) Data: 0.0019 (0.0029) Loss: 0.0000 (0.0255)
[2023/01/16 08:24] | TRAIN(145): [1200/2211] Batch: 0.0989 (0.1012) Data: 0.0019 (0.0029) Loss: 0.0887 (0.0253)
[2023/01/16 08:25] | TRAIN(145): [1250/2211] Batch: 0.0911 (0.1009) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0254)
[2023/01/16 08:25] | TRAIN(145): [1300/2211] Batch: 0.0977 (0.1006) Data: 0.0018 (0.0028) Loss: 0.0499 (0.0252)
[2023/01/16 08:25] | TRAIN(145): [1350/2211] Batch: 0.0882 (0.1005) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0255)
[2023/01/16 08:25] | TRAIN(145): [1400/2211] Batch: 0.0901 (0.1003) Data: 0.0019 (0.0027) Loss: 0.0000 (0.0256)
[2023/01/16 08:25] | TRAIN(145): [1450/2211] Batch: 0.0879 (0.1000) Data: 0.0018 (0.0027) Loss: 0.0671 (0.0258)
[2023/01/16 08:25] | TRAIN(145): [1500/2211] Batch: 0.0960 (0.1005) Data: 0.0016 (0.0027) Loss: 0.0478 (0.0255)
[2023/01/16 08:25] | TRAIN(145): [1550/2211] Batch: 0.0948 (0.1007) Data: 0.0018 (0.0026) Loss: 0.0873 (0.0254)
[2023/01/16 08:25] | TRAIN(145): [1600/2211] Batch: 0.0931 (0.1007) Data: 0.0019 (0.0026) Loss: 0.0065 (0.0253)
[2023/01/16 08:25] | TRAIN(145): [1650/2211] Batch: 0.0952 (0.1006) Data: 0.0018 (0.0026) Loss: 0.0468 (0.0254)
[2023/01/16 08:25] | TRAIN(145): [1700/2211] Batch: 0.0941 (0.1005) Data: 0.0018 (0.0026) Loss: 0.0345 (0.0254)
[2023/01/16 08:25] | TRAIN(145): [1750/2211] Batch: 0.0951 (0.1002) Data: 0.0017 (0.0026) Loss: 0.0517 (0.0254)
[2023/01/16 08:25] | TRAIN(145): [1800/2211] Batch: 0.0956 (0.1000) Data: 0.0019 (0.0025) Loss: 0.0174 (0.0252)
[2023/01/16 08:26] | TRAIN(145): [1850/2211] Batch: 0.1017 (0.0998) Data: 0.0017 (0.0025) Loss: 0.0057 (0.0250)
[2023/01/16 08:26] | TRAIN(145): [1900/2211] Batch: 0.0883 (0.0997) Data: 0.0017 (0.0025) Loss: 0.0443 (0.0252)
[2023/01/16 08:26] | TRAIN(145): [1950/2211] Batch: 0.1046 (0.0996) Data: 0.0016 (0.0025) Loss: 0.0000 (0.0252)
[2023/01/16 08:26] | TRAIN(145): [2000/2211] Batch: 0.0989 (0.0994) Data: 0.0019 (0.0025) Loss: 0.0224 (0.0250)
[2023/01/16 08:26] | TRAIN(145): [2050/2211] Batch: 0.0885 (0.0992) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0248)
[2023/01/16 08:26] | TRAIN(145): [2100/2211] Batch: 0.0987 (0.0990) Data: 0.0018 (0.0024) Loss: 0.0566 (0.0247)
[2023/01/16 08:26] | TRAIN(145): [2150/2211] Batch: 0.0872 (0.0989) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0247)
[2023/01/16 08:26] | TRAIN(145): [2200/2211] Batch: 0.0988 (0.0988) Data: 0.0016 (0.0024) Loss: 0.0000 (0.0247)
[2023/01/16 08:26] | ------------------------------------------------------------
[2023/01/16 08:26] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:26] | ------------------------------------------------------------
[2023/01/16 08:26] |   TRAIN(145)     0:03:38     0:00:05     0:03:32      0.0247
[2023/01/16 08:26] | ------------------------------------------------------------
[2023/01/16 08:26] | **************************************************
[2023/01/16 08:26] | TRAIN(146): [  50/2211] Batch: 0.1053 (0.1488) Data: 0.0017 (0.0269) Loss: 0.0000 (0.0162)
[2023/01/16 08:26] | TRAIN(146): [ 100/2211] Batch: 0.0911 (0.1209) Data: 0.0017 (0.0144) Loss: 0.0037 (0.0170)
[2023/01/16 08:26] | TRAIN(146): [ 150/2211] Batch: 0.1013 (0.1125) Data: 0.0018 (0.0102) Loss: 0.1493 (0.0182)
[2023/01/16 08:26] | TRAIN(146): [ 200/2211] Batch: 0.1084 (0.1075) Data: 0.0018 (0.0081) Loss: 0.0257 (0.0173)
[2023/01/16 08:27] | TRAIN(146): [ 250/2211] Batch: 0.0849 (0.1045) Data: 0.0019 (0.0069) Loss: 0.1303 (0.0191)
[2023/01/16 08:27] | TRAIN(146): [ 300/2211] Batch: 0.0959 (0.1025) Data: 0.0016 (0.0060) Loss: 0.0328 (0.0213)
[2023/01/16 08:27] | TRAIN(146): [ 350/2211] Batch: 0.0896 (0.1008) Data: 0.0017 (0.0054) Loss: 0.0473 (0.0220)
[2023/01/16 08:27] | TRAIN(146): [ 400/2211] Batch: 0.0916 (0.1003) Data: 0.0015 (0.0049) Loss: 0.0734 (0.0229)
[2023/01/16 08:27] | TRAIN(146): [ 450/2211] Batch: 0.0983 (0.0998) Data: 0.0017 (0.0046) Loss: 0.0000 (0.0237)
[2023/01/16 08:27] | TRAIN(146): [ 500/2211] Batch: 0.0910 (0.0993) Data: 0.0017 (0.0043) Loss: 0.0860 (0.0242)
[2023/01/16 08:27] | TRAIN(146): [ 550/2211] Batch: 0.1023 (0.0989) Data: 0.0018 (0.0041) Loss: 0.0000 (0.0254)
[2023/01/16 08:27] | TRAIN(146): [ 600/2211] Batch: 0.1010 (0.0989) Data: 0.0018 (0.0039) Loss: 0.0072 (0.0250)
[2023/01/16 08:27] | TRAIN(146): [ 650/2211] Batch: 0.0896 (0.0984) Data: 0.0018 (0.0037) Loss: 0.0000 (0.0251)
[2023/01/16 08:27] | TRAIN(146): [ 700/2211] Batch: 0.0881 (0.0980) Data: 0.0020 (0.0036) Loss: 0.0197 (0.0253)
[2023/01/16 08:27] | TRAIN(146): [ 750/2211] Batch: 0.0962 (0.0978) Data: 0.0015 (0.0035) Loss: 0.0531 (0.0256)
[2023/01/16 08:27] | TRAIN(146): [ 800/2211] Batch: 0.0963 (0.0975) Data: 0.0018 (0.0033) Loss: 0.0000 (0.0258)
[2023/01/16 08:27] | TRAIN(146): [ 850/2211] Batch: 0.0981 (0.0977) Data: 0.0026 (0.0033) Loss: 0.0172 (0.0257)
[2023/01/16 08:28] | TRAIN(146): [ 900/2211] Batch: 0.1014 (0.0979) Data: 0.0018 (0.0032) Loss: 0.0020 (0.0261)
[2023/01/16 08:28] | TRAIN(146): [ 950/2211] Batch: 0.0881 (0.0977) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0261)
[2023/01/16 08:28] | TRAIN(146): [1000/2211] Batch: 0.0992 (0.0975) Data: 0.0018 (0.0030) Loss: 0.0595 (0.0259)
[2023/01/16 08:28] | TRAIN(146): [1050/2211] Batch: 0.1014 (0.0973) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0260)
[2023/01/16 08:28] | TRAIN(146): [1100/2211] Batch: 0.1183 (0.0972) Data: 0.0021 (0.0029) Loss: 0.0154 (0.0259)
[2023/01/16 08:28] | TRAIN(146): [1150/2211] Batch: 0.1024 (0.0974) Data: 0.0017 (0.0029) Loss: 0.0865 (0.0260)
[2023/01/16 08:28] | TRAIN(146): [1200/2211] Batch: 0.0912 (0.0972) Data: 0.0017 (0.0028) Loss: 0.0135 (0.0258)
[2023/01/16 08:28] | TRAIN(146): [1250/2211] Batch: 0.0980 (0.0972) Data: 0.0017 (0.0028) Loss: 0.1664 (0.0260)
[2023/01/16 08:28] | TRAIN(146): [1300/2211] Batch: 0.1024 (0.0976) Data: 0.0017 (0.0028) Loss: 0.0361 (0.0260)
[2023/01/16 08:28] | TRAIN(146): [1350/2211] Batch: 0.1067 (0.0980) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0261)
[2023/01/16 08:28] | TRAIN(146): [1400/2211] Batch: 0.0922 (0.0980) Data: 0.0018 (0.0027) Loss: 0.0294 (0.0260)
[2023/01/16 08:28] | TRAIN(146): [1450/2211] Batch: 0.0971 (0.0982) Data: 0.0016 (0.0027) Loss: 0.0278 (0.0258)
[2023/01/16 08:29] | TRAIN(146): [1500/2211] Batch: 0.0903 (0.0981) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0256)
[2023/01/16 08:29] | TRAIN(146): [1550/2211] Batch: 0.0896 (0.0979) Data: 0.0018 (0.0026) Loss: 0.0976 (0.0255)
[2023/01/16 08:29] | TRAIN(146): [1600/2211] Batch: 0.0888 (0.0979) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0251)
[2023/01/16 08:29] | TRAIN(146): [1650/2211] Batch: 0.1213 (0.0982) Data: 0.0021 (0.0026) Loss: 0.0000 (0.0252)
[2023/01/16 08:29] | TRAIN(146): [1700/2211] Batch: 0.0976 (0.0985) Data: 0.0018 (0.0026) Loss: 0.0065 (0.0253)
[2023/01/16 08:29] | TRAIN(146): [1750/2211] Batch: 0.1126 (0.0984) Data: 0.0016 (0.0025) Loss: 0.0745 (0.0256)
[2023/01/16 08:29] | TRAIN(146): [1800/2211] Batch: 0.0940 (0.0984) Data: 0.0018 (0.0025) Loss: 0.0649 (0.0255)
[2023/01/16 08:29] | TRAIN(146): [1850/2211] Batch: 0.1039 (0.0983) Data: 0.0018 (0.0025) Loss: 0.0217 (0.0254)
[2023/01/16 08:29] | TRAIN(146): [1900/2211] Batch: 0.0919 (0.0984) Data: 0.0018 (0.0025) Loss: 0.0304 (0.0252)
[2023/01/16 08:29] | TRAIN(146): [1950/2211] Batch: 0.0944 (0.0984) Data: 0.0018 (0.0025) Loss: 0.0272 (0.0252)
[2023/01/16 08:29] | TRAIN(146): [2000/2211] Batch: 0.0970 (0.0983) Data: 0.0018 (0.0024) Loss: 0.0382 (0.0252)
[2023/01/16 08:29] | TRAIN(146): [2050/2211] Batch: 0.1176 (0.0981) Data: 0.0029 (0.0024) Loss: 0.0000 (0.0250)
[2023/01/16 08:30] | TRAIN(146): [2100/2211] Batch: 0.0974 (0.0982) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0253)
[2023/01/16 08:30] | TRAIN(146): [2150/2211] Batch: 0.0996 (0.0982) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0251)
[2023/01/16 08:30] | TRAIN(146): [2200/2211] Batch: 0.0903 (0.0981) Data: 0.0016 (0.0024) Loss: 0.0172 (0.0253)
[2023/01/16 08:30] | ------------------------------------------------------------
[2023/01/16 08:30] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:30] | ------------------------------------------------------------
[2023/01/16 08:30] |   TRAIN(146)     0:03:36     0:00:05     0:03:31      0.0253
[2023/01/16 08:30] | ------------------------------------------------------------
[2023/01/16 08:30] | **************************************************
[2023/01/16 08:30] | TRAIN(147): [  50/2211] Batch: 0.1186 (0.1234) Data: 0.0018 (0.0243) Loss: 0.0298 (0.0420)
[2023/01/16 08:30] | TRAIN(147): [ 100/2211] Batch: 0.1201 (0.1153) Data: 0.0022 (0.0131) Loss: 0.0000 (0.0307)
[2023/01/16 08:30] | TRAIN(147): [ 150/2211] Batch: 0.1017 (0.1079) Data: 0.0017 (0.0094) Loss: 0.0664 (0.0323)
[2023/01/16 08:30] | TRAIN(147): [ 200/2211] Batch: 0.0877 (0.1084) Data: 0.0018 (0.0075) Loss: 0.0195 (0.0321)
[2023/01/16 08:30] | TRAIN(147): [ 250/2211] Batch: 0.1024 (0.1054) Data: 0.0019 (0.0064) Loss: 0.0833 (0.0337)
[2023/01/16 08:30] | TRAIN(147): [ 300/2211] Batch: 0.1038 (0.1074) Data: 0.0020 (0.0057) Loss: 0.0000 (0.0317)
[2023/01/16 08:30] | TRAIN(147): [ 350/2211] Batch: 0.1207 (0.1088) Data: 0.0022 (0.0052) Loss: 0.0366 (0.0308)
[2023/01/16 08:30] | TRAIN(147): [ 400/2211] Batch: 0.0977 (0.1083) Data: 0.0018 (0.0048) Loss: 0.0000 (0.0301)
[2023/01/16 08:31] | TRAIN(147): [ 450/2211] Batch: 0.0836 (0.1065) Data: 0.0018 (0.0044) Loss: 0.0740 (0.0299)
[2023/01/16 08:31] | TRAIN(147): [ 500/2211] Batch: 0.0823 (0.1045) Data: 0.0018 (0.0042) Loss: 0.0058 (0.0303)
[2023/01/16 08:31] | TRAIN(147): [ 550/2211] Batch: 0.1260 (0.1051) Data: 0.0022 (0.0040) Loss: 0.0334 (0.0302)
[2023/01/16 08:31] | TRAIN(147): [ 600/2211] Batch: 0.0896 (0.1058) Data: 0.0017 (0.0038) Loss: 0.0000 (0.0295)
[2023/01/16 08:31] | TRAIN(147): [ 650/2211] Batch: 0.1033 (0.1049) Data: 0.0017 (0.0037) Loss: 0.0000 (0.0291)
[2023/01/16 08:31] | TRAIN(147): [ 700/2211] Batch: 0.0832 (0.1043) Data: 0.0018 (0.0035) Loss: 0.0000 (0.0288)
[2023/01/16 08:31] | TRAIN(147): [ 750/2211] Batch: 0.0982 (0.1034) Data: 0.0017 (0.0034) Loss: 0.0215 (0.0282)
[2023/01/16 08:31] | TRAIN(147): [ 800/2211] Batch: 0.1289 (0.1044) Data: 0.0021 (0.0033) Loss: 0.0000 (0.0284)
[2023/01/16 08:31] | TRAIN(147): [ 850/2211] Batch: 0.1228 (0.1055) Data: 0.0021 (0.0033) Loss: 0.0218 (0.0280)
[2023/01/16 08:31] | TRAIN(147): [ 900/2211] Batch: 0.0876 (0.1055) Data: 0.0018 (0.0032) Loss: 0.0020 (0.0277)
[2023/01/16 08:31] | TRAIN(147): [ 950/2211] Batch: 0.0995 (0.1055) Data: 0.0020 (0.0031) Loss: 0.0390 (0.0270)
[2023/01/16 08:31] | TRAIN(147): [1000/2211] Batch: 0.0921 (0.1049) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0267)
[2023/01/16 08:32] | TRAIN(147): [1050/2211] Batch: 0.1046 (0.1048) Data: 0.0019 (0.0030) Loss: 0.0783 (0.0263)
[2023/01/16 08:32] | TRAIN(147): [1100/2211] Batch: 0.0923 (0.1045) Data: 0.0020 (0.0030) Loss: 0.0650 (0.0263)
[2023/01/16 08:32] | TRAIN(147): [1150/2211] Batch: 0.0978 (0.1049) Data: 0.0018 (0.0029) Loss: 0.0977 (0.0264)
[2023/01/16 08:32] | TRAIN(147): [1200/2211] Batch: 0.1001 (0.1046) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0267)
[2023/01/16 08:32] | TRAIN(147): [1250/2211] Batch: 0.1180 (0.1052) Data: 0.0022 (0.0029) Loss: 0.0000 (0.0263)
[2023/01/16 08:32] | TRAIN(147): [1300/2211] Batch: 0.0984 (0.1049) Data: 0.0019 (0.0028) Loss: 0.0000 (0.0263)
[2023/01/16 08:32] | TRAIN(147): [1350/2211] Batch: 0.1240 (0.1045) Data: 0.0022 (0.0028) Loss: 0.0615 (0.0259)
[2023/01/16 08:32] | TRAIN(147): [1400/2211] Batch: 0.0943 (0.1044) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0257)
[2023/01/16 08:32] | TRAIN(147): [1450/2211] Batch: 0.0905 (0.1041) Data: 0.0017 (0.0027) Loss: 0.0000 (0.0254)
[2023/01/16 08:32] | TRAIN(147): [1500/2211] Batch: 0.1187 (0.1043) Data: 0.0022 (0.0027) Loss: 0.0583 (0.0252)
[2023/01/16 08:32] | TRAIN(147): [1550/2211] Batch: 0.0934 (0.1040) Data: 0.0017 (0.0027) Loss: 0.0760 (0.0250)
[2023/01/16 08:32] | TRAIN(147): [1600/2211] Batch: 0.0971 (0.1038) Data: 0.0018 (0.0026) Loss: 0.0633 (0.0248)
[2023/01/16 08:33] | TRAIN(147): [1650/2211] Batch: 0.1081 (0.1037) Data: 0.0019 (0.0026) Loss: 0.0098 (0.0245)
[2023/01/16 08:33] | TRAIN(147): [1700/2211] Batch: 0.0881 (0.1036) Data: 0.0019 (0.0026) Loss: 0.0000 (0.0246)
[2023/01/16 08:33] | TRAIN(147): [1750/2211] Batch: 0.1166 (0.1035) Data: 0.0022 (0.0026) Loss: 0.0404 (0.0245)
[2023/01/16 08:33] | TRAIN(147): [1800/2211] Batch: 0.0889 (0.1037) Data: 0.0016 (0.0026) Loss: 0.0000 (0.0245)
[2023/01/16 08:33] | TRAIN(147): [1850/2211] Batch: 0.0959 (0.1034) Data: 0.0019 (0.0025) Loss: 0.0000 (0.0245)
[2023/01/16 08:33] | TRAIN(147): [1900/2211] Batch: 0.0878 (0.1031) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0245)
[2023/01/16 08:33] | TRAIN(147): [1950/2211] Batch: 0.0972 (0.1028) Data: 0.0018 (0.0025) Loss: 0.0505 (0.0246)
[2023/01/16 08:33] | TRAIN(147): [2000/2211] Batch: 0.0875 (0.1025) Data: 0.0017 (0.0025) Loss: 0.0230 (0.0247)
[2023/01/16 08:33] | TRAIN(147): [2050/2211] Batch: 0.0879 (0.1022) Data: 0.0017 (0.0025) Loss: 0.0690 (0.0248)
[2023/01/16 08:33] | TRAIN(147): [2100/2211] Batch: 0.1176 (0.1023) Data: 0.0020 (0.0025) Loss: 0.0816 (0.0248)
[2023/01/16 08:33] | TRAIN(147): [2150/2211] Batch: 0.0933 (0.1024) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0245)
[2023/01/16 08:33] | TRAIN(147): [2200/2211] Batch: 0.0908 (0.1023) Data: 0.0015 (0.0024) Loss: 0.0000 (0.0246)
[2023/01/16 08:33] | ------------------------------------------------------------
[2023/01/16 08:33] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:33] | ------------------------------------------------------------
[2023/01/16 08:33] |   TRAIN(147)     0:03:46     0:00:05     0:03:40      0.0246
[2023/01/16 08:33] | ------------------------------------------------------------
[2023/01/16 08:33] | **************************************************
[2023/01/16 08:34] | TRAIN(148): [  50/2211] Batch: 0.0976 (0.1370) Data: 0.0019 (0.0251) Loss: 0.0593 (0.0212)
[2023/01/16 08:34] | TRAIN(148): [ 100/2211] Batch: 0.0927 (0.1147) Data: 0.0017 (0.0136) Loss: 0.0000 (0.0211)
[2023/01/16 08:34] | TRAIN(148): [ 150/2211] Batch: 0.0872 (0.1075) Data: 0.0016 (0.0097) Loss: 0.0197 (0.0202)
[2023/01/16 08:34] | TRAIN(148): [ 200/2211] Batch: 0.1035 (0.1036) Data: 0.0017 (0.0077) Loss: 0.0608 (0.0229)
[2023/01/16 08:34] | TRAIN(148): [ 250/2211] Batch: 0.0920 (0.1016) Data: 0.0016 (0.0065) Loss: 0.1545 (0.0248)
[2023/01/16 08:34] | TRAIN(148): [ 300/2211] Batch: 0.0930 (0.1005) Data: 0.0019 (0.0057) Loss: 0.0043 (0.0244)
[2023/01/16 08:34] | TRAIN(148): [ 350/2211] Batch: 0.0963 (0.1016) Data: 0.0017 (0.0052) Loss: 0.0738 (0.0243)
[2023/01/16 08:34] | TRAIN(148): [ 400/2211] Batch: 0.0972 (0.1003) Data: 0.0017 (0.0048) Loss: 0.1040 (0.0259)
[2023/01/16 08:34] | TRAIN(148): [ 450/2211] Batch: 0.0888 (0.0996) Data: 0.0018 (0.0044) Loss: 0.0000 (0.0268)
[2023/01/16 08:34] | TRAIN(148): [ 500/2211] Batch: 0.0978 (0.0988) Data: 0.0019 (0.0042) Loss: 0.0000 (0.0267)
[2023/01/16 08:34] | TRAIN(148): [ 550/2211] Batch: 0.0878 (0.0981) Data: 0.0018 (0.0040) Loss: 0.0115 (0.0269)
[2023/01/16 08:34] | TRAIN(148): [ 600/2211] Batch: 0.0895 (0.0976) Data: 0.0019 (0.0038) Loss: 0.0000 (0.0263)
[2023/01/16 08:35] | TRAIN(148): [ 650/2211] Batch: 0.1177 (0.0975) Data: 0.0021 (0.0036) Loss: 0.0000 (0.0263)
[2023/01/16 08:35] | TRAIN(148): [ 700/2211] Batch: 0.1027 (0.0980) Data: 0.0019 (0.0035) Loss: 0.0000 (0.0256)
[2023/01/16 08:35] | TRAIN(148): [ 750/2211] Batch: 0.1185 (0.0982) Data: 0.0021 (0.0034) Loss: 0.0036 (0.0257)
[2023/01/16 08:35] | TRAIN(148): [ 800/2211] Batch: 0.1191 (0.0996) Data: 0.0025 (0.0033) Loss: 0.0053 (0.0255)
[2023/01/16 08:35] | TRAIN(148): [ 850/2211] Batch: 0.0866 (0.0987) Data: 0.0017 (0.0032) Loss: 0.0098 (0.0255)
[2023/01/16 08:35] | TRAIN(148): [ 900/2211] Batch: 0.0918 (0.0981) Data: 0.0018 (0.0032) Loss: 0.0000 (0.0255)
[2023/01/16 08:35] | TRAIN(148): [ 950/2211] Batch: 0.0898 (0.0975) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0251)
[2023/01/16 08:35] | TRAIN(148): [1000/2211] Batch: 0.0826 (0.0970) Data: 0.0018 (0.0030) Loss: 0.0022 (0.0250)
[2023/01/16 08:35] | TRAIN(148): [1050/2211] Batch: 0.0979 (0.0966) Data: 0.0017 (0.0030) Loss: 0.0014 (0.0248)
[2023/01/16 08:35] | TRAIN(148): [1100/2211] Batch: 0.0898 (0.0967) Data: 0.0018 (0.0029) Loss: 0.0000 (0.0248)
[2023/01/16 08:35] | TRAIN(148): [1150/2211] Batch: 0.0878 (0.0967) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0253)
[2023/01/16 08:35] | TRAIN(148): [1200/2211] Batch: 0.0935 (0.0965) Data: 0.0018 (0.0028) Loss: 0.0092 (0.0249)
[2023/01/16 08:35] | TRAIN(148): [1250/2211] Batch: 0.0935 (0.0965) Data: 0.0018 (0.0028) Loss: 0.0000 (0.0251)
[2023/01/16 08:36] | TRAIN(148): [1300/2211] Batch: 0.0844 (0.0963) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0253)
[2023/01/16 08:36] | TRAIN(148): [1350/2211] Batch: 0.1169 (0.0963) Data: 0.0022 (0.0027) Loss: 0.0595 (0.0250)
[2023/01/16 08:36] | TRAIN(148): [1400/2211] Batch: 0.0886 (0.0963) Data: 0.0018 (0.0027) Loss: 0.0180 (0.0249)
[2023/01/16 08:36] | TRAIN(148): [1450/2211] Batch: 0.1027 (0.0964) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0246)
[2023/01/16 08:36] | TRAIN(148): [1500/2211] Batch: 0.1020 (0.0964) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0247)
[2023/01/16 08:36] | TRAIN(148): [1550/2211] Batch: 0.0901 (0.0963) Data: 0.0017 (0.0026) Loss: 0.0077 (0.0247)
[2023/01/16 08:36] | TRAIN(148): [1600/2211] Batch: 0.0964 (0.0965) Data: 0.0018 (0.0026) Loss: 0.0000 (0.0246)
[2023/01/16 08:36] | TRAIN(148): [1650/2211] Batch: 0.1085 (0.0968) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0247)
[2023/01/16 08:36] | TRAIN(148): [1700/2211] Batch: 0.0924 (0.0968) Data: 0.0017 (0.0025) Loss: 0.0162 (0.0251)
[2023/01/16 08:36] | TRAIN(148): [1750/2211] Batch: 0.0884 (0.0967) Data: 0.0017 (0.0025) Loss: 0.0233 (0.0250)
[2023/01/16 08:36] | TRAIN(148): [1800/2211] Batch: 0.0946 (0.0969) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0248)
[2023/01/16 08:36] | TRAIN(148): [1850/2211] Batch: 0.0924 (0.0968) Data: 0.0017 (0.0025) Loss: 0.1371 (0.0250)
[2023/01/16 08:37] | TRAIN(148): [1900/2211] Batch: 0.0931 (0.0967) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0252)
[2023/01/16 08:37] | TRAIN(148): [1950/2211] Batch: 0.0881 (0.0966) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0254)
[2023/01/16 08:37] | TRAIN(148): [2000/2211] Batch: 0.0897 (0.0965) Data: 0.0019 (0.0024) Loss: 0.0000 (0.0251)
[2023/01/16 08:37] | TRAIN(148): [2050/2211] Batch: 0.0884 (0.0966) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0251)
[2023/01/16 08:37] | TRAIN(148): [2100/2211] Batch: 0.0885 (0.0964) Data: 0.0018 (0.0024) Loss: 0.0849 (0.0252)
[2023/01/16 08:37] | TRAIN(148): [2150/2211] Batch: 0.0896 (0.0963) Data: 0.0017 (0.0024) Loss: 0.0783 (0.0253)
[2023/01/16 08:37] | TRAIN(148): [2200/2211] Batch: 0.0965 (0.0963) Data: 0.0016 (0.0024) Loss: 0.0326 (0.0254)
[2023/01/16 08:37] | ------------------------------------------------------------
[2023/01/16 08:37] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:37] | ------------------------------------------------------------
[2023/01/16 08:37] |   TRAIN(148)     0:03:33     0:00:05     0:03:27      0.0255
[2023/01/16 08:37] | ------------------------------------------------------------
[2023/01/16 08:37] | **************************************************
[2023/01/16 08:37] | TRAIN(149): [  50/2211] Batch: 0.1250 (0.1178) Data: 0.0022 (0.0222) Loss: 0.0035 (0.0245)
[2023/01/16 08:37] | TRAIN(149): [ 100/2211] Batch: 0.0897 (0.1083) Data: 0.0019 (0.0122) Loss: 0.0288 (0.0261)
[2023/01/16 08:37] | TRAIN(149): [ 150/2211] Batch: 0.0982 (0.1032) Data: 0.0018 (0.0087) Loss: 0.0345 (0.0228)
[2023/01/16 08:37] | TRAIN(149): [ 200/2211] Batch: 0.0923 (0.1004) Data: 0.0019 (0.0070) Loss: 0.0000 (0.0221)
[2023/01/16 08:37] | TRAIN(149): [ 250/2211] Batch: 0.0890 (0.0989) Data: 0.0016 (0.0060) Loss: 0.0080 (0.0216)
[2023/01/16 08:38] | TRAIN(149): [ 300/2211] Batch: 0.0890 (0.0979) Data: 0.0019 (0.0053) Loss: 0.0000 (0.0240)
[2023/01/16 08:38] | TRAIN(149): [ 350/2211] Batch: 0.1422 (0.1026) Data: 0.0022 (0.0048) Loss: 0.1101 (0.0241)
[2023/01/16 08:38] | TRAIN(149): [ 400/2211] Batch: 0.0921 (0.1043) Data: 0.0019 (0.0045) Loss: 0.0000 (0.0248)
[2023/01/16 08:38] | TRAIN(149): [ 450/2211] Batch: 0.1264 (0.1053) Data: 0.0020 (0.0042) Loss: 0.0310 (0.0254)
[2023/01/16 08:38] | TRAIN(149): [ 500/2211] Batch: 0.0977 (0.1057) Data: 0.0019 (0.0040) Loss: 0.0000 (0.0252)
[2023/01/16 08:38] | TRAIN(149): [ 550/2211] Batch: 0.1232 (0.1060) Data: 0.0018 (0.0038) Loss: 0.0046 (0.0249)
[2023/01/16 08:38] | TRAIN(149): [ 600/2211] Batch: 0.1152 (0.1079) Data: 0.0018 (0.0036) Loss: 0.0077 (0.0250)
[2023/01/16 08:38] | TRAIN(149): [ 650/2211] Batch: 0.1043 (0.1082) Data: 0.0017 (0.0035) Loss: 0.0000 (0.0254)
[2023/01/16 08:38] | TRAIN(149): [ 700/2211] Batch: 0.1026 (0.1077) Data: 0.0019 (0.0034) Loss: 0.0024 (0.0250)
[2023/01/16 08:38] | TRAIN(149): [ 750/2211] Batch: 0.1316 (0.1092) Data: 0.0021 (0.0033) Loss: 0.0000 (0.0251)
[2023/01/16 08:38] | TRAIN(149): [ 800/2211] Batch: 0.1074 (0.1088) Data: 0.0019 (0.0032) Loss: 0.0000 (0.0255)
[2023/01/16 08:39] | TRAIN(149): [ 850/2211] Batch: 0.0978 (0.1082) Data: 0.0017 (0.0031) Loss: 0.0000 (0.0252)
[2023/01/16 08:39] | TRAIN(149): [ 900/2211] Batch: 0.0882 (0.1075) Data: 0.0019 (0.0030) Loss: 0.1241 (0.0250)
[2023/01/16 08:39] | TRAIN(149): [ 950/2211] Batch: 0.0877 (0.1070) Data: 0.0018 (0.0030) Loss: 0.0000 (0.0254)
[2023/01/16 08:39] | TRAIN(149): [1000/2211] Batch: 0.0969 (0.1063) Data: 0.0019 (0.0029) Loss: 0.0087 (0.0250)
[2023/01/16 08:39] | TRAIN(149): [1050/2211] Batch: 0.0867 (0.1055) Data: 0.0017 (0.0029) Loss: 0.0000 (0.0250)
[2023/01/16 08:39] | TRAIN(149): [1100/2211] Batch: 0.0911 (0.1049) Data: 0.0019 (0.0028) Loss: 0.0235 (0.0250)
[2023/01/16 08:39] | TRAIN(149): [1150/2211] Batch: 0.0947 (0.1043) Data: 0.0018 (0.0028) Loss: 0.0819 (0.0248)
[2023/01/16 08:39] | TRAIN(149): [1200/2211] Batch: 0.0947 (0.1039) Data: 0.0017 (0.0027) Loss: 0.0678 (0.0248)
[2023/01/16 08:39] | TRAIN(149): [1250/2211] Batch: 0.1004 (0.1036) Data: 0.0018 (0.0027) Loss: 0.0760 (0.0252)
[2023/01/16 08:39] | TRAIN(149): [1300/2211] Batch: 0.0882 (0.1035) Data: 0.0018 (0.0027) Loss: 0.0000 (0.0249)
[2023/01/16 08:39] | TRAIN(149): [1350/2211] Batch: 0.0879 (0.1031) Data: 0.0019 (0.0026) Loss: 0.0030 (0.0250)
[2023/01/16 08:39] | TRAIN(149): [1400/2211] Batch: 0.0986 (0.1028) Data: 0.0017 (0.0026) Loss: 0.0264 (0.0248)
[2023/01/16 08:40] | TRAIN(149): [1450/2211] Batch: 0.0909 (0.1029) Data: 0.0017 (0.0026) Loss: 0.0000 (0.0248)
[2023/01/16 08:40] | TRAIN(149): [1500/2211] Batch: 0.1094 (0.1028) Data: 0.0018 (0.0025) Loss: 0.0458 (0.0246)
[2023/01/16 08:40] | TRAIN(149): [1550/2211] Batch: 0.0871 (0.1024) Data: 0.0018 (0.0025) Loss: 0.0402 (0.0247)
[2023/01/16 08:40] | TRAIN(149): [1600/2211] Batch: 0.0879 (0.1021) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0248)
[2023/01/16 08:40] | TRAIN(149): [1650/2211] Batch: 0.0909 (0.1021) Data: 0.0018 (0.0025) Loss: 0.0000 (0.0250)
[2023/01/16 08:40] | TRAIN(149): [1700/2211] Batch: 0.0959 (0.1019) Data: 0.0018 (0.0024) Loss: 0.0724 (0.0250)
[2023/01/16 08:40] | TRAIN(149): [1750/2211] Batch: 0.0924 (0.1019) Data: 0.0018 (0.0024) Loss: 0.0000 (0.0252)
[2023/01/16 08:40] | TRAIN(149): [1800/2211] Batch: 0.1011 (0.1018) Data: 0.0014 (0.0024) Loss: 0.0000 (0.0252)
[2023/01/16 08:40] | TRAIN(149): [1850/2211] Batch: 0.0962 (0.1017) Data: 0.0018 (0.0024) Loss: 0.0046 (0.0253)
[2023/01/16 08:40] | TRAIN(149): [1900/2211] Batch: 0.1029 (0.1015) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0250)
[2023/01/16 08:40] | TRAIN(149): [1950/2211] Batch: 0.0870 (0.1013) Data: 0.0017 (0.0024) Loss: 0.0000 (0.0249)
[2023/01/16 08:40] | TRAIN(149): [2000/2211] Batch: 0.0969 (0.1012) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0250)
[2023/01/16 08:40] | TRAIN(149): [2050/2211] Batch: 0.0885 (0.1010) Data: 0.0017 (0.0023) Loss: 0.0000 (0.0249)
[2023/01/16 08:41] | TRAIN(149): [2100/2211] Batch: 0.0880 (0.1008) Data: 0.0020 (0.0023) Loss: 0.0000 (0.0248)
[2023/01/16 08:41] | TRAIN(149): [2150/2211] Batch: 0.0924 (0.1006) Data: 0.0018 (0.0023) Loss: 0.0000 (0.0249)
[2023/01/16 08:41] | TRAIN(149): [2200/2211] Batch: 0.0866 (0.1004) Data: 0.0017 (0.0023) Loss: 0.0024 (0.0248)
[2023/01/16 08:41] | ------------------------------------------------------------
[2023/01/16 08:41] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/16 08:41] | ------------------------------------------------------------
[2023/01/16 08:41] |   TRAIN(149)     0:03:41     0:00:05     0:03:36      0.0249
[2023/01/16 08:41] | ------------------------------------------------------------
[2023/01/16 08:42] | idx 0 case case0008 mean_dice 0.359195 mean_hd95 34.639345
[2023/01/16 08:43] | idx 1 case case0022 mean_dice 0.509040 mean_hd95 51.316441
[2023/01/16 08:44] | idx 2 case case0038 mean_dice 0.499066 mean_hd95 18.765571
[2023/01/16 08:46] | idx 3 case case0036 mean_dice 0.462905 mean_hd95 45.828594
[2023/01/16 08:48] | idx 4 case case0032 mean_dice 0.536607 mean_hd95 43.801233
[2023/01/16 08:49] | idx 5 case case0002 mean_dice 0.513100 mean_hd95 24.261629
[2023/01/16 08:50] | idx 6 case case0029 mean_dice 0.320723 mean_hd95 38.278491
[2023/01/16 08:53] | idx 7 case case0003 mean_dice 0.379989 mean_hd95 110.921084
[2023/01/16 08:54] | idx 8 case case0001 mean_dice 0.495818 mean_hd95 61.901063
[2023/01/16 08:56] | idx 9 case case0004 mean_dice 0.418703 mean_hd95 40.301115
[2023/01/16 08:57] | idx 10 case case0025 mean_dice 0.435462 mean_hd95 46.178469
[2023/01/16 08:58] | idx 11 case case0035 mean_dice 0.347326 mean_hd95 22.331115
[2023/01/16 08:58] | Mean class 1 mean_dice 0.773987 mean_hd95 15.889704
[2023/01/16 08:58] | Mean class 2 mean_dice 0.074624 mean_hd95 37.391465
[2023/01/16 08:58] | Mean class 3 mean_dice 0.542707 mean_hd95 22.631390
[2023/01/16 08:58] | Mean class 4 mean_dice 0.568585 mean_hd95 29.846782
[2023/01/16 08:58] | Mean class 5 mean_dice 0.889833 mean_hd95 21.868074
[2023/01/16 08:58] | Mean class 6 mean_dice 0.126015 mean_hd95 95.815589
[2023/01/16 08:58] | Mean class 7 mean_dice 0.522778 mean_hd95 35.357028
[2023/01/16 08:58] | Mean class 8 mean_dice 0.020094 mean_hd95 100.216069
[2023/01/16 08:58] | Testing performance>>  mean_dice : 0.439828  mean_hd95 : 44.877012
[2023/01/16 08:58] | **************************************************
