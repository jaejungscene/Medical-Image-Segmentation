[2023/01/15 23:07] | ---------------------------------------------------------------------------------
[2023/01/15 23:07] |                                    INFORMATION
[2023/01/15 23:07] | ---------------------------------------------------------------------------------
[2023/01/15 23:07] | Project Name              | MESEG
[2023/01/15 23:07] | Project Administrator     | jaejung
[2023/01/15 23:07] | Experiment Name           | btcv_v2_transunet_v24
[2023/01/15 23:07] | Experiment Start Time     | 2023-01-15 23:07:22
[2023/01/15 23:07] | Experiment Model Name     | transunet
[2023/01/15 23:07] | Experiment Log Directory  | log/btcv_v2_transunet_v24
[2023/01/15 23:07] | ---------------------------------------------------------------------------------
[2023/01/15 23:07] |                                 EXPERIMENT SETUP
[2023/01/15 23:07] | ---------------------------------------------------------------------------------
[2023/01/15 23:07] | train_size                | (224, 224)
[2023/01/15 23:07] | test_size                 | (224, 224)
[2023/01/15 23:07] | center_crop_ptr           | 0.875
[2023/01/15 23:07] | interpolation             | bicubic
[2023/01/15 23:07] | mean                      | (0.485, 0.456, 0.406)
[2023/01/15 23:07] | std                       | (0.229, 0.224, 0.225)
[2023/01/15 23:07] | hflip                     | 0.5
[2023/01/15 23:07] | auto_aug                  | False
[2023/01/15 23:07] | cutmix                    | None
[2023/01/15 23:07] | mixup                     | None
[2023/01/15 23:07] | remode                    | None
[2023/01/15 23:07] | model_name                | transunet
[2023/01/15 23:07] | lr                        | 0.001
[2023/01/15 23:07] | epoch                     | 1
[2023/01/15 23:07] | criterion                 | dicece
[2023/01/15 23:07] | optimizer                 | adamw
[2023/01/15 23:07] | weight_decay              | 1e-05
[2023/01/15 23:07] | scheduler                 | None
[2023/01/15 23:07] | warmup_epoch              | 5
[2023/01/15 23:07] | batch_size                | 1
[2023/01/15 23:07] | ---------------------------------------------------------------------------------
[2023/01/15 23:07] |                                   DATA & MODEL
[2023/01/15 23:07] | ---------------------------------------------------------------------------------
[2023/01/15 23:07] | Model Parameters(M)       | 105277081
[2023/01/15 23:07] | Number of Train Examples  | 2211
[2023/01/15 23:07] | Number of Valid Examples  | 12
[2023/01/15 23:07] | Number of Class           | 9
[2023/01/15 23:07] | ---------------------------------------------------------------------------------
[2023/01/15 23:07] | TRAIN(000): [  50/2211] Batch: 0.1027 (0.1823) Data: 0.0030 (0.0090) Loss: 0.5111 (0.7900)
[2023/01/15 23:07] | TRAIN(000): [ 100/2211] Batch: 0.1095 (0.1447) Data: 0.0021 (0.0057) Loss: 0.5182 (0.6773)
[2023/01/15 23:07] | TRAIN(000): [ 150/2211] Batch: 0.1006 (0.1294) Data: 0.0022 (0.0045) Loss: 0.6287 (0.6237)
[2023/01/15 23:07] | TRAIN(000): [ 200/2211] Batch: 0.1111 (0.1238) Data: 0.0022 (0.0039) Loss: 0.4861 (0.6014)
[2023/01/15 23:07] | TRAIN(000): [ 250/2211] Batch: 0.1503 (0.1222) Data: 0.0026 (0.0036) Loss: 0.4745 (0.5850)
[2023/01/15 23:08] | TRAIN(000): [ 300/2211] Batch: 0.0889 (0.1194) Data: 0.0022 (0.0034) Loss: 0.5746 (0.5729)
[2023/01/15 23:08] | TRAIN(000): [ 350/2211] Batch: 0.0930 (0.1156) Data: 0.0023 (0.0032) Loss: 0.5221 (0.5620)
[2023/01/15 23:08] | TRAIN(000): [ 400/2211] Batch: 0.0999 (0.1141) Data: 0.0021 (0.0031) Loss: 0.4529 (0.5542)
[2023/01/15 23:08] | TRAIN(000): [ 450/2211] Batch: 0.1050 (0.1132) Data: 0.0017 (0.0030) Loss: 0.4590 (0.5469)
[2023/01/15 23:08] | TRAIN(000): [ 500/2211] Batch: 0.0998 (0.1122) Data: 0.0022 (0.0029) Loss: 0.4523 (0.5433)
[2023/01/15 23:08] | TRAIN(000): [ 550/2211] Batch: 0.0968 (0.1116) Data: 0.0023 (0.0029) Loss: 0.4664 (0.5389)
[2023/01/15 23:08] | TRAIN(000): [ 600/2211] Batch: 0.1024 (0.1110) Data: 0.0019 (0.0028) Loss: 0.4591 (0.5351)
[2023/01/15 23:08] | TRAIN(000): [ 650/2211] Batch: 0.0991 (0.1107) Data: 0.0020 (0.0028) Loss: 0.4805 (0.5325)
[2023/01/15 23:08] | TRAIN(000): [ 700/2211] Batch: 0.0999 (0.1104) Data: 0.0024 (0.0027) Loss: 0.5854 (0.5295)
[2023/01/15 23:08] | TRAIN(000): [ 750/2211] Batch: 0.1322 (0.1103) Data: 0.0029 (0.0027) Loss: 0.5467 (0.5267)
[2023/01/15 23:08] | TRAIN(000): [ 800/2211] Batch: 0.0956 (0.1098) Data: 0.0021 (0.0027) Loss: 0.4752 (0.5243)
[2023/01/15 23:08] | TRAIN(000): [ 850/2211] Batch: 0.1008 (0.1094) Data: 0.0018 (0.0026) Loss: 0.4578 (0.5227)
[2023/01/15 23:09] | TRAIN(000): [ 900/2211] Batch: 0.0975 (0.1086) Data: 0.0027 (0.0026) Loss: 0.4589 (0.5221)
[2023/01/15 23:09] | TRAIN(000): [ 950/2211] Batch: 0.1008 (0.1080) Data: 0.0028 (0.0026) Loss: 0.4586 (0.5204)
[2023/01/15 23:09] | TRAIN(000): [1000/2211] Batch: 0.1056 (0.1080) Data: 0.0020 (0.0026) Loss: 0.5260 (0.5185)
[2023/01/15 23:09] | TRAIN(000): [1050/2211] Batch: 0.1050 (0.1081) Data: 0.0024 (0.0026) Loss: 0.5028 (0.5167)
[2023/01/15 23:09] | TRAIN(000): [1100/2211] Batch: 0.1175 (0.1088) Data: 0.0019 (0.0026) Loss: 0.4658 (0.5148)
[2023/01/15 23:09] | TRAIN(000): [1150/2211] Batch: 0.1265 (0.1090) Data: 0.0024 (0.0026) Loss: 0.4668 (0.5129)
[2023/01/15 23:09] | TRAIN(000): [1200/2211] Batch: 0.0994 (0.1095) Data: 0.0024 (0.0026) Loss: 0.4657 (0.5113)
[2023/01/15 23:09] | TRAIN(000): [1250/2211] Batch: 0.1058 (0.1093) Data: 0.0024 (0.0025) Loss: 0.4609 (0.5101)
[2023/01/15 23:09] | TRAIN(000): [1300/2211] Batch: 0.1158 (0.1090) Data: 0.0023 (0.0025) Loss: 0.4500 (0.5086)
[2023/01/15 23:09] | TRAIN(000): [1350/2211] Batch: 0.0978 (0.1091) Data: 0.0024 (0.0025) Loss: 0.4544 (0.5069)
[2023/01/15 23:09] | TRAIN(000): [1400/2211] Batch: 0.1260 (0.1090) Data: 0.0028 (0.0025) Loss: 0.4033 (0.5056)
[2023/01/15 23:10] | TRAIN(000): [1450/2211] Batch: 0.0950 (0.1089) Data: 0.0022 (0.0025) Loss: 0.4602 (0.5039)
[2023/01/15 23:10] | TRAIN(000): [1500/2211] Batch: 0.0900 (0.1087) Data: 0.0018 (0.0025) Loss: 0.4666 (0.5026)
[2023/01/15 23:10] | TRAIN(000): [1550/2211] Batch: 0.0926 (0.1086) Data: 0.0022 (0.0025) Loss: 0.5425 (0.5014)
[2023/01/15 23:10] | TRAIN(000): [1600/2211] Batch: 0.0932 (0.1084) Data: 0.0016 (0.0025) Loss: 0.4614 (0.5002)
[2023/01/15 23:10] | TRAIN(000): [1650/2211] Batch: 0.0897 (0.1081) Data: 0.0019 (0.0025) Loss: 0.4534 (0.4994)
[2023/01/15 23:10] | TRAIN(000): [1700/2211] Batch: 0.0905 (0.1078) Data: 0.0019 (0.0025) Loss: 0.5365 (0.4982)
[2023/01/15 23:10] | TRAIN(000): [1750/2211] Batch: 0.0907 (0.1074) Data: 0.0020 (0.0024) Loss: 0.5079 (0.4977)
[2023/01/15 23:10] | TRAIN(000): [1800/2211] Batch: 0.0932 (0.1071) Data: 0.0024 (0.0024) Loss: 0.4985 (0.4968)
[2023/01/15 23:10] | TRAIN(000): [1850/2211] Batch: 0.0910 (0.1068) Data: 0.0018 (0.0024) Loss: 0.4658 (0.4960)
[2023/01/15 23:10] | TRAIN(000): [1900/2211] Batch: 0.0914 (0.1065) Data: 0.0022 (0.0024) Loss: 0.4996 (0.4952)
[2023/01/15 23:10] | TRAIN(000): [1950/2211] Batch: 0.0943 (0.1064) Data: 0.0022 (0.0024) Loss: 0.4527 (0.4942)
[2023/01/15 23:10] | TRAIN(000): [2000/2211] Batch: 0.0910 (0.1063) Data: 0.0017 (0.0024) Loss: 0.4716 (0.4934)
[2023/01/15 23:11] | TRAIN(000): [2050/2211] Batch: 0.1031 (0.1062) Data: 0.0021 (0.0024) Loss: 0.4342 (0.4922)
[2023/01/15 23:11] | TRAIN(000): [2100/2211] Batch: 0.0968 (0.1061) Data: 0.0019 (0.0024) Loss: 0.3646 (0.4906)
[2023/01/15 23:11] | TRAIN(000): [2150/2211] Batch: 0.1037 (0.1059) Data: 0.0020 (0.0024) Loss: 0.4123 (0.4885)
[2023/01/15 23:11] | TRAIN(000): [2200/2211] Batch: 0.0906 (0.1056) Data: 0.0019 (0.0024) Loss: 0.5747 (0.4871)
[2023/01/15 23:11] | ------------------------------------------------------------
[2023/01/15 23:11] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/15 23:11] | ------------------------------------------------------------
[2023/01/15 23:11] |     TRAIN(0)     0:03:53     0:00:05     0:03:48      0.4868
[2023/01/15 23:11] | ------------------------------------------------------------
[2023/01/15 23:12] | idx 0 case case0008 mean_dice 0.093931 mean_hd95 82.089767
[2023/01/15 23:13] | idx 1 case case0022 mean_dice 0.090079 mean_hd95 64.555613
[2023/01/15 23:13] | idx 2 case case0038 mean_dice 0.081678 mean_hd95 76.200531
[2023/01/15 23:15] | idx 3 case case0036 mean_dice 0.119386 mean_hd95 69.691921
[2023/01/15 23:16] | idx 4 case case0032 mean_dice 0.107698 mean_hd95 105.965216
[2023/01/15 23:17] | idx 5 case case0002 mean_dice 0.092845 mean_hd95 69.783883
[2023/01/15 23:18] | idx 6 case case0029 mean_dice 0.093750 mean_hd95 70.535623
[2023/01/15 23:19] | idx 7 case case0003 mean_dice 0.099289 mean_hd95 83.969823
[2023/01/15 23:20] | idx 8 case case0001 mean_dice 0.142829 mean_hd95 80.702554
[2023/01/15 23:21] | idx 9 case case0004 mean_dice 0.095554 mean_hd95 66.991804
[2023/01/15 23:22] | idx 10 case case0025 mean_dice 0.109052 mean_hd95 62.963682
[2023/01/15 23:23] | idx 11 case case0035 mean_dice 0.072859 mean_hd95 96.694112
[2023/01/15 23:23] | Mean class 1 mean_dice 0.155325 mean_hd95 124.918472
[2023/01/15 23:23] | Mean class 2 mean_dice 0.000000 mean_hd95 0.000000
[2023/01/15 23:23] | Mean class 3 mean_dice 0.032876 mean_hd95 179.525454
[2023/01/15 23:23] | Mean class 4 mean_dice 0.001886 mean_hd95 81.034944
[2023/01/15 23:23] | Mean class 5 mean_dice 0.609186 mean_hd95 188.133691
[2023/01/15 23:23] | Mean class 6 mean_dice 0.000000 mean_hd95 0.000000
[2023/01/15 23:23] | Mean class 7 mean_dice 0.000029 mean_hd95 46.483793
[2023/01/15 23:23] | Mean class 8 mean_dice 0.000000 mean_hd95 0.000000
[2023/01/15 23:23] | Testing performance>>  mean_dice : 0.099913  mean_hd95 : 77.512044
[2023/01/15 23:23] | **************************************************
