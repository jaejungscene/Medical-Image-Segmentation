[2023/01/16 16:44] | ---------------------------------------------------------------------------------
[2023/01/16 16:44] |                                    INFORMATION
[2023/01/16 16:44] | ---------------------------------------------------------------------------------
[2023/01/16 16:44] | Project Name              | MESEG
[2023/01/16 16:44] | Project Administrator     | jaejung
[2023/01/16 16:44] | Experiment Name           | btcv_v2_transunet_inception_d192_p3_f2_v18
[2023/01/16 16:44] | Experiment Start Time     | 2023-01-16 16:44:27
[2023/01/16 16:44] | Experiment Model Name     | transunet_inception_d192_p3_f2
[2023/01/16 16:44] | Experiment Log Directory  | log/btcv_v2_transunet_inception_d192_p3_f2_v18
[2023/01/16 16:44] | ---------------------------------------------------------------------------------
[2023/01/16 16:44] |                                 EXPERIMENT SETUP
[2023/01/16 16:44] | ---------------------------------------------------------------------------------
[2023/01/16 16:44] | train_size                | (224, 224)
[2023/01/16 16:44] | test_size                 | (224, 224)
[2023/01/16 16:44] | center_crop_ptr           | 0.875
[2023/01/16 16:44] | interpolation             | bicubic
[2023/01/16 16:44] | mean                      | (0.485, 0.456, 0.406)
[2023/01/16 16:44] | std                       | (0.229, 0.224, 0.225)
[2023/01/16 16:44] | hflip                     | 0.5
[2023/01/16 16:44] | auto_aug                  | False
[2023/01/16 16:44] | cutmix                    | None
[2023/01/16 16:44] | mixup                     | None
[2023/01/16 16:44] | remode                    | None
[2023/01/16 16:44] | model_name                | transunet_inception_d192_p3_f2
[2023/01/16 16:44] | lr                        | 0.01
[2023/01/16 16:44] | epoch                     | 1
[2023/01/16 16:44] | criterion                 | dicece
[2023/01/16 16:44] | optimizer                 | sgd
[2023/01/16 16:44] | weight_decay              | 0.0001
[2023/01/16 16:44] | scheduler                 | None
[2023/01/16 16:44] | warmup_epoch              | 5
[2023/01/16 16:44] | batch_size                | 24
[2023/01/16 16:44] | ---------------------------------------------------------------------------------
[2023/01/16 16:44] |                                   DATA & MODEL
[2023/01/16 16:44] | ---------------------------------------------------------------------------------
[2023/01/16 16:44] | Model Parameters(M)       | 107559769
[2023/01/16 16:44] | Number of Train Examples  | 2211
[2023/01/16 16:44] | Number of Valid Examples  | 12
[2023/01/16 16:44] | Number of Class           | 9
[2023/01/16 16:44] | ---------------------------------------------------------------------------------
[2023/01/16 16:44] | TRAIN(000): [ 1/93] Batch: 9.5999 (9.5999) Data: 0.0383 (0.9898) Loss: 1.5052 (1.5278)
[2023/01/16 16:44] | TRAIN(000): [ 2/93] Batch: 0.7051 (5.1525) Data: 0.0139 (0.6645) Loss: 1.4220 (1.4926)
[2023/01/16 16:44] | TRAIN(000): [ 3/93] Batch: 0.6562 (3.6538) Data: 0.0122 (0.5014) Loss: 1.3182 (1.4490)
[2023/01/16 16:44] | TRAIN(000): [ 4/93] Batch: 0.6463 (2.9019) Data: 0.0178 (0.4047) Loss: 1.1867 (1.3965)
[2023/01/16 16:44] | TRAIN(000): [ 5/93] Batch: 0.6556 (2.4526) Data: 0.0125 (0.3394) Loss: 1.0405 (1.3372)
[2023/01/16 16:44] | TRAIN(000): [ 6/93] Batch: 0.6493 (2.1521) Data: 0.0146 (0.2930) Loss: 0.9069 (1.2757)
[2023/01/16 16:44] | TRAIN(000): [ 7/93] Batch: 0.6543 (1.9381) Data: 0.0129 (0.2580) Loss: 0.7799 (1.2137)
[2023/01/16 16:44] | TRAIN(000): [ 8/93] Batch: 0.6662 (1.7791) Data: 0.0162 (0.2311) Loss: 0.7037 (1.1571)
[2023/01/16 16:44] | TRAIN(000): [ 9/93] Batch: 0.6564 (1.6544) Data: 0.0318 (0.2112) Loss: 0.6438 (1.1057)
[2023/01/16 16:44] | TRAIN(000): [10/93] Batch: 0.7065 (1.5596) Data: 0.0102 (0.1929) Loss: 0.6196 (1.0615)
[2023/01/16 16:44] | TRAIN(000): [11/93] Batch: 0.6510 (1.4770) Data: 0.0115 (0.1778) Loss: 0.5821 (1.0216)
[2023/01/16 16:44] | TRAIN(000): [12/93] Batch: 0.6522 (1.4083) Data: 0.0144 (0.1652) Loss: 0.5965 (0.9889)
[2023/01/16 16:44] | TRAIN(000): [13/93] Batch: 0.6723 (1.3516) Data: 0.0164 (0.1546) Loss: 0.5326 (0.9563)
[2023/01/16 16:44] | TRAIN(000): [14/93] Batch: 0.6853 (1.3041) Data: 0.0147 (0.1453) Loss: 0.5987 (0.9325)
[2023/01/16 16:44] | TRAIN(000): [15/93] Batch: 0.6464 (1.2602) Data: 0.0126 (0.1370) Loss: 0.5300 (0.9073)
[2023/01/16 16:44] | TRAIN(000): [16/93] Batch: 0.6454 (1.2218) Data: 0.0189 (0.1300) Loss: 0.5645 (0.8871)
[2023/01/16 16:44] | TRAIN(000): [17/93] Batch: 0.6546 (1.1884) Data: 0.0144 (0.1236) Loss: 0.4922 (0.8652)
[2023/01/16 16:44] | TRAIN(000): [18/93] Batch: 0.6639 (1.1593) Data: 0.0147 (0.1179) Loss: 0.5837 (0.8504)
[2023/01/16 16:44] | TRAIN(000): [19/93] Batch: 0.6605 (1.1330) Data: 0.0205 (0.1130) Loss: 0.5308 (0.8344)
[2023/01/16 16:44] | TRAIN(000): [20/93] Batch: 0.6568 (1.1092) Data: 0.0181 (0.1085) Loss: 0.5312 (0.8200)
[2023/01/16 16:44] | TRAIN(000): [21/93] Batch: 0.6525 (1.0875) Data: 0.0158 (0.1043) Loss: 0.5323 (0.8069)
[2023/01/16 16:44] | TRAIN(000): [22/93] Batch: 0.6749 (1.0687) Data: 0.0193 (0.1006) Loss: 0.5384 (0.7952)
[2023/01/16 16:44] | TRAIN(000): [23/93] Batch: 0.6619 (1.0510) Data: 0.0171 (0.0971) Loss: 0.5465 (0.7848)
[2023/01/16 16:44] | TRAIN(000): [24/93] Batch: 0.6561 (1.0346) Data: 0.0162 (0.0939) Loss: 0.5205 (0.7743)
[2023/01/16 16:44] | TRAIN(000): [25/93] Batch: 0.6600 (1.0196) Data: 0.0168 (0.0909) Loss: 0.5467 (0.7655)
[2023/01/16 16:44] | TRAIN(000): [26/93] Batch: 0.6714 (1.0062) Data: 0.0147 (0.0881) Loss: 0.5366 (0.7570)
