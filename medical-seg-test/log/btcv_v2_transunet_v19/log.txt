[2023/01/15 22:51] | ---------------------------------------------------------------------------------
[2023/01/15 22:51] |                                    INFORMATION
[2023/01/15 22:51] | ---------------------------------------------------------------------------------
[2023/01/15 22:51] | Project Name              | MESEG
[2023/01/15 22:51] | Project Administrator     | jaejung
[2023/01/15 22:51] | Experiment Name           | btcv_v2_transunet_v19
[2023/01/15 22:51] | Experiment Start Time     | 2023-01-15 22:51:19
[2023/01/15 22:51] | Experiment Model Name     | transunet
[2023/01/15 22:51] | Experiment Log Directory  | log/btcv_v2_transunet_v19
[2023/01/15 22:51] | ---------------------------------------------------------------------------------
[2023/01/15 22:51] |                                 EXPERIMENT SETUP
[2023/01/15 22:51] | ---------------------------------------------------------------------------------
[2023/01/15 22:51] | train_size                | (224, 224)
[2023/01/15 22:51] | test_size                 | (224, 224)
[2023/01/15 22:51] | center_crop_ptr           | 0.875
[2023/01/15 22:51] | interpolation             | bicubic
[2023/01/15 22:51] | mean                      | (0.485, 0.456, 0.406)
[2023/01/15 22:51] | std                       | (0.229, 0.224, 0.225)
[2023/01/15 22:51] | hflip                     | 0.5
[2023/01/15 22:51] | auto_aug                  | False
[2023/01/15 22:51] | cutmix                    | None
[2023/01/15 22:51] | mixup                     | None
[2023/01/15 22:51] | remode                    | None
[2023/01/15 22:51] | model_name                | transunet
[2023/01/15 22:51] | lr                        | 0.001
[2023/01/15 22:51] | epoch                     | 1
[2023/01/15 22:51] | criterion                 | dicece
[2023/01/15 22:51] | optimizer                 | adamw
[2023/01/15 22:51] | weight_decay              | 1e-05
[2023/01/15 22:51] | scheduler                 | None
[2023/01/15 22:51] | warmup_epoch              | 5
[2023/01/15 22:51] | batch_size                | 1
[2023/01/15 22:51] | ---------------------------------------------------------------------------------
[2023/01/15 22:51] |                                   DATA & MODEL
[2023/01/15 22:51] | ---------------------------------------------------------------------------------
[2023/01/15 22:51] | Model Parameters(M)       | 105277081
[2023/01/15 22:51] | Number of Train Examples  | 2211
[2023/01/15 22:51] | Number of Valid Examples  | 12
[2023/01/15 22:51] | Number of Class           | 9
[2023/01/15 22:51] | ---------------------------------------------------------------------------------
[2023/01/15 22:51] | TRAIN(000): [  50/2211] Batch: 0.1028 (0.1846) Data: 0.0025 (0.0077) Loss: 0.5285 (0.7942)
[2023/01/15 22:51] | TRAIN(000): [ 100/2211] Batch: 0.1048 (0.1463) Data: 0.0021 (0.0051) Loss: 0.5209 (0.6809)
[2023/01/15 22:51] | TRAIN(000): [ 150/2211] Batch: 0.1072 (0.1333) Data: 0.0026 (0.0043) Loss: 0.6177 (0.6284)
[2023/01/15 22:51] | TRAIN(000): [ 200/2211] Batch: 0.0988 (0.1268) Data: 0.0024 (0.0038) Loss: 0.4953 (0.6049)
[2023/01/15 22:51] | TRAIN(000): [ 250/2211] Batch: 0.1083 (0.1239) Data: 0.0024 (0.0035) Loss: 0.4756 (0.5898)
[2023/01/15 22:51] | TRAIN(000): [ 300/2211] Batch: 0.1318 (0.1255) Data: 0.0030 (0.0034) Loss: 0.5791 (0.5773)
[2023/01/15 22:52] | TRAIN(000): [ 350/2211] Batch: 0.1039 (0.1237) Data: 0.0043 (0.0033) Loss: 0.4602 (0.5653)
[2023/01/15 22:52] | TRAIN(000): [ 400/2211] Batch: 0.1305 (0.1221) Data: 0.0029 (0.0032) Loss: 0.4544 (0.5563)
[2023/01/15 22:52] | TRAIN(000): [ 450/2211] Batch: 0.1009 (0.1225) Data: 0.0026 (0.0032) Loss: 0.4577 (0.5492)
[2023/01/15 22:52] | TRAIN(000): [ 500/2211] Batch: 0.1054 (0.1228) Data: 0.0027 (0.0031) Loss: 0.4540 (0.5450)
[2023/01/15 22:52] | TRAIN(000): [ 550/2211] Batch: 0.1356 (0.1218) Data: 0.0031 (0.0031) Loss: 0.4698 (0.5401)
[2023/01/15 22:52] | TRAIN(000): [ 600/2211] Batch: 0.1341 (0.1213) Data: 0.0030 (0.0030) Loss: 0.4584 (0.5358)
[2023/01/15 22:52] | TRAIN(000): [ 650/2211] Batch: 0.1352 (0.1217) Data: 0.0031 (0.0030) Loss: 0.4890 (0.5332)
[2023/01/15 22:52] | TRAIN(000): [ 700/2211] Batch: 0.1012 (0.1212) Data: 0.0044 (0.0030) Loss: 0.5738 (0.5304)
[2023/01/15 22:52] | TRAIN(000): [ 750/2211] Batch: 0.0934 (0.1201) Data: 0.0018 (0.0029) Loss: 0.5787 (0.5275)
[2023/01/15 22:52] | TRAIN(000): [ 800/2211] Batch: 0.1295 (0.1201) Data: 0.0031 (0.0029) Loss: 0.4662 (0.5253)
[2023/01/15 22:53] | TRAIN(000): [ 850/2211] Batch: 0.1045 (0.1200) Data: 0.0024 (0.0029) Loss: 0.4482 (0.5236)
[2023/01/15 22:53] | TRAIN(000): [ 900/2211] Batch: 0.1084 (0.1195) Data: 0.0024 (0.0029) Loss: 0.4453 (0.5226)
[2023/01/15 22:53] | TRAIN(000): [ 950/2211] Batch: 0.1398 (0.1193) Data: 0.0026 (0.0029) Loss: 0.4518 (0.5204)
[2023/01/15 22:53] | TRAIN(000): [1000/2211] Batch: 0.1063 (0.1192) Data: 0.0026 (0.0028) Loss: 0.4917 (0.5183)
[2023/01/15 22:53] | TRAIN(000): [1050/2211] Batch: 0.1011 (0.1184) Data: 0.0021 (0.0028) Loss: 0.4759 (0.5160)
[2023/01/15 22:53] | TRAIN(000): [1100/2211] Batch: 0.0972 (0.1175) Data: 0.0020 (0.0028) Loss: 0.4550 (0.5136)
[2023/01/15 22:53] | TRAIN(000): [1150/2211] Batch: 0.0974 (0.1174) Data: 0.0021 (0.0028) Loss: 0.4694 (0.5117)
[2023/01/15 22:53] | TRAIN(000): [1200/2211] Batch: 0.1030 (0.1168) Data: 0.0021 (0.0027) Loss: 0.4637 (0.5100)
[2023/01/15 22:53] | TRAIN(000): [1250/2211] Batch: 0.1330 (0.1165) Data: 0.0025 (0.0027) Loss: 0.4669 (0.5088)
[2023/01/15 22:53] | TRAIN(000): [1300/2211] Batch: 0.1291 (0.1171) Data: 0.0021 (0.0027) Loss: 0.4511 (0.5075)
[2023/01/15 22:54] | TRAIN(000): [1350/2211] Batch: 0.1391 (0.1172) Data: 0.0040 (0.0027) Loss: 0.4529 (0.5061)
[2023/01/15 22:54] | TRAIN(000): [1400/2211] Batch: 0.1208 (0.1167) Data: 0.0023 (0.0027) Loss: 0.4168 (0.5048)
[2023/01/15 22:54] | TRAIN(000): [1450/2211] Batch: 0.1057 (0.1162) Data: 0.0021 (0.0027) Loss: 0.4626 (0.5030)
[2023/01/15 22:54] | TRAIN(000): [1500/2211] Batch: 0.1414 (0.1164) Data: 0.0031 (0.0027) Loss: 0.4545 (0.5016)
[2023/01/15 22:54] | TRAIN(000): [1550/2211] Batch: 0.1215 (0.1163) Data: 0.0023 (0.0027) Loss: 0.5564 (0.5004)
[2023/01/15 22:54] | TRAIN(000): [1600/2211] Batch: 0.0984 (0.1165) Data: 0.0021 (0.0027) Loss: 0.4619 (0.4993)
[2023/01/15 22:54] | TRAIN(000): [1650/2211] Batch: 0.1226 (0.1161) Data: 0.0021 (0.0026) Loss: 0.4549 (0.4984)
[2023/01/15 22:54] | TRAIN(000): [1700/2211] Batch: 0.1014 (0.1156) Data: 0.0021 (0.0026) Loss: 0.5202 (0.4971)
[2023/01/15 22:54] | TRAIN(000): [1750/2211] Batch: 0.0908 (0.1151) Data: 0.0021 (0.0026) Loss: 0.4824 (0.4962)
[2023/01/15 22:54] | TRAIN(000): [1800/2211] Batch: 0.1040 (0.1148) Data: 0.0025 (0.0026) Loss: 0.4892 (0.4953)
[2023/01/15 22:54] | TRAIN(000): [1850/2211] Batch: 0.1082 (0.1147) Data: 0.0026 (0.0026) Loss: 0.4526 (0.4946)
[2023/01/15 22:54] | TRAIN(000): [1900/2211] Batch: 0.0982 (0.1145) Data: 0.0024 (0.0026) Loss: 0.5346 (0.4935)
[2023/01/15 22:55] | TRAIN(000): [1950/2211] Batch: 0.0987 (0.1144) Data: 0.0024 (0.0026) Loss: 0.3935 (0.4914)
[2023/01/15 22:55] | TRAIN(000): [2000/2211] Batch: 0.1239 (0.1142) Data: 0.0023 (0.0026) Loss: 0.4133 (0.4897)
[2023/01/15 22:55] | TRAIN(000): [2050/2211] Batch: 0.1284 (0.1145) Data: 0.0024 (0.0026) Loss: 0.4017 (0.4878)
[2023/01/15 22:55] | TRAIN(000): [2100/2211] Batch: 0.0988 (0.1145) Data: 0.0025 (0.0026) Loss: 0.3482 (0.4859)
[2023/01/15 22:55] | TRAIN(000): [2150/2211] Batch: 0.0962 (0.1143) Data: 0.0023 (0.0026) Loss: 0.4386 (0.4839)
[2023/01/15 22:55] | TRAIN(000): [2200/2211] Batch: 0.1036 (0.1141) Data: 0.0022 (0.0026) Loss: 0.4221 (0.4826)
[2023/01/15 22:55] | ------------------------------------------------------------
[2023/01/15 22:55] |        Stage       Batch        Data       F+B+O        Loss
[2023/01/15 22:55] | ------------------------------------------------------------
[2023/01/15 22:55] |     TRAIN(0)     0:04:12     0:00:05     0:04:06      0.4822
[2023/01/15 22:55] | ------------------------------------------------------------
